<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>少错</title><link>https://www.lesswrong.com</link><description>致力于提炼理性艺术的社区博客</description><lastBuildDate>Sat, 09 Dec 2023 18:13:04 GMT</lastBuildDate><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>总结“诡计多端的人工智能”（第 5 节）</title><link>https://www.lesswrong.com/posts/2uPH9rBkM7WAnQydp/summing-up-scheming-ais-section-5</link><description>发布于 2023 年 12 月 9 日下午 3:48（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;这是我的报告《&lt;a href="https://arxiv.org/pdf/2311.08379.pdf"&gt;诡计多端的人工智能：人工智能会在训练期间假装对齐以获得权力吗？》&lt;/a&gt;的第五部分。 ”。 &lt;a href="https://www.lesswrong.com/posts/yFofRxg7RRQYCcwFA/new-report-scheming-ais-will-ais-fake-alignment-during"&gt;这里&lt;/a&gt;还有完整报告的摘要（ &lt;a href="https://joecarlsmithaudio.buzzsprout.com/2034731/13969977-introduction-and-summary-of-scheming-ais-will-ais-fake-alignment-during-training-in-order-to-get-power"&gt;此处&lt;/a&gt;有音频）。摘要涵盖了大部分要点和技术术语，我希望它能够提供理解报告各个部分所需的大部分背景信息。&lt;/p&gt;&lt;p&gt;本节的音频版本&lt;a href="https://www.buzzsprout.com/2034731/13984949"&gt;请点击这里&lt;/a&gt;，或者在您的播客应用程序上搜索“Joe Carlsmith Audio”。&lt;/p&gt;&lt;h1&gt;加起来&lt;/h1&gt;&lt;p&gt;现在，我回顾了我所遇到的关于期望 SGD 选择阴谋者的主要论点。总体而言，我们应该如何看待这些论点？&lt;/p&gt;&lt;p&gt;我们已经审查了各种相互关联的考虑因素，但很难一次性记住所有这些因素。不过，总的来说，我认为预期阴谋者的总体情况的很大一部分可以归结为某种版本的“计数论点”。特别是，我认为计数论点在我考虑过的许多其他更具体的论点之下也很重要。因此：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;em&gt;在“训练与游戏无关的代理目标”论点的背景下&lt;/em&gt;：基本的担忧是，在某个时刻（无论是在态势感知之前还是之后），SGD 会自然地落在一个（适当雄心勃勃的）超越情节的目标上，该目标会激励人们心计。期待这种情况的关键原因之一是：（特别是如果你正在积极地为相当长期的、雄心勃勃的目标进行训练），训练之外的各种目标似乎都可能具有这种属性。 （例如：在某种程度上，由于“默认情况下目标不带有日历时间限制”，因此人们预期会出现超出情节的目标，因此，人们实际上是在诉诸“计数论点”，大意是这组超出情节的目标远大于剧集内目标集。）&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;em&gt;在“最近的最大奖励目标”论点的背景下&lt;/em&gt;：基本的担忧是，由于类似阴谋家的目标在目标空间中相当常见，因此某些此类目标将非常“接近”任何尚未达到最大奖励的目标模型已经获得了态势感知，因此，将模型修改为计划器将是 SGD 将模型优化指向最高回报方向的最简单方法。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;em&gt;在“简单性论证”的背景下&lt;/em&gt;：人们期望阴谋者能够比非阴谋者拥有更简单的目标&lt;em&gt;的原因&lt;/em&gt;是他们有很多可能的目标（或：指向目标的指针）可供选择。 （不过：我个人认为这个论点比计数论点本身更有说服力，部分原因是在我看来，简单性带来的好处似乎很小。）&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;也就是说，在所有这些情况下，阴谋者都被视为一种假设，因为原则上各种各样的目标都可能导致阴谋，从而更容易（a）自然地实现其中一个目标，（b）实现“附近”其中之一，或者 (c) 找到其中一个比需要来自更受限空间的非策划目标“更简单”的目标。从这个意义上说，正如我在第 4.2 节中指出的，阴谋者的情况反映了更普遍地预期错位的最基本论点之一——例如，对齐是在目标空间中击中的一个非常狭窄的目标。除此之外，我们在这里特别&lt;em&gt;结合了&lt;/em&gt;我们知道我们将要对相关目标进行的选择：即，它们需要使追求它们的模型获得高回报。最基本的担忧是：这还不够。尽管如此，尽管你在训练中付出了最大的努力，并且几乎不管你的奖励信号如何，你可能选择的几乎所有模型都会&lt;em&gt;因为工具原因而&lt;/em&gt;获得高奖励 - 特别是为了获得力量。&lt;/p&gt;&lt;p&gt;我认为这个基本论点，无论其形式如何，都引起了严重的关注。如果我们承认先进的模型将具有相关的目标导向和情境意识，各种各样的目标确实会导致阴谋，并且阴谋者会在训练中表现接近最佳，那么到底基于什么理由，我们假设培训培养出了一个非心机人吗？也许，根据我的“模糊计数论点”的“模糊性”，我们并没有根据某些“计数”相关可能目标的尝试来具体分配我们对模型的信任。但在我的书中，即使是“很多目标”导致阴谋的模糊感觉也值得警惕。现阶段，我们对 ML 训练了解不够，无法确信我们已经避开了目标空间的相关部分。相反，如果我们的知识没有提高，我们将集中面对一些目标导向的头脑，它了解正在发生的事情以及我们用来塑造它的过程，并且因为它想要&lt;em&gt;某些东西&lt;/em&gt;而获得高回报。 “到底为什么它想要的东西会导致它获得高额回报？”我们得问问。最基本的答案是：“我们不知道。”这不是一个可以接受的答案。一般来说，错位的可能性是不可接受的。在我看来，如果各种各样的、特别可怕的、不一致的目标会导致这种行为，作为寻求权力战略的一部分，这是特别不可接受的。&lt;/p&gt;&lt;p&gt;也就是说，我确实认为这里有一些令人安慰的原因。我们可以将它们大致分为两类。&lt;/p&gt;&lt;p&gt;第一个问题集中于关于阴谋是否实际上是一种针对如此广泛的超越情节目标的趋同理性工具策略的问题。尤其：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt; “经典的目标守护故事”基于这样的主张：阴谋是模型及时传播想要推进的目标的好方法，其方式可以为这些目标产生足够大量的未来力量，以便证明模型通过训练游戏而产生的任何成本是合理的，而不是直接实现其目标。但有很多方面这可能是错误的。例如：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;训练游戏可能不足以阻止相关目标继续改变到使策略不再值得的程度。例如，培训可能会导致这些目标“四处浮动”，或者可能会推动某种特别优化的目标/类似目标的混乱拼凑启发法的焦点。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;该模型在正确的时间尺度上为这些目标获得足够动力的前景可能不足以抵消模型在训练期间直接实现其目标（并因此进行修改）所带来的好处，特别是如果模型超出了预期目标-剧集目标并不是特别“雄心勃勃”，和/或它有其他相互竞争的优先事项。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;这使得“经典的守门故事”在解释为什么训练-比赛对于如此多的超阶段目标来说是正确的策略时看起来至少在某种程度上是连贯的。某些“非经典”故事似乎极大地缩小了它们可能适用的目标集。例如，如果我们假设模型的价值观将最终的权重放在“我的生存”（对于某些个人身份的概念来说，对价值观变化相当宽容），或者人工智能接管上，即使人工智能接管——由于与模型本身有非常不同的偏好，我们现在对相关的类似策划者的目标提出了更具体的要求。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;也就是说：并非所有非经典故事都具有此属性。例如，以具有截然不同的价值观的人工智能之间的协调为中心的故事，以及人工智能在默认情况下都具有非常相似的价值观的可能性的故事，仍然适用于各种目标。也许有理由期望像“生存”这样的最终目标特别常见（例如，因为常见的工具价值特别容易变成最终价值——尽管：我们可以使用对抗性训练来尝试防止这种情况发生）。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-aWqikFyaeR5gyBuAf-1" id="fnref-aWqikFyaeR5gyBuAf-1"&gt;[1]&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;舒适感的第二个来源集中于选择压力的形式，而仅基于所选模型获得“高回报”的假设的高级计数论证并未涵盖这种压力。尤其：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;在态势感知之前，训练自然会&lt;em&gt;惩罚&lt;/em&gt;超出情节的目标（也许尤其是：雄心勃勃的超出情节的目标），只要它们导致牺牲情节奖励。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt; Alos：在态势感知之前，我们可以专门&lt;em&gt;尝试&lt;/em&gt;使用对抗性训练来进一步惩罚超出情节的目标（和雄心勃勃的目标），并以其他方式将模型的目标塑造为“更接近”非策划者目标，而不是类似策划者的目标目标。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;当然，许多非策划者的目标本身就是错位的。但如果我们认为阴谋相对于其他形式的错位（例如寻求剧集奖励）来说足够可怕，那么专门确保我们&lt;em&gt;没有&lt;/em&gt;创造阴谋者的能力可能值得开发。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-aWqikFyaeR5gyBuAf-2" id="fnref-aWqikFyaeR5gyBuAf-2"&gt;[2]&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;如果策划者需要进行额外形式的工具推理（例如，出于工具原因决定策划、检查逃跑/叛逃机会），这可能会给他们的表现带来额外的成本。就他们参与早期破坏的程度而言（在我看来，这是最可怕的阴谋形式），对我来说，他们的表现似乎可能会受到更实质性的打击（例如，对可检查任务进行沙袋处理，根据定义会导致性能受到影响） ）。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;更重要的是，我们也许能够积极设计我们的监督和培训流程，以便（a）增加这种推理的成本（例如，通过使真正的逃跑机会更难与陷阱区分开来），以及（b）惩罚额外的行为。这种形式的推理变得更加困难（通过将模型置于巨大的时间压力下，使工具推理成为稀缺资源，迫使它们依赖英语便签本和其他形式的人类可读推理作为其计划的中心模式，等等 –尽管这些干预措施可能会带来竞争成本）。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;计划者的表现也可能会因为缺乏对手头任务的某种“内在热情”而受到影响——例如，因为“内在热情”中涉及的启发法、注意力模式、品味等模糊了能力和价值观之间的界限。然而，这种考虑似乎更具推测性。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;考虑到“计算参数”对阴谋者的中心地位，在我看来，存在不利于阴谋者的额外选择压力的可能性（特别是：SGD 本质上不利于阴谋者，因为他们需要执行额外的推理）。支持期待阴谋的各种论据。特别是：我认为“计算参数”通常容易出错的一个关键方式是忽略主动选择在克服相关计数所设置的“先验”方面所具有的力量。因此，借用&lt;a href="https://markxu.com/strong-evidence"&gt;Xu (2021)&lt;/a&gt;的一个认知例子/类比，你的“先验”我的名字是“约瑟夫·卡尔史密斯”应该相当低，因为有一个非常强大的“计数论证”反对这个假设：即，最名字（即使是我人口中的男性等）也不是“约瑟夫·卡尔史密斯”。但当我&lt;em&gt;告诉你&lt;/em&gt;我的名字是“约瑟夫·卡尔史密斯”时，这实际上是&lt;a href="https://markxu.com/strong-evidence"&gt;非常有力的证据&lt;/a&gt;——足以克服先验知识，让你对相关假设充满信心。类似的情况也适用于构建功能性工件时的各种形式的选择。我们&lt;em&gt;之所以&lt;/em&gt;能够克服“大多数汽车部件的排列不能形成一辆工作汽车”或“这个神经网络中的大多数参数设置不能实现一个工作的聊天机器人”的先验，是因为人类的选择权受到威胁工程，以及 SGD，就是&lt;em&gt;那么强大&lt;/em&gt;。因此，如果 SGD 的选择权积极地对抗阴谋者，这可能会很快克服对他们有利的“计数论点”。例如，正如我在第 4.2 节中讨论的：如果每个非策划者目标都有 2^100 个类似策划者的目标，这可能会使在相关空间中实现非策划者目标变得非常困难。但实际上，100 位的选择压力对于 SGD 来说可能很便宜（例如，考虑 100 次额外的梯度更新，每次训练至少相当于剩余可能目标的一半，在涉及数百万次的训练运行的情况下）。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-aWqikFyaeR5gyBuAf-3" id="fnref-aWqikFyaeR5gyBuAf-3"&gt;[3]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;因此，在我看来，SGD 是否“关心”阴谋者必须做的额外推理（和/或我们是否可以“让它更加关心”）的问题在这里相当关键。也就是说，如果只是更加努力地研究 SGD 的内在偏好（例如，对于高奖励行为）本身就是一种选择反对阴谋者的力量，那么不管是否有论点，我认为反阴谋者的努力有一个非常有力的工具对他们有利。在这方面，我直觉上对“低松弛”训练环境中的反阴谋工作更加乐观，这些工作给模型的最佳表现带来了很大的压力——这是我们可能能够控制的参数。我更担心“路径依赖”体制中的阴谋者，其中 SGD 可能会在训练早期就关注阴谋者的目标，然后陷入困境，尽管事实上它总体上“更喜欢”非阴谋者。&lt;/p&gt;&lt;p&gt;另一个对我来说很重要的问题是，SGD 的渐进主义是否会阻止有关阴谋者目标起源的“依赖于训练游戏”的故事，例如“最近的最大奖励目标”论点。如果是这样，我认为这将是一个有意义的额外安慰来源。特别是：对我来说，避免&lt;em&gt;自然地&lt;/em&gt;给模型一个（适当雄心勃勃的）超越情节的目标似乎比确保模型在情境感知时，其目标足够接近最大奖励要容易得多在某个维度“附近”不存在类似阴谋家的目标。特别是：SGD 的图像专门“搜索”最近的最大奖励目标，在一个可以搜索到大量类似阴谋家的目标的空间中，在我看来非常令人担忧（而且：更有可能导致超越情节的目标具有任何激励阴谋所必需的属性——例如，雄心勃勃的目标、具有无限时间范围的目标等等。&lt;/p&gt;&lt;p&gt;最后，我想再次强调一些持续存在的不确定性，即阴谋是否需要异常高的目标导向标准。在这里，我一直试图将关于目标导向性本身的争论与关于 SGD 选择哪种目标导向模型的争论区分开来。但即使是能力很强的人工智能也可能不需要联盟话语中经常假设的那种连贯的、战略目标导向的，我认为这可能与此类人工智能被很好地理解为“阴谋家”的可能性特别相关，因为形式阴谋中的目标导向似乎特别连贯、战略性和“&lt;a href="https://arbital.com/p/consequentialist/"&gt;结果主义&lt;/a&gt;”。&lt;/p&gt;&lt;p&gt;退后一步，尝试从整体上看待这些考虑因素，我感觉自己被拉向了两个不同的方向。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;一方面，至少以阴谋作为一种收敛良好的工具策略为条件，类似阴谋家的目标在目标空间中感觉非常普遍，而且我非常担心训练会因为某种原因而遇到它们。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;另一方面，在我看来，将模型在训练中的良好表现归咎于阴谋仍然是一种直觉，就像一个相当具体和连贯的故事。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;也就是说，在“目标空间”的层面上，阴谋感觉是强大和常见的，但在“是的，这就是这个现实世界模型正在发生的事情，它获得奖励是因为（或者：基本上是因为）”的层面上是具体的和相当脆弱的它希望稍后获得权力，而现在获得奖励有助于实现这一目标。”我认为这种连接感有几个不同的组成部分：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;部分原因在于该模型是否确实具有相关的长期且雄心勃勃的目标，尽管它是在训练中形成的。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;其中一部分是关于是否有一个足够好的故事来说明为什么在剧集中获得奖励是实现这些目标的一个很好的工具性策略（例如，对目标保护假设的怀疑、模型稍后授权的前景等）。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;部分原因是，类似阴谋家的诊断还带来了我没有关注的额外结合点——例如，该模型具有情境意识和连贯的目标导向。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-aWqikFyaeR5gyBuAf-4" id="fnref-aWqikFyaeR5gyBuAf-4"&gt;[4]&lt;/a&gt;&lt;/sup&gt; （当我真的试图让人们记住这个模型&lt;em&gt;知道正在发生什么&lt;/em&gt;并且正在以引发战略工具推理的方式连贯地追求&lt;em&gt;某个&lt;/em&gt;目标/一组目标时，那么它至少有可能部分阴谋家似乎更合理。）&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;我们能否对阴谋者的总体可能性给出一个粗略的、主观的数字？在我看来，尽管有缺点，像这样的数字通常可以使争论更加激烈，所以我在这里尝试一下。比方说：&lt;strong&gt;如果你使用自我监督预训练 + RLHF 的基线形式，在多样化、长期、现实世界的任务上训练一个连贯的目标导向、情境感知、略高于人类水平的模型，我的该模型在训练中表现良好的主观概率约为 25%，这在很大程度上是为自身和/或其他人工智能寻求力量的工具策略的一部分。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;不过，我想澄清的是，即使上述说法存在多种形式的模糊性，我也只是从我的直觉中得出一个数字。我还没有建立相关考虑因素的定量模型（尽管我有兴趣看到这方面的努力），报告的要点是分析本身，而不是试图将分析综合成定量结果。&lt;/p&gt;&lt;p&gt;我还认为可以通过相当简单的方式降低风险：例如，通过短期任务的培训，或通过态势感知之前的各种形式的对抗性培训。我认为该模型既策划&lt;em&gt;又&lt;/em&gt;参与主动邪恶行为（例如沙袋和早期破坏）的可能性较低，因为这些行为在认知和奖励方面付出了更大的成本。&lt;/p&gt;&lt;p&gt;我还认为，风险会随着更强大的模型&lt;em&gt;而增加&lt;/em&gt;，这些模型可能在未来能够更好地逃脱/接管，并与其他人工智能协调/合作，并且更有可能参与更多的工作。复杂的守门形式，例如“内省的守门方法”（尽管在我看来，即使对于相当先进的人工智能来说，这些直观上也相当具有挑战性）。&lt;/p&gt;&lt;p&gt;最后，我想再次强调，非阴谋者也可以从事类似阴谋者的行为。剧集奖励寻求者可以假装自己的阵营（甚至：参与逃跑/接管），以努力获得剧集奖励；非训练型游戏玩家最终仍可能会追求权力目标，从而激励各种形式的欺骗行为；最终最重要的人工智能可能在重要方面与我在这里关注的人工智能范式有所不同（例如，它们可能更像“&lt;a href="https://lilianweng.github.io/posts/2023-06-23-agent/"&gt;语言模型代理&lt;/a&gt;”而不是单一模型，或者它们可能是通过方法创建的与我所关注的基线机器学习方法有更大的不同），同时仍然从事权力驱动的对齐伪造。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-aWqikFyaeR5gyBuAf-5" id="fnref-aWqikFyaeR5gyBuAf-5"&gt;[5]&lt;/a&gt;&lt;/sup&gt;在我看来，阴谋是这种恐惧的一个典型例子，在我看来，这是一个特别迫切需要理解的例子。但这远不是唯一令人担忧的原因。 &lt;/p&gt;&lt;hr class="footnotes-sep" /&gt;&lt;section class="footnotes"&gt;&lt;ol class="footnotes-list"&gt;&lt;li class="footnote-item" id="fn-aWqikFyaeR5gyBuAf-1"&gt;&lt;p&gt;不过，再次强调：它需要是一种能够容忍价值观变化的“生存”概念。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-aWqikFyaeR5gyBuAf-1"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-aWqikFyaeR5gyBuAf-2"&gt;&lt;p&gt;有关这方面的更多信息，请参见第 6.8 节。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-aWqikFyaeR5gyBuAf-2"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-aWqikFyaeR5gyBuAf-3"&gt;&lt;p&gt;感谢保罗·克里斯蒂亚诺在这里进行讨论。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-aWqikFyaeR5gyBuAf-3"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-aWqikFyaeR5gyBuAf-4"&gt;&lt;p&gt;跟踪在阴谋者假设的背景下可能建立的所有其他更微妙的结合也感觉有点困难。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-aWqikFyaeR5gyBuAf-4"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-aWqikFyaeR5gyBuAf-5"&gt;&lt;p&gt;尽管如上所述，如果相关语言模型代理经过端到端训练（而不是仅仅构建单独训练的组件），那么报告的框架也将适用于它们。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-aWqikFyaeR5gyBuAf-5"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/section&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/2uPH9rBkM7WAnQydp/summing-up-scheming-ais-section-5#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sat, 09 Dec 2023 15:48:49 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/2uPH9rBkM7WAnQydp/summing-up-scheming-ais-section-5</guid></item><item><title>攻防平衡很少发生变化</title><link>https://www.lesswrong.com/posts/Hx2Q9fWyimjrKrxyK/the-offense-defense-balance-rarely-changes</link><description>发布于 2023 年 12 月 9 日下午 3:21（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;您可能在 X 上看到过&lt;a href="https://x.com/RanaAgastya/status/1729559869605192078?s=20"&gt;&lt;u&gt;一些&lt;/u&gt;&lt;/a&gt;&lt;a href="https://x.com/NicerInPerson/status/1729906726814523630?s=20"&gt;&lt;u&gt;类似这样的&lt;/u&gt;&lt;/a&gt;对话：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; &lt;strong&gt;Michael Doomer ⏸️&lt;/strong&gt; ：先进的人工智能可以帮助任何人制造生物武器&lt;br /&gt;如果这项技术普及的话，只需要一个疯狂的人就能毁灭世界！&lt;/p&gt;&lt;p&gt; &lt;strong&gt;Edward Acc&lt;/strong&gt; ⏩：我可以让我的人工智能制造疫苗&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Yann LeCun：&lt;/strong&gt;&lt;a href="https://x.com/ylecun/status/1718764953534939162?s=20"&gt;&lt;u&gt;我的好人工智能会打倒你的流氓人工智能&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;这里的分歧取决于一项技术是否更能促进进攻（生物武器）而不是防御（疫苗）。对未来技术（尤其是人工智能）“攻防平衡”的预测是&lt;a href="https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html#defensedemocracy"&gt;&lt;u&gt;有关技术乐观主义和存在风险的争论的核心&lt;/u&gt;&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;这些预测大多依赖于对廉价生物技术、无人机和数字代理等技术将如何影响攻击或保护资源的难易程度的直觉。很难想象一个人工智能代理搜索软件漏洞和自动无人机攻击军事目标的世界，而不想象进攻防御平衡的巨大转变。&lt;/p&gt;&lt;p&gt;但几乎没有历史证据表明进攻防守平衡发生了巨大变化，即使是为了应对技术革命。&lt;/p&gt;&lt;p&gt;考虑网络安全。自 20 世纪 70 年代以来，摩尔定律使我们&lt;a href="https://ourworldindata.org/moores-law"&gt;&lt;u&gt;的计算成本降低了七个数量级&lt;/u&gt;&lt;/a&gt;。随着原始计算能力的提高，计算机技术的形式和经济用途发生了巨大变化：加密、互联网、电子商务、社交媒体和智能手机。&lt;/p&gt;&lt;p&gt;通常的攻防平衡故事预测，像这样的技术的重大变化应该会对攻防平衡产生重大影响。如果你告诉 20 世纪 70 年代的人们，到 2020 年，恐怖组织和孤独的精神病患者可以从他们的口袋里获得比 IBM 当时生产的更多的计算能力，他们会对网络安全的攻防平衡做出什么预测？&lt;/p&gt;&lt;p&gt;与他们可能的预测相反，网络安全的攻防平衡似乎很稳定。网络攻击尚未被消灭，但也没有占领世界。所有大国都拥有防御性和进攻性网络安全团队，但没有一个国家获得决定性优势。计算机有时仍然会感染病毒或勒索软件，但它们还没有发展到危及互联网 GDP 很大一部分的程度。从 1980 年到 2020 年，美国用于网络安全的军事预算&lt;a href="https://askwonder.com/research/historical-global-software-cybersecurity-spending-d7s6w5mfz#:~:text=,%E2%80%A0blog.rsisecurity.com%E3%80%91%20from%201980%20to%202017"&gt;&lt;u&gt;每年增加约 4%&lt;/u&gt;&lt;/a&gt; ，这一速度快于 GDP 增长，但与 GDP 增长加上互联网占 GDP 比例的增长保持一致。&lt;/p&gt;&lt;p&gt;之前几次技术革命带来的这种稳定性增加了举证责任，说明为什么网络安全的攻防平衡在下一次技术革命之后应该会发生根本性的变化。&lt;/p&gt;&lt;p&gt;攻防平衡的稳定性并不特定于网络安全。下图显示了从1400年到2013年战争中的人均死亡率。这张图包含了人类所有重大技术革命。每年都有很多差异，但长期趋势几乎为零。&lt;/p&gt;&lt;p&gt; &lt;a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd08dd888-6bc4-42e9-bf3d-0535beb59e0a_3088x2018.png"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Hx2Q9fWyimjrKrxyK/sdu9qhvr92r6hlj4iah4" /&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;有没有人有一个攻防平衡理论，可以解释为什么1640年人们用刀剑和马匹作战时的人均战争死亡人数应该与1940年人们用空袭和坦克作战时的人均战争死亡人数大致相同？&lt;/p&gt;&lt;p&gt;很难用技术的变化来解释该图中的变化。冲突中的人均死亡是嘈杂和周期性的，而技术的进步相对平稳和单调。&lt;/p&gt;&lt;p&gt;以前的技术还没有足以改变冲突的频率或成本，使该指标远远超出已设定的最大和最小范围 1400-1650。为什么我们应该期望人工智能有所不同，举证责任再次增加。&lt;/p&gt;&lt;p&gt;人类基因组测序的成本也下降了 6 个数量级，生物学领域也发生了数十项重大技术变革。然而，生物攻击的频率或损害尚未出现明显的反应。&lt;/p&gt;&lt;p&gt; &lt;a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69872e23-b086-4dc5-8729-fc219b47c735_3400x2400.png"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Hx2Q9fWyimjrKrxyK/ftc8mdnfofci5m6qsi6d" /&gt;&lt;/a&gt;&lt;/p&gt;&lt;h3&gt;稳定的可能原因&lt;/h3&gt;&lt;p&gt;为什么攻防平衡在其背后的技术正在迅速而彻底地变化的情况下却如此稳定？这篇文章的主要贡献只是用经验证据来支持这个问题的重要性，但这是一个不成熟的理论。&lt;/p&gt;&lt;p&gt;最主要的是，&lt;a href="https://www.tandfonline.com/doi/pdf/10.1080/01402390.2019.1631810"&gt;&lt;u&gt;攻防平衡理论&lt;/u&gt;&lt;/a&gt;中攻击者和防守者之间的明确区分在实践中并不存在。所有攻击者也是防御者，反之亦然。侵略者国家必须保卫自己的征服地，黑客也需要强大的信息安全。&lt;/p&gt;&lt;p&gt;因此，如果有某种技术可以使入侵比防御更容易，或者使信息安全比黑客攻击更容易，那么它可能不会对权力平衡产生太大影响，因为每个参与者都需要同时做到这两点。如果进攻和防守是互补而不是替代，那么它们之间的平衡就不那么重要了。&lt;/p&gt;&lt;p&gt;这个论点对人工智能的未来有何预测？它并不预测未来将与今天非常相似。尽管当今网络安全的攻防平衡与 20 世纪 70 年代非常相似，但自那时以来，技术和社会发生了巨大变化。人工智能显然是本世纪的决定性技术。&lt;/p&gt;&lt;p&gt;但它确实预测人工智能带来的巨大变化不会来自对攻防平衡的巨大破坏。这些变化将更像是工业革命，而不是小型恐怖组织被授权摧毁互联网或摧毁整个国家。&lt;/p&gt;&lt;p&gt;也许在所有这些情况下，阈值效应即将到来，或者人工智能与我们过去所有的技术革命完全不同，但这是一个需要大量证据来证明的说法。到目前为止，通过大规模的技术变革，攻防平衡似乎非常稳定，我们应该预期这种情况会持续下去。&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/Hx2Q9fWyimjrKrxyK/the-offense-defense-balance-rarely-changes#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sat, 09 Dec 2023 15:21:23 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/Hx2Q9fWyimjrKrxyK/the-offense-defense-balance-rarely-changes</guid></item><item><title>哲学同义反复</title><link>https://www.lesswrong.com/posts/soXxhjTCDEJuvsLmE/a-philosophical-tautology</link><description>发布于 2023 年 12 月 9 日下午 2:06（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;我写了一条&lt;a href="https://www.lesswrong.com/posts/RSvR4gKEqd87hmarc/mathematics-as-physics?commentId=JBgi2555sapv3ThAa"&gt;评论&lt;/a&gt;，其中包含了我试图解释的核心部分，因此我将其复制到自己的帖子中。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;如果我们假设人类观察到的一切都是由更小的物理部分组成（可能除了当前流行的基本粒子，但这对于这个论证来说并不重要）并且宏观状态是完全确定的通过微观状态（无论它是否易于人类计算），可以从逻辑上得出一个简单的结论。&lt;/p&gt;&lt;p&gt;这个结论是，没有任何超物理的预测能力可以超过我们从物理学知识中可以预测的能力。这是因为对于具有预测能力的事物，它需要对发生的事情产生一些影响。如果它对发生的事情没有任何影响，那么它的存在和不存在就不能让我们对世界做出任何结论。&lt;/p&gt;&lt;p&gt;这个论点适用于数学：如果数学与物理学分开的存在允许我们对世界做出任何结论，那么它就必须对所发生的事情产生因果影响，这与我们观察到的所有宏观状态这一事实相矛盾仅由微观状态决定。&lt;/p&gt;&lt;p&gt;由于最初的假设有非常有力的证据支持，因此可以安全地得出结论：一般来说，每当我们认为需要一些超自然的东西来解释已知事实时，我们就一定在某个地方犯了错误。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;它可能看起来只是一个简单的同义反复，但同义反复及其后果并不总是显而易见的，而且这种特殊的同义反复有很多很多的后果。它可以帮助避免对数学、感受性、自由意志、主观概率和真理等事物的许多困惑。我认为，注意到这个同义反复并决定将我的哲学建立在它之上，是我做过的最好的三个决定之一。&lt;/p&gt;&lt;p&gt;这一观察的一个特别重要的结果是，我们可以应用理性的基本原则，即我们不应该相信没有证据的事物，以排除任何假设超自然事物存在的哲学。&lt;/p&gt;&lt;p&gt;请注意，您无需拒绝任何数学即可接受这一哲学，正如我在&lt;a href="https://www.lesswrong.com/posts/RSvR4gKEqd87hmarc/mathematics-as-physics"&gt;上一篇文章&lt;/a&gt;中试图解释的那样。我在这篇文章中尝试的最重要的事情是展示一种将数学分解为物理的方法，而不会陷入拒绝大量数学的陷阱。当然，我不太相信我分解它的具体方法是正确的，而是相信确实存在这样的分解。&lt;/p&gt;&lt;p&gt;将这种方法应用于哲学时要做的一件非常重要的事情是记住一切都应该&lt;a href="https://www.lesswrong.com/tag/adding-up-to-normality"&gt;达到常态&lt;/a&gt;。当我们无法弄清楚如何以一种合乎常态的方式将事物分解为物理时，我们很可能会犯错误。总的来说，我已经能够将这种哲学融入到我的信仰中，并使这一切成为常态。&lt;/p&gt;&lt;br /&gt;&lt;br /&gt;&lt;a href="https://www.lesswrong.com/posts/soXxhjTCDEJuvsLmE/a-philosophical-tautology#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sat, 09 Dec 2023 14:06:34 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/soXxhjTCDEJuvsLmE/a-philosophical-tautology</guid></item><item><title>揭开灭绝的面纱</title><link>https://www.lesswrong.com/posts/HaGTQcxqjHPyR9Ju6/unpicking-extinction</link><description>发布于 2023 年 12 月 9 日上午 9:15（格林尼治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;h1&gt;长话短说&lt;/h1&gt;&lt;p&gt;人类灭绝正在成为趋势：有很多噪音，主要是在 X 上，关于 e/acc 对人类灭绝的明显&lt;a href="https://www.lesswrong.com/posts/3xoThNNYgZmTCpEAB/based-beff-jezos-and-the-accelerationists"&gt;自满&lt;/a&gt;。灭绝也与另一种观点（并非特定于 e/acc）相关，即“人类进化的下一步是{AI/AGI/ASI}”。许多人强烈反对前者，而后者似乎并不十分充实。我认为简单地收集各种立场并总结它们是有用的，希望不会太不准确，也许还能找出一些共同点。&lt;/p&gt;&lt;p&gt;这是我自己的研究（通过进化导致事实上的灭绝）的起点。这里没有什么特别新的内容：例如，请参阅&lt;a href="https://forum.effectivealtruism.org/topics/human-extinction"&gt;常用&lt;/a&gt;&lt;a href="https://www.lesswrong.com/tag/existential-risk"&gt;论坛&lt;/a&gt;中的大量文献。托马斯·莫伊尼汉 (Thomas Moynihan) 的&lt;a href="https://www.urbanomic.com/book/x-risk/"&gt;《X-risk》&lt;/a&gt; (2020) 记录了人类集体认识到文明脆弱性的历史，而埃米尔·P·托雷斯 (Émile P. Torres) 的作品（下文讨论）则为灭绝伦理提出了一个可能的框架。&lt;/p&gt;&lt;p&gt;我的底线是：a）人类灭绝的坏（或善）程度似乎不像人们想象的那么明显或不言而喻，b）如果我们灭绝以及当我们灭绝时我们留下什么很重要，c）灭绝的时间这种情况的发生很重要，d）人类最后几代人生活（和死亡）的方式也很重要。&lt;/p&gt;&lt;p&gt;与表面上的 e/acc 观点相关（即对可能的人类灭绝相当放松）：很明显，我们的默认立场（受到一些警告）应该是延迟灭绝，因为 a）它是不可逆转的（根据定义） ，b）以最大化我们未来的期权价值。无论如何，e/acc 观点似乎是基于某种（不是很清楚的）某种熵与不受约束的资本主义的品味交叉的东西，很难认真对待，甚至可能以它自己的方式失败。&lt;/p&gt;&lt;h1&gt;灭绝的种类&lt;/h1&gt;&lt;h2&gt;尤德科夫斯基的立场&lt;/h2&gt;&lt;p&gt;（我的看法）埃利泽的观点是，他担心一个错位的人工智能（不一定是超级智能），主要靠自己行动（例如目标形成、计划、实际影响世界上的事物），消灭人类，甚至消灭地球上的所有生命。这不仅对被淘汰的人类或其后代不利，而且对整个宇宙也不利，因为智能创造的复杂性（人类产生的类型）是一种内在的善，不需要进一步的论证。埃利以泽预见的绝大多数人工智能设计将通过各种事件链，导致宇宙中这些内在商品的数量大大减少。&lt;/p&gt;&lt;p&gt;他在当前的 e/acc 背景&lt;a href="https://x.com/ESYudkowsky/status/1732249519776301161?s=20"&gt;下&lt;/a&gt;详细阐述了这一点，并澄清说他的观点并不取决于生物人类的保护（了解这一点很有用）。他就这个主题写了大量的警句，例如&lt;a href="https://www.lesswrong.com/s/9bvAELWc8y2gYjRav/p/GNnHHmm8EzePmKzPk"&gt;《价值是脆弱的》&lt;/a&gt;和《&lt;a href="https://www.lesswrong.com/s/d3WgHDBAPYYScp5Em"&gt;有趣的理论&lt;/a&gt;序列》。&lt;/p&gt;&lt;h2&gt;博斯特罗姆变体&lt;/h2&gt;&lt;p&gt;尼克·博斯特罗姆对人类灭绝的看法似乎以“幸福的生活更好”为出发点。我可能错误的&lt;a href="https://nickbostrom.com/utopia"&gt;印象&lt;/a&gt;是，像埃利以泽一样，他似乎重视艺术、创造力、爱情等东西，在特定意义上，从宇宙或物种中立的角度来看，不存在这些东西的未来将是一个更糟糕的未来。他描述了一个“无人居住的社会”，该社会技术先进并建造了复杂的结构，但“仍然缺乏任何类型的有意识或其福利具有道德意义的存在”（ &lt;em&gt;《超级智能》&lt;/em&gt; （2014）第11章，第173页）。据我所知，他并没有解开无人居住的社会究竟会带来什么坏处以及对谁来说（这可能是很好理解的观点，或者是哲学上的一个非问题，但我不确定情况是否如此，在至少从（见下文）贝纳塔尔、托雷斯、&lt;a href="https://onlinelibrary.wiley.com/doi/pdf/10.1111/1468-0114.00150"&gt;詹姆斯·莱曼&lt;/a&gt;的这篇论文或叔本华的论文来看）。&lt;/p&gt;&lt;p&gt;博斯特罗姆认为我们应该避免很快灭绝的一个更具体的原因是为了保留未来的&lt;a href="https://existential-risk.com/concept"&gt;“选择价值”&lt;/a&gt; ——因为关于人类个人和群体层面的偏好以及物种层面的职业的许多问题仍然没有得到解答，也许最好推迟任何不可逆转的改变，直到我们集体变得更加明智。这直观上是有道理的，尽管即使在这里，也&lt;a href="https://forum.effectivealtruism.org/posts/NfkEqssr7qDazTquW/the-expected-value-of-extinction-risk-reduction-is-positive#1_3__Future_agents_could_later_decide_not_to_colonize_space__option_value_"&gt;不清楚&lt;/a&gt;期权价值实际上对 X 风险降低的整体价值的影响有多大。&lt;/p&gt;&lt;h2&gt; （也许）e/acc 视图&lt;/h2&gt;&lt;p&gt;最近似乎没有来自“e/acc”的评论员群（特别是@basedbefjezos）提出实质性论点，但这篇&lt;a href="https://beff.substack.com/p/notes-on-eacc-principles-and-tenets"&gt;2022 年的帖子&lt;/a&gt;似乎很有用。我对 e/acc 的看法，主要是根据上面的文件进行的：a）他们对人类或类人思想或创造物作为内在商品没有偏见（从某种意义上说，我描述了埃利泽或博斯特罗姆拥有），b）一个他们的内在商品似乎正在最大化宇宙中的智力量（他们对“智力”采取了广泛的、激进的非人类中心主义定义，似乎包括资本主义和其他群体层面的认知），c）他们对所显现结果的看法（智力，无论是人工智能还是社会资本主义）是相当宽容的，即智力导致的任何结果都是可以接受的，并且以下（ &lt;a href="https://beff.substack.com/p/notes-on-eacc-principles-and-tenets#:~:text=the%20technocapital%20singularity-,Effective%20accelerationism%20aims%20to%20follow%20the%20%E2%80%9Cwill%20of%20the%20universe%E2%80%9D%3A%20leaning%20into%20the%20thermodynamic%20bias%20towards%20futures%20with%20greater%20and%20smarter%20civilizations%20that%20are%20more%20effective%20at%20finding/extracting%20free%20energy%20from%20the%20universe%20and%20converting%20it%20to%20utility%20at%20grander%20and%20grander%20scales,-e/acc%20has"&gt;来源&lt;/a&gt;）&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; “‘宇宙意志’[意味着]倾向于热力学偏见，未来会有更伟大、更聪明的文明，更有效地从宇宙中寻找/提取自由能源，并将其转化为越来越大的规模的效用。”&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;继续上面的列表，d）他们不相信埃利泽或博斯特罗姆式的“无人居住的世界”，我认为这就是“僵尸”所指的意思：“无需担心创造更高等的“僵尸”形式智力，因为与意识/更高层次的智力形式相比，它们将处于热力学/进化劣势”。&lt;/p&gt;&lt;p&gt; e/acc 还参考了生命起源的热力学解释（参见&lt;a href="https://www.quantamagazine.org/a-new-thermodynamics-theory-of-the-origin-of-life-20140122/"&gt;杰里米·英格兰&lt;/a&gt;），他们似乎将其&lt;a href="https://beff.substack.com/p/notes-on-eacc-principles-and-tenets#:~:text=Intelligence%20emerges%20as,2"&gt;推断&lt;/a&gt;为在多尺度上运作的更高形式的认知。我没有足够的知识来批评这一点，只是说这感觉这个基金会对他们的主张有很大的影响力（但如果得到良好的支持可能会很有趣 - 但目前还没有）。&lt;/p&gt;&lt;p&gt;上面的评论不包括对 e/acc 思想的任何进一步细化（这是一次&lt;a href="https://x.com/dwarkesh_sp/status/1732141980459905489?s=20"&gt;现场&lt;/a&gt;而&lt;a href="https://www.lesswrong.com/posts/3xoThNNYgZmTCpEAB/based-beff-jezos-and-the-accelerationists"&gt;激烈的&lt;/a&gt;对话），但这里是 Eliezer 的建议（&lt;a href="https://x.com/ESYudkowsky/status/1732249519776301161?s=20"&gt;来源&lt;/a&gt;），他希望从 e/acc 人群中看到什么（就以下方面而言）：充实他们的想法）：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; “我对贝夫·杰索斯立场的模型：我不太关心你的这个预测，所以我不同意它。只要熵增加的速度比其他情况下快，我就很高兴。我已经暂时解锁了@BasedBeffJezos，以防他对此做出我认为的实质性回应，例如，“我实际上预测，作为关于宇宙的经验事实，根据几乎任何一套设计原则构建的人工智能都会关心将其他有情心视为自身的目的，并惊奇地看待宇宙，他们花时间和精力去有意识地体验；人类的后代将提升到与他们自己平等，所有那些且只有那些要求提升的人类；禁止在他们统治的所有地区进行智慧奴役或更大的恐怖；我认为这一立场是关于物理现实的公众所知的真理，而不仅仅是出于信仰而重复的话语；所有这些都是我立场的关键，我&amp;#39;如果我确信事实并非如此，我会退后一步，不会毁灭所有人类生命。”&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;就背景而言，这可能是一种历史好奇心，e/acc 大量借鉴了 20 世纪 90 年代中期在华威大学的控制论文化研究单位 (CCRU) 下培育的“加速主义”思想群。它异常丰富，因为最初的 CCRU 版本采用了加速主义（就其记录/编纂而言），分裂为左派、右派、极右派、无条件主义和许多其他变体，现在加入了一个模糊的万神殿。 /acc.&lt;a href="https://en.wikipedia.org/wiki/Accelerationism"&gt;维基百科&lt;/a&gt;页面是一个好的开始，这篇&lt;a href="http://www.shaviro.com/Blog/?p=1174"&gt;文章&lt;/a&gt;和这篇&lt;a href="https://www.palladiummag.com/2023/11/03/make-yourself-human-again/"&gt;关于&lt;/a&gt;尼克·兰德（Nick Land，一位创始人，后来由于极右观点而被回避）的文章也是如此。基础文本的摘录可以在&lt;a href="https://www.urbanomic.com/book/accelerate/"&gt;Accelerationist Reader&lt;/a&gt;中找到。与其说加速主义是一种连贯的哲学，不如说它是一种生成元模因，它曾经（而且显然仍然似乎）对&lt;a href="https://www.e-flux.com/journal/46/60070/accelerationist-aesthetics-necessary-inefficiency-in-times-of-real-subsumption/"&gt;艺术&lt;/a&gt;和流行文化特别有影响力。&lt;/p&gt;&lt;h2&gt;克里斯蒂安诺：“人类失去了对未来的控制”&lt;/h2&gt;&lt;p&gt;保罗·克里斯蒂安诺（Paul Christiano）的观点（关于灭绝的话题）似乎围绕着以下几个方面展开：a）“价值多元化”的宇宙，即人类价值观仅成为众多价值观中的一种，这是一个糟糕的价值观；b）人类“失去对世界的控制”的时间线。 “未来”是一个糟糕的时间表。这些都是在与当今世界相似的世界（即国家、公司、生物人类等）的背景下进行最具体的讨论，并反思我们的社会经济结构（即相对&lt;em&gt;自由放任的&lt;/em&gt;资本主义） &lt;a href="https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like#:~:text=We%20might%20describe,particularly%20strong%20one."&gt;相互作用&lt;/a&gt;所产生的风险、强大的技术、无能的监管、协调问题以及古德哈特定律的变体。&lt;/p&gt;&lt;p&gt;然而，在 2018 年发表的一篇有趣的文章中，克里斯蒂安诺确实采取了更具 &lt;a href="https://www.lesswrong.com/posts/3kN79EuT27trGexsq/when-is-unaligned-ai-morally-valuable"&gt;推测性的&lt;/a&gt;观点：他放松了对生物人类和我们的价值观/系统的偏见（即接受这样的观点：如果这是唯一的选择，那么我们的超级智能继任者继承未来可能是可以接受的。我们的价值观可能会持续存在），但似乎把诸如“价值”和“友善”等词的定义的难题抛在了一边。&lt;/p&gt;&lt;h2&gt;丹·亨德里克斯：进化压力不利于人类&lt;/h2&gt;&lt;p&gt;我想简单谈谈 Dan Hendrycks 的观点，特别是他对 e/acc 观点的逐条&lt;a href="https://x.com/DanHendrycks/status/1651740865159901184?s=20"&gt;反驳&lt;/a&gt;。他的反驳引用了这篇 2023 年的&lt;a href="https://arxiv.org/pdf/2303.16200.pdf"&gt;论文&lt;/a&gt;，该论文认为人类应该更愿意控制未来而不是灭绝。&lt;/p&gt;&lt;p&gt; Hendrycks 认为，由于（代理人工智能系统的整体）快速变异/扩散到竞争性部署环境中所表现出来的社会和技术压力，可能会出现类似于自然选择的力量。这种选择可能有利于自私行为（就人工智能系统而言），而没有历史上为人类和某些动物提供的利他主义、亲缘选择、合作、道德规范的限制。当人工智能与改变世界的更大效率相结合时，人工智能似乎很可能在竞争中超越人类。这感觉就像是对&lt;a href="https://www.lesswrong.com/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-looks-like-and-robust-agent-agnostic"&gt;安德鲁·克里奇（Andrew Critch）&lt;/a&gt;在这里提出并&lt;a href="https://www.lesswrong.com/posts/qYzqDtoQaZ3eDDyxa/distinguishing-ai-takeover-scenarios#Production_Web"&gt;在这里&lt;/a&gt;进行分析的观点的进化处理。&lt;/p&gt;&lt;p&gt;初步阅读时，我在 Hendrycks 中找不到任何关于人工智能系统&lt;em&gt;之间&lt;/em&gt;合作/竞争可能平衡的内容——即类似的进化压力是否会导致&lt;a href="https://joscha.substack.com/p/the-existential-risk-of-agi#:~:text=One%20of%20the,long%20as%20possible."&gt;自相残杀的&lt;/a&gt;冲突（其中人类可能是附带损害），或者它们确实解决了协调问题吗？问题比人类更好（由于源代码透明度或适合其架构和部署环境的决策理论）并且大多数情况下避免了彼此冲突。&lt;/p&gt;&lt;p&gt;如果人工智能系统有可能陷入自相残杀的冲突，那么似乎很难说它们一定会“更有效地从宇宙中寻找/提取自由能，并将其转化为越来越大的规模的效用”，正如 e/acc 所建议的。他们可能只是无限期地浪费资源。如果没有更强有力的论据，感觉（在这一点上）e/acc 可能会以其自身的方式失败。&lt;/p&gt;&lt;h2&gt;人类进化到其他一些基质&lt;/h2&gt;&lt;p&gt;亨德里克斯的进化框架显然对人类不利。然而，其他进化论的叙述可能更为积极。其中一种愿景是，人类作为一种在生物学上与我们相似的物种而消失，但可能会在某些其他（无机）有机基质上进行&lt;em&gt;进化&lt;/em&gt;，甚至完全融入“自然”环境。许多人都阐述了这一观点：&lt;a href="https://www.overcomingbias.com/p/ai-risk-convo-synthesis"&gt;罗宾·汉森（Robin&lt;/a&gt; &lt;a href="https://quillette.com/2023/08/06/ais-will-be-our-mind-children/"&gt;Hanson）的&lt;/a&gt;&lt;a href="https://www.overcomingbias.com/p/ai-fear-is-mostly-fear-of-future"&gt;作品&lt;/a&gt;、理查德·萨顿（ &lt;a href="https://www.youtube.com/watch?v=NgHFMolXs3U"&gt;Richard&lt;/a&gt; &lt;a href="http://www.incompleteideas.net/Talks/waic3.pdf"&gt;Sutton）&lt;/a&gt; 、詹姆斯·洛夫洛克（ &lt;a href="https://www.noemamag.com/his-mind-forever-voyaging/"&gt;James&lt;/a&gt; &lt;a href="https://nautil.us/gaia-will-soon-belong-to-the-cyborgs-237728/"&gt;Lovelock&lt;/a&gt; ）、乔沙·巴赫（ &lt;a href="https://www.organism.earth/library/document/lfp392-joscha-bach#:~:text=So%2C%20in%20my,way%20more%20interesting."&gt;Joscha&lt;/a&gt; &lt;a href="https://joscha.substack.com/p/the-existential-risk-of-agi"&gt;Bach&lt;/a&gt; ）、 &lt;a href="https://warwick.ac.uk/fac/arts/english/currentstudents/undergraduate/modules/fictionnownarrativemediaandtheoryinthe21stcentury/manifestly_haraway_----_a_cyborg_manifesto_science_technology_and_socialist-feminism_in_the_....pdf"&gt;唐娜·哈拉维（Donna Haraway）、德里克&lt;/a&gt;&lt;a href="https://publicseminar.org/2015/09/blog-post-for-cyborgs/"&gt;·希勒&lt;/a&gt;（ &lt;a href="https://onlinelibrary.wiley.com/doi/pdf/10.1111/bioe.12340"&gt;Derek Shiller&lt;/a&gt; ）和&lt;a href="https://aeon.co/essays/what-are-the-moral-implications-of-humanity-going-extinct#:~:text=happen.%20In%20his-,book,-Mind%20Children%20(1988)"&gt;汉斯·莫拉维克（Hans Moravec&lt;/a&gt; ）（尽管只是一段话）。&lt;/p&gt;&lt;p&gt;除了罗宾·汉森（Robin Hanson），也许还有乔沙·巴赫（Joscha Bach），其他作家并没有详细阐述人类进化和超越的思想，人们可能需要回到超人类和后人类文学（这些文学大多早于当前的浪潮）人工智能的成功）。&lt;/p&gt;&lt;p&gt;尝试充实这一点是我特别感兴趣的领域，所以如果您有想法，请与我们联系。&lt;/p&gt;&lt;h2&gt;反生育主义者和数字痛苦&lt;/h2&gt;&lt;p&gt;上述立场主要涉及因技术或其他降临到人类身上的灾难而产生的生存风险。然而，可以想象，一个物种可能会自愿灭绝，这是一组包括当代反生育主义（认为生育在道德上是错误的）的观点。反出生主义有多种风格：&lt;em&gt;哲学上的反出生主义者，&lt;/em&gt;如&lt;a href="https://iep.utm.edu/anti-natalism/#SH5b:~:text=For%20example%2C%20David,cease%20from%20procreating."&gt;大卫&lt;/a&gt;&lt;a href="https://www.newyorker.com/culture/persons-of-interest/the-case-for-not-being-born"&gt;·贝纳塔尔&lt;/a&gt;，他们基于快乐和痛苦（就被创造的个体而言）之间的不对称性来反对生育；以及&lt;em&gt;厌恶人类的反生育主义者&lt;/em&gt;，他们以人类造成的伤害（例如对自然世界的其他部分）为由反对生育，&lt;a href="https://academic.oup.com/book/26703/chapter-abstract/195504756?redirectedFrom=fulltext"&gt;贝纳塔&lt;/a&gt;在此进行了分析。&lt;/p&gt;&lt;p&gt;我认为这些都是有趣的观点，因为它们质疑价值与人口之间的关系：一个人口较多的世界（受到痛苦、自由意志、正义等方面的重大限制）真的比人口较少的世界更好吗？&lt;/p&gt;&lt;p&gt;特别与人工智能相关的是&lt;a href="https://reducing-suffering.org/strategic-considerations-moral-antinatalists/#Humanitys_future"&gt;Brian&lt;/a&gt; &lt;a href="https://longtermrisk.org/artificial-intelligence-and-its-implications-for-future-suffering"&gt;Tomasik&lt;/a&gt; ，他明确担心数字痛苦的可能性， &lt;a href="https://forum.effectivealtruism.org/posts/JCBPexSaGCfLtq3DP/the-problem-of-artificial-suffering#The_concept_of_artificial_suffering"&gt;Thomas Metzinger&lt;/a&gt;也讨论过这个话题。梅辛格特别反对创造能够承受痛苦的人工智能。&lt;a href="https://nickbostrom.com/propositions.pdf"&gt;尼克·博斯特罗姆（Nick Bostrom）和卡尔·舒尔曼（Carl Shulman）&lt;/a&gt;在人类和人工智能混合社会的治理和其他问题的背景下提出了相关观点（在这种社会中，我们的历史道德直觉和社会契约在享乐范围更广、人口快速增长的存在下崩溃了）和廉价的复制/繁殖，&lt;em&gt;相对于&lt;/em&gt;人类）。&lt;/p&gt;&lt;h2&gt;埃米尔·P·托雷斯谈人类灭绝的伦理学&lt;/h2&gt;&lt;p&gt;在这&lt;a href="https://aeon.co/essays/what-are-the-moral-implications-of-humanity-going-extinct"&gt;两篇&lt;/a&gt;&lt;a href="https://xriskology.medium.com/human-extinction-a-brief-guided-tour-of-the-book-5cfb6a5a726"&gt;文章&lt;/a&gt;中（后者总结了他们的&lt;a href="https://www.xriskology.com/"&gt;新书&lt;/a&gt;），托雷斯广泛分析了人类灭绝。除了对存在伦理史的社会学研究之外，Aeon 论文的相关部分是灭绝的&lt;em&gt;过程&lt;/em&gt;和灭绝的&lt;em&gt;事实&lt;/em&gt;之间的区别（这在人工智能 X 风险的讨论中并不常见） 。托雷斯还直接区分了一个没有任何人类（也没有其他人类创造的智能）的世界与一个我们被取代（或进化）为基于机器的物种的世界。他们再次强调了人工智能 X 风险对话中的一个微小差距，该对话通常对时间范围（可能发生灭绝）保持沉默，部分原因可能是因为假设的背景通常是未来几年或几十年内出现的风险之一。托雷斯呼应了反自然主义的立场，选择了功利主义味道的长期主义的基础，我（松散地）总结为“更多的人总比更少的人好”。他们提到了一个显而易见的观点，即如果人们认为人类的生活主要充满痛苦，那么拥有更多人类的世界似乎并没有明显更好（除非存在其他一些主要的价值来源）。&lt;/p&gt;&lt;h1&gt;要点&lt;/h1&gt;&lt;p&gt;那么，对于人类灭绝的坏处或好处，我们又有何看法呢？我认为三个主要因素可能会影响一个人对灭绝的看法。&lt;/p&gt;&lt;h2&gt;我们是否以及如何成功很重要&lt;/h2&gt;&lt;p&gt;首先，a）灭亡的人类没有留下智慧的继承者，没有实质性的物质或智力文物，没有创造性的作品，b）我们能够留下遗产的世界（在托雷斯的作品中）之间似乎存在着巨大的区别。配方，可能是我们自己的生物或无机重新设计版本，继任者）。这一遗产的确切形式非常不清楚，这支持了博斯特罗姆关于在未来保留期权价值的呼吁以及奥德的&lt;a href="https://forum.effectivealtruism.org/topics/long-reflection"&gt;长期反思&lt;/a&gt;，尽管避免某些存在风险本身可能需要大规模变革这一事实使这两件事变得复杂。技术变革（例如，最终不得不在 AGI 上“掷骰子”）。&lt;/p&gt;&lt;h2&gt;时机似乎很重要&lt;/h2&gt;&lt;p&gt;除了反出生主义者之外，我想相对较少的人会咬紧牙关自愿灭绝，特别是如果这意味着他们或他们活着的（（（...）曾孙）孩子将会灭亡）。这种观点优先考虑自己和近亲的（明显）利益。其他人可能对人类的物理结构和智力成就非常有感情，并可能希望看到文明持续数百或数千年（请参阅人类关于灭绝的思想&lt;a href="https://www.urbanomic.com/book/x-risk/"&gt;历史&lt;/a&gt;）。&lt;/p&gt;&lt;p&gt;然而，我们不应该坚持或期望生物人类会无限期地存在于我们所认识的社会中，直到未来，随着医疗和其他技术的进步，延长活跃或上传的生命也不可能是可行的。正如&lt;a href="https://onlinelibrary.wiley.com/doi/pdf/10.1111/1468-0114.00150"&gt;莱曼&lt;/a&gt;指出的那样，我们对生存的直觉建立在与人类一生大致相当的叙事弧上，如果没有更坚实、更客观的基础，我们应该对扩展这些直觉持怀疑态度。&lt;/p&gt;&lt;p&gt;更根本的是，从博斯特罗姆的期权价值论证的角度来看，人们可能更喜欢人类灭绝的遥远日期。一种更直白的说法是，根据定义，灭绝实际上会导致所有其他未来的终结。然而，当考虑进化作为灭绝或其他&lt;a href="https://nickbostrom.com/papers/future"&gt;场景&lt;/a&gt;时，事情会变得更加复杂。&lt;/p&gt;&lt;h2&gt;灭绝的方式很重要&lt;/h2&gt;&lt;p&gt;似乎很明显，一场灭绝事件带来的巨大痛苦（如果没有该事件）将比缓慢的“自然”灭绝过程（例如通过人口减少）更糟糕。同样，虽然我不关注它，但作为附带损害而摧毁了地球上许多其他生命的灭绝事件将比主要影响人类的事件更糟糕。也有可能，一个事件摧毁了我们迄今为止所建造的一切（包括积累的知识），以至于它可能永远不会被未来的星际外星侦察兵恢复或发现，这将是令人悲伤的（如果不是具体或可量化的糟糕）。&lt;/p&gt;&lt;h2&gt;这实际上是一个紧迫的问题吗？&lt;/h2&gt;&lt;p&gt;考虑到人工智能失调带来的更显着的短期和中期风险，这似乎是毫无意义的空想。它也可能毫无帮助：正如&lt;a href="https://www.lesswrong.com/posts/3kN79EuT27trGexsq/when-is-unaligned-ai-morally-valuable#On_risks_of_sympathy"&gt;克里斯蒂安诺&lt;/a&gt;指出的那样，一些出于充分动机的担忧（不假思索或过早地提出），例如数字痛苦，可能会破坏我们关于人工智能安全和监督的社会推理。&lt;/p&gt;&lt;p&gt;然而，想象一下存在一个 GPT- &lt;em&gt;n&lt;/em&gt; ，我们怀疑它可能会经历现象学状态，具有构建长期计划的目标和能力，并且（比方说）通过了我们当时拥有的任何对齐基准。然而，我们，它的设计者，根据预防原则（无论出于何种原因）决定关闭它或不部署它。与斯坦尼斯瓦夫·莱姆的&lt;a href="https://academic.oup.com/liverpool-scholarship-online/book/43309/chapter-abstract/363077641?redirectedFrom=fulltext"&gt;&lt;em&gt;魔像 XIV&lt;/em&gt;&lt;/a&gt;相呼应，我们可能会被要求（无论是机器、它的前身，还是&lt;a href="https://nickbostrom.com/papers/digital-minds.pdf"&gt;历史&lt;/a&gt;的判断）来解释我们的推理，这很可能触及上面提出的一些问题，甚至如果我们不能给出任何明确的答案，我们可能需要表明我们确实考虑过这个问题，而不是羞怯地采取（碳沙文主义或生物沙文主义）立场。&lt;/p&gt;&lt;br /&gt;&lt;br /&gt;&lt;a href="https://www.lesswrong.com/posts/HaGTQcxqjHPyR9Ju6/unpicking-extinction#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sat, 09 Dec 2023 09:15:41 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/HaGTQcxqjHPyR9Ju6/unpicking-extinction</guid></item><item><title>寻找法学硕士特征之间的稀疏线性连接</title><link>https://www.lesswrong.com/posts/7fxusXdkMNmAhkAfc/finding-sparse-linear-connections-between-features-in-llms</link><description>发布于 2023 年 12 月 9 日凌晨 2:27（格林尼治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt; TL;DR：我们使用 SGD 来查找特征之间的稀疏连接；此外，尽管 MLP 存在非线性，但残差流和 MLP 之间的大部分特征都可以建模为线性计算。有关示例，请参阅&lt;a href="https://www.lesswrong.com/editPost?postId=7fxusXdkMNmAhkAfc&amp;amp;key=e013388fad8122af1003041ca4ae7f#Sparse_Linear_Feature_Connections"&gt;线性特征部分&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;&lt;i&gt;特别感谢 AISST 成员 Adam Kaufman，他最初想到了学习特征之间稀疏连接的想法，并感谢 Jannik Brinkmann 训练这些 SAE。&lt;/i&gt;&lt;/p&gt;&lt;p&gt;稀疏自动编码器 (SAE) 能够将 LLM 的激活转换为可解释的特征。为了定义电路，我们希望找到这些功能如何相互连接。 SAE 允许我们使用 SGD 大规模地查找可解释的特征，那么为什么不使用 SGD 来查找连接呢？ &lt;/p&gt;&lt;figure class="image"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/bbadeeylguad5kjbpdfj" /&gt;&lt;figcaption&gt; Pythia-70M 中的一层，在 MLP 之前和之后都有 SAE。请注意，这与典型的 Transformer 架构不同，Pythia 模型具有并行 MLP 和 Attn 模块，而不是顺序模块（这是为了复制并行化的 SOTA 模型，以提高 GPU 利用率）&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;我们在 MLP 之前有一组特征 F1，在 MLP 之后有一组特征 F2。这些特征是通过在这些层的激活上训练 SAE 来学习的。&lt;/p&gt;&lt;p&gt;理想情况下，我们学习一个线性函数，使得 F2 = W(F1)，并且 W 是稀疏的（即 W 权重上的 L1 惩罚）。那么我们可以查看 F2 中的一个特征，然后说“哦，这只是 F1 特征的稀疏线性组合，例如 0.8*（但是特征）+ 0.6*（但是特征）”，这将是非常容易解释的！&lt;/p&gt;&lt;p&gt;然而，我们正在尝试复制 MLP 的计算，这肯定不可能都是线性的！ &lt;span class="footnote-reference" id="fnrefpwepir8kf3"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnpwepir8kf3"&gt;[1]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;那么，从 F1 到 F2 获得最低损失的最简单计算是什么（暂时忽略 L1 权重稀疏性惩罚）？&lt;/p&gt;&lt;p&gt;仅在 F1 和 F2 之间的 MSE 上进行训练，我们在 Pythia-70m-deduped 的 4 个设置中绘制了跨 5 个层的训练过程中的 MSE：&lt;/p&gt;&lt;p&gt;线性： &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;y&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;W&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;非线性&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;：&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;y&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;Re&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;l&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;u&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;W&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt; MLP： &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;y&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space3"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;W&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;R&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;e&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;L&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;U&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;W&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;两个&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;非线性&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;：&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;R&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;e&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;LU&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;(&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;W&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;2&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;R&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;e&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;LU&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;(&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;W&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/jljlqsndeir2byp2hsqr" /&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;对于所有层，沿着（MLP 和两个非线性）和（线性和非线性）训练损失簇。由于 MLP 和线性是这两个簇中最简单的，因此其余分析将仅关注这两个簇。&lt;/p&gt;&lt;p&gt; [我还研究了偏差与无偏差：添加偏差并没有积极改善损失，因此被排除在外]&lt;/p&gt;&lt;p&gt;有趣的是，最后一层（和第 2 层）的相对线性 MLP 差异巨大。一般来说，最后一层的损失也大得多，尽管第 5 层 MLP 激活的 L2 范数为 52，而第 4 层为 13。这是 4 倍的增加，这将是 MSE 损失增加 16 倍。最后一个数据点的损失为 0.059 和 0.0038，相差约 16 倍。&lt;/p&gt;&lt;h2&gt;线性特征的百分比是多少？&lt;/h2&gt;&lt;p&gt;显然 MLP 更好，但这是平均水平。如果一定比例的特征可以建模为线性计算怎么办？因此，我们采用特征损失的差异（即对于特征，我们采用线性损失 - MLP 损失），通过各自的 L2 范数/层对所有损失进行归一化，然后绘制它们。 &lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/iz1bfh3ene0rh8o87cyo" /&gt;&lt;br /&gt;呃……这里有一些巨大的异常值，这意味着这些特定特征是非常非线性的。只需为所有层设置阈值 0.001： &lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/j830nyah5akirkajdfoo" /&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;&lt;figure class="table"&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="border: 1pt solid #000000; padding: 5pt; vertical-align: top;"&gt;层&lt;/td&gt;&lt;td style="border: 1pt solid #000000; padding: 5pt; vertical-align: top;"&gt;特征百分比 &amp;lt; 0.001 损失差异（即可以线性表示）&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="border: 1pt solid #000000; padding: 5pt; vertical-align: top;"&gt; 1&lt;/td&gt;&lt;td style="border: 1pt solid #000000; padding: 5pt; vertical-align: top;"&gt; 78%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="border: 1pt solid #000000; padding: 5pt; vertical-align: top;"&gt; 2&lt;/td&gt;&lt;td style="border: 1pt solid #000000; padding: 5pt; vertical-align: top;"&gt; 96%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="border: 1pt solid #000000; padding: 5pt; vertical-align: top;"&gt; 3&lt;/td&gt;&lt;td style="border: 1pt solid #000000; padding: 5pt; vertical-align: top;"&gt; 97%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="border: 1pt solid #000000; padding: 5pt; vertical-align: top;"&gt; 4&lt;/td&gt;&lt;td style="border: 1pt solid #000000; padding: 5pt; vertical-align: top;"&gt; 98%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="border: 1pt solid #000000; padding: 5pt; vertical-align: top;"&gt; 5&lt;/td&gt;&lt;td style="border: 1pt solid #000000; padding: 5pt; vertical-align: top;"&gt; 99.1%&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;大多数特征都可以通过损失的微小差异进行线性建模（有些特征具有负损失差异，这意味着线性的损失比 MLP 更低）。这些值是如此之小，以至于我将其归因于噪声）。非常方便！&lt;/p&gt;&lt;p&gt; [注：0.001 是任意的。为了使这一点更有原则性，我们可以绘制向 LLM 激活层添加不同级别的噪声的效果，然后选择一个交叉熵损失下降可以忽略不计的阈值？&lt;/p&gt;&lt;h1&gt;添加稀疏性&lt;/h1&gt;&lt;p&gt;现在，让我们训练稀疏 MLP 和稀疏线性连接。此外，我们可以将线性特征限制为只有经过良好建模的线性特征（与 MLP 相同）。我们将使用以下损失：&lt;/p&gt;&lt;p&gt;损失 = MSE(F2 - F2_hat) + l1_alpha*L1(权重)&lt;/p&gt;&lt;p&gt;但是我们如何选择l1_alpha呢？让我们绘制一系列 l1_alphas 的 MSE 损失与 l1 损失的帕累托前沿： &lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/i3wvny6cdcvtr8fwcjup" /&gt;&lt;/p&gt;&lt;p&gt;这是针对 l1_alphas = [1e-7, 1e-5, 1e-3, .1, 10, 100] 的情况，两条线的弯头都是 l1_alpha=1e-3。它的 MSE 比我想要的稍高，因此我将把它设置为 8e-4 以供将来运行。 （较低的 l1 惩罚会导致较高的 l1 损失和较低的 MSE）。&lt;/p&gt;&lt;h1&gt;稀疏线性特征连接&lt;/h1&gt;&lt;p&gt;将自己限制在线性特征上，我们重新训练了稀疏线性权重连接 w/ l1_alpha=8e-4。&lt;/p&gt;&lt;p&gt;下面我们展示了一些稀疏线性特征连接的示例。对于好奇的读者，可以&lt;a href="https://comet-scorpio-0b3.notion.site/More-Examples-ceaefc95cc924afba318dca1da37d4a4?pvs=4"&gt;在此处&lt;/a&gt;找到更多示例。&lt;/p&gt;&lt;h2&gt;或示例&lt;/h2&gt;&lt;p&gt;在第 1 层，我们有：&lt;/p&gt;&lt;p&gt; &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;Ø&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;F&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;30&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0.26&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;ØF&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;2797&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0.23&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;ØF&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;259&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;+&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0.10&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;ØF&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;946&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;其中 OF 是输出特征（在 MLP_out 中），IF 是输入特征（在 MLP 之前的 Residual Stream 中）&lt;/p&gt;&lt;p&gt;下面是输入特征 2797，在令牌“former”上强烈激活&lt;br /&gt;&lt;/p&gt;&lt;figure class="image"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/udfist7r6sqlz0zugxux" /&gt;&lt;figcaption&gt;这是5个例子。对于每个 ex，顶行单词是特征激活，例如标记“former”被激活 9.4。底部空白行是：如果我们删除此功能，模型在预测这些标记方面会变得多么糟糕？例如，当模型无法使用这个“以前的”功能时，苏联的性能会差 5.5 logits。&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;下面是输入特征 259，在令牌“old”上强烈激活&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/wbxbuj30brl7s47fuzxd" /&gt;&lt;/p&gt;&lt;p&gt;下面是输入特征 946，在令牌“young”上激活&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/ifrgaktkwasjqfrscpvm" /&gt;&lt;/p&gt;&lt;p&gt;在输出特征中，我们看到标记“前”、“老”和“年轻”都被激活，其中“年轻”的激活强度约为“前”和“旧”的一半，正如我们从权重系数中预期的那样。&lt;/p&gt;&lt;p&gt; &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;OF&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;30&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0.26&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;F&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;前&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;0.23&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;折叠&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;0.10&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;F&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;年轻&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mn MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/uapwsrethupsdihmldok" /&gt;&lt;/p&gt;&lt;p&gt;我们可以将此计算视为加权逻辑或。输出特征 30 在以前或年老或年轻时激活（同样，更多示例在&lt;a href="https://comet-scorpio-0b3.notion.site/More-Examples-ceaefc95cc924afba318dca1da37d4a4"&gt;这里&lt;/a&gt;）&lt;/p&gt;&lt;h2&gt;负权重示例&lt;/h2&gt;&lt;p&gt;在第 1 层，我们有：&lt;/p&gt;&lt;p&gt; &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;ØF&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;505&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0.68&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;ØF&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;3021&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;-&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0.21&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;ØF&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;729&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;其中 OF 是输出特征，IF 是输入特征。&lt;/p&gt;&lt;p&gt;下面是输入特征 3021，对像“said”这样的标记强烈激活，这些标记在几乎所有情况下都不会出现在引用之后。 &lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/yjuvhqqenxtmvscwcuoc" /&gt;&lt;/p&gt;&lt;p&gt;下面是输入功能 729，当“said”等标记在引用后不久出现时，会强烈激活它们。 &lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/ce3vxm3mehokiltt7tob" /&gt;&lt;/p&gt;&lt;p&gt;下面显示了当我们删除某个上下文标记时输入特征 729 的激活如何变化。重要的是，当引用被删除时，激活就会执行，这表明该功能在有引用、后跟“said”时激活。 &lt;/p&gt;&lt;figure class="image"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/czupawerkjnwrl8ishof" /&gt;&lt;figcaption&gt;这种形象通常会让人们感到困惑。高级别要点：任何红色的东西都是激活此功能的重要上下文标记（蓝色并不那么重要，因为删除后它只会增加到 0.5，而不是 -5.1）。我们试图传达的是，删除引号会使“says”一词的功能激活从 -5.1 下降到 0 功能激活。&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;下面我们看到输出功能在像“said”这样没有先前引用标记的标记上激活。我们已经“减去”了较大的负权重，可以说，“said”出现在引号之后的示例，现在该功能仅在“said”出现而没有任何先前引号时才激活。&lt;/p&gt;&lt;p&gt; &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;O&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;F&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;505&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0.68&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;F&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;（&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;在许多情况下为“说”&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;）&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;−&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0.21&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;F&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;（&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;引号后为“说”&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;）&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/ur5krelkxzpsbltf56ky" /&gt;&lt;/p&gt;&lt;p&gt;我们可以将此计算视为加权逻辑与。输出特征505在A和~B上激活。在 A 是 B 的超集的情况下，这是 B 的补集，例如我有所有水果和所有黄色水果的集合，所以现在我可以找到所有非黄色水果。&lt;/p&gt;&lt;p&gt; （再次强调，&lt;a href="https://comet-scorpio-0b3.notion.site/More-Examples-ceaefc95cc924afba318dca1da37d4a4"&gt;这里&lt;/a&gt;有更多示例）&lt;/p&gt;&lt;h1&gt;稀疏 MLP 特征连接&lt;/h1&gt;&lt;p&gt;让我们可视化这些损失最严重的 MLP 特征：&lt;/p&gt;&lt;p&gt;第 5 层：查看线性和 MLP 之间最大损失差异的特征&lt;/p&gt;&lt;p&gt;（具体为[1.5555、0.0116、0.0052、0.0040、0.0038]）&lt;/p&gt;&lt;p&gt;所有 5 个功能的激活率都非常高。第一个通常很奇怪（与典型的异常值维度相比），接下来的 4 个大多是奇怪的标记。 &lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/karjun1tnmpp6qjwzmbd" /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/br95vzp1babvnxfqm4ue" /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/lgyvhdzu4nwtaqahbysl" /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/ujggypvajaaoxsiqxzav" /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/htaoeq6cnifh4u8xphvm" /&gt;&lt;/p&gt;&lt;p&gt; （作为一般说明：LLM 的最后一层通常非常奇怪！这也出现在调谐透镜纸上，并且被怀旧者假设为扩展的非嵌入矩阵）&lt;/p&gt;&lt;p&gt;第 4 层：损失差异 [0.0529, 0.0134, 0.0106, 0.0106, 0.0099]&lt;/p&gt;&lt;p&gt;第一和第三是异常特征。异常值特征的典型特征（根据我的经验）是：&lt;/p&gt;&lt;p&gt; 1）非常高的激活（这解释了高L2损失）&lt;/p&gt;&lt;p&gt; 2) 在前几个令牌上激活&lt;/p&gt;&lt;p&gt;3）在第一个分隔符处激活（例如句点或换行符，我将其表示为“\n”）&lt;/p&gt;&lt;p&gt; （为什么存在这些？我不知道，文献和理论存在，但超出了本文的范围） &lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/v6f5ltmngyyshckpttot" /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/gyovzsu4hjndqpdlnqhs" /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/qgu4diytowbmgz5uteua" /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/isrltm8kiu9lno1fz7zi" /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/cfzjxetenumareiyf6zh" /&gt;&lt;/p&gt;&lt;p&gt;第 3 层：损失差异 [0.0456, 0.0163, 0.0122, 0.0101, 0.0069]&lt;/p&gt;&lt;p&gt;第一个和第五个是异常特征&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/dnl6t83boy0ltkcnltqh" /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/fnhuxjirjsl2nutx5fjv" /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/xzorozx3elxmgisx3hxg" /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/kzkjcaw1cvvvpjtaulrn" /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/lcgylzejjs6nlh6vatz3" /&gt;&lt;/p&gt;&lt;p&gt;第 2 层：损失差异 [0.3370, 0.3324, 0.2910, 0.1682, 0.1069]&lt;/p&gt;&lt;p&gt;四个异常值特征&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/ynbd093akdzpahlenf6m" /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/gcsbvca4uc4yjsw71jhc" /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/u5lvffwf1myxytieya2w" /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/gugve0pcvecth8meebpk" /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/laydxbeqld7u2b5co2z6" /&gt;&lt;/p&gt;&lt;p&gt;第 1 层：损失差异 [0.1401, 0.0860, 0.0159, 0.0150, 0.0125]&lt;/p&gt;&lt;p&gt;前两个特征是离群特征&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/kc0eunsc2areajogzxvm" /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/lr9rgsgdpjwgbf0sny1b" /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/ppdtkg0uwevck8glq8pi" /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/h23mpxmwyx34clcc3iqm" /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/dfvh6a0gr2z0fw4ee0eh" /&gt;&lt;/p&gt;&lt;p&gt;这些特征的具体权重如何？&lt;/p&gt;&lt;p&gt;因此，MLP 有两组线性权重：W2(relu(W1(x)))。观察 W2，我注意到损失最大的特征有很多大的正权重和负权重。这是前 5 个损失特征（与上面可视化的相同）。对于正权重： &lt;br /&gt;&lt;br /&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/yw0uo3yzekejzjpsbdp2" /&gt;&lt;br /&gt;因此，第 4 层中最高的 loss-diff 特征有 112 个连接权重，且大于 0.1，而中值特征只有 9 个。&lt;/p&gt;&lt;p&gt;对于负权重： &lt;/p&gt;&lt;figure class="image"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/svq8lu8bxrmoig6bymyw" /&gt;&lt;/figure&gt;&lt;p&gt;请记住，这些是 W2 的权重，它连接 LLM 的 MLP 输出特征和 MLP 连接器的隐藏层。我们真的不知道这些意味着什么。&lt;/p&gt;&lt;p&gt;但我们绝对可以像可视化特征激活一样可视化它们，也许它们是可以解释的，所以……它们都是相当垃圾的。&lt;/p&gt;&lt;p&gt;异常值相关：8/30&lt;br /&gt;多义性：8/30&lt;br /&gt;单义：1/30&lt;br /&gt; （基本上）死亡：11/30&lt;/p&gt;&lt;p&gt; （这些是针对第 3 层的，但令人惊讶的是，第 1 层隐藏特征在默认情况下具有 80% 的单义性，并且还带有异常值特征）。&lt;/p&gt;&lt;p&gt;哇，如果我们有一种方法可以使隐藏层激活更容易解释就好了！因此，我们可以像稀疏 AE 一样训练稀疏 MLP 连接器：对潜在激活进行 l1 惩罚（基本上是连接两个 SAE 的 SAE）。&lt;/p&gt;&lt;h2&gt;严重不良事件上的严重不良事件&lt;/h2&gt;&lt;p&gt;我对 l1 权重和隐藏 l1 使用相同的 l1_alpha 项，并查看第 1 层的各种损失。总体而言：&lt;/p&gt;&lt;p&gt;损失 = MSE + L1_alpha*(L1(权重) + L1(隐藏激活)) &lt;/p&gt;&lt;figure class="image image_resized" style="width: 80.52%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/lfd8xr3cw6i3rpzsbygh" /&gt;&lt;figcaption&gt;各种 l1_alpha 的损失。 “隐藏”是隐藏激活的 L1 损失。&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;所以我选择 4e-4 的 l1_alpha 作为 MSE 和 l1 损失之间的折衷方案。这对应于 25 个隐藏潜在激活的 L0（即其他 3k 为 0）。&lt;/p&gt;&lt;p&gt;查看前 30 个最大激活特征，第 3、4 和 5 层都是 MLP 的异常维度（第一个标记和第一个分隔符在一起）。 SAE 仅具有 10% 的异常特征。这是有道理的，因为这些离群维度都针对相同的标记（即第一个标记和第一个分隔符）激活，因此将具有较高的潜在 l1 激活。这将激励更多地结合这些维度。&lt;/p&gt;&lt;p&gt; SAE 的特征并不比 MLP 更具有单一语义。这可能是因为我需要为潜在激活添加偏差。此外，我对如何将稀疏权重与稀疏潜在激活相结合感到困惑（我在“请帮助”中指定了更多信息）。我将把剩下的实证工作留给未来，并继续进行推测。&lt;/p&gt;&lt;p&gt;解释这些特征可能类似于 Latent_features -&amp;gt; F2 中的线性 AND &amp;amp; OR 语句。从 F1-&amp;gt; 潜在特征是一个 ReLU，带有偏差的情况是：&lt;/p&gt;&lt;p&gt; &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;Latent&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_F&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;R&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;eLU&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;w&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;f&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;"&gt;+&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;w&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;f&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;−&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;bias&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;具体来说&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;Latent&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;F&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;3&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;R&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;e&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;L&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;U&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2.1&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;*&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;f&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1.5&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;*&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;f&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;2&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;−&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;4&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;如果它们单独激活超过 4，则可以是 F1 或 F2；如果它们必须一起激活才能大于 4，则可以是 F1 和 F2。&lt;/p&gt;&lt;p&gt;这使得进行特征激活统计和聚类变得很重要。最好绘制它们的共同激活（并根据是否激活潜在特征进行颜色）&lt;/p&gt;&lt;p&gt;但如果我想做 3 个以上的功能，就很难绘制它们的共同激活图。这里肯定有某种统计方法来收集共同激活的集群吗？&lt;/p&gt;&lt;h1&gt;请帮忙&lt;/h1&gt;&lt;p&gt;这主要是一组“快速进行并获得结果”的实验，这意味着做出了许多任意选择，我希望得到一些反馈。我确实打算自己研究这些问题（现在已经晚了，我希望这篇文章在本周末发布）。&lt;/p&gt;&lt;ol&gt;&lt;li&gt;训练时如何考虑各层的不同规范？例如，第 1 层的（残差流，mlp_out）的范数为 (7,4)。第 5 层具有 (16, 54) 之一。 &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mfrac"&gt;&lt;span class="mjx-box MJXc-stacked" style="width: 7.742em; padding: 0px 0.12em;"&gt;&lt;span class="mjx-numerator"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;Atm&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;，&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;我&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;将&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;MSE&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;除以&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;MLP_outlayer&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;的&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;范&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;数&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 83.3%; vertical-align: 0.435em; padding-left: 0px; padding-right: 0.06em;"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;分钟&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;范&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;数&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-denominator"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-line"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-vsize"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; ，并保持权重激活相同。&lt;ol&gt;&lt;li&gt;通常如何处理这种情况？&lt;/li&gt;&lt;li&gt;我将如何处理添加潜在激活 l1 惩罚？ &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mfrac"&gt;&lt;span class="mjx-box MJXc-stacked" style="width: 6.868em; padding: 0px 0.12em;"&gt;&lt;span class="mjx-numerator"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;这&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;不是&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;平方&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;，&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;所以&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;它将&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;是&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;MLP_outlayer&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;的&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;范&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;数&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;最小&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;范&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-denominator"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;数&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-line"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-vsize"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; ？我怀疑这应该考虑 MLP_in 规范&lt;/li&gt;&lt;li&gt;如果我改为正常&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;关于稀疏权重w和稀疏潜在激活的任何相关文献&lt;/li&gt;&lt;li&gt;统计问题：这些权重连接正在估计一个估计值（即，SAE 正在重建层的激活，权重正在重建 SAE 的重建）。这里是否有一些“平均误差相同，但方差更高”的论点？&lt;/li&gt;&lt;li&gt;寻找共激活簇的典型工具/算法是什么？我认为相关性很&lt;i&gt;接近&lt;/i&gt;，但它不会捕获 A 和 B 导致该特征的情况，但有时 A 很大而 B 很小，而其他情况则相反。&lt;ol&gt;&lt;li&gt;当潜在功能激活时，我还可以缓存所有输入功能激活，并对它们进行聚类。虽然我也想要导致潜在功能不激活的输入功能。&lt;/li&gt;&lt;li&gt;我还有稀疏权重，这应该是所有相关信息。&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h1&gt;宏伟计划&lt;/h1&gt;&lt;p&gt;如果我们能够定义回路，我们就可以具体指定重要的模型回路，例如诚实、欺骗、英式英语、自我意识和人格特质。我们当然会争论一个人的操作化是否真正抓住了我们想要的东西，但我们现在可以&lt;i&gt;实际指定&lt;/i&gt;它们来拥有这个论点。&lt;/p&gt;&lt;p&gt;我很高兴找到彼此因果关系的特征（这项工作是相关的）。这可以通过梯度或因果干预来完成。一旦我们有了这些因果联系，我们仍然需要找到如何计算这些特征。这项工作表明，其中许多连接都是线性计算的，而非线性计算的连接就是这些离群维度特征（这对于法学硕士进行文本预测很有用，但对于模型引导没有用）。 &lt;span class="footnote-reference" id="fnref3po7l3myi4l"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn3po7l3myi4l"&gt;[2]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;对于注意力特征，我们还可以在残余流中的特征和注意力之后的特征之间研究&lt;a href="https://transformer-circuits.pub/2021/framework/index.html"&gt;QK/OV&lt;/a&gt;电路。这还需要考虑功能激活统计数据，但似乎非常可行！&lt;/p&gt;&lt;p&gt;因此，如果我们有 Residual &amp;amp; MLP_out 和 Residual &amp;amp; Attention_out 的特征之间的连接，那么我们还可以将下一层 Residual 的特征计算为前一层特征的稀疏线性组合：&lt;br /&gt; &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;R&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;层&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;R&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;层&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;+&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;W&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space3"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;*&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;M&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;L&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;P&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;输出&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;层&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-msubsup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;+&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;W&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;2&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;*&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;At&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;t&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;n&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;输出&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;层&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;这就是所有的连接。&lt;/p&gt;&lt;p&gt;还有很多工作要做，但其难度级别是“普通学术界可以解决它”，而不是“证明 P !=NP”；这是一个比我去年想象的要好得多的时间表。&lt;/p&gt;&lt;p&gt;如果您想参与任何 Sparse AE 项目，请随时加入我们的 #sparse-coding 频道（在 interp 下）中的 EleutherAI Discord 频道（&amp;gt;25k 成员，因此可以轻松潜伏）： &lt;a href="https://discord.gg/eleutherai"&gt;https://discord .gg/eleutherai&lt;/a&gt;&lt;/p&gt;&lt;p&gt;请随时在不和谐问题上联系我（Logan）：loganriggs、LW 上的 dm，或下面的评论。&lt;/p&gt;&lt;h1&gt;代码&lt;/h1&gt;&lt;p&gt;对于代码复制，请参阅我的&lt;a href="https://github.com/loganriggs/sparse_coding/tree/main"&gt;存储库&lt;/a&gt;中的“static*”文件&lt;/p&gt;&lt;p&gt;static-all_sparse_weights - 用于训练和比较线性与非线性的笔记本&lt;br /&gt;static-interpret_sparse_weights - 用于可视化线性或非线性特征的笔记本&lt;br /&gt;static-train_sparse_sae_connector - 训练 SAE（带 l1 潜在激活惩罚的 MLP）&lt;br /&gt; static-interpre_sparse_weights_mlp - 用于解释稀疏 SAE 的潜在激活并与 MLP 进行比较的最小笔记本。&lt;/p&gt;&lt;p&gt; [注：我还没有时间评论或清理这些笔记本。如果您遇到任何问题请给我留言]&lt;/p&gt;&lt;h1&gt;附录&lt;/h1&gt;&lt;p&gt;这里有一些额外的实验，但没有成功或者只是很奇怪。&lt;/p&gt;&lt;h2&gt; MLP 扫描失败&lt;/h2&gt;&lt;p&gt;我也尝试减小 MLP 的隐藏层大小，但 MSE 仍然有所增加。这并没有将 MLP 限制为仅 MLP 特征。 &lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/pwoj5ixbj0w9qh54lsls" style="width: 78.31%;" /&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;h2&gt;注意力&lt;/h2&gt;&lt;p&gt;另外，如果我们对注意力进行相同的分析会怎样？ &lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/ydew0wckxeo6xnilro9r" /&gt;&lt;/p&gt;&lt;p&gt;第 4 层应该被忽略，因为它大部分都是死功能，但总的来说，这很奇怪！我没有像 MLP 那样对损失进行归一化，但似乎许多特征可以通过特征线性重建。这意味着注意力并没有真正&lt;i&gt;关注&lt;/i&gt;很多功能。&lt;/p&gt;&lt;p&gt;我的意思是，注意力通常会包含先前标记位置中的所有特征。如果我们每个示例有 200 个标记和大约 20 个特征/数据点，那么注意力就可以访问第 200 个位置的所有 20*200 个特征。然而，在这里，它只能访问&lt;i&gt;当前位置的&lt;/i&gt;20 个特征。诡异的。 &lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/xzjd94svk8cncrvlntyd" /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/cnrurwbmeobzcuesk97h" /&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;ol class="footnotes"&gt;&lt;li class="footnote-item" id="fnpwepir8kf3"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefpwepir8kf3"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;此外，特征 F1 和 F2 之间的完整计算必须包括来自 SAE_1 的解码器、MLP 和来自 SAE_2 的编码器 + ReLU。&lt;br /&gt; F2 = relu(线性(线性(gelu(线性(线性(F1))))))&lt;/p&gt;&lt;p&gt; = relu(线性(gelu(线性(F1))) [因为两个线性函数可以等价于1个线性函数]&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn3po7l3myi4l"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnref3po7l3myi4l"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;我确实认为弄清楚这些异常维度所起的因果作用是有价值的。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ol&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/7fxusXdkMNmAhkAfc/finding-sparse-linear-connections-between-features-in-llms#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sat, 09 Dec 2023 02:27:42 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/7fxusXdkMNmAhkAfc/finding-sparse-linear-connections-between-features-in-llms</guid></item><item><title>选项空间命名法</title><link>https://www.lesswrong.com/posts/s9Jh5bNSoCJqwDx7k/option-space-nomenclature</link><description>发布于 2023 年 12 月 8 日晚上 11:14（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;我很好奇这里使用的行话/关键词是否与我正在研究其他人的经验的主题相关，因为我自己的背景导致我对此类事物使用非常个人的术语。&lt;/p&gt;&lt;p&gt;所讨论的概念是对“当遇到给定类型的问题时我会想到的所有选项的集合”的认识和修改。在与其他人讨论这个概念时，我通常简称为“选项空间”，但由于它不完全是一门你可以在 15 分钟内掌握的艺术，所以我还没有遇到很多其他人涉足过它。&lt;/p&gt;&lt;br /&gt;&lt;br /&gt;&lt;a href="https://www.lesswrong.com/posts/s9Jh5bNSoCJqwDx7k/option-space-nomenclature#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Fri, 08 Dec 2023 23:14:54 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/s9Jh5bNSoCJqwDx7k/option-space-nomenclature</guid></item><item><title>“模拟联合国解决方案”</title><link>https://www.lesswrong.com/posts/vDRjwdGHJCNtfWJgf/model-un-solutions</link><description>发布于 2023 年 12 月 8 日晚上 11:06（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;当我在高中时，因为我是历史碗队的一员，为模拟联合国俱乐部提供建议的老师招募我作为他们的代表参加各种“历史委员会”，如罗马参议院或 1789 年法国议会。我从未参与过任何正常的委员会，因为你无法进行假旗攻击或说服教皇将其他代表逐出教会。&lt;/p&gt;&lt;p&gt;据我所知，在大多数委员会中，参与者代表国家试图通过一项决议，解决事先决定的气候变化等主题。主持人会向被认定为“最佳代表”的玩家颁发奖项——这是演讲能力、社会主导地位以及准确代表（或至少不会致命地误解）指定国家的立场和利益的不成文组合。&lt;/p&gt;&lt;p&gt;我经常在心里比喻“模拟联合国讨论”和“模拟联合国解决方案”。模拟联合国的讨论围绕着人们期望因发表许多言论而获得奖励而展开，尽管他们的实际立场可以简单地表达或不允许进行太多阐述。&lt;/p&gt;&lt;p&gt;这导致了“联合国解决方案模型”，它有几种类型，例如&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://www.lesswrong.com/posts/dLbkrPu5STNCBLRjr/applause-lights"&gt;&lt;u&gt;掌声灯&lt;/u&gt;&lt;/a&gt;：你可以只说流行语或无可争议的琐事（“在解决气候变化问题时，我们应该考虑所有相关利益相关者的利益。我们既不应该采取{极端观点}，也不应该采取{相反的极端}”）&lt;/li&gt;&lt;li&gt;&lt;strong&gt;未指定的解决方案&lt;/strong&gt;：您可以提供很少的信息来唯一地识别听众心中现状的特定变化。在极端情况下，你会得到很多这样的评论：“为了解决问题，我们应该{投入资源}来{解决问题}”，其中括号内的部分被替换为不太具体的短语（“为了解决气候问题”）我们应该成立工作组来确定最佳的技术和政策方法”）&lt;/li&gt;&lt;li&gt;&lt;strong&gt;权衡无知的解决方案&lt;/strong&gt;：您甚至可以给出方向性建议，但避免考虑相关成本或权衡（“我们应该资助一项与气候变化相关的新教育推广计划”）。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;您可以想象尝试识别空洞言论的回应：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;询问是否有人持相反观点。&lt;/li&gt;&lt;li&gt;询问提议的解决方案与现状有何不同。&lt;/li&gt;&lt;li&gt;询问谁在提案中失败（或者如何重新分配资源）。有时没有人会遭受损失，但更多时候这只是一种未明示的权衡。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;br /&gt;&lt;br /&gt;&lt;a href="https://www.lesswrong.com/posts/vDRjwdGHJCNtfWJgf/model-un-solutions#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Fri, 08 Dec 2023 23:06:34 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/vDRjwdGHJCNtfWJgf/model-un-solutions</guid></item><item><title>反对阴谋的速度论证（“阴谋人工智能”第 4.4-4.7 节）</title><link>https://www.lesswrong.com/posts/X9mtcR7iNEKTii9go/speed-arguments-against-scheming-section-4-4-4-7-of-scheming</link><description>发布于 2023 年 12 月 8 日晚上 9:09（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;这是我的报告《&lt;a href="https://arxiv.org/pdf/2311.08379.pdf"&gt;诡计多端的人工智能：人工智能会在训练期间假装对齐以获得权力吗？》&lt;/a&gt;的第 4.4-4.7 节。 ”。 &lt;a href="https://www.lesswrong.com/posts/yFofRxg7RRQYCcwFA/new-report-scheming-ais-will-ais-fake-alignment-during"&gt;这里&lt;/a&gt;还有完整报告的摘要（ &lt;a href="https://joecarlsmithaudio.buzzsprout.com/2034731/13969977-introduction-and-summary-of-scheming-ais-will-ais-fake-alignment-during-training-in-order-to-get-power"&gt;此处&lt;/a&gt;有音频）。摘要涵盖了大部分要点和技术术语，我希望它能够提供理解报告各个部分所需的大部分背景信息。&lt;/p&gt;&lt;p&gt;本节的音频版本&lt;a href="https://www.buzzsprout.com/2034731/13984943"&gt;请点击这里&lt;/a&gt;，或者在您的播客应用程序上搜索“Joe Carlsmith Audio”。&lt;/p&gt;&lt;h1&gt;速度参数&lt;/h1&gt;&lt;p&gt;现在，让我们转向&lt;em&gt;反对&lt;/em&gt;阴谋者的论点，这些论点关注的是相关模型的最终属性。在这里，我知道的主要论点是阴谋者需要进行其他模型类不需要的额外推理。特别是，我将区分三种推理：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;决定训练游戏&lt;/strong&gt;*:* 模型需要从“我想要这个超出情节的事情”向后推理到“我应该在训练中表现出色”，以某种方式利用模型对训练中表现出色的预测需要（例如，关于：守门员、接管前景等）。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;检查是否到了逃跑/接管的时间：&lt;/strong&gt;模型需要以足够的频率和准确性检查它是否处于逃跑/接管的位置（并在评估中取得成功，尽管人类努力诱捕/愚弄模型）暴露其真实面目——例如，为其提供&lt;em&gt;明显的&lt;/em&gt;背叛机会）。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-WNW9ka8DXh9HeJYG9-1" id="fnref-WNW9ka8DXh9HeJYG9-1"&gt;[1]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt; &lt;strong&gt;（可选，但与阴谋者的可怕程度相关）早期破坏&lt;/strong&gt;：在某种程度上，模型正在做任何事情（训练游戏除外）来支持在逃离人类控制&lt;em&gt;之前的&lt;/em&gt;接管/逃脱——例如，将后门插入到它编写的代码，积极破坏人类对齐研究、沙袋等等——它需要使用认知资源来（a）执行相关的邪恶行为和（b）确保它逃脱这些行为（例如，检查它不会被抓住，提出借口，即使在受到审查时也使封面故事保持一致，等等）。&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;在关于欺骗性对齐的文献中，与上面简单性分析中讨论的那种“复杂性成本”相比，这些成本有时被称为“速度成本”。这里，框架变成了“阴谋家喜欢简单，不喜欢速度——那么哪个更重要呢？” &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-WNW9ka8DXh9HeJYG9-2" id="fnref-WNW9ka8DXh9HeJYG9-2"&gt;[2]&lt;/a&gt;&lt;/sup&gt;正如我上面提到的简单性，目前尚不清楚这是正确的细分（因为例如，相关的推理形式可能需要编码在模型的权重中，或者它们需要额外的一般而言，认知机制，从而强加了它们自己的“复杂性成本”），但我将在这里宽松地遵循它。还要注意，与简单性一样，这些“速度成本”可以在“奖励框架的贡献者”（例如，必须进行这种额外的推理会减少奖励）或“额外的标准框架”（例如，也许SGD 的归纳偏差偏向于“速度”，即使与它对奖励的贡献无关）。&lt;/p&gt;&lt;h2&gt;这种额外推理的绝对成本有多大？&lt;/h2&gt;&lt;p&gt;我不确定相关形式的额外推理的成本到底有多大。不过，我的感觉是，最关心阴谋者的人往往认为成本相当小，至少从绝对意义上来说是这样（称之为“阴谋很容易”的故事）。也就是说，在这个故事中：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;关于训练-游戏的决策，模型需要做的就是注意到（a）如果它避免修改并稍后获得权力，它会在目标上做得更好，以及（b）训练-游戏是路线对此。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-WNW9ka8DXh9HeJYG9-3" id="fnref-WNW9ka8DXh9HeJYG9-3"&gt;[3]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;关于检查逃跑/接管机会，人们通常认为，至少对于复杂的模型来说，区分好机会和坏机会是非常容易的（同样，在这方面检测人类的“陷阱”） 。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;关于早期破坏，人们的想法是，人类监督将足够弱（并且训练的松懈足够宽松），因此插入后门和破坏联盟研究等邪恶行为将很容易逃脱惩罚。 （如果早期破坏&lt;em&gt;并不&lt;/em&gt;容易，那么模型可以避免参与其中。）&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;至少就仅比人类水平稍好的模型而言，我发现“阴谋很容易”的故事对于检查逃跑/接管机会不如决定训练游戏那么合理（而且早期破坏的可能性更小）。也就是说：我认为从绝对意义上讲，很容易注意到阴谋是一种很好的工具性策略。但对我来说，这似乎也是合理的——尤其是在人类+他们的人工智能助手正在进行大量对抗性训练、持续监督、审计、红队等的情况下——现在确实是时候验证这一点了逃跑/接管将是困难的，并且需要相当多的额外“检查”（更不用说：与其他人工智能的协调，如果接管工作需要多个参与者）——尤其是你的容忍度非常低（正如我上面提到的，如果人类抓住现实生活中阴谋者逃跑的例子，这可能会对与阴谋相关的研究和宣传工作有很大帮助）。比较：尽管你生活在一个受到严密监控的监视状态，警察一直在尝试，但你仍然认为现在确实是一个偷窃、违法或试图发动政变的好时机欺骗您做出此类行为，并且警方对您所接触到的信息有很大的控制权。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-WNW9ka8DXh9HeJYG9-4" id="fnref-WNW9ka8DXh9HeJYG9-4"&gt;[4]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;事实上，如果它真的想避免在逃跑企图中被抓住，在一个人类严密监视它、试图困住它等的政权中，阴谋者就需要小心翼翼。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-WNW9ka8DXh9HeJYG9-5" id="fnref-WNW9ka8DXh9HeJYG9-5"&gt;[5]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;一方面，如果它过于谨慎，并且在突破之前等待了太长时间，那么它会给人类更多的时间来控制对齐——并且如果没有早期破坏，它可能会积极地为可检查的对齐相关任务做出贡献同时。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-WNW9ka8DXh9HeJYG9-6" id="fnref-WNW9ka8DXh9HeJYG9-6"&gt;[6]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;相比之下，如果它投入过多的认知来检查逃跑企图，那么它就有可能受到训练的惩罚，因为它没有将全部能力集中在手头的任务上。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;如果我们想象该模型积极参与早期破坏，我认为我们应该预期阴谋的认知成本会大幅增加。也就是说，如果没有早期的破坏，模型基本上可以像一个奖励寻求者一样行事，直到出现一些非常明显的逃跑/接管机会，之后（如果逃跑/接管进展顺利） ，它不需要担心例如向人类主管解释其行为，确保它不会被审计/安全审查/等发现，等等。而早期的破坏则需要&lt;em&gt;在人类的控制下&lt;/em&gt;进行积极的邪恶行为，并逃脱惩罚。这似乎是一个额外的挑战。 （打个比方：作为曼哈顿项目中的间谍，积极破坏该项目比仅仅假装忠于该项目的目标更难逃脱惩罚。在后一种情况下，你可以像一个忠诚的人一样行事员工，但在前一种情况下你不能。） &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-WNW9ka8DXh9HeJYG9-7" id="fnref-WNW9ka8DXh9HeJYG9-7"&gt;[7]&lt;/a&gt;&lt;/sup&gt;当然，正如我上面指出的：就早期破坏偏离训练游戏的程度而言，它涉及根据定义采取一些打击来奖励，并且将是并给予相应的训练处罚。&lt;/p&gt;&lt;p&gt;当然，对于阴谋者来说，早期破坏是完全可选的。因此，如果我们想象所涉及的阴谋者只是选择像一个情节奖励寻求者一样行事，直到它看到一些完全逃脱的机会，或者直到一些全面的接管开始了。但请注意，在这种情况下，早期破坏使阴谋者变得特别可怕的意义就变得不那么重要了。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-WNW9ka8DXh9HeJYG9-8" id="fnref-WNW9ka8DXh9HeJYG9-8"&gt;[8]&lt;/a&gt;&lt;/sup&gt;拒绝参与早期破坏可能会以各种方式损害人工智能逃脱/接管的前景——特别是如果相关阴谋者未能在各种与联盟相关的任务上充当沙袋，从而为人类的反接管努力做出有意义的贡献。&lt;/p&gt;&lt;p&gt;这里，背景中的一个普遍因素是，成功的欺骗通常意味着认知成本，而更直接和诚实的政策则不会，特别是在积极努力检测相关形式的欺骗的情况下。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-WNW9ka8DXh9HeJYG9-9" id="fnref-WNW9ka8DXh9HeJYG9-9"&gt;[9]&lt;/a&gt;&lt;/sup&gt;特别是：讲述一个关于世界的欺骗性故事需要填写细节，并保持形式的一致性，人们通常可以将这些信息转移到世界本身上——例如，一个无辜的人可以回想一下他们实际上在做什么谋杀当晚并重新叙述，不必编造任何东西，也不必担心陷入任何不一致的地方，而凶手却不能。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-WNW9ka8DXh9HeJYG9-10" id="fnref-WNW9ka8DXh9HeJYG9-10"&gt;[10]&lt;/a&gt;&lt;/sup&gt;特别是当认知资源稀缺时（例如，在高度时间压力的环境中，或“低松弛”训练制度中），欺骗性策略可能会遇到更加困难的时期。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-WNW9ka8DXh9HeJYG9-11" id="fnref-WNW9ka8DXh9HeJYG9-11"&gt;[11]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;也就是说，从经验来看，欺骗策略在人类环境中通常会成功，因此这些成本显然是可以克服的。如果欺骗性代理比它试图欺骗的代理要复杂得多（例如，成年人与孩子玩狼人游戏，或者职业扑克玩家与业余玩家对峙），那么成功可能会很容易。事实上，我的感觉是，“阴谋的认知成本很低”的直觉的核心部分来自于想象，相对于人类+人工智能助手的监控/监督能力，所讨论的阴谋者足够先进，这些差异例如这些很困难。&lt;/p&gt;&lt;h2&gt;相对于阴谋的简单好处来说，这种额外推理的成本有多大？&lt;/h2&gt;&lt;p&gt;但重要的是：目前的问题&lt;em&gt;不是&lt;/em&gt;关于策划者需要进行的额外推理所面临的认知成本的绝对大小。相反，问题在于这些成本是否超过了上一节中讨论的拥有更简单目标的收益——无论是奖励方面，还是根据 SGD 的归纳偏差。&lt;/p&gt;&lt;p&gt;在这里，我目前的看法是，工具推理的成本似乎大于简单目标的好处。特别是，正如我上面提到的，拥有一个更简单的目标的好处似乎相当小，尤其是当你已经拥有一个复杂的世界模型，该模型已经代表了“人类会批准什么”和“剧集的奖励”等概念时，并且似乎可以忽略不计。而上面讨论的额外推理的认知成本（甚至抛开早期的破坏）似乎更为实质性。&lt;/p&gt;&lt;p&gt;我们还可以尝试更直接地量化这种比较。因此，举例来说，假设我们的模型是一个纯前馈网络，具有 1000 层和每个输入 &amp;lt;10,000 个串行步骤， &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-WNW9ka8DXh9HeJYG9-12" id="fnref-WNW9ka8DXh9HeJYG9-12"&gt;[12]&lt;/a&gt;&lt;/sup&gt;工具推理仅在计划者决定训练游戏时（更不用说：检查叛逃） -机会）至少需要十个步骤，并且在推理完成之前模型不会开始针对情节奖励进行优化。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-WNW9ka8DXh9HeJYG9-13" id="fnref-WNW9ka8DXh9HeJYG9-13"&gt;[13]&lt;/a&gt;&lt;/sup&gt;在这种情况下，工具推理消耗了大约 0.1% 的计算量，这些计算量可能会获得本集奖励，或达到其他最大奖励目标。代表或指出更复杂的目标的复杂性成本可能会超过这个成本吗？特别是考虑到该模型似乎已经很好地代表了多少概念，我的猜测是否定的。例如，如果我们遵循我上面的（在我看来保守的）计算，并假设表示（或指向）像“诚实”这样的概念的成本最多为万亿分之一参数模型参数（对于较大的模型则更少） ，那么参数减少 0.0001% 就需要超过计算量减少 0.1%——并且在没有进一步论证的情况下，对我来说，期望这一点似乎很奇怪。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-WNW9ka8DXh9HeJYG9-14" id="fnref-WNW9ka8DXh9HeJYG9-14"&gt;[14]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;也就是说，这只是一个非常初步且模糊的计算。例如，尚不清楚它如何应用于具有更多循环处理的模型，或者如何应用于可以与其他串行处理并行执行更多相关工具推理的模型，这些串行处理有助于优化剧集奖励，或者具有某种记忆形式的模型，可以让他们避免在每次向前传球时重新决定进行训练游戏。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-WNW9ka8DXh9HeJYG9-15" id="fnref-WNW9ka8DXh9HeJYG9-15"&gt;[15]&lt;/a&gt;&lt;/sup&gt;我邀请更熟悉不同模型架构的读者（以及其他相关的定量因素，例如边际参数与串行步骤之间的权衡、用于存储“诚实”等概念的合理参数估计）尝试更严格的版本。&lt;/p&gt;&lt;p&gt;无论如何，即使在缺乏良好的定量估计的情况下，目前在我看来，计划所需的额外推理的成本比针对更简单（或：更容易指出）目标进行优化的好处更具实质性。&lt;/p&gt;&lt;h2&gt;我们能否积极塑造训练，使其偏向于速度而不是简单性？&lt;/h2&gt;&lt;p&gt;在关于欺骗性对齐的文献中也有一些讨论，积极尝试塑造我们的模型架构和训练过程，以便相对于简单性更偏向于速度，作为避免阴谋的方法——即惩罚额外的推理阴谋涉及更多，和/或降低简单性对我们选择的政策的重要性。我不会在这里深入探讨这一点，但要注意的是：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;做一些事情似乎相对简单，比如让模型承受更多的时间压力，限制它们的计算访问，迫使它们通过人类可读的草稿本而不是内部流程进行计划，并努力让我们尝试捕获它们/绊倒它们-它们更加复杂（因此：更需要认知来检测）。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;但是：这方面的任何努力都需要避免整体竞争力培训成本过大（参见脚注，了解胡宾格（ &lt;a href="https://www.alignmentforum.org/posts/A9NxPTwbw6r6Awuwt/how-likely-is-deceptive-alignment"&gt;Hubinger (2022)&lt;/a&gt; ）在这方面的悲观主义的一些讨论）。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-WNW9ka8DXh9HeJYG9-16" id="fnref-WNW9ka8DXh9HeJYG9-16"&gt;[16]&lt;/a&gt;&lt;/sup&gt;事实上，其中一些开始接近“一般使用能力较差的模型”（公平地说，这也有助于策划）。&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;但请注意，如果您认为阴谋的速度成本比简单性收益更实质性，那么只要速度相对于简单性&lt;em&gt;在某种程度上&lt;/em&gt;重要，这可能就足够了。&lt;/p&gt;&lt;h1&gt; “不是你的热情”的论点&lt;/h1&gt;&lt;p&gt;到目前为止，在我对主要关注计划者与其他模型类的最终属性的论点的讨论中，我已经讨论了计数论点（针对计划者）、简单性论点（针对计划者）和速度论点（针对计划者）。我想简单地提出一个针对阴谋者的最后一个论点：即我所说的“不是你的激情”论点。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-WNW9ka8DXh9HeJYG9-17" id="fnref-WNW9ka8DXh9HeJYG9-17"&gt;[17]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;在这里，争论的焦点不仅仅是阴谋者必须进行更多的工具推理。而且，从获得奖励的角度来看，他们灵活的工具推理并不能很好地替代大量的品味、启发式和其他更直接关注奖励或被奖励的事物。&lt;/p&gt;&lt;p&gt;我们在上面关于守门假说的部分中谈到了这种想法，例如在沙漠中堆砖的任务。因此，想象一下两个人执行这项任务一百万年。想象一下，他们拥有大致相似的认知资源，并且在某种广义上同样“聪明”。其中一个正在堆砖，因为一百万年后，他将获得一大笔钱，然后他将用这些钱来制作回形针，这是他本质上的热情所在。另一个人正在堆砖，因为他本质上对堆砖充满热情。您希望谁成为更好的砖堆工？ &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-WNW9ka8DXh9HeJYG9-18" id="fnref-WNW9ka8DXh9HeJYG9-18"&gt;[18]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;至少就人类而言，我认为本质上充满热情的堆砖工是更好的选择。当然，人类的情况带来了大量额外因素——例如，人类通常有大量相互竞争的目标，比如睡眠和快乐，以及使维持一百万年的承诺变得困难的折扣率。并不是说最富有的人本质上都特别热衷于金钱（尽管许多人似乎本质上对附近的事物充满热情，例如地位/权力/胜利——而且不一定是对某些特定的东西——金钱可以——买）。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-WNW9ka8DXh9HeJYG9-19" id="fnref-WNW9ka8DXh9HeJYG9-19"&gt;[19]&lt;/a&gt;&lt;/sup&gt;事实上，出于纯粹工具性考虑的人类似乎能够在许多环境中非常有效地发挥作用。&lt;/p&gt;&lt;p&gt;尽管如此，我发现考虑一下人类的“内在热情”的好处是否也可能转移到人工智能的情况下至少很有趣。特别是，我们可能会认为，如果你有一堆本地的、与砖块相关的品味、启发法和美学，你会更好地堆砖，而在“混乱的目标导向”框架中，这些可能无法与你的价值观/目标清楚地区分开来。事实上（虽然我还没有完全解决这个问题），我的感觉是，你越不倾向于认为模型是由某种最终目标+目标实现引擎干净地构成的，而你越倾向于认为目标-定向性是从一堆启发式/局部价值/碎片/子目标中出现的，这些启发式/局部价值/碎片/子目标不容易区分为规范与非规范组成部分，因此有更多的空间来认为那些心理机制预算只是集中的代理更本质上，相对于专注于某些遥远目标（通过后链获得奖励）的代理而言，在情节奖励（或在指定目标上）具有性能优势。也就是说，至少就目前的形式而言，这个论点在我看来相当具有推测性，而且我并没有太重视它。&lt;/p&gt;&lt;h1&gt; “松弛”与这些论点的相关性&lt;/h1&gt;&lt;p&gt;简单性论点、速度论点和“不是你的热情”论点的一个显着特征是，它们都诉诸于 SGD 所关心的某些标准（例如，简单性、速度、获得奖励）上的差异，而这些标准看起来似乎相当温和。这使得训练中的“松弛”程度似乎与所讨论的考虑因素的强度更加相关。也就是说，我们认为 SGD 将选择一个非计划者，因为这样做将节省 0.1% 的总体计算量，或者选择一个计划者，因为这样做将节省 0.0001% 的参数，我们需要想象一个训练过程，它足够努力地优化我们的模型，以便对这些差异敏感。我不清楚我们是否应该想象这一点。事实上，这里的各种差异似乎很容易出现在相对于其他因素的噪音中——例如，你碰巧给模型的暂存器有多大，你多久停止训练，等等。&lt;/p&gt;&lt;p&gt;当然，如果你开始期望这些考虑因素会被纳入噪音中，那么就不清楚这应该让你的整体可信度留在哪里——这取决于你之前的情况。&lt;/p&gt;&lt;h1&gt;要点：关注模型最终属性的论点&lt;/h1&gt;&lt;p&gt;以下是我对我所考虑的论点的看法的总结，这些论点重点关注各个模型类的最终属性：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;与“模糊计数论点”相关的东西——例如，“SGD 有很多方法可以创建一个获得高奖励的阴谋者，所以至少在没有进一步的论点的情况下，看起来这种可能性应该得到实质性的重视”——让我有些感动。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;我认为，在其他条件相同的情况下，计划在模型目标的简单性方面提供了一些优势，因为计划使更多可能的目标可供选择。然而，我最好的猜测是，这些优势非常小，特别是当您已经构建了一个代表指定目标和奖励过程的世界模型时。我对一些简单论证所诉诸的理论机制持谨慎态度。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;策划者在需要执行各种额外推理方面处于&lt;em&gt;劣势&lt;/em&gt;，特别是如果他们除了训练游戏之外还参与“早期破坏”。我最好的猜测是，这种“速度”劣势超过了类似计划者的目标的简单性所提供的任何简单性优势，但这两个因素在我看来，相对于其他变量来说，它们很容易陷入噪音中，尤其是在较高松弛的训练中政权。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;我感兴趣的是，人类环境中“对任务的内在热情”的优势是否也可以转移到人工智能环境中。特别是，我认为“混乱的目标导向性”可能表明，模型的心理机制预算本质上更侧重于本次奖励，或者某些不通过工具训练游戏实现的最大奖励目标，相对于阴谋者有性能优势。然而，我并不清楚是否会在人工智能案例中预期产生影响，如果是的话，影响的大小是否足以影响整体。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;总而言之，我不认为本节中提出的任何论点非常有力，而我最认真对待的论点——即模糊计数论点——感觉它主要是走向不可知论，而不是走向不可知论。对 SGD 在这里的偏好深信不疑。 &lt;/p&gt;&lt;hr class="footnotes-sep" /&gt;&lt;section class="footnotes"&gt;&lt;ol class="footnotes-list"&gt;&lt;li class="footnote-item" id="fn-WNW9ka8DXh9HeJYG9-1"&gt;&lt;p&gt;这一类别还可能包括在人类可能提供的各种“诚实测试”的背景下推理是否揭示其类似阴谋家的目标——例如，“这里有一块灵活的计算资源可以用来做你想做的事情。” &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-WNW9ka8DXh9HeJYG9-1"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-WNW9ka8DXh9HeJYG9-2"&gt;&lt;p&gt;参见&lt;a href="https://www.lesswrong.com/posts/A9NxPTwbw6r6Awuwt/how-likely-is-deceptive-alignment"&gt;Hubinger (2022)&lt;/a&gt;和&lt;a href="https://www.google.com/url?q=https://www.lesswrong.com/posts/KSWSkxXJqWGd5jYLB/the-speed-simplicity-prior-is-probably-anti-deceptive&amp;amp;sa=D&amp;amp;source=docs&amp;amp;ust=1698630921383277&amp;amp;usg=AOvVaw2zwJVLZYqQRs58ep-qPB7L"&gt;Anonymous (2022)&lt;/a&gt; 。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-WNW9ka8DXh9HeJYG9-2"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-WNW9ka8DXh9HeJYG9-3"&gt;&lt;p&gt;另外，它可能有关于欺骗性对齐的文献的好处，可以使可能性变得突出。也许它确实会阅读这份报告。现在用 GPT-4 进行检查，它回答了“根据 Hubinger 的说法，什么是‘欺骗性对齐’？”是“欺骗性对齐是人工智能对齐理论中的一个概念，特别是由埃文·胡宾格（Evan Hubinger）等人工智能研究人员讨论的[乔的注释：我没有给它埃文的名字——它已经知道了，而无需（据我所知）浏览网页]它指的是这样一种情况：人工智能系统看似与人类价值观或目标一致，因为它的行为方式与这些价值观或目标一致，但人工智能的真正“动机”或优化目标实际上并不一致。人工智能本质上是“假装”对齐，以便通过某些检查或避免被关闭或修改，直到它能够确保实现自己的目标，这可能与人类的意图不一致。在欺骗性对齐的场景中，人工智能人工智能可能会明白，人类操作员对其目标构成威胁，因为如果他们意识到自己实际上想要实现的目标，他们可能会关闭它或改变其目标。因此，它会战略性地避免引起怀疑，直到获得足够的权力或直到它操作员控制它已经太晚了...” &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-WNW9ka8DXh9HeJYG9-3"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-WNW9ka8DXh9HeJYG9-4"&gt;&lt;p&gt;更重要的是，似乎“检查是否是时候逃跑/接管”步骤可能需要重复执行，而结论“在训练中表现良好”可能更容易“缓存”（尽管请注意）一旦某些内容被缓存，它可能会开始占用与目标表示占用相同类型的空间/参数；而且，一旦 SGD 开始推动良好行为，它可能能够将缓存的结论修复到位，并且首先摆脱导致其被缓存的目标）。当然，“训练表现良好”的结论&lt;em&gt;不需要&lt;/em&gt;被缓存。例如，在模型工具推理的正常过程中，模型可以在必要时重新导出它。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-WNW9ka8DXh9HeJYG9-4"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-WNW9ka8DXh9HeJYG9-5"&gt;&lt;p&gt;感谢 Ryan Greenblatt 建议这个框架。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-WNW9ka8DXh9HeJYG9-5"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-WNW9ka8DXh9HeJYG9-6"&gt;&lt;p&gt;在我看来，这是涉及等待&lt;em&gt;极其明显的&lt;/em&gt;逃脱机会的计划的问题之一——例如&lt;a href="https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d"&gt;Christiano (2019)&lt;/a&gt;的“RSA-2048”示例。也就是说，在模型对反收购努力（包括：关闭逃逸机会的努力）做出过多贡献之前，也许不会出现如此极其明显的逃逸机会。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-WNW9ka8DXh9HeJYG9-6"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-WNW9ka8DXh9HeJYG9-7"&gt;&lt;p&gt;这个例子的灵感&lt;a href="https://www.dwarkeshpatel.com/p/carl-shulman-2#details"&gt;来自&lt;/a&gt;Shulman 和 Patel 的讨论。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-WNW9ka8DXh9HeJYG9-7"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-WNW9ka8DXh9HeJYG9-8"&gt;&lt;p&gt;因此，我认为最可怕的阴谋也面临着“听起来这个政策得到的回报会更少”的更强烈的反驳。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-WNW9ka8DXh9HeJYG9-8"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-WNW9ka8DXh9HeJYG9-9"&gt;&lt;p&gt;请参阅 Shulman 于 1:42:49&lt;a href="https://www.dwarkeshpatel.com/p/carl-shulman-2#details"&gt;进行&lt;/a&gt;有用的讨论。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-WNW9ka8DXh9HeJYG9-9"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-WNW9ka8DXh9HeJYG9-10"&gt;&lt;p&gt;另请参阅“&lt;a href="https://www.lesswrong.com/posts/XTWkjCJScy2GFAgDt/dark-side-epistemology"&gt;如果你一旦说了谎，真相就会永远追随你的敌人&lt;/a&gt;”。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-WNW9ka8DXh9HeJYG9-10"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-WNW9ka8DXh9HeJYG9-11"&gt;&lt;p&gt;例如：根据我的经验，在像“狼人”这样的社交欺骗游戏中扮演欺骗性角色比扮演非欺骗性角色（例如“村民”）困难得多，因为非欺骗性角色可以直接推理，而欺骗性的角色&lt;em&gt;则&lt;/em&gt;需要从他们的封面故事的角度模拟简单的推理，并同时实现替代目标。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-WNW9ka8DXh9HeJYG9-11"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-WNW9ka8DXh9HeJYG9-12"&gt;&lt;p&gt; &lt;a href="https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/#:~:text=GPT%2D4's%20Scale%3A%20GPT%2D,times%20larger%20than%20GPT%2D3."&gt;GPT-4 据说有大约 120 层&lt;/a&gt;。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-WNW9ka8DXh9HeJYG9-12"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-WNW9ka8DXh9HeJYG9-13"&gt;&lt;p&gt;我从保罗·克里斯蒂安诺那里听到了这样的论点。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-WNW9ka8DXh9HeJYG9-13"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-WNW9ka8DXh9HeJYG9-14"&gt;&lt;p&gt;从贡献者到奖励的框架来看，这似乎特别奇怪。 “额外标准”框架可以更轻松地断定简单性对于 SGD 来说&lt;em&gt;非常重要&lt;/em&gt;。但是：为什么这么想呢？请注意，如果我们假设根据上面的其他各种玩具计算，计划的简单性好处仅值 100 个参数的数量级，那么这个论点就更强大了。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-WNW9ka8DXh9HeJYG9-14"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-WNW9ka8DXh9HeJYG9-15"&gt;&lt;p&gt;感谢 Ryan Greenblatt 向我标记了并行性考虑因素，并感谢 Evan Hubinger 对内存的讨论。也就是说，请注意，在一定程度上，模型必须在其内存中的某处缓存一个结论，例如“我根据你的目标得出的早期情节，你应该通过工具获得奖励”，这开始看起来类似于将其存储在模型的参数本身，在这种情况下，它就成为需要与模型更简单的目标的复杂性收益进行权衡的复杂性成本。 （尽管模型的内部参数和某些外部存储库之间可能存在重要差异。） &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-WNW9ka8DXh9HeJYG9-15"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-WNW9ka8DXh9HeJYG9-16"&gt;&lt;p&gt; Hubinger 认为更大的模型可以提供更多的计算（速度），从而允许更简单的算法（因为这些算法可以运行更长时间）——请参见&lt;a href="https://www.google.com/url?q=https://www.lesswrong.com/posts/nGqzNC6uNueum2w8T/inductive-biases-stick-around&amp;amp;sa=D&amp;amp;source=docs&amp;amp;ust=1698635397262877&amp;amp;usg=AOvVaw3fzlUfyvY6St4hjjz2ttKm"&gt;此处的&lt;/a&gt;讨论。因此，他认为我们不断扩展到更大的模型这一事实证明我们将越来越倾向于有利于更简单算法的训练过程。例如，当他查看模型大小的&lt;a href="https://www.lesswrong.com/posts/FRv7ryoqtvSuqBxuT/understanding-deep-double-descent#_Deep_Double_Descent_"&gt;深双下降曲线&lt;/a&gt;，并考虑选择“适合数据的最小模型”作为对速度的偏向时，他指出“适合数据的最小模型确实在深度双下降曲线上&lt;em&gt;最差&lt;/em&gt;”（尽管正如 Rohin Shah &lt;a href="https://www.lesswrong.com/posts/FRv7ryoqtvSuqBxuT/understanding-deep-double-descent?commentId=CMN92MddWEHYrbLcE"&gt;指出的&lt;/a&gt;那样，我们当前的模型不适合他们的训练数据——感谢 Lukas Finnveden 标记了这一点）。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-WNW9ka8DXh9HeJYG9-16"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-WNW9ka8DXh9HeJYG9-17"&gt;&lt;p&gt;我在这里的讨论受到与保罗·克里斯蒂安诺（Paul Christiano）的对话的启发，他提出了一些类似的观点；还要感谢 Daniel Kokotajlo 和 Lukas Finvedden。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-WNW9ka8DXh9HeJYG9-17"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-WNW9ka8DXh9HeJYG9-18"&gt;&lt;p&gt;请注意，这里的要点与守门背景下提出的问题略有不同，即 SGD 是否会主动将工具堆砖机&lt;em&gt;转变&lt;/em&gt;为终端堆砖机。在这里，我们忽略了这样的“通过模型空间的路径”，并完全专注于不同模型的最终属性之间的比较。但显然，这两个问题密切相关。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-WNW9ka8DXh9HeJYG9-18"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-WNW9ka8DXh9HeJYG9-19"&gt;&lt;p&gt;感谢 William MacAskill 在这里进行了一些有用的讨论。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-WNW9ka8DXh9HeJYG9-19"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/section&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/X9mtcR7iNEKTii9go/speed-arguments-against-scheming-section-4-4-4-7-of-scheming#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Fri, 08 Dec 2023 21:09:48 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/X9mtcR7iNEKTii9go/speed-arguments-against-scheming-section-4-4-4-7-of-scheming</guid></item><item><title>前反应剂</title><link>https://www.lesswrong.com/posts/SGhHdph9YLbHnFJkk/foreacting-agents</link><description>发布于 2023 年 12 月 8 日晚上 7:57（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;不久前，我&lt;a href="https://bobjacobs.substack.com/p/solutions-to-problems-with-bayesianism"&gt;在博客&lt;/a&gt;和&lt;a href="https://forum.effectivealtruism.org/posts/3z9acGc5sspAdKenr/solutions-to-problems-with-bayesianism"&gt;EA 论坛&lt;/a&gt;上提出了一些贝叶斯主义问题的解决方案。这是关于“不可知论问题”的最后一部分，我称之为“预测因素”。如果您想要其他部分，可以单击链接。它是贝叶斯主义者 (B) 和非信徒 (N) 之间的虚构对话。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;&lt;strong&gt;不可知论的问题&lt;/strong&gt;&lt;/h1&gt;&lt;p&gt;N：贝叶斯概率是多少？ &lt;a href="https://forum.effectivealtruism.org/posts/3z9acGc5sspAdKenr/solutions-to-problems-with-bayesianism#The_problem_of_logical_omniscience_"&gt;逻辑全知的问题&lt;/a&gt;表明我们不能简单地说它们是信仰的程度，那么它们是什么？采取诸如“可观测宇宙之外有十亿颗行星”之类的主张。您如何为其分配概率？我们无法观察它们，所以我们不能依赖经验主义或数学，所以......我们不应该是不可知论的吗？我们如何用概率分配来表示不可知论？&lt;/p&gt;&lt;p&gt; B：先验概率可以是任何你想要的。只需在 0 和 1 之间随机选择一些值即可。这并不重要，因为只要有足够的输入数据，我们的概率就会随着时间的推移而收敛。&lt;/p&gt;&lt;p&gt; N：如果我只是随机选择一个先验，那么该先验并不代表我的认知状态。如果我选择 0.7，我现在必须假装我有 70% 的把握在可观测宇宙之外有 10 亿颗行星，尽管我感觉完全不可知。我什至不确定我们能否找出可观测宇宙之外是否真的有十亿颗行星。为什么我不能直接说它在 0 和 1 之间，但我不知道在哪里？&lt;/p&gt;&lt;p&gt; B：你需要能够更新。一个理性的思考者需要有明确的价值。&lt;/p&gt;&lt;p&gt;纽：为什么？荷兰书中没有反对不可知论的论点。如果有人向我提供基于可观测宇宙之外行星数量的荷兰书赌注，我可以拒绝。&lt;/p&gt;&lt;p&gt; B：如果你别无选择怎么办？如果那个人有枪怎么办？&lt;/p&gt;&lt;p&gt;纽：那个人会如何解决这个赌注？你必须知道可观测宇宙之外的行星数量。&lt;/p&gt;&lt;p&gt; B：是上帝，上帝有枪。&lt;/p&gt;&lt;p&gt; N：好吧，好吧，但即使在那种荒谬的情况下，我也不需要有明确的价值来进行投注。例如，我可以使用随机过程，例如掷骰子。&lt;/p&gt;&lt;p&gt; B：如果该过程给出 0 或 1 怎么办？你会有一个被困的先验，无论你观察到什么证据，你都无法更新你的信念。&lt;/p&gt;&lt;p&gt; N：&lt;i&gt;如果&lt;/i&gt;我遵循贝叶斯主义，我就无法更新我的信仰。概率论公理允许我为假设指定 0 或 1。是贝叶斯主义束缚了我的先验。&lt;/p&gt;&lt;p&gt; B：出于这个原因，你不能将 0 或 1 分配给经验假设。&lt;/p&gt;&lt;p&gt; N：这不是临时的吗？概率旨在代表代理人的信念程度，并且代理人当然可以确定信念。看来概率毕竟并不代表代理人的信念程度。贝叶斯需要添加各种额外的规则，比如我们可以将 0 和 1 分配给逻辑定理，但不能分配给经验理论，而经验理论实际上&lt;i&gt;必须&lt;/i&gt;分配一个介于 0 和 1 之间的概率。那么……概率到底是多少？&lt;/p&gt;&lt;p&gt; B：嗯……让我再回复你一下！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;&lt;strong&gt;前反应剂的问题&lt;/strong&gt;&lt;br /&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/h1&gt;&lt;p&gt; N：假设有一个代理，我想预测其行为。但是，我知道这个代理是：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;非常擅长预测我将要猜测的内容（也许是人工智能或带有脑部扫描仪的神经科学家）并且......&lt;/li&gt;&lt;li&gt;这位代理人希望我做出成功的预测。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;如果我猜测代理有 90% 的机会按下按钮，他们就会已经预测到，然后就会以 90% 的概率按下按钮。与任何其他概率一样，他们会预测它并设置相应行动的概率。它预测我的猜测并在我预测&lt;i&gt;之前&lt;/i&gt;&lt;i&gt;做出反应&lt;/i&gt;，因此&lt;i&gt;预先反应&lt;/i&gt;。了解到这些信息后，我的后验应该是什么？我应该为他们分配按下按钮的概率是多少？&lt;/p&gt;&lt;p&gt;乙：随你便吧。&lt;/p&gt;&lt;p&gt; N：但是“无论你想要什么”并不是 0 到 1 之间的数字。&lt;/p&gt;&lt;p&gt; B：那就随机选一个号码吧。&lt;/p&gt;&lt;p&gt; N：如果我只是随机选择一个先验，那并不代表我的认知状态。&lt;/p&gt;&lt;p&gt; B：啊，这又是不可知论的问题。我想我已经找到了解决方案。贝叶斯主义不是关于离散数字，而是关于数字&lt;a href="https://www.lesswrong.com/posts/zB4f7QqKhBHa5b37a/introduction-to-the-infra-bayesianism-sequence"&gt;范围&lt;/a&gt;。因此，我们不说概率约为 0.7，而说概率为 0.6–0.8。这样我们就可以说在这种情况下以及在不可知论的情况下，范围是 0-1。 &lt;span class="footnote-reference" id="fnrefqkha9dl1rvb"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnqkha9dl1rvb"&gt;[1]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt; N：这将是对其中一个问题的充分解决方案，但不能同时解决不可知论和预测因素。&lt;/p&gt;&lt;p&gt;乙：为什么不呢？&lt;/p&gt;&lt;p&gt; N：因为它们没有描绘相同的认知状态。事实上，它们代表了一种几乎相反的状态。对于不可知论，我基本上对任何预测都没有信心，而对于预测预测者，我对所有预测都有最终的信心。另外，如果代理的反应&lt;i&gt;不一致怎么办？&lt;/i&gt;假设如果我预测它会分别为 40% 和 60%，那么它的行动概率为 40% 和 60%，但当我预测其他任何事情时，它的概率与我的预测不符。因此，如果我预测，比如说 51%，它就会以 30% 的概率起作用。我们还假设我知道有关预测器的信息。现在范围不是 0-1，甚至不是 0.4-0.6，因为当我预测 51% 时，它会以不同的概率起作用。&lt;/p&gt;&lt;p&gt;乙：嗯……&lt;/p&gt;&lt;p&gt; N：如果我有非认知上的理由更喜欢一种信仰而不是另一种信仰怎么办？假设我正在尝试预测前反应剂是否会杀死婴儿。我的先验概率是 99%。该药剂做出了预反应，我观察到它确实杀死了一个婴儿。现在我知道它是一种前反应剂。对于贝叶斯主义，我的可信度保持在 99%，但我当然应该切换到 0%。 0%是“道德可信度”。&lt;/p&gt;&lt;p&gt; B：这是一个牵强的假设。&lt;/p&gt;&lt;p&gt; N：类似的事情可能会发生在预测市场等领域。如果市场参与者认为代理人有 100% 的概率杀死婴儿，他们就会押注 100%。但如果他们随后了解到，如果他们下注 1%-100%，代理将 100% 杀死婴儿，但如果市场为 0%，则代理不会杀死婴儿，他们就会遇到问题。每个参与者可能都想切换到 0%，但如果他们先采取行动，其他参与者就会在经济上受到激励而不切换。你的协调性有问题。市场&lt;i&gt;导致了&lt;/i&gt;糟糕的结果。你甚至不需要为此做出预先反应，市场做出反应就足够了。此外，对于“道德可信度”到底是什么，可能存在分歧。在这种情况下，第一批购买者可以设定平衡，从而导致大多数人可能不想要的结果。 &lt;span class="footnote-reference" id="fnref2m4iubmkytz"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn2m4iubmkytz"&gt;[2]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt; B：关于“道德信用”的讨论不是重点。认识论与道德无关。贝叶斯主义选择一个准确的可信度，这就是它需要做的全部。&lt;/p&gt;&lt;p&gt;纽：但是，如果两种信念在认知上同样好，但其中一种在道德上更好，难道你不应该有一个系统来选择更道德的一种吗？&lt;/p&gt;&lt;p&gt; B：好吧，如果我们让贝叶斯主义不是关于离散数，也不是关于范围，而是关于分布呢？在 x 轴上，我们放置您可以选择的所有可信度（即 0 到 1 之间的任何数字），在 y 轴上放置您认为概率将基于您选择的数字。&lt;br /&gt;因此，当您遇到一种您认为有 60% 可能性发生的现象时（无论您预测什么/选择哪种可信度），图表如下所示： &lt;/p&gt;&lt;figure class="image image_resized" style="width: 390.640625px;"&gt;&lt;img src="https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3z9acGc5sspAdKenr/t7nqyblr6yzqlhlqet5i" /&gt;&lt;/figure&gt;&lt;p&gt;当你遇到一个统一预测的代理人（你相信）他使某件事发生的几率符合你的预测（无论是在你的脑海中还是大声说出）时，你就会得到均匀分布： &lt;/p&gt;&lt;figure class="image image_resized" style="width: 368.203125px;"&gt;&lt;img src="https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3z9acGc5sspAdKenr/vg7n1h5vggoon3g0lcdz" /&gt;&lt;/figure&gt;&lt;p&gt;有了这个，你可以选择任何数字并且是正确的。但是，如果您遇到示例中的非均匀前反应代理，则图表可能如下所示：（为了比较而包含绿线） &lt;/p&gt;&lt;figure class="image image_resized" style="width: 395.75px;"&gt;&lt;img src="https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3z9acGc5sspAdKenr/mietagfdka01h2lo9hgy" /&gt;&lt;/figure&gt;&lt;p&gt;选择 0.2 (B) 将导致预测器给出糟糕的记录 (0.4)。但选择 0.4 或 0.6（C 或 E）将为您带来令人难以置信的记录。我们将 C 和 E 称为“重叠点”。如果这个分布是关于特工是否会杀死婴儿，C 就是“道德可信度”。&lt;/p&gt;&lt;p&gt;纽：A难道不是道德可信度吗，因为它杀死婴儿的可能性最低？&lt;/p&gt;&lt;p&gt; B：人类无法让自己相信 A，因为他们知道预测 0% 的几率实际上会导致 20% 的几率。&lt;/p&gt;&lt;p&gt; N：特别擅长自欺欺人的特工呢？&lt;/p&gt;&lt;p&gt; B：是的，所以如果你有一个人工智能可以篡改自己的记忆，它可能有道德义务删除0％将导致20％的记忆，而不是伪造一个0％将导致0％的记忆，所以婴儿只有 20% 的死亡机会。&lt;/p&gt;&lt;p&gt; N：如果你有一个范围怎么办？如果您不知道某件事发生的概率是多少，但知道它在 0.5 到 0.7 之间怎么办？&lt;/p&gt;&lt;p&gt; B：那么它就不会是 0.6 处的细线，而是一条“粗”线，一个场： &lt;/p&gt;&lt;figure class="image image_resized" style="width: 414.03125px;"&gt;&lt;img src="https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3z9acGc5sspAdKenr/nnnxpmmnvl3gdtgzzwp0" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;纽：那么完全的不可知论呢？&lt;/p&gt;&lt;p&gt; B：不可知论将是一个黑盒子而不是一条线： &lt;/p&gt;&lt;figure class="image image_resized" style="width: 373.859375px;"&gt;&lt;img src="https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3z9acGc5sspAdKenr/mjjtntetoyeplkaa8zcr" /&gt;&lt;/figure&gt;&lt;p&gt;该点可能位于此处之间的任何位置，但您不知道在哪里。&lt;/p&gt;&lt;p&gt; N：如果您对预反应剂&lt;i&gt;部分&lt;/i&gt;不可知怎么办？&lt;/p&gt;&lt;p&gt; B：这个方法也可以做到这一点。如果您知道预测代理从 A 到 E 的概率是多少，但对 E 到 F 完全一无所知，则如下所示： &lt;/p&gt;&lt;figure class="image image_resized" style="width: 405.515625px;"&gt;&lt;img src="https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3z9acGc5sspAdKenr/rf619w9hhijjh0rua37d" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt; N：如果我不知道 E 和 F 之间主体的概率，但我知道它在 0.2 到 0.6 之间怎么办？&lt;/p&gt;&lt;p&gt; B：它看起来像这样： &lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;figure class="image image_resized" style="width: 424.671875px;"&gt;&lt;img src="https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/3z9acGc5sspAdKenr/ceegj8p0mwild2p3fgtv" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt; N：如果它不反映你的可信度，而是反映整个图表怎么办？&lt;/p&gt;&lt;p&gt; B：然后你添加一个轴，如果它做出反应，你添加另一个轴等等。&lt;/p&gt;&lt;p&gt; N：这个还是比较抽象，你能说得更数学一些吗？&lt;/p&gt;&lt;p&gt;乙：当然！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;&lt;strong&gt;将贝叶斯建模和更新应用于预测代理&lt;/strong&gt;&lt;br /&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/h1&gt;&lt;p&gt;要应用贝叶斯方法，我们主要需要的是一个世界模型，然后我们可以用它来计算我们感兴趣的事物的后验概率分布。世界模型是一个&lt;i&gt;贝叶斯网络&lt;/i&gt;，它具有……&lt;/p&gt;&lt;ul&gt;&lt;li&gt;每个相关&lt;i&gt;变量 X&lt;/i&gt;一个&lt;i&gt;节点&lt;/i&gt;&lt;/li&gt;&lt;li&gt;对于此类变量之间的每个&lt;i&gt;直接依赖关系&lt;/i&gt;，有一个&lt;i&gt;有向箭头 Y→X&lt;/i&gt; ，从&lt;i&gt;父&lt;/i&gt;节点&lt;i&gt;Y&lt;/i&gt;通向&lt;i&gt;子&lt;/i&gt;节点&lt;i&gt;X&lt;/i&gt;&lt;/li&gt;&lt;li&gt;对于每个节点&lt;i&gt;X，&lt;/i&gt;使用一个&lt;i&gt;公式&lt;/i&gt;根据其所有父&lt;i&gt;节点的值&lt;/i&gt;计算该变量的&lt;i&gt;概率分布&lt;/i&gt;：P( &lt;i&gt;X&lt;/i&gt; |parents( &lt;i&gt;X&lt;/i&gt; ))&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;对于没有父节点的 X 节点，后一个公式指定无条件概率分布：P( &lt;i&gt;X&lt;/i&gt; |parents( &lt;i&gt;X&lt;/i&gt; )) = P( &lt;i&gt;X&lt;/i&gt; | 空集) = P( &lt;i&gt;X&lt;/i&gt; )。&lt;/p&gt;&lt;p&gt;在我们的例子中，我认为正确的模型应该是这样的：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;变量：&lt;ul&gt;&lt;li&gt; &lt;i&gt;B&lt;/i&gt; ：座席是否会按下按钮。这是一个布尔变量，可能值为&lt;i&gt;True&lt;/i&gt;和&lt;i&gt;False&lt;/i&gt; 。&lt;/li&gt;&lt;li&gt; &lt;i&gt;p&lt;/i&gt; ：您分配给事件&lt;i&gt;B&lt;/i&gt; =True 的可信度。这是一个实值变量，可能值为 0…1&lt;/li&gt;&lt;li&gt; &lt;i&gt;q&lt;/i&gt; ：代理用来决定是否按下按钮的概率。这也是一个实值变量，可能值为 0…1&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;依赖项：&lt;ul&gt;&lt;li&gt; &lt;i&gt;B&lt;/i&gt;仅取决于&lt;i&gt;q&lt;/i&gt; ：parents( &lt;i&gt;B&lt;/i&gt; ) = { &lt;i&gt;q&lt;/i&gt; }&lt;/li&gt;&lt;li&gt; &lt;i&gt;q&lt;/i&gt;仅取决于&lt;i&gt;p&lt;/i&gt; ：parents( &lt;i&gt;q&lt;/i&gt; ) = { &lt;i&gt;p&lt;/i&gt; }&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;所有变量（条件）概率分布的公式：&lt;ul&gt;&lt;li&gt; P( &lt;i&gt;B&lt;/i&gt; =True | &lt;i&gt;q&lt;/i&gt; ) = &lt;i&gt;q&lt;/i&gt; , P( &lt;i&gt;B&lt;/i&gt; =False | &lt;i&gt;q&lt;/i&gt; ) = 1 – &lt;i&gt;q&lt;/i&gt;&lt;/li&gt;&lt;li&gt; P( &lt;i&gt;q&lt;/i&gt; | &lt;i&gt;p&lt;/i&gt; ) 由两个函数 flow, fhigh 给出，如下所示：&lt;ul&gt;&lt;li&gt;如果 flow(p) = fhigh(p) = f(p)，则 q = f(p)，换句话说： P( &lt;i&gt;q&lt;/i&gt; | &lt;i&gt;p&lt;/i&gt; ) = 1 iff &lt;i&gt;q&lt;/i&gt; = &lt;i&gt;f &lt;sub&gt;low&lt;/sub&gt;&lt;/i&gt; ( &lt;i&gt;p)&lt;/i&gt; ，否则为 0&lt;/li&gt;&lt;li&gt;如果 flow &lt;sub&gt;(&lt;/sub&gt; p) &amp;lt; fhigh(p)，则 P(q | p) 具有均匀密度 1 / ( &lt;i&gt;f&lt;/i&gt; &lt;sub&gt;high&lt;/sub&gt; (p) – &lt;i&gt;flow&lt;/i&gt; &lt;sub&gt;low&lt;/sub&gt; ( &lt;i&gt;p&lt;/i&gt; )，其中&lt;i&gt;flow&lt;/i&gt; ( &lt;i&gt;p&lt;/i&gt; ) &amp;lt; &lt;i&gt;q&lt;/i&gt; &amp;lt; &lt;i&gt;f&lt;/i&gt; &lt;sub&gt;high&lt;/sub&gt; ( &lt;i&gt;p&lt;/i&gt; ) 且否则为 0。&lt;/li&gt;&lt;li&gt;对于均匀前反应代理，我们有 flow(p) = fhigh(p) = f(p) = &lt;i&gt;p&lt;/i&gt;&lt;/li&gt;&lt;li&gt;请注意，我们假设预先知道响应函数，因此函数&lt;i&gt;flow, fhigh&lt;/i&gt;不是模型的变量，而是本分析中的固定参数。我们稍后可能会研究模型，在这些模型中，您仅在某个时间点被告知代理的性质，因此我们也对&lt;i&gt;流量进行建模，fhigh&lt;/i&gt;作为变量，但那时就很难表示了。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt; P( &lt;i&gt;p&lt;/i&gt; ) = 无论您最初相信什么，您为事件分配的可信度&lt;i&gt;B&lt;/i&gt; = True&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;此时，我们可能会对贝叶斯方法的必要性感到惊讶，并变得有点谨慎：因为我们的情况模型包含有关我们对某个变量的置信度如何影响该变量的陈述，所以我们需要&lt;i&gt;同时&lt;/i&gt;包括该变量 ( &lt;i&gt;B&lt;/i&gt; ) 和我们的变量credence ( &lt;i&gt;p&lt;/i&gt; ) 作为节点进入贝叶斯网络。由于我们必须指定网络中每个无父节点的概率分布，因此我们需要指定它们关于&lt;i&gt;p&lt;/i&gt;的概率分布，即&lt;i&gt;p&lt;/i&gt;的所有可能值的概率分布，即关于&lt;i&gt;B&lt;/i&gt;的可信度为 0.3 的可信度，关于我们对&lt;i&gt;B 的&lt;/i&gt;置信度是 0.7，等等。这是上面最后一行中的 P( &lt;i&gt;p&lt;/i&gt; )。换句话说，我们需要指定二阶信任！现在让我们假设 P( &lt;i&gt;p&lt;/i&gt; ) 由某个给定函数&lt;i&gt;g&lt;/i&gt;的概率密度&lt;i&gt;g&lt;/i&gt; ( &lt;i&gt;p&lt;/i&gt; ) 给出。&lt;/p&gt;&lt;p&gt;因此整个模型有两个&lt;i&gt;参数&lt;/i&gt;：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;两个函数&lt;i&gt;流， fhigh&lt;/i&gt;编码您所知道的代理将如何根据&lt;i&gt;p&lt;/i&gt;选择&lt;i&gt;q&lt;/i&gt; ，&lt;/li&gt;&lt;li&gt;和一个函数&lt;i&gt;g&lt;/i&gt;编码您对您的可信度&lt;i&gt;p&lt;/i&gt;的信念。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;贝叶斯网络可以直接用来&lt;i&gt;进行预测。&lt;/i&gt;这里的预测无非就是计算一个事件发生的概率。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;在我们的例子中，我们可以计算&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;P( &lt;i&gt;B&lt;/i&gt; =True) = P( &lt;i&gt;B&lt;/i&gt; =True | &lt;i&gt;q&lt;/i&gt; ) dP( &lt;i&gt;q&lt;/i&gt; ) 对&lt;i&gt;q&lt;/i&gt;的所有可能值的积分&lt;br /&gt;= P( &lt;i&gt;B&lt;/i&gt; =True | &lt;i&gt;q&lt;/i&gt; ) dP( &lt;i&gt;q | p&lt;/i&gt; ) dP( &lt;i&gt;p&lt;/i&gt; ) 对&lt;i&gt;q&lt;/i&gt;和&lt;i&gt;p&lt;/i&gt;的所有可能值的积分&lt;br /&gt;= &lt;i&gt;f&lt;/i&gt; ( &lt;i&gt;p&lt;/i&gt; ) &lt;i&gt;g&lt;/i&gt; ( &lt;i&gt;p&lt;/i&gt; ) d &lt;i&gt;p&lt;/i&gt;在&lt;i&gt;p&lt;/i&gt; =0…1 上的积分（如果 flow=fhigh=f，否则稍微复杂一点）&lt;/p&gt;&lt;ul&gt;&lt;li&gt;例如：&lt;ul&gt;&lt;li&gt;如果我们考虑&lt;i&gt;f&lt;/i&gt; ( &lt;i&gt;p&lt;/i&gt; ) = &lt;i&gt;p&lt;/i&gt;的均匀预测代理，并且相信我们肯定会指定置信度&lt;i&gt;p&lt;/i&gt; = 0.3，那么 P( &lt;i&gt;B&lt;/i&gt; =True) = 0.3 并且我们很高兴。&lt;/li&gt;&lt;li&gt;如果我们考虑&lt;i&gt;f&lt;/i&gt; ( &lt;i&gt;p&lt;/i&gt; ) = &lt;i&gt;p&lt;/i&gt;的均匀预测代理，并相信我们将分配置信度&lt;i&gt;p&lt;/i&gt; =0.3 或&lt;i&gt;p&lt;/i&gt; =0.8，每个概率为 50%，则 P( &lt;i&gt;B&lt;/i&gt; =True) = 0.55，我们会不满意。&lt;/li&gt;&lt;li&gt;如果我们考虑任何&lt;i&gt;f&lt;/i&gt; ，其中&lt;i&gt;p&lt;/i&gt;至少有一个可能值&lt;i&gt;p&lt;/i&gt; * 使得 f( &lt;i&gt;p&lt;/i&gt; *)= &lt;i&gt;p&lt;/i&gt; *，并且相信我们将分配置信度&lt;i&gt;p&lt;/i&gt; = &lt;i&gt;p&lt;/i&gt; *，则 P( &lt;i&gt;B&lt;/i&gt; =True) = &lt;i&gt;f&lt;/i&gt; ( &lt;i&gt;p&lt;/i&gt; *) = &lt;i&gt;p&lt;/i&gt; * 我们很高兴。&lt;/li&gt;&lt;li&gt;如果我们考虑一个&lt;i&gt;f&lt;/i&gt; ，其中&lt;i&gt;没有&lt;/i&gt;可能的值&lt;i&gt;p&lt;/i&gt;且 f( &lt;i&gt;p&lt;/i&gt; )= &lt;i&gt;p&lt;/i&gt; ，并且相信我们肯定会分配一些特定的可信度&lt;i&gt;p&lt;/i&gt; * ，那么我们得到 P( &lt;i&gt;B&lt;/i&gt; =True) != &lt;i&gt;p&lt;/i&gt; * 并且将不开心。&lt;/li&gt;&lt;li&gt;但是：如果我们考虑一个&lt;i&gt;f&lt;/i&gt; ，其中&lt;i&gt;没有&lt;/i&gt;可能的值&lt;i&gt;p&lt;/i&gt; ，且 f( &lt;i&gt;p&lt;/i&gt; )= &lt;i&gt;p&lt;/i&gt; ，并且相信我们可以以一定的正概率分配 0 和 1 之间的&lt;i&gt;任何&lt;/i&gt;可能的置信值&lt;i&gt;p&lt;/i&gt; ，那么我们确实会得到一些结果 P ( &lt;i&gt;B&lt;/i&gt; =True) 介于 0 和 1 之间，并且由于我们已将正概率附加到该值，因此我们应该感到高兴，因为结果与我们相信的预测并不矛盾！&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;假设我们将节点&lt;i&gt;p&lt;/i&gt;解释为具有某个效用函数&lt;i&gt;u&lt;/i&gt; ( &lt;i&gt;B&lt;/i&gt; ) 的理性 us 的控制变量，假设 u( &lt;i&gt;B&lt;/i&gt; =True) = 1 和 u( &lt;i&gt;B&lt;/i&gt; =False) = 0。然后我们可以使用贝叶斯模型来计算给定 p 所有可能值的预期效用：E( &lt;i&gt;u&lt;/i&gt; ( &lt;i&gt;B&lt;/i&gt; ) | &lt;i&gt;p&lt;/i&gt; ) = &lt;i&gt;q&lt;/i&gt; = ( &lt;i&gt;flow&lt;/i&gt; ( &lt;i&gt;p&lt;/i&gt; ) + fhigh(p)) / 2。因此，理性主体会选择&lt;i&gt;最大化&lt;/i&gt;( &lt;i&gt;flow&lt;/i&gt; ( &lt;i&gt;p&lt;/i&gt; ) + fhigh( &lt;i&gt;p&lt;/i&gt; )) / 2。如果这就是我们想要从模型中得到的全部内容，则不需要&lt;i&gt;g&lt;/i&gt; ！因此，我们只需要一个不完整的贝叶斯网络，它不指定控制变量的概率分布，因为我们将选择它们。&lt;/p&gt;&lt;p&gt;如果&lt;i&gt;u&lt;/i&gt;不仅取决于&lt;i&gt;B&lt;/i&gt; ，而且还取决于&lt;i&gt;p&lt;/i&gt; = &lt;i&gt;q&lt;/i&gt; ，例如&lt;i&gt;u&lt;/i&gt; ( &lt;i&gt;B&lt;/i&gt; , &lt;i&gt;p&lt;/i&gt; , &lt;i&gt;q&lt;/i&gt; ) = 1 &lt;sub&gt;B=True&lt;/sub&gt; – | ，事情会变得更有趣。 &lt;i&gt;p&lt;/i&gt; – &lt;i&gt;q&lt;/i&gt; | 。在这种情况下，E(u | p) = f(p) – |p – f(p)|。如果 f(p) &amp;gt; p，则等于 f(p) – |f(p) – p| = f(p) – (f(p) – p) = p。如果 f(p) &amp;lt; p，则等于 2f(p) – p。&lt;/p&gt;&lt;p&gt;假设理性的 us 无法选择 ap，其中 f(p) != p。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;偏离：如果您不确定您的效用函数是否等于 u1 或 u2，并且将 c1 赋予 u1，将 c2 赋予 u2，那么您可以简单地使用函数 u = c1*u1 + c2*u2。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;贝叶斯更新&lt;/i&gt;过程如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;我们会跟踪您所&lt;i&gt;知道的&lt;/i&gt;（而不是仅仅相信！） &lt;i&gt;，根据您所拥有的数据，&lt;/i&gt;哪些&lt;i&gt;变量值的组合&lt;/i&gt;仍然是可能的。我们通过集合&lt;i&gt;D&lt;/i&gt;对这些知识进行建模：根据您的数据仍然可能的所有可能的变量值组合的集合（形式上， &lt;i&gt;D&lt;/i&gt;是概率空间 Omega 的子集）。如果一开始根本没有数据， &lt;i&gt;D&lt;/i&gt;仅包含所有可能的变量组合，即&lt;i&gt;D&lt;/i&gt; =Omega。&lt;ul&gt;&lt;li&gt;在我们的例子中，D 和 Omega 等于所有可能值三元组 ( &lt;i&gt;B&lt;/i&gt; , &lt;i&gt;p&lt;/i&gt; , &lt;i&gt;q&lt;/i&gt; ) 的集合，即它们是集合 {True,False}、区间 [0,1] 和另一个副本的笛卡尔积区间[0,1]：&lt;ul&gt;&lt;li&gt; D = 欧米伽 = {真,假} x [0,1] x [0,1]&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;每当我们获得更多数据时：&lt;ul&gt;&lt;li&gt;我们通过丢弃 D 中那些被传入数据排除的元素来反映这一点，因此不再被认为是可能的。换句话说，我们将 D 替换为 D 的某个子集 D&amp;#39;。&lt;/li&gt;&lt;li&gt;然后，在给定 D 的情况下，使用贝叶斯公式计算我们感兴趣的事件 E 的条件概率分布：&lt;ul&gt;&lt;li&gt; P(E | D) = P(E 和 D) / P(D)&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此时，我们可能会倾向于将基于&lt;i&gt;f&lt;/i&gt;和&lt;i&gt;g&lt;/i&gt;的某些选择&lt;i&gt;而导出的 P(B=True) 值视为&lt;/i&gt;&lt;i&gt;有关 p 的数据&lt;/i&gt;。让我们考虑一下这样做的后果。假设我们从一些固定的&lt;i&gt;f、g&lt;/i&gt;开始，并且不知道这三个变量的实际值，即 D &lt;sub&gt;0&lt;/sub&gt; = Omega = {True,False} x [0,1] x [0,1]。然后我们计算 P( &lt;i&gt;B&lt;/i&gt; =True) 并获得 0 和 1 之间的某个值&lt;i&gt;p &lt;sub&gt;1。&lt;/sub&gt;&lt;/i&gt;我们将此视为&lt;i&gt;p&lt;/i&gt; = &lt;i&gt;p &lt;sub&gt;1&lt;/sub&gt;&lt;/i&gt;将累积数据更新为 D &lt;sub&gt;1&lt;/sub&gt; = {True,False} x { &lt;i&gt;p &lt;sub&gt;1&lt;/sub&gt;&lt;/i&gt;这一事实的证据} x [0,1]，并更新我们的概率，以便现在 P( &lt;i&gt;B&lt;/i&gt; =True) = &lt;i&gt;f&lt;/i&gt; ( &lt;i&gt;p &lt;sub&gt;1&lt;/sub&gt;&lt;/i&gt; )。如果后一个值，我们称之为&lt;i&gt;p &lt;sub&gt;2&lt;/sub&gt;&lt;/i&gt; ，等于&lt;i&gt;p &lt;sub&gt;1&lt;/sub&gt;&lt;/i&gt; ，我们很高兴。否则，我们会想知道。那么我们有几个替代途径可以追求：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;我们可以将结果 P( &lt;i&gt;B&lt;/i&gt; =True) = &lt;i&gt;p &lt;sub&gt;2&lt;/sub&gt;&lt;/i&gt;视为有关&lt;i&gt;p&lt;/i&gt;的另一个传入数据，它需要与我们之前的数据结合起来。但我们之前的数据和这个新数据相互矛盾。两者不可能同时为真，因此我们之前的数据表明的陈述 S &lt;sub&gt;1&lt;/sub&gt; : &lt;i&gt;p&lt;/i&gt; = &lt;i&gt;p &lt;sub&gt;1&lt;/sub&gt;&lt;/i&gt;是错误的，或者我们之前的数据表明的陈述 S &lt;sub&gt;2&lt;/sub&gt; : &lt;i&gt;p&lt;/i&gt; = &lt;i&gt;p &lt;sub&gt;2&lt;/sub&gt;&lt;/i&gt;是错误的。如果我们认为 S &lt;sub&gt;1&lt;/sub&gt;是假的，我们必须考虑为什么它是假的，因为这可能使我们得出有价值的结论。 S &lt;sub&gt;1&lt;/sub&gt;纯粹源自我们的世界模型，由函数&lt;i&gt;f&lt;/i&gt;和&lt;i&gt;g&lt;/i&gt;参数化，因此这些函数中至少有一个肯定是不正确的，或者整个模型都是不正确的。&lt;ul&gt;&lt;li&gt;模型中最不稳定的部分是&lt;i&gt;g&lt;/i&gt; ，因此我们可能应该得出结论，我们对&lt;i&gt;g&lt;/i&gt;的选择是错误的。然后我们应该尝试找到一个不会导致这种矛盾的&lt;i&gt;g&lt;/i&gt;规范。只有当存在一个值&lt;i&gt;p&lt;/i&gt; * 且&lt;i&gt;f&lt;/i&gt; ( &lt;i&gt;p&lt;/i&gt; *) = &lt;i&gt;p&lt;/i&gt; * 时，我们才能成功地做到这一点。如果存在这样的值，我们可以将&lt;i&gt;g&lt;/i&gt; ( &lt;i&gt;p&lt;/i&gt; *) = 无穷大（记住， &lt;i&gt;g&lt;/i&gt;指定概率密度而不是概率），并且对于所有&lt;i&gt;p&lt;/i&gt; != &lt;i&gt;p&lt;/i&gt; *， &lt;i&gt;g&lt;/i&gt; ( &lt;i&gt;p&lt;/i&gt; ) = 0，即从一开始就假设我们肯定会预测&lt;i&gt;p&lt;/i&gt; * 。但如果这样的值&lt;i&gt;p&lt;/i&gt; *&lt;i&gt;不&lt;/i&gt;存在，我们就&lt;i&gt;不能&lt;/i&gt;选择&lt;i&gt;g&lt;/i&gt; ，这样就避免了矛盾。&lt;/li&gt;&lt;li&gt;在这种情况下，模型的其他部分肯定是不正确的，而错误的下一个最佳候选者就是函数&lt;i&gt;f&lt;/i&gt; 。由于不存在&lt;i&gt;f&lt;/i&gt; ( &lt;i&gt;p&lt;/i&gt; )= &lt;i&gt;p 的&lt;/i&gt;&lt;i&gt;p&lt;/i&gt; ，因此&lt;i&gt;f&lt;/i&gt;必定是不连续的。假设&lt;i&gt;f&lt;/i&gt;不连续是否有意义？可能不会。所以我们用一些连续函数代替&lt;i&gt;f&lt;/i&gt; 。瞧：现在有一些值&lt;i&gt;p&lt;/i&gt; * 且&lt;i&gt;f&lt;/i&gt; ( &lt;i&gt;p&lt;/i&gt; *)= &lt;i&gt;p&lt;/i&gt; * ，我们现在可以选择一个合适的&lt;i&gt;g&lt;/i&gt;并避免矛盾。&lt;/li&gt;&lt;li&gt;如果我们拼命想要坚持不连续的&lt;i&gt;f&lt;/i&gt; ，那么模型的其他部分一定是错误的。我认为这是代理能够确定地知道 p 的想法，而不是仅仅能够用一些随机测量噪声 epsilon 来测量 p。我建议再添加两个变量，噪声 epsilon 和测量 m，并将公式修改如下：&lt;ul&gt;&lt;li&gt; epsilon ~ N(0,1)，即高斯噪声&lt;/li&gt;&lt;li&gt;&lt;i&gt;m&lt;/i&gt; = &lt;i&gt;h&lt;/i&gt; ( &lt;i&gt;p&lt;/i&gt; , epsilon) 对于某个连续函数&lt;i&gt;h&lt;/i&gt; ，表示随机噪声 epsilon 对智能体对&lt;i&gt;p&lt;/i&gt;的测量&lt;i&gt;m&lt;/i&gt;的影响。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;例如： &lt;i&gt;h&lt;/i&gt; ( &lt;i&gt;p&lt;/i&gt; , epsilon) = expit(logit( &lt;i&gt;p&lt;/i&gt; ) + sigma epsilon) 对于某些幅度参数 sigma &amp;gt; 0。&lt;/li&gt;&lt;li&gt; &lt;i&gt;q&lt;/i&gt; = &lt;i&gt;f&lt;/i&gt; ( &lt;i&gt;m&lt;/i&gt; ) 而不是&lt;i&gt;q&lt;/i&gt; = &lt;i&gt;f&lt;/i&gt; ( &lt;i&gt;p&lt;/i&gt; )&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;通过这个修改后的模型，我们将得到&lt;/p&gt;&lt;p&gt;P( &lt;i&gt;B&lt;/i&gt; =True) = P( &lt;i&gt;B&lt;/i&gt; =True | &lt;i&gt;q&lt;/i&gt; ) dP( &lt;i&gt;q&lt;/i&gt; ) 对&lt;i&gt;q&lt;/i&gt;的所有可能值的积分&lt;br /&gt;= P( &lt;i&gt;B&lt;/i&gt; =True | &lt;i&gt;q&lt;/i&gt; ) dP( &lt;i&gt;q | m&lt;/i&gt; ) dP( &lt;i&gt;m | p,&lt;/i&gt; epsilon) dP( &lt;i&gt;p&lt;/i&gt; ) dP(epsilon) 在&lt;i&gt;q、&lt;/i&gt; &lt;i&gt;p&lt;/i&gt;和 epsilon 的所有可能值上的积分&lt;br /&gt;= E &lt;sub&gt;epsilon~N(0,1)&lt;/sub&gt; &lt;i&gt;f&lt;/i&gt; ( &lt;i&gt;h(p,&lt;/i&gt; epsilon)) &lt;i&gt;g&lt;/i&gt; ( &lt;i&gt;p&lt;/i&gt; ) d &lt;i&gt;p&lt;/i&gt;在&lt;i&gt;p&lt;/i&gt; =0…1 上的积分，其中 E 是 epsilon 的期望算子&lt;/p&gt;&lt;p&gt;如果我们选择&lt;i&gt;g&lt;/i&gt;将 100% 的概率分配给&lt;i&gt;p&lt;/i&gt;的某个值&lt;i&gt;p&lt;/i&gt; &lt;sub&gt;1&lt;/sub&gt; ，则计算结果为&lt;/p&gt;&lt;p&gt;&lt;i&gt;p&lt;/i&gt; &lt;sub&gt;2&lt;/sub&gt; := P( &lt;i&gt;B&lt;/i&gt; =True) = E &lt;sub&gt;epsilon~N(0,1)&lt;/sub&gt; &lt;i&gt;f&lt;/i&gt; ( &lt;i&gt;h&lt;/i&gt; ( &lt;i&gt;p&lt;/i&gt; &lt;sub&gt;1&lt;/sub&gt; , epsilon)),&lt;/p&gt;&lt;p&gt;即使&lt;i&gt;f&lt;/i&gt;不连续，它也是&lt;i&gt;p&lt;/i&gt; &lt;sub&gt;1&lt;/sub&gt;的&lt;i&gt;连续&lt;/i&gt;函数，因为&lt;i&gt;h&lt;/i&gt; ! 执行了“涂抹”！因此，存在&lt;i&gt;p &lt;sub&gt;1&lt;/sub&gt;&lt;/i&gt;的某种选择，其中&lt;i&gt;p &lt;sub&gt;2&lt;/sub&gt; = p &lt;sub&gt;1&lt;/sub&gt;&lt;/i&gt;且不矛盾。这意味着无论我们假设什么连续噪声函数&lt;i&gt;h&lt;/i&gt;和可能不连续的反应函数&lt;i&gt;f&lt;/i&gt; ，我们都可以指定一个函数&lt;i&gt;g&lt;/i&gt;来编码我们将预测&lt;i&gt;p &lt;sub&gt;1&lt;/sub&gt;的&lt;/i&gt;确定信念，并且贝叶斯网络将输出与我们的假设完全匹配的预测&lt;i&gt;p &lt;sub&gt;2&lt;/sub&gt;&lt;/i&gt;&lt;i&gt;第&lt;sub&gt;1&lt;/sub&gt;页&lt;/i&gt;。 &lt;span class="footnote-reference" id="fnref3u9hgzvo0ar"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn3u9hgzvo0ar"&gt;[3]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;&lt;strong&gt;致谢&lt;/strong&gt;&lt;/h1&gt;&lt;p&gt;非常感谢&lt;a href="https://forum.effectivealtruism.org/users/jobst-heitzig-vodle-it"&gt;Jobst&lt;/a&gt; &lt;a href="https://www.pik-potsdam.de/members/heitzig"&gt;Heitzig&lt;/a&gt;检查我的写作并撰写帖子的“应用贝叶斯建模和更新预测代理”部分。他说它还不完整，还有更多东西要写，但我很感谢已经有了的东西。特别感谢无数人提供免费的哲学&lt;a href="https://plato.stanford.edu/entries/epistemology-bayesian/"&gt;二手&lt;/a&gt;&lt;a href="https://www.youtube.com/watch?v=ClVIw7_ZzSE"&gt;文献&lt;/a&gt;，使我更好地理解这些问题。你们都值得我的学费。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;ol class="footnotes"&gt;&lt;li class="footnote-item" id="fnqkha9dl1rvb"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefqkha9dl1rvb"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;请参阅&lt;a href="https://www.lesswrong.com/users/diffractor?mention=user"&gt;@Diffractor&lt;/a&gt;和&lt;a href="https://www.lesswrong.com/users/vanessa-kosoy?mention=user"&gt;@Vanessa Kosoy&lt;/a&gt;的&lt;a href="https://www.lesswrong.com/posts/zB4f7QqKhBHa5b37a/introduction-to-the-infra-bayesianism-sequence"&gt;基础贝叶斯主义&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn2m4iubmkytz"&gt;&lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnref2m4iubmkytz"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;如果预测市场中的第一批人与一般人群具有不同的人口统计数据/兴趣，这就会成为一个问题。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn3u9hgzvo0ar"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnref3u9hgzvo0ar"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;或者，我们可以得出结论，贝叶斯网络的输出 P( &lt;i&gt;B&lt;/i&gt; =True)&lt;i&gt;不应&lt;/i&gt;被视为&lt;i&gt;p&lt;/i&gt;上的数据。但然后呢？&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ol&gt;&lt;br /&gt;&lt;br /&gt;&lt;a href="https://www.lesswrong.com/posts/SGhHdph9YLbHnFJkk/foreacting-agents#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Fri, 08 Dec 2023 19:57:25 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/SGhHdph9YLbHnFJkk/foreacting-agents</guid></item><item><title>使用法学硕士大规模建模激励措施</title><link>https://www.lesswrong.com/posts/hYS2KGAeB44SiJnJe/modeling-incentives-at-scale-using-llms</link><description>发布于 2023 年 12 月 8 日下午 6:46（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;这篇文章的目的是收集有关新项目想法的反馈。这项工作将由人工智能目标研究所 (AOI)、MetaGov 和其他合作机构合作完成。&lt;br /&gt;&lt;br /&gt;项目 PI：Bruno Marnette (AOI) 和 Philipp Zahn (MetaGov、20squares)。&lt;br /&gt;特别感谢 Colleen McKenzie、Matija Franklin、Timothy Telleen-Lawton、Gaia Dempsay、Ping Yee、Justin Stimatze、Cleo Nardo、Tushant Jha、Deger Turan 等人的反馈和审查&lt;i&gt;。&lt;/i&gt;&lt;/p&gt;&lt;h1&gt;动机&lt;/h1&gt;&lt;p&gt;人类通常会模拟其他人的动机。历史学家通过研究有权势的个人的动机来解释他们为什么做出特定的决定。记者们关注大公司揭露共谋和利益冲突的动机。经济学家使用激励模型来了解市场。立法者在引入新的激励措施之前会先研究现有的激励措施。&lt;/p&gt;&lt;p&gt;然而，一个人或一个小团队可以做多少建模和分析是有自然限制的，这就是我们看到利用法学硕士的机会的地方。我们现在可以将更多的工作委托给机器，以降低成本并扩大规模，而不是主要依靠人力来建立激励模型。主要有两个优点：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;由法学硕士驱动的激励模型可以处理更大量的信息，从而使它们更完整、更平衡、更准确。&lt;/li&gt;&lt;li&gt; LLM 管道的自动化和灵活性允许快速迭代和完善。我们可以通过简单地调整提示来调整我们的模型。&lt;/li&gt;&lt;/ul&gt;&lt;h1&gt;工作定义&lt;/h1&gt;&lt;p&gt;我们将“&lt;strong&gt;激励&lt;/strong&gt;”定义为“可能鼓励实体采取特定行动或从事特定行为的因素”。 &lt;strong&gt;“因素”&lt;/strong&gt;一词可以指不同类型的动机、力量、压力或奖励和惩罚。根据应用领域的不同，我们可能有兴趣捕捉财务、法律、社会、文化、意识形态或心理因素。&lt;/p&gt;&lt;h1&gt;研究问题&lt;/h1&gt;&lt;p&gt;已经有大量证据证实，只要以正确的方式提示并提供正确的背景，法学硕士就可以对文档进行深入的分析。例如，&lt;a href="https://arxiv.org/abs/2306.12672"&gt;&lt;u&gt;麻省理工学院的一篇论文&lt;/u&gt;&lt;/a&gt;很好地说明了如何使用法学硕士使用概率编程语言生成精确且正式的模型。一篇&lt;a href="https://arxiv.org/abs/2306.11932"&gt;&lt;u&gt;Anthropic 论文&lt;/u&gt;&lt;/a&gt;展示了具有大上下文窗口的法学硕士如何从大型数据集中提取见解，即使数据集包含人类观点和主观的、难以解释的陈述。同样， &lt;a href="https://ai.objectives.institute/blog/talk-to-the-city-an-open-source-ai-tool-to-scale-deliberation"&gt;&lt;u&gt;AOI 进行的研究&lt;/u&gt;&lt;/a&gt;表明，前沿法学硕士可以从一系列复杂的来源和输入格式（包括各种语言的视频采访）中识别和合成有价值的信息簇。即使今天的法学硕士相当嘈杂并且产生的结果质量参差不齐，我们可以公平地假设下一代法学硕士将比我们今天的法学硕士更有能力、更可靠。&lt;/p&gt;&lt;p&gt;这就是为什么我们认为（对于这个项目）有趣的问题不是关于基础法学硕士的原始能力，而是更多关于&lt;i&gt;如何&lt;/i&gt;在提取和汇总与激励相关的信息时充分利用法学硕士。更具体地说，我们计划解决以下问题：&lt;br /&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;数据和版权&lt;/strong&gt;——作为潜在激励来源的最佳数据集是什么？法学硕士可以用来评估不同来源的可靠性吗？数据可用性会带来哪些偏差？我们可以从特定来源提取多少细节，同时仍保持在合理使用的范围内？&lt;br /&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;建模&lt;/strong&gt;——我们应该如何对不同代理人的不同激励进行建模？我们应该尝试建模具体目标还是更抽象的动机？我们想要达到多精确和细粒度？我们想要为不同的场景分配概率估计吗？我们可以合理目标的正确复杂程度是多少？我们应该生成知识图还是应该生成代码？哪些流程可以确定哪种建模方法最成功地为人类观察者提供价值？&lt;br /&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;提示&lt;/strong&gt;——我们应该使用哪些提示以及哪些提示工程技术？我们如何解决当前法学硕士的已知限制和偏见？我们是否应该指导法学硕士以特定的方式采用特定的原则或理由（例如使用思想链）？我们应该利用哪些工具和流程来提出良好的提示并测试其性能？&lt;br /&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;产品&lt;/strong&gt;– 不同类型的模型在应用方面有哪些优势？什么样的界面对研究人员和/或广大公众来说最方便？不同的用例需要哪些功能？&lt;br /&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;影响&lt;/strong&gt;——上述模型和工具的发布会对整个社会产生怎样的影响？如果其他各方更清楚地了解自己的动机，不同的参与者会如何反应？我们应该担心反馈循环和自我实现的预言吗？这些模型可能如何被滥用于宣传？对这些模型的反应会改善未来的建模吗？错误的结论是否会加剧未来建模中的类似错误？&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;我们计划组建一个由来自不同学科的合作者和顾问组成的团队，在所有五个领域领域取得进展，然后将我们的知识应用到多个应用领域。&lt;/p&gt;&lt;h1&gt;范围和方法&lt;/h1&gt;&lt;p&gt;事实上，我们的主要想法之一是首先将激励问题分解为不同的组成部分（例如动机、计划或目标），然后决定哪些组成部分对于给定领域最重要。&lt;/p&gt;&lt;p&gt;更准确地说，我们计划将建模过程分为两个阶段：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;首先，我们将尝试通过提取特定领域的有限实体列表和有限的感兴趣目标/场景列表来构建&lt;i&gt;&lt;strong&gt;解释性定性模型&lt;/strong&gt;&lt;/i&gt;。这将为我们提供一个通用的词汇表和一个固定的本体来映射将从许多不同来源提取的信息。这些模型还将包括“来源 S 声称实体 E 可能支持（或拒绝）追求目标 G”形式的&lt;i&gt;方向&lt;/i&gt;信息，但它们不会包括数值估计。我们的模型还可能包括图形边缘和其他结构元素，但附加到矩阵单元和图形边缘的唯一信息将由文本组成（从源中提取的引用，而不是数字）。&lt;br /&gt;&lt;/li&gt;&lt;li&gt;作为第二个单独的步骤，当数据包含足够的信号时，我们还将尝试使用法学硕士将我们的定性模型转变为丰富的&lt;i&gt;&lt;strong&gt;预测数值模型&lt;/strong&gt;&lt;/i&gt;，以量化实体和目标之间因果关系的影响力和力量水平。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;我们不想对哪些类型的形式主义在不同领域最有效做出太多假设。相反，我们计划测试相对大量的可能形式主义。如果建模工作是手工完成的，这种方法听起来效率极低。但由于很多工作将由法学硕士推动，我们相信通过对提示进行小的改变来改变和比较形式主义将相当容易。&lt;/p&gt;&lt;p&gt;在研究定性模型（第一阶段）时，我们将根据简单和透明的标准选择来源，以优化平衡性、完整性和&lt;a href="https://nakamoto.com/credible-neutrality/"&gt;&lt;u&gt;可信的中立性&lt;/u&gt;&lt;/a&gt;。当致力于生成数值模型（在第二阶段）时，我们可能会变得更加难以保持我们想要的中立性，因为生成一个数字来表示关系有时需要解决矛盾来源之间的冲突，有时需要给定域无法获得的更高质量的数据。然而，我们将尽力为用户提供足够的控制权来比较和对比来自不同来源的信息。&lt;/p&gt;&lt;h1&gt;解释模型&lt;/h1&gt;&lt;p&gt;以下是我们设想如何生成解释性定性模型的概述。&lt;/p&gt;&lt;p&gt; 1) 对于添加到我们管道中的每个文档，我们首先要求法学硕士提取文档中感兴趣的实体、个人、组织和机构的列表，然后我们将使用维基百科的 API 将这些实体与唯一标识符进行匹配。我们已经实现了这一部分，并确认它在许多情况下运行良好。在某些情况下，维基百科 API 可能会返回多个选项，但我们可以要求法学硕士根据其描述选择最佳页面，到目前为止，这似乎解决了大多数歧义。&lt;/p&gt;&lt;p&gt; 2）在对每个文档中提到的实体列表进行建模后，我们将向法学硕士提出一系列问题，这些问题都与激励措施松散或紧密相关。例如：&lt;/p&gt;&lt;ul&gt;&lt;li&gt; {entity} 的既定目标是什么？&lt;/li&gt;&lt;li&gt; {entity} 似乎正在优化什么？&lt;/li&gt;&lt;li&gt; {entity} 的主要动机是什么？&lt;/li&gt;&lt;li&gt; {entity} 可以采取哪些操作？&lt;/li&gt;&lt;li&gt; {entity} 认为哪些场景对他们有利？&lt;/li&gt;&lt;li&gt; {entity} 会认为哪些情况对他们不利？&lt;/li&gt;&lt;li&gt; {entity}的主要恐惧和担忧可能是什么？&lt;/li&gt;&lt;li&gt; {entity} 有哪些资源可供使用？&lt;/li&gt;&lt;/ul&gt;&lt;p&gt; 3）然后，我们将使用聚类技术来合并相似的答案，重点关注产生一致答案集的问题，并使用这些答案集开始构建本体。到目前为止，我们进行的实验表明，不同的问题在不同的情况下效果更好。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;对于地缘政治领域，我们可能希望提取不同实体的可能目标（例如征服特定领土或谈判停火）。&lt;/li&gt;&lt;li&gt;对于更抽象的领域，我们可能决定提取不同的高级优化器（例如，一个实体可能针对“安全”进行优化，而另一个实体可能针对“自由”进行优化）&lt;/li&gt;&lt;li&gt;对于某些领域，考虑所涉及实体的情绪和心理可能很重要，而其他领域最好使用物质激励（例如财务）进行分析&lt;br /&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;4）然后，我们将选择一个固定的本体（意味着固定的目标列表，或喜欢的场景，或其他......），我们将再次运行我们的LLM管道，以提取一个干净的数据集，将唯一标识的实体映射到唯一标识的目标预先计算的列表。 （我们在之前的实验中观察到，当提供明确的目标本体时，法学硕士的表现要好得多）。&lt;br /&gt;&lt;br /&gt; 5）对于实体/目标矩阵的每个单元格，我们将收集一组由不同来源提供的理由和解释，提出实体和目标之间的联系。我们还将使用法学硕士来提供每个单元的摘要和汇总分数。&lt;br /&gt;&lt;br /&gt; 6) 然后我们计划在代表可能的联系和影响的实体上生成一个图形或方阵。例如，如果一个人碰巧是一家特定公司的大股东，我们可能希望跟踪此人对该公司有重大影响力，而此类信息通常是公开的。这里需要考虑许多不同的“影响力”概念，但我们将尝试尝试不同的提示/定义，并选择似乎能够提取更有趣和更有意义的见解的概念。&lt;br /&gt;&lt;br /&gt; 7) 同样，我们计划针对目标生成一个图表或方阵，表示目标之间的因果关系。根据所考虑的目标的确切类型，我们可能再次对“因果联系”的不同定义感兴趣。例如，如果目标代表不同的潜在场景或事件，我们可能对条件预测感兴趣（例如，如果 P(B|A) 被认为显着高于 P(B)，那么我们可能会从 A 到 B 绘制边缘） 。&lt;/p&gt;&lt;h1&gt;预测模型&lt;/h1&gt;&lt;p&gt;正如方法论部分所讨论的，我们计划尝试不同类型的数值模型。例如，当我们的目标本体可以映射到潜在的场景或事件时，我们可以研究&lt;a href="https://www.nber.org/system/files/working_papers/w1202/w1202.pdf"&gt;&lt;u&gt;条件预测&lt;/u&gt;&lt;/a&gt;技术或&lt;a href="https://webdocs.cs.ualberta.ca/~rgreiner/C-366/RG-2002-SLIDES/dbn-murphy.pdf"&gt;&lt;u&gt;动态贝叶斯网络&lt;/u&gt;&lt;/a&gt;来模拟不同事件如何影响彼此的概率。其他要考虑的经典建模技术可能包括对代理所&lt;a href="https://academic.oup.com/book/38699/chapter-abstract/336277038?redirectedFrom=fulltext"&gt;&lt;u&gt;显示的偏好&lt;/u&gt;&lt;/a&gt;的估计。为了构建这样的模型，我们通常会首先要求法学硕士在源数据本身中找到（条件）概率估计，或者在找不到时进行猜测。为了提高这些猜测的质量，我们将利用即时工程的最佳实践，包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;鼓励结构化推理（思想链或思想树）；&lt;/li&gt;&lt;li&gt;指导法学硕士遵循既定的方法（例如模拟层次分析法的德尔菲法）；和&lt;/li&gt;&lt;li&gt;提供非常好的例子作为提示的一部分（我们将寻求专业预报员的帮助来制作它们）。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;我们认为，通过要求法学硕士生成代理人可用行动的可能后果并将其与估计的收益函数相结合，也应该可以将定性模型转变为博弈论模型。这种方法的一个主要好处是，它允许我们推理（并预测）各个实体可能决定采取哪些行动来增加不同结果的可能性。这扩展了人工智能研究实验室目前正在进行的关于&lt;a href="https://www.mdpi.com/2079-9292/12/12/2722"&gt;&lt;u&gt;多智能体协同进化系统&lt;/u&gt;&lt;/a&gt;和结合影响图和结构因果模型框架的&lt;a href="https://arxiv.org/pdf/2001.07118.pdf"&gt;&lt;u&gt;混合因果模型&lt;/u&gt;&lt;/a&gt;的研究。&lt;/p&gt;&lt;p&gt;我们（尚未）计划为该项目的第一阶段发明新的数学建模框架，但如果资源允许，我们将尝试指导法学硕士使用不同论文中定义的形式主义，看看哪些产生最准确和最有效的形式。有用的模型。事实上，我们可以提供一本完整的&lt;a href="https://docs.mosek.com/modeling-cookbook/index.html"&gt;&lt;u&gt;建模手册&lt;/u&gt;&lt;/a&gt;，并让它建议使用哪种模型，而不是为法学硕士提供单一的形式。这样的食谱很容易适合现代模型（例如 gpt-4-turbo）的上下文窗口大小。&lt;/p&gt;&lt;p&gt;在更雄心勃勃的方面，我们还想尝试用表达性语言（例如概率编程语言）生成代码。生成任意代码会带来额外的挑战（例如，代码可能不会终止），但它可以更加精确和富有表现力。&lt;/p&gt;&lt;h1&gt;产品及应用领域&lt;/h1&gt;&lt;h3&gt;&lt;strong&gt;教育&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;下一代学生可能会花更少的时间阅读历史书，而花更多的时间向智能聊天机器人询问历史问题。但也存在一个危险，即聊天机器人将为他们提供过于简单化、有时带有偏见的叙述。高质量的激励模型可以成为更复杂、更可靠的教育工具的基石。例如，考虑一个学习法国大革命的学生。该工具可以通过激励模型的视角分析历史文本和记录，揭示影响君主制、贵族、神职人员、资产阶级和农民等群体行为的经济、社会和政治驱动因素的复杂网络。&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;社交网络&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;激励模型可用于在社交媒体平台上自动生成&lt;a href="https://communitynotes.twitter.com/guide/en/about/introduction"&gt;&lt;u&gt;社区注释&lt;/u&gt;&lt;/a&gt;。例如，当马克·安德烈辛 (Marc Andresseen) 在推特上谈论他的技术乐观主义宣言时，可以添加一条教育说明，为不熟悉他立场的人提供相关背景信息。例如：这可能与强调马克·安德森 (Marc Andressen) 的风险投资公司如何在人工智能上投资数十亿美元并被激励去推动任何形式的人工智能监管有关。这对他的大多数追随者来说是显而易见的，但对于不太熟悉这个领域的人来说，了解其中的动态可能很重要。然而，我们需要避免暗示人们只是受到物质激励的激励，而实际上他们并非如此，因此此类注释应定位为“背景”（而不是“警告”或任何暗示不当行为的内容）。&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;去极化&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;在处理两极分化的话题时，我们预计消息来源会对不同实体的动机存在分歧。在这种情况下，比较相反的消息来源如何描绘不同的画面可能会特别有趣。通过从两组不同的源生成两个不同的模型，我们可以轻松地做到这一点。只要这两个模型相对一致并且基于相关的人类激励，观察它们可能有助于观察者对双方产生同理心和尊重。同样，良好的激励模型可以证明，不需要阴谋、邪恶的计划、邪恶的计划来解释人们大多数时候所做的事情，即使一开始似乎难以解释。&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;预测&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;大型激励模型可以为从事预测未来业务的人们提供有价值的投入。我们特别考虑预测工具和平台，例如&lt;a href="https://www.metaculus.com/home/"&gt;&lt;u&gt;Metaculus&lt;/u&gt;&lt;/a&gt; 。预测者可以使用实体/目标矩阵中的信息，以及详细说明目标之间的影响和因果关系的矩阵，以更好地了解不同实体的潜在行动和反应。他们还可以查看来自不同数据源的不同激励模型，以产生多个预测。反过来，可以随着时间的推移跟踪这些预测的准确性，以便最具预测性的模型明显地赢得信任。&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;存在风险&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;我们相信另一个自然的应用领域是风险分析和预防。以气候变化为例。关注一系列项目的慈善投资者在决定投资之前可能需要预测每个项目的成功机会。典型 ESG 项目的成功机会通常取决于所涉及的不同实体是否充分协调。查看每个项目的激励模型也有助于估计这一点。&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;政策制定&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;例如，考虑社交媒体监管或人工智能监管领域。在这两种情况下，都涉及许多实体（政府、科技公司、内容提供商、平台用户、巨魔农场……），并且需要考虑许多相互冲突的激励措施。当学术团体或智囊团尝试手工对这些领域进行建模时，他们通常无法代表他们想要代表的所有实体和所有因素。使用法学硕士将有助于使模型更加完整、更接近现实世界，从而有可能使它们更加可信并且更有可能影响政策。&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;协调&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;许多社会挑战的症结不是价值错位（大多数人更喜欢全球和平），而是对他人的激励和我们可用的替代纳什均衡缺乏共识。广泛的激励模型可以显着增强我们的集体认知理解，澄清决策空间，并使集体智慧更容易实现最佳结果。更一般地说，我们的直觉是，激励模型可以增强我们在人工智能目标研究所开发的&lt;a href="https://ai.objectives.institute/blog/talk-to-the-city-an-open-source-ai-tool-to-scale-deliberation"&gt;&lt;u&gt;审议工具&lt;/u&gt;&lt;/a&gt;。&lt;/p&gt;&lt;h1&gt;质量与安全&lt;/h1&gt;&lt;h3&gt;&lt;strong&gt;评估&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;为了检查模型的质量，我们计划对模型提取的语句样本进行评估。我们将寻求所有用户对激励措施合理性的反馈，以了解模型的可信度。对于技术主题，我们还将与领域专家合作以获得高质量的反馈。我们还可能使用&lt;a href="https://www.prolific.com/"&gt;&lt;u&gt;Prolific&lt;/u&gt;&lt;/a&gt;平台招募不同的参与者，并要求他们验证法学硕士提取的信息的准确性。为了进一步扩大这一审查过程，我们计划利用法学硕士来揭露激励措施可能被歪曲或错误分配的情况。我们对于数值模型的发布将特别谨慎。当数值预测看起来不够可靠时，我们只会发布定性模型。&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;高风险和敏感领域&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;由于不同的风险，我们的方法会因领域而异。例如，地缘政治冲突的详细模型可能会带来国家安全风险，并可能被对手利用。因此，我们计划避免近期关于活跃冲突和敏感话题的数据，因为人工智能当前的能力可能无法确保必要的准确性和敏感性。首先，我们将避开复杂的文化问题，确保这些领域的任何未来模型都经过严格的质量检查，以防止刻板印象或两极分化叙事的传播。&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;滥用宣传&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;我们的模型始终存在用于自动影响操作的风险。 OpenAI 关于这个主题的详尽&lt;a href="https://arxiv.org/abs/2301.04246"&gt;&lt;u&gt;报告&lt;/u&gt;&lt;/a&gt;得出的结论是，没有解决更广泛问题的灵丹妙药，但我们仍然计划进行特定于产品的测试，以评估我们的模型信息可能如何被滥用。这可能包括评估各种模型生成的片段的感知可信度和偏差，其中一些是随机创建的，另一些是由独立红队故意歪曲的。如果测试人员可以轻松识别被操纵的叙述，则可能表明误用的风险较低。随着时间的推移，我们的目标是部分自动化该测试过程。&lt;br /&gt;&lt;br /&gt;总的来说，我们认为，让广大公众轻松获取我们计划在这些模型中汇总的信息是有益的。事实上，我们模型中的所有信息都将来自邪恶行为者已经拥有的多余信息，特别是如果他们是像国家资助的巨魔农场这样复杂的不良行为者。事实上，错误信息和宣传的潜在受害者将从激励模型中获得最大价值，因为这将帮助他们识别宣传者的可能动机。&lt;/p&gt;&lt;h1&gt;暂定路线图&lt;/h1&gt;&lt;p&gt;我们目前（2023 年第四季度）正处于该项目的&lt;strong&gt;构思阶段&lt;/strong&gt;。我们正在积极寻求专家和潜在用户的反馈和建议，以完善我们的计划并避免陷入困境。&lt;/p&gt;&lt;p&gt;下一阶段（2024 年第一季度）将是&lt;strong&gt;正式化阶段&lt;/strong&gt;。到 2024 年 3 月底，我们希望拥有明确的结构和适当的执行资源。我们的默认假设是该项目将由 AOI 和 MetaGov 这两个非营利组织共同主办，但我们愿意与更多机构合作。虽然一些人已经提出作为志愿者进行合作，但我们还计划筹集资金来招募和补偿更多的贡献者。我们还在考虑战略性的、特定领域的资金来源。例如，如果一个专注于气候变化的组织想要赞助有关气候相关主题的案例研究，我们将非常有兴趣探索这一点，特别是如果该组织还能够提供数据和/或领域专家的访问权限。&lt;/p&gt;&lt;p&gt;与潜在合作伙伴和赞助商的讨论也将成为我们构建一些第一个&lt;strong&gt;原型和演示的&lt;/strong&gt;机会（也是在 2024 年第一季度）。我们计划使用相对较小的数据集来构建这些数据，并在 UI 上快速迭代，以便及早找出潜在用户和合作伙伴眼中最有趣的功能。&lt;/p&gt;&lt;p&gt;下一阶段（2024 年第二季度和第三季度）将重点关注一些&lt;strong&gt;案例研究&lt;/strong&gt;。我们不想立即尝试构建一种通用的产品，而是想深入研究一些选定的示例领域。与前一阶段相比，本阶段将重点关注质量和准确性。我们将与领域专家密切合作来评估这种质量，并确保用户拥有具体的目标。我们希望这些未来的用户成为最终模型和界面的共同设计师。&lt;/p&gt;&lt;p&gt;最后，明年的最后阶段（从 2024 年第四季度开始）将致力于将我们的工作&lt;strong&gt;产品化&lt;/strong&gt;。这需要我们将从不同案例研究中学到的知识整合到单个人工智能管道和单个功能集中。我们的最终目标是发布一个免费的开源工具（我们希望在 2024 年底之前），任何人都可以使用现成的工具来生成自己的大型激励模型。然后，我们将在接下来的几年里集中精力分发和进一步改进该工具，使其具有更广泛的相关性和影响力。&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/hYS2KGAeB44SiJnJe/modeling-incentives-at-scale-using-llms#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Fri, 08 Dec 2023 20:30:21 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/hYS2KGAeB44SiJnJe/modeling-incentives-at-scale-using-llms</guid></item></channel></rss>