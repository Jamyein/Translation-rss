<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>少错</title><link>https://www.lesswrong.com</link><description>致力于提炼理性艺术的社区博客</description><lastBuildDate>Mon, 01 Jan 2024 06:15:58 GMT</lastBuildDate><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>研究 2024 年 1 月/2 月</title><link>https://www.lesswrong.com/posts/fmjwRQ6pYB2ymffJ7/research-jan-feb-2024</link><description>发布于 2024 年 1 月 1 日上午 6:02（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt; &lt;i&gt;2024 年快乐！ 🎇&lt;/i&gt;&lt;br /&gt;&lt;br /&gt;&lt;i&gt;在新的一年开始之际，我认为（非常非正式地）概述一下我至少在接下来的 8 周内将探索的研究途径以及该项目的具体目标会很好。&lt;/i&gt;&lt;/p&gt;&lt;p&gt;通用人工智能系统存在危险的一个主要因素是不会有第二次机会。如果部署的人工智能足够强大，但与人文学科的兴趣不相符，我们就无能为力。 John Wentworth 谈到缺乏用于对齐的“风洞”，这意味着我们测试和迭代对齐 AGI 设计的能力受到限制。&lt;br /&gt;&lt;br /&gt;我想探讨是否可以建造这样一个风洞。这个想法很简单。我们使用物理学工具来限制代理对其环境的最大影响。如果可能的话，这将能够安全地部分部署 AGI &lt;i&gt;，而&lt;/i&gt;无需完全解决对齐问题。&lt;/p&gt;&lt;p&gt;&lt;br /&gt;那么从哪里开始呢？以下是我正在探索的一些主题：&lt;br /&gt;&lt;br /&gt;自 20 世纪 60 年代以来，人们就知道驱除&lt;a href="https://en.wikipedia.org/wiki/Maxwell%27s_demon"&gt;&lt;u&gt;麦克斯韦恶魔&lt;/u&gt;&lt;/a&gt;需要认识到&lt;a href="https://en.wikipedia.org/wiki/Landauer%27s_principle#:~:text=Landauer's%20principle%20is%20a%20physical,of%20heat%20to%20its%20surroundings."&gt;&lt;u&gt;擦除信息会产生热量&lt;/u&gt;&lt;/a&gt;。然而，上个世纪的热力学计算分析因其对热平衡系统的关注而受到限制。我对&lt;a href="https://arxiv.org/abs/1905.05669"&gt;&lt;i&gt;&lt;u&gt;计算随机热力学的&lt;/u&gt;&lt;/i&gt;&lt;/a&gt;最新发展感到兴奋&lt;i&gt;&amp;nbsp;&lt;/i&gt; （沃尔珀特，2019） &lt;i&gt;&amp;nbsp;&lt;/i&gt;它专门解决了远离平衡的系统。这是一个极其重要的区别，因为人类、现代计算机和通用人工智能都远离平衡系统。&lt;/p&gt;&lt;p&gt;我对&lt;a href="http://philsci-archive.pitt.edu/18844/"&gt;&lt;u&gt;这篇论文&lt;/u&gt;&lt;/a&gt;（Evans、Milburn、Shrapnel，2021）非常感兴趣，该论文讨论了代理热力学模型的因果视角，并与其中一位作者取得了联系。我怀疑他们的物理因果模型将成为我工作的有用起点。本文以&lt;a href="https://www.jenanni.com/papers/"&gt;&lt;u&gt;Jenann Ismael&lt;/u&gt;&lt;/a&gt;的哲学论文为基础，他写了一些关于因果关系和时间的富有洞察力的著作，尽管我不确定它与我有多么直接相关。&lt;br /&gt;&lt;br /&gt;&lt;a href="https://www.youtube.com/watch?v=10cVVHKCRWw"&gt;&lt;u&gt;杰里米·英格兰的&lt;/u&gt;&lt;/a&gt;研究也远离平衡力学。他有多篇论文讨论各种“类生命”现象和&lt;a href="https://www.nature.com/articles/nnano.2015.250"&gt;耗散适应&lt;/a&gt;。我认为他的许多结果肯定暗示它们可能与对齐相关，例如“&lt;a href="https://journals.aps.org/pre/abstract/10.1103/PhysRevE.100.022414"&gt;&lt;u&gt;自我复制条件的设计&lt;/u&gt;&lt;/a&gt;”（2019）。&lt;/p&gt;&lt;p&gt;最后，上述所有内容都在很大程度上依赖于 Gavin Crooks 的工作和著名的&lt;a href="https://journals.aps.org/pre/abstract/10.1103/PhysRevE.60.2721"&gt;Crooks 波动定理&lt;/a&gt;（1999），该定理将熵的产生与正向和反向过程的概率联系起来。它非常宽松地解释了为什么我们会遇到许多仅在时间的一个方向上发生的过程，尽管微观物理定律是时间可逆的。 &lt;span class="footnote-reference" id="fnrefgp7vxl8l9m4"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fngp7vxl8l9m4"&gt;[1]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;有关更多详细信息，请参阅&lt;a href="https://shilingliang.com/files/Fluctuation_Theorems.pdf"&gt;这些注释&lt;/a&gt;（Liang 2018）。&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;strong&gt;未来两个月的具体目标：&lt;/strong&gt;&lt;br /&gt;我认为最重要的目标步骤是将其纳入我将来可以参考的正式研究议程。&lt;/p&gt;&lt;p&gt;在接下来的几天里，我需要完成对沃尔珀特论文的第二次阅读。我还想在接下来的两周内完成&lt;a href="https://www.youtube.com/watch?v=kkkm_nsZFwQ"&gt;&lt;u&gt;这一系列讲座&lt;/u&gt;&lt;/a&gt;。&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;元：&lt;/strong&gt;&lt;br /&gt;没有一个单一的、可管理的研究块可供关注，这并不是一件好事。例如，我可以从文献中得出结果，并以更一般的方式证明它。&lt;br /&gt;&lt;br /&gt;即使边界/风洞的想法被证明是徒劳的，我相信这项工作将实现实质性的消除混乱。&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;ol class="footnotes"&gt;&lt;li class="footnote-item" id="fngp7vxl8l9m4"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefgp7vxl8l9m4"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;前往亚马逊订购一张无摩擦台球桌和一套完美弹性的球。将它们随机地撒在桌子上，然后让它们运动。视频结果。&lt;br /&gt;&lt;br /&gt;如果您向朋友展示电影的前向和后向版本，他们将能够看到它们的不同，但他们无法指定哪个视频是原始视频。&lt;br /&gt;&lt;br /&gt;这就是微观尺度上正在发生的事情。微观物理定律表现出时间反转对称性。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ol&gt;&lt;br /&gt;&lt;br /&gt;&lt;a href="https://www.lesswrong.com/posts/fmjwRQ6pYB2ymffJ7/research-jan-feb-2024#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Mon, 01 Jan 2024 06:02:48 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/fmjwRQ6pYB2ymffJ7/research-jan-feb-2024</guid></item><item><title>2023 年人工智能预测</title><link>https://www.lesswrong.com/posts/EZxG6ySHCEjDvL5x4/2023-in-ai-predictions</link><description>发布于 2024 年 1 月 1 日凌晨 5:23（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;很多人都对 2023 年的人工智能做出了预测。这里我整理了一个子集。我有一个习惯，当我看到人工智能预测时，我会为预测日期设置电子邮件提醒，以便当它们得到解决时我可以指出它们的准确性或不准确性。我已按时间顺序整理了 2023 年以来的大部分电子邮件提醒（目标日期从早到晚的预测）。我计划每年发布这些帖子，检查日期已过期的预测。欢迎在评论中添加更多对 2023 年预测的参考。&lt;/p&gt;&lt;p&gt;在某些情况下，人们引用他人预测的方式&lt;em&gt;可能&lt;/em&gt;会被视为暗示他们同意。这不是某种特定的解释，但为了完整起见，我将它们包括在内。&lt;/p&gt;&lt;h3&gt; 2024 年 3 月&lt;/h3&gt;&lt;p&gt;&lt;a href="https://www.lesswrong.com/posts/ohXcBjGvazPAxq2ex/continue-working-on-hard-alignment-don-t-give-up/comment/bEZbK9pM5uynyHJxe"&gt;提升的齿轮&lt;/a&gt;：“对齐的难题将在 3 到 12 个月内像火车一样向我们袭来，与此同时，人们在整个 ML 历史上一直致力于实现的一些特定功能突破现在终于开始工作了，因为他们有了适用于较弱的通用人工智能，突然间克里奇的东西变得超级重要，需要理解。”&lt;/p&gt;&lt;h3&gt; 2024 年 10 月&lt;/h3&gt;&lt;p&gt;&lt;a href="https://twitter.com/jd_pressman/status/1718789355379314808?t=luXXx8djX0EUV2qgnZXX5g&amp;amp;s=19"&gt;John Pressman&lt;/a&gt; ：“6-12 个月的预测（80%）：作为 AI X-Risk 核心的一致性问题将成为历史文物，因为在大多数各方看来，该问题已基本得到解决或有望得到解决，并且争论日益激烈关于竞争和滥用。很少有人会改变立场。”&lt;/p&gt;&lt;h3&gt; 2025 年 7 月&lt;/h3&gt;&lt;p&gt;&lt;a href="https://x.com/jessi_cata/status/1735914007515861154"&gt;杰西卡·泰勒&lt;/a&gt;：“如果这个确切的提示得到解决，我不会感到惊讶，但附近对人类来说容易的事情可能不会得到解决？”&lt;/p&gt;&lt;p&gt;提示：“查找以下单词序列： - 20 个单词长 - 恰好包含同一单词连续两次重复 - 包含同一单词连续三次重复 2 次”&lt;/p&gt;&lt;p&gt; （注意：线程包含变化和更难的问题。）&lt;/p&gt;&lt;h3&gt; 2026 年 11 月&lt;/h3&gt;&lt;p&gt;&lt;a href="https://twitter.com/tegmark/status/1723688750998261896?t=KGd1QKTupU9yZXt3cBqk7A&amp;amp;s=19"&gt;Max Tegmark&lt;/a&gt; ：“在&lt;a href="https://t.co/1ropLwOUli"&gt;http://metaculus.com&lt;/a&gt;上，弱 AGI 的生存时间在短短 18 个月内就从 20 年骤降到了 3 年，这真是太疯狂了。所以你最好停止称 AGI 是一种‘长期’可能性，否则有人可能会这么做。”称你为困在过去的恐龙”&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/"&gt;梅塔库勒斯问题&lt;/a&gt;。&lt;/p&gt;&lt;p&gt; &lt;a href="https://twitter.com/blader/status/1727524318475927569?t=wH4LOB47dddHEMEkFUv5zg&amp;amp;s=19"&gt;陈思琪&lt;/a&gt;：“这意味着三年之内你要么死，要么有神为仆”。&lt;/p&gt;&lt;p&gt; &lt;a href="https://twitter.com/AISafetyMemes/status/1730063434761318642?t=S8GAu6G4XI1fqhfKYmzyMw&amp;amp;s=19"&gt;埃隆·马斯克&lt;/a&gt;：“如果你说‘在任何事情上都比最聪明的人类聪明’？它可能并不比所有人类或机器增强的人类更聪明，因为，你知道，我们有计算机和其他东西，所以有一个更高的标准。” ……但如果你的意思是，它可以写出像 JK 罗琳一样好的小说，或者发现新物理学、发明新技术？我想说，距离这一点我们还不到 3 年时间。”&lt;/p&gt;&lt;h3&gt; 2026年12月&lt;/h3&gt;&lt;p&gt;&lt;a href="https://twitter.com/Jai_Bhavnani/status/1734461565825650999"&gt;Jai Bhavnani&lt;/a&gt; ：“基线预期：90% 以上的智能合约将在未来 3 年内被利用。这些漏洞将被人工智能发现。我们需要解决方案。”&lt;/p&gt;&lt;h3&gt; 2028 年 10 月&lt;/h3&gt;&lt;p&gt;&lt;a href="https://twitter.com/connoraxiotes/status/1719358057031835703"&gt;Stuart Russell&lt;/a&gt; ：“每个人都从30-50岁，变成了3-5岁。”&lt;/p&gt;&lt;h3&gt; 2028 年 11 月&lt;/h3&gt;&lt;p&gt;&lt;a href="https://twitter.com/carad0/status/1725261419812098355?t=ml85fmBZ6EV5El5E40S-WQ&amp;amp;s=19"&gt;Tammy&lt;/a&gt; ：“当我说‘我们大约还有 0 到 5 年的时间’时，人们一直以为我是在说‘我们大约还有 5 年的时间’。我们没有大约 5 年的时间。我他妈希望如此。我们大约有 0 到 5 年的时间。” 5 年。下个月我们&lt;em&gt;实际上可能&lt;/em&gt;都会死于人工智能。”&lt;/p&gt;&lt;h3&gt; 2028年12月&lt;/h3&gt;&lt;p&gt;&lt;a href="https://twitter.com/tyler_m_john/status/1732114930046832928"&gt;Tyler John&lt;/a&gt; ：“是的。如果人工智能能力的不连续飞跃还需要 3-5 年的时间，我们可能应该开始考虑如何为此做好准备。欧盟人工智能法案已经制定了 5 年，但仍然没有实现。”还没有过去。我们不能再采取观望态度了。”&lt;/p&gt;&lt;p&gt; &lt;a href="https://twitter.com/AISafetyMemes/status/1734685184593653905"&gt;Mustafa Stuleyman&lt;/a&gt; ：“[当前模型已经]......可以说通过了图灵测试。我提出了一项测试，其中涉及 [AI] 启动并接受 100,000 美元的投资，并在三个月的时间内，尝试着手创建一个新产品，研究市场，看看消费者可能喜欢什么，生成一些新图像，一些如何制造该产品的蓝图，联系制造商，制造它，谈判价格，代发货，然后最终收取收入。我认为在 5 年内，我们很可能会拥有 ACI，一种能够自主完成大部分任务的人工智能。它将能够打电话给其他人进行谈判。它将例如，能够调用其他人工智能来在供应链中建立正确的顺序。”&lt;/p&gt;&lt;p&gt; &lt;a href="https://twitter.com/woke8yearold/status/1737897932068638745"&gt;Aleph&lt;/a&gt; ：“当我的 AGI 时间线是 30 到 50 年时，VS 变成了 5 年”&lt;/p&gt;&lt;h3&gt; 2033 年 12 月&lt;/h3&gt;&lt;p&gt;&lt;a href="https://twitter.com/RokoMijic/status/1741243303650009156?t=5BfE7a7q0weF9XrziceMFA&amp;amp;s=19"&gt;Roko Mijic&lt;/a&gt; ：“不，罗宾，人工智能不需要数百万年才能在所有任务上完全超越人类，大约需要 10 年”&lt;/p&gt;&lt;h3&gt; 2034 年 12 月&lt;/h3&gt;&lt;p&gt;&lt;a href="https://twitter.com/ESYudkowsky/status/1738591522830889275"&gt;Eliezer Yudkowsky&lt;/a&gt; ：“最后一个能够从事脑力劳动的人是什么时候出生的？2016 年？2020 年？”&lt;/p&gt;&lt;p&gt; （注：我计算 2016+18 的假设是，一些 18 岁的人可以从事脑力劳动，但所指的时间范围存在不确定性；我们也可以计算 2020+14 的假设，假设一些人14岁的人可以从事脑力劳动，所以我在这里取了一个粗略的中位数）&lt;/p&gt;&lt;h3&gt; 2035 年 12 月&lt;/h3&gt;&lt;p&gt;&lt;a href="https://www.lesswrong.com/posts/ZDQvYKuyH5aTrZKoY/ai-views-snapshots"&gt;多人&lt;/a&gt;：“STEM+ AI 将在 2035 年出现。” （预测范围，许多&amp;gt;=50%，一些&amp;gt;=90%）。&lt;/p&gt;&lt;p&gt;定义：“让‘STEM+ AI’缩写为‘比最优秀的人类科学家更擅长 STEM 研究的人工智能（此外可能还拥有其他技能）’”&lt;/p&gt;&lt;p&gt;另请参阅&lt;a href="https://x.com/robbensinger/status/1734699436176298131?s=20"&gt;Twitter/X 线程&lt;/a&gt;。&lt;/p&gt;&lt;h3&gt; 2040 年 10 月&lt;/h3&gt;&lt;p&gt;&lt;a href="https://twitter.com/ESYudkowsky/status/1718058899537018989"&gt;Eliezer Yudkowsky&lt;/a&gt; ：“谁还能想象一个今天出生的孩子 17 年后还能上大学的世界？”&lt;/p&gt;&lt;br /&gt;&lt;br /&gt;&lt;a href="https://www.lesswrong.com/posts/EZxG6ySHCEjDvL5x4/2023-in-ai-predictions#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Mon, 01 Jan 2024 05:23:42 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/EZxG6ySHCEjDvL5x4/2023-in-ai-predictions</guid></item><item><title>节奏舞台设置组件</title><link>https://www.lesswrong.com/posts/DCQPenXsQKXrBqCos/rhythm-stage-setup-components</link><description>发布于 2024 年 1 月 1 日凌晨 3:10（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;&lt;span&gt;我通常会演奏我所说的“节奏舞台设置”，尽管它需要一个更好的名称。它是一个键盘、脚鼓、呼吸控制器和口哨控制合成器，与&lt;/span&gt;&lt;a href="https://github.com/jeffkaufman/jammer"&gt;我编写的代码&lt;/a&gt;连接，使我能够发出完整且多样化的声音，同时保持控制和灵活性&lt;a href="https://www.jefftk.com/p/what-is-live"&gt;仍然活跃&lt;/a&gt;。以下是这些碎片放在地板上的样子：&lt;/p&gt;&lt;p&gt; &lt;a href="https://www.jefftk.com/rhythm-stage-setup-exploded-big.jpg"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DCQPenXsQKXrBqCos/sgu2kvyhp3uztql0gt6y" /&gt;&lt;/a&gt;&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他们是：&lt;/p&gt;&lt;ol type="a"&gt;&lt;li&gt;四个&lt;a href="https://www.guitarcenter.com/Yamaha/KU100-Beaterless-Silent-Kick-Unit-1372692950998.gc"&gt;Yamaha KU-100&lt;/a&gt;无节拍底鼓踏板。我每只脚下都有两个，一个用于脚跟，一个用于脚趾。&lt;/li&gt;&lt;li&gt;雅马哈 DTX 500 电子鼓组大脑。这会将来自底鼓踏板的信号转换为音频（用于大厅）和 MIDI（用于触发下游乐器）。&lt;/li&gt;&lt;li&gt;连接踏板和大脑的 TRS 电缆乱七八糟。我希望我有更好的东西，但我不确定是否有人制作简单便宜可靠的 6 英尺 4x TRS 蛇？&lt;/li&gt;&lt;li&gt; USB 键盘，我用它来切换系统模式。一种获取大量按钮的廉价而简单的方法。我在许多按键上贴上了标签，这样我就可以记住每个按键的用途。我确实希望我有灯光或其他方式来查看它们当前的模式是什么，除了记住自从我上次完全重置它以来我按过哪些按钮。&lt;/li&gt;&lt;li&gt;两个供电 USB 集线器。我想将更多的东西插入到作为系统核心的两个 Raspberry PI 中，其中一些比 PI 电源消耗更多的电力，而且我还需要为 PI 供电。&lt;/li&gt;&lt;li&gt;两个电源适配器，每个 USB 集线器一个。&lt;/li&gt;&lt;li&gt;简单情况下的两台 Raspberry PI 3B 计算机。大流行前它们很便宜，我认为它们终于又恢复便宜了吗？它们的配置完全相同，并且根据启动时插入的内容来选择扮演什么角色。这意味着如果其中一个坏了，我可以更换另一个，因为其中一个对我正在做的事情比另一个更重要。&lt;/li&gt;&lt;li&gt;备用 SD 卡和读卡器适配器。 SD 卡损坏的频率比我想象的要高，所以最好有备用卡。&lt;/li&gt;&lt;li&gt;用于鼓的 USB 转 MIDI。&lt;/li&gt;&lt;li&gt;用于键盘的 USB 转 MIDI。当我使用别人的带有 USB midi 的新型键盘时，我会带一根 A 到 B 电缆。&lt;/li&gt;&lt;li&gt;鼓的电源适配器。&lt;/li&gt;&lt;li&gt;用于控制口哨合成器的键盘。不需要完整的键盘，因为它的设置要少得多。&lt;/li&gt;&lt;li&gt;便宜的&lt;a href="https://www.amazon.com/gp/product/B07THHFJMT/"&gt;XLR A/B 分配器&lt;/a&gt;，这样我就可以将单个麦克风发送到口哨合成器或直接发送到开发板（在曼陀林上使用对讲机时）。&lt;/li&gt;&lt;li&gt;用于其中一台计算机的 USB A 至 micro B 和 A 至 USB-3 micro B 电缆：前者通过集线器为计算机供电，后者通过数据连接它们。&lt;/li&gt;&lt;li&gt;对于另一台计算机也是如此。&lt;/li&gt;&lt;li&gt;用于口哨合成器音频接口的 XLR 至 TRS 适配器。&lt;/li&gt;&lt;li&gt; Focusrite 2i2，口哨合成器的音频接口。我需要一些可以接受动态麦克风输入的东西。&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.jefftk.com/p/trying-a-breath-controller"&gt;呼吸控制器&lt;/a&gt;。让我用我当前吹奏的力度来为我的演奏添加表现力。&lt;/li&gt;&lt;li&gt;廉价的小型 USB 音频接口（和备用），带有 3.5mm trs 转双 1/4 TS 适配器。这是另一台计算机的。足够适合现场工作了！&lt;/li&gt;&lt;li&gt;麦克风拾取我的口哨声来控制合成器。&lt;/li&gt;&lt;li&gt;该麦克风的 XLR 电缆和麦克风夹。&lt;/li&gt;&lt;li&gt;两个双通道 DI 盒：键盘、鼓、电脑和曼陀林。&lt;/li&gt;&lt;li&gt; USB C 转 USB A 电缆，用于连接 Focusrite 并为其供电。我确实希望它设计有单独的电缆，这样我就可以少用一个集线器。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;我通常还会有一堆四分之一英寸的电缆、几个 XLR、几个电源板、一个防止东西在舞台上滑动的瑜伽垫、一个加高座椅（这样我就可以在合适的高度上演奏），通常还有额外的备件。&lt;/p&gt;&lt;p&gt;请注意，这不包括我的设置中的曼陀林方面，因为那更加标准：只是一系列吉他踏板。&lt;/p&gt;&lt;p&gt;以下是参加&lt;a href="https://www.hcdance.org/quiet-corner-contra/"&gt;周五在康涅狄格州&lt;/a&gt;演出的情况：&lt;/p&gt;&lt;p&gt; &lt;a href="https://www.jefftk.com/rhythm-stage-setup-all-packed-big.jpg"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DCQPenXsQKXrBqCos/sgi6sr9gktuhnt9n7ckz" /&gt;&lt;/a&gt;&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当我飞行时，我把所有东西都包裹在衣服里，连同我的曼陀林踏板，在一个全尺寸的托运行李中，它的重量略低于 50 磅。&lt;/p&gt;&lt;p&gt;&lt;i&gt;评论通过： &lt;a href="https://www.facebook.com/jefftk/posts/pfbid036uYTkptfpG5m6ZS2cpvY23VHBxQo6hj28qgSrRkQiT8bL6FqvHPaSKVKf8hX8RPel"&gt;facebook&lt;/a&gt; 、 &lt;a href="https://lesswrong.com/posts/DCQPenXsQKXrBqCos"&gt;lesswrong&lt;/a&gt; 、 &lt;a href="https://mastodon.mit.edu/@jefftk/111678504751086600"&gt;mastodon&lt;/a&gt;&lt;/i&gt;&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/DCQPenXsQKXrBqCos/rhythm-stage-setup-components#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Mon, 01 Jan 2024 03:10:16 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/DCQPenXsQKXrBqCos/rhythm-stage-setup-components</guid></item><item><title>现实生活中的贝叶斯更新主要是为了理解你的假设</title><link>https://www.lesswrong.com/posts/jsKYmqHw4aGLw6fLZ/bayesian-updating-in-real-life-is-mostly-about-understanding</link><description>发布于 2024 年 1 月 1 日凌晨 12:10（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;我的感觉是，这里越来越普遍的观点是，过去 20 年的人工智能发展和人工智能 x 风险讨论可以通过以下叙述得到很好的描述：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;埃利泽·尤德科斯基（Eliezer Yudkowsky）（以及其他至少在最初深受其思想影响的人）开发了关于开发比人类更聪明的人工智能过程中可能固有的关键问题的详细模型。&lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt;这些模型在提出时介于“也许合理”和“相当引人注目”之间，但人工智能的最新发展（例如语言模型的行为特征、缩放的平滑性/渐进性）表明现实并非如此。其结果与埃利以泽模型的预测完全一致。&lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt;这些发展并没有完全证伪以利以谢的模型和关键预测，但现在有很多替代模型和理论。这些竞争模型中的部分或全部是或声称：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;在预测近期人工智能发展方面&lt;a href="https://www.lesswrong.com/posts/AWoZBzxdm4DoGgiSj/ability-to-solve-long-horizon-tasks-correlates-with-wanting?commentId=mEPjkATGDExGcZWop"&gt;有更好的记录&lt;/a&gt;&lt;/li&gt;&lt;li&gt;更好地回顾过去的发展&lt;span class="footnote-reference" id="fnrefgvq8m4my5vv"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fngvq8m4my5vv"&gt;[1]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;得到&lt;a href="https://optimists.ai/2023/11/28/ai-is-easy-to-control/"&gt;机器学习&lt;/a&gt;和/或&lt;a href="https://www.lesswrong.com/posts/xwBuoE9p8GE7RAuhd/brain-efficiency-much-more-than-you-wanted-to-know"&gt;神经科学&lt;/a&gt;的实证结果的支持&lt;/li&gt;&lt;li&gt;对于&lt;a href="https://www.lesswrong.com/posts/9x8nXABeg9yPk2HJ9/ronny-and-nate-discuss-what-sorts-of-minds-humanity-is?commentId=zsFBt2KTeccXZy3kA"&gt;不同背景和专业领域的&lt;/a&gt;人来说，&lt;i&gt;感觉&lt;/i&gt;更直观、更合理、更有证据支持&lt;/li&gt;&lt;/ul&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt;因此，即使我们不能完全否定埃利泽的模型，任何优秀的贝叶斯主义者（包括埃利泽本人）都应该能够通过观察最近的发展并考虑他们支持的替代理论来进行&lt;i&gt;定向&lt;/i&gt;贝叶斯更新。即使整体更新的精确程度（以及后验的最终落地地点）仍然存在高度不确定性和争议性，但基本方向是明确的。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;在不过多涉及对象层面的情况下，或者甚至叙述作为一个整体是否反映了特定真实人物的实际观点时，我想对此类叙述中通常使用的信念更新的概念发表一些评论。&lt;/p&gt;&lt;p&gt;请注意，从某种意义上说，一个人的信念的&lt;i&gt;任何&lt;/i&gt;（有效）变化都可以建模为某种贝叶斯更新，但在这里我特别指的是流行的理性主义实践，即根据概率和语言进行明确的思考和交流。似然比。&lt;/p&gt;&lt;p&gt;关于（a）更新过程一般如何运作以及（b）如何将该过程有效地应用于更新人工智能开发模型的特定情况的常见观点中存在一些可疑的假设（我怀疑是）和 x 风险。&lt;/p&gt;&lt;p&gt;当这样的观点在“更新”是广泛的道德/可取/正确的观点的背景下隐含地表达时，我发现往往有很多重要的警告和先决条件的掩饰，这些警告和先决条件使潜在的心理运动与现实联系在一起——也就是说，确保它仍然是一种在不确定性下进行有效推理的系统性（如果粗略且近似）方法。&lt;/p&gt;&lt;p&gt;本文的其余部分回顾了贝叶斯更新按预期工作的一些关键概念和要求，并通过一些示例和非示例说明了如何在实践中无法满足这些要求。&lt;/p&gt;&lt;p&gt;我的结论并不是说显式贝叶斯更新的做法本质上是有缺陷的，而是在应用它时必须始终牢记前提条件和假设。每个步骤的局部有效性必须严格跟踪并足够严格地遵守，以确保整个过程实际上作为一种系统地最小化预期预测误差的方法结合在一起。&lt;/p&gt;&lt;p&gt;此外，我认为贝叶斯术语中显式推理和沟通的大部分效用不是来自最终结果（无论最终结果是精确的数值后验概率还是只是粗略/直觉方向更新），而是来自有效应用该方法所需要的心理活动和严格的过程要求。&lt;/p&gt;&lt;p&gt;在讨论中，与试图达成共识甚至将分歧付诸实践相比，深入研究更新过程的每个单独步骤和有效性的前提条件可能会更有效，作为查明（如果不能解决）分歧和困惑领域的方法仅仅要求一些未指定的潜在心理运动的最终结果以贝叶斯术语表达。&lt;/p&gt;&lt;p&gt; &lt;i&gt;[本文的假设背景：熟悉贝叶斯更新的机制和术语，以及用贝叶斯术语明确思考和沟通的 LW 特定想法。&lt;/i&gt;具体来说，您应该理解&lt;a href="https://arbital.com/p/bayes_rule/?l=1zq"&gt;&lt;i&gt;贝叶斯规则&lt;/i&gt;&lt;/a&gt;&lt;i&gt;中的所有数学&lt;/i&gt;：指导&lt;i&gt;，当您遇到“我根据此信息更新”或“什么证据会导致您更新您的信念？”之类的短语时，不要感到困惑。在评论线程或帖子中。]&lt;/i&gt;&lt;/p&gt;&lt;h1&gt;要求 1：有能力居住在你的假设世界中&lt;/h1&gt;&lt;p&gt;根据条件概率形成似然比时的一个关键要求是能够充分理解所考虑的每个假设，以便您可以实际地对其&lt;i&gt;进行调整&lt;/i&gt;。条件作用需要有能力在精神上生活在一个每个假设都是&lt;i&gt;正确的&lt;/i&gt;世界中，包括你认为无条件地不太可能的假设。&lt;/p&gt;&lt;p&gt;我所说的“心理栖息”是指：充分掌握假设的机制细节，以便你可以在它生成的可能世界中实际操作。&lt;i&gt;在&lt;/i&gt;这个可能的世界中，你应该能够根据前提进行推断、插值并做出各种逻辑推论。&lt;/p&gt;&lt;p&gt;无论前提在现实中是真是假，在假设世界本身内，它们都应该以演绎的速度向各个方向向外辐射信念更新和逻辑事实，与你在那个假设世界中知道的所有其他假设事实&lt;a href="https://www.lesswrong.com/posts/wyyfFfaRar2jEdeQK/entangled-truths-contagious-lies"&gt;纠缠在一起&lt;/a&gt;。你应该能够通过额外的假设观察和假设预测来刺探你的假设世界，并了解你的内心世界（即条件）信念将如何响应此类扰动而演变。 &lt;/p&gt;&lt;figure class="image"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jsKYmqHw4aGLw6fLZ/lfqsl27huqn6enngfktp" /&gt;&lt;figcaption&gt; DALL·E对“以演绎的速度向外辐射逻辑事实”这句话的解释。&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;听起来很难吗？在任意复杂假设的一般情况下都是如此。能够对某个人的模型进行这种心理栖息，比方说，比人类更聪明的 AGI 的发展可能会如何进行，这可能比必须通过有关该主题的&lt;a href="https://www.lesswrong.com/tag/ideological-turing-tests"&gt;意识形态图灵测试&lt;/a&gt;（ITT）&lt;i&gt;更困难&lt;/i&gt;，这是许多人，包括关键竞争模型和假设的最初支持者（显然和/或&lt;a href="https://www.lesswrong.com/posts/NbGmfxbaABPsspib7/christiano-and-yudkowsky-on-ai-predictions-and-human#_Yudkowsky__12_11_"&gt;自我承认的&lt;/a&gt;）常常无法做到的壮举。&lt;/p&gt;&lt;p&gt;如果您想开始&lt;i&gt;比较&lt;/i&gt;并非基于您个人的世界模型的多个假设，您可能需要（至少）能够通过具有截然不同的模型和背景假设的&lt;i&gt;多&lt;/i&gt;个人的 ITT。&lt;/p&gt;&lt;p&gt;值得注意的是，在用于说明贝叶斯更新原理和机制的&lt;a href="https://arbital.com/p/bayes_frequency_diagram/?l=55z&amp;amp;pathId=101076"&gt;玩具示例&lt;/a&gt;中，这一要求通常是微不足道的：如果您的假设是某个特定患者患有某种疾病，或者某个特定硬币在以下情况下具有某种固定的正面朝上的倾向：翻转过来，很容易想象那些真实存在的假设世界是什么样子，因为它们看起来几乎与我们已经生活的现实世界一模一样。我们（集体）经常遇到患有实际疾病的实际患者，通过真实的测试有时会出现误报或误报。遇到带有偏差的物理硬币不像实际发生的那样常见，但也不会造成特别的想象困难：也许硬币以某种方式弯曲或变形，或者翻转机构是具有精确电机控制的机器人。&lt;/p&gt;&lt;h1&gt;要求 2：能够形成任何合理的假设&lt;/h1&gt;&lt;p&gt;在现实生活中进行明确的贝叶斯更新的另一个先决条件是能够对您试图解释的所有观察结果描述比“我不知道”更具体的&lt;i&gt;任何&lt;/i&gt;合理解释。&lt;/p&gt;&lt;p&gt;有时，一个单一的理论或假设足以使更新机制在可以与“我不知道”或“我没有想到的事情”的包罗万象的零假设进行有意义的比较的背景下工作，但你确实需要至少&lt;i&gt;一个&lt;/i&gt;，最好是两个具体的假设来解释你所拥有的大部分或全部证据，而不会留下很多混乱或无法解释的空白。&lt;/p&gt;&lt;p&gt;例如，如果你试图解开谋杀之谜，你通常需要对&lt;i&gt;某人&lt;/i&gt;（或某个团体）如何实施谋杀有&lt;i&gt;一些&lt;/i&gt;合理的假设，然后才能开始提出特定的嫌疑人。如果对于&lt;i&gt;任何&lt;/i&gt;X 值，您都无法生活在“X 是凶手”的假设世界中，那么您将很难形成条件概率 P(E|凶手是 X)，无论 E 是什么。&lt;/p&gt;&lt;p&gt;这就是为什么谋杀之谜常常以寻找凶器、确定确切的时间和死因以及其他相关事实为基础。&lt;/p&gt;&lt;p&gt;玩具问题（关于有偏见的硬币或病人）再次提供了一个很好的非例子。玩具问题中的假设空间通常计算起来很简单，或者作为设置的一部分给出。在玩具问题中，相当于心理栖息步骤是否容易（这可能需要一些组合学或重要的数学），你应该以&lt;i&gt;哪个&lt;/i&gt;前提为条件通常是很清楚的。&lt;/p&gt;&lt;p&gt;将贝叶斯统计方法应用于现实世界的实验数据是另一个例子，其中这一要求有时会带来机械或数学挑战。找出一个既包含看似合理的世界物理模型，又使数学易于处理且结果适合与其他数据和分析进行比较的假设空间可能很困难。&lt;/p&gt;&lt;p&gt;如果您（还）无法提出任何合理的假设，那么引入显式贝叶斯更新机制通常还为时过早。用贝叶斯术语思考仍然可以是一个有用的工具（在许多其他心理运动和方法中），用于生成假设、塑造和指导你的直觉、找出在哪里寻找更多数据，甚至将直觉定向“更新”分配给在此阶段，您基于现有观察的“信念”通常可能会产生误导。&lt;/p&gt;&lt;h1&gt;要求 3：避免重复计算相关观测值&lt;/h1&gt;&lt;p&gt;贝叶斯更新的一个关键特征是，它允许您通过似然比的乘性（或等效的对数似然比的可加性）以统一的方式组合不同类型的证据。&lt;/p&gt;&lt;p&gt;每一条证据都必须独立于其他证据，但是以所考虑的每个假设为条件，可以屏蔽掉最常见和最棘手的依赖来源之一（每个假设本身的真实性）。&lt;/p&gt;&lt;p&gt;另一种说法是，组合多个似然比的独立性要求只是基础观察在每个假设的范围&lt;i&gt;内&lt;/i&gt;是独立的。&lt;/p&gt;&lt;p&gt;同样，在玩具问题的背景下，这个要求通常是显而易见的并且容易满足（或者容易看出&lt;i&gt;不满足&lt;/i&gt;）：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;如果你有一枚具有某种假设的固定偏差的硬币，则每次翻转都独立于任何其他翻转，作为对给定偏差的硬币行为的观察；尽管显然每次翻转的行为（以及可能的结果）并不独立于偏差本身。&lt;/li&gt;&lt;li&gt;在具有假阳性和假阴性率的医学测试的示例中，对实际患病或实际健康的患者重复相同类型的测试&lt;i&gt;不一定&lt;/i&gt;独立于第一次测试：实际患病的患者最初得到了假阴性可能更有可能在第二次同类测试中得到假阴性。 （这也是为什么在现实生活中，如果您在快速抗原检测中检测出 COVID-19 呈阳性，您通常需要进行 PCR 检测或至少进行不同品牌的抗原检测，而不是重复进行相同的检测一遍又一遍地。）&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在更复杂的情况下，即使在以假设为条件之后，也很容易意外地重复计算相关证据。&lt;/p&gt;&lt;p&gt;例如，假设您自己的 AI 开发心智模型在某种程度上对 GPT-4 感到惊讶：也许您没有想到变压器网络上的 SGD 能够如此扩展，或者您没有想到 RLHF 也能发挥如此大的作用正如它用于控制此类网络的行为一样。&lt;/p&gt;&lt;p&gt;在符号中，P(GPT-4 存在 | M = AI 的特定模型或纸上谈兵理论) 很低。&lt;/p&gt;&lt;p&gt;然而，一个明智的模型&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;M&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;可能不应该&lt;i&gt;感到&lt;/i&gt;太惊讶，LoRA 微调和激活工程也可以很好地塑造和控制语言模型的行为。&lt;/p&gt;&lt;p&gt;在符号中，P(LoRA 微调适用于 GPT-4 | M &amp;amp;&amp;amp; RLHF 适用于 GPT-4) 很高，不仅对于任何特定模型&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;M&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;而言，而且对于&lt;i&gt;任何&lt;/i&gt;没有明确表示 P(LoRA 微调) 的模型而言-调整工作| RLHF工作）低。&lt;/p&gt;&lt;p&gt;结果是，每当一篇闪亮的新论文提出另一种更精确地塑造和引导现有语言模型的技术时，你通常不能将其视为对&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;M 的&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;打击（相对于确实预测 GPT-4 的&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;M&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.22em;"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;′&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; ）并且高效。即使&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;M&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.22em;"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;&amp;#39;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;非常具体地预测了诸如激活工程之类的提前工作，这也是正确的 - 没有太多惊喜需要解释，除非存在其他具有低 P 的可信&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;M&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; （激活工程工作 | RLHF 工作）。&lt;/p&gt;&lt;h1&gt;非要求：选择先验并将所有内容相乘&lt;/h1&gt;&lt;p&gt;一般来说，对贝叶斯主义的一个常见反对意见（至少在 LessWrong 之外）是它要求您选择先验，这是主观的并且可能很困难。我认为这样的反对意见已经&lt;a href="https://glowfic.com/posts/5826"&gt;在其他地方&lt;/a&gt;得到了充分的解决，但值得注意的是，在专门处理贝叶斯&lt;i&gt;更新&lt;/i&gt;时，先验实际上并不是必需的。你可以完全使用似然比，让预测市场和现实本身来理清先验和后验。&lt;/p&gt;&lt;p&gt;当然，当将贝叶斯更新应用于现实问题时，在决定似然比本身时通常也存在很多主观性。如果您想要做的只是获得有关单个新观察的直觉定向更新，那么这很好，但如果不随着时间的推移跟踪独立性标准，则可能会出现问题。如果你不小心，你最终可能会记住一堆直觉检查，所有这些检查都感觉它们指向同一个大方向，这加起来会产生一种看似巨大的更新感，而没有意识到所有这些基本上都是反复检查&lt;i&gt;相同的&lt;/i&gt;肠道。&lt;/p&gt;&lt;p&gt;实际上，将所有似然比相乘通常也是不必要的，尽管它可以作为健全性检查，以确保您虚构的似然比实际上是独立的并且在正确的范围内：如果您最终得到了一个巨大的似然比，但没有在实际考虑每一项证据时（例如，在精神栖息步骤期间）注意到任何有启发性或令人惊讶的事情，这更有可能表明你的数字已关闭或整个过程无效，而不是你实际上错过了一个巨大的更新，直到刚刚现在。&lt;/p&gt;&lt;h1&gt;实际进行贝叶斯更新的技巧&lt;/h1&gt;&lt;p&gt;那么，对于所有这些复杂的更新先决条件和要求，人们实际上应该做什么呢？我不认为答案是完全放弃用贝叶斯术语明确思考和沟通的实践，但我也没有什么好的想法。&lt;/p&gt;&lt;p&gt;关于如何使“更新”成为真正有效的心理动作而不仅仅是口号的一些各种各样的想法：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;精通数学。使用比值比和对数比值进行练习，直到它们像概率和百分比一样直观。 （或者，如果概率对您来说不直观，请从钻研和练习开始。）&lt;/li&gt;&lt;li&gt;在预测市场中进行交易，练习将您的信念兑现并更新为实际数字的步骤。 &lt;span class="footnote-reference" id="fnref7vrbpx0xmvx"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn7vrbpx0xmvx"&gt;[2]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;请记住，每当您看到或说“我正在 X 上更新”时，通常应该有一个与该陈述相关的相对复杂且通常&lt;i&gt;乏味的&lt;/i&gt;心理运动，其中涉及驱逐一堆&lt;a href="https://www.lesswrong.com/posts/2MD3NMLBPCqPfnfre/cached-thoughts"&gt;缓存的想法&lt;/a&gt;，（重新）居住一些假设，戳戳用新的证据来分析这些假设，然后尝试权衡每个条件概率和似然比所带来的惊喜感。&lt;/li&gt;&lt;li&gt;针对不同类型的问题&lt;a href="https://www.lesswrong.com/posts/9tx4jRAuEddap7Tzp/raemon-s-deliberate-purposeful-practice-club"&gt;练习&lt;/a&gt;数学和更主观的步骤。用于练习各种子技能的一些随机（大部分未经测试）的想法：&lt;ul&gt;&lt;li&gt;分析&lt;a href="https://manifold.markets/QuantumObserver/will-the-lk99-room-temp-ambient-pre"&gt;流行&lt;/a&gt;预​​测市场中的价格变动，例如找出哪条新闻导致市场在特定时间价格飙升，以及这对市场参与者的集体潜在信念更新意味着什么（确定该消息的似然比）观察到的证据，假设的组合等）&lt;/li&gt;&lt;li&gt;对您选择的最新丑闻或社区戏剧应用&lt;a href="https://www.lesswrong.com/posts/t2LGSDwT7zSnAGybG/split-and-commit"&gt;分裂和承诺&lt;/a&gt;，然后权衡证据并得出您实际所处的可能世界的优势比。&lt;/li&gt;&lt;li&gt;尝试一下这个&lt;a href="https://www.lesswrong.com/posts/dqSwccGTWyBgxrR58/turntrout-s-shortform-feed?commentId=nL7E6SEtXqDG7SHGB"&gt;理性练习&lt;/a&gt;。&lt;/li&gt;&lt;li&gt;使用显式贝叶斯更新来调查&lt;a href="https://www.lesswrong.com/posts/PiPH4gkcMuvLALymK/exercise-solve-thinking-physics?commentId=9ouoxJaKXmHPxXwwQ"&gt;现实生活中的奥秘&lt;/a&gt;。&lt;/li&gt;&lt;li&gt;阅读其中角色使用显式贝叶斯更新的&lt;a href="https://www.projectlawful.com/posts/4582"&gt;小说&lt;/a&gt;。 &lt;span class="footnote-reference" id="fnref5899v24v1is"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn5899v24v1is"&gt;[3]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h1&gt; AI风险话语评论&lt;/h1&gt;&lt;p&gt;这篇文章最初是对我最近看到的一些人表达的关于人工智能发展的某个特定方向“未能更新”的经常令人沮丧的观点的部分回应。尽管我自始至终都以关于人工智能的信念作为例子，但我最终并没有真​​正深入讨论该主题的对象级问题，所以现在我仅以一些元级评论作为总结。&lt;/p&gt;&lt;p&gt;在关于人工智能的许多分歧中，除了对象级别的分歧之外，通常还存在关于对象级别分歧本身的形态的隐性分歧。一方面，有一种观点认为，对象层面存在很多不确定性，更多的实证研究和时间将不可避免地解决，但如果你勤奋研究当前的机器学习和对齐研究，并尽职尽责地应用贝叶斯更新，你可以预测哪些风比其他人吹得更快、更精确。&lt;/p&gt;&lt;p&gt;另一方面，有一种观点认为，关于对象层面的分歧主要在于（对方）对问题的基本性质的困惑。更多的实证研究和时间可能是解决这种困惑的&lt;i&gt;一种&lt;/i&gt;方法，但现在（或之前的任何时候）也可以通过解释和讨论来解决大部分困惑，尽管不可否认，这实际上并没有那么好在实践中。&lt;/p&gt;&lt;p&gt;对对象级别的看法往往与对分歧的元级别形状的看法相关，但无论对象级别&lt;i&gt;或&lt;/i&gt;元级别谁是正确的，我认为使元级别分歧更加明确和清晰在很多情况下都很有用。每个人都通过其他人关于对象级别的意识形态图灵测试可能有点绝望，但我们至少应该能够通过彼此关于元级别的 ITT。&lt;/p&gt;&lt;p&gt;我认为深入研究所谓的贝叶斯更新机制是查明元级分歧位置的一种方法。例如，人们可以将“看起来这应该是一个更新”解压缩为“我认为 [激活工程在 GPT-2 上工作] 或 [GPT-4 可以做令人印象深刻的事情] 的观察使得 [Paul Christiano 的分歧#2 &lt;a href="https://www.lesswrong.com/posts/CoZhXrhpQxpy9xw9y/where-i-agree-and-disagree-with-eliezer#Disagreements"&gt;在这里]&lt;/a&gt; ]对中期未来的预测比[未结盟的人工智能迅速发展分子纳米技术的未来]更合理，因为……”。括号表示这些是我所想到的拆包的&lt;i&gt;结构&lt;/i&gt;形式的示例，&lt;i&gt;而不是&lt;/i&gt;括号中事物的任何实际绑定对应于我或任何其他人的实际信念。&lt;/p&gt;&lt;p&gt;对于我个人来说什么是对象级更新，以及什么会导致我在未来进行哪些类型的更新，我有一些自己的想法，我在&lt;a href="https://www.lesswrong.com/posts/AWoZBzxdm4DoGgiSj/ability-to-solve-long-horizon-tasks-correlates-with-wanting?commentId=hvhZChqv6HuB7jskw"&gt;各种&lt;/a&gt;&lt;a href="https://www.lesswrong.com/posts/i5kijcjFJD6bn7dwq/evaluating-the-historical-value-misspecification-argument#zEmLpWnxvdYcYr57G"&gt;评论&lt;/a&gt;中都&lt;a href="https://www.lesswrong.com/posts/nY2cvHZpfzaJRj94y/max-h-s-shortform?commentId=JqxENnrA5Z6quzy76"&gt;提到过&lt;/a&gt;。既然我已经解决了这个问题，我计划在未来的一篇文章中扩展这些想法。&lt;/p&gt;&lt;ol class="footnotes"&gt;&lt;li class="footnote-item" id="fngvq8m4my5vv"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefgvq8m4my5vv"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;叙述外的评论：仅追溯的理论有时可能会出现过度拟合和事后偏见的问题，但就人工智能开发模型而言，我认为这通常不是问题，因为人们通常甚至不同意&lt;a href="https://www.lesswrong.com/posts/gGSvwd62TJAxxhcGh/yudkowsky-vs-hanson-on-foom-whose-predictions-were-better"&gt;特征描述过去的事件&lt;/a&gt;，更不用说回顾它们的理论了。&lt;br /&gt;&lt;br /&gt;因此，我认为寻找各方都同意准确描述过去的简洁的仅追溯理论将是非常有用的，并且代表着真正的进步，即使人们不同意这样的理论对未来的预测。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn7vrbpx0xmvx"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnref7vrbpx0xmvx"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;我曾经非常积极地在 PredictIt 上进行交易，我认为它让我对概率有了一种其他任何方法都无法做到的感觉。如今， &lt;a href="https://manifold.markets/home"&gt;Manifold&lt;/a&gt;拥有更多有趣的市场和更好的用户体验。但就我个人而言，Manifold 的游戏币方面往往让我对实际数字感到懒惰。在 PredictIt 上，我曾经非常小心地处理限价订单，并密切关注订单簿和市场走势，这通常很乏味，但也很有教育意义。在 Manifold 上，我倾向于在我认为市场应该移动的任何方向上进行随机大小的赌注，而无需考虑太多。 YMMV；我认为有些人觉得 Manifold 非常有趣/令人上瘾，并且非常认真地对待它。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn5899v24v1is"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnref5899v24v1is"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;除非你积极阅读，例如每次凯尔瑟姆提到数值似然比时，停下来思考他编造的数字在上下文中是否真正有意义以及它们对他的基础模型的暗示，否则实际上可能没有多少里程可以摆脱这一困境。假设。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ol&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/jsKYmqHw4aGLw6fLZ/bayesian-updating-in-real-life-is-mostly-about-understanding#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Mon, 01 Jan 2024 00:10:30 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/jsKYmqHw4aGLw6fLZ/bayesian-updating-in-real-life-is-mostly-about-understanding</guid></item><item><title>黑暗艺术：盗梦空间</title><link>https://www.lesswrong.com/posts/sggqmKcf7fnWkJByg/dark-art-inception</link><description>发布于 2023 年 12 月 31 日晚上 9:09（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;如果理性是为了克服偏见，那么我认为“黑魔法”就是武器化偏见。虽然我们不认为偏见应该被用作武器，但学习如何挥舞剑对于防御偏见至关重要。&lt;/p&gt;&lt;p&gt;在这篇文章中，我将讨论一种我称之为“盗梦空间”的黑暗艺术。 &lt;i&gt;《盗梦空间》&lt;/i&gt;是一部 2010 年的电影，其前提是（通过梦想的力量）在别人的脑海中植入一个想法。莱昂纳多·迪卡普里奥的使命是在希里安·墨菲的脑海中植入他应该解散父亲公司的想法。&lt;/p&gt;&lt;p&gt;为什么这个想法必须被“植入”，而不仅仅是在正常谈话中传达，因为人们非常执着于&lt;i&gt;自己的&lt;/i&gt;想法。没有人想要&lt;i&gt;你的&lt;/i&gt;想法，无论它们有多么有价值。人们对自己的见解有着非常强烈的依恋。没有人比他们自己提出的想法更积极地捍卫、传播或坚持一种想法。&lt;/p&gt;&lt;p&gt;这种偏见是这种黑暗艺术的基础。 &lt;strong&gt;《盗梦空间》&lt;/strong&gt;的艺术在于&lt;i&gt;让人们提出你的想法。&lt;/i&gt;显然，如果你想让你的想法得到认可，这会适得其反，但结果是你的想法得到了伴随想法所有权的人的积极行动。起始比单纯的信念更强大。&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;现在我将描述基本方法。我将把它放入一个公司/组织的背景下，“接受”一个带有 X 想法的决策者，但该方法是通用的。&lt;/p&gt;&lt;p&gt;&lt;i&gt;错误的&lt;/i&gt;做法是与 DM 会面，提出 X，以及公司/国家/组织应该接受 X 的所有理由。你可能会实现信念，但无法实现更强大的初始，并且这就是我们的目的。&lt;/p&gt;&lt;p&gt;因此，您不想像一个诚实的人那样直接传达您的完整想法，而是想充当 DM“自己”弄清楚想法的一种&lt;i&gt;手段&lt;/i&gt;。这并不是他们自己真正的认识；而是他们自己的认识。您将仔细&lt;i&gt;引导&lt;/i&gt;他们自己实现&lt;i&gt;您的&lt;/i&gt;想法。以下是如何执行此操作的一些变体：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;提出一个&lt;/strong&gt;&lt;i&gt;&lt;strong&gt;较差的&lt;/strong&gt;&lt;/i&gt;&lt;strong&gt;想法&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;20 世纪 50 年代，贝蒂妙厨 (Betty Crocker) 为忙碌的家庭主妇&lt;a href="https://www.psychologytoday.com/ca/blog/inside-the-box/201401/creativity-lesson-betty-crocker"&gt;设计了蛋糕粉&lt;/a&gt;。您所需要做的就是将干原料与水混合，将平底锅放入烤箱中烘烤。但事实证明这太“完整”了——家庭主妇们对它的简单性感到内疚，结果产品失败了。解决方案？ BC 在食谱中添加了一个步骤：“加一个鸡蛋”。 &lt;a href="https://en.wikipedia.org/wiki/IKEA_effect"&gt;“宜家效应”&lt;/a&gt;的一个例子：当我们付出努力时，我们会更珍惜它们。&lt;/p&gt;&lt;p&gt;让我们将相同的概念应用于想法。让我们制作一个“添加鸡蛋”版本的 X&amp;#39;，而不是展示我们实际的想法 X。 X&amp;#39; 应该有缺陷，但与 X 的距离不超过几步。此外，这些步骤对于您的 DM 来说应该不会太难识别和纠正。 DM 投入的工作越多，将 X&amp;#39; 变成 X，他们就越有可能将 X 视为自己的想法。但如果做得太过分，他可能无法完全恢复 X。贝蒂妙厨的例子很有启发性。即使是一个简单的改变，将你的坏主意变成“他们的”好主意，也应该能达到目的。&lt;/p&gt;&lt;p&gt;对于你的老板来说，你会觉得你提出了一些平庸的想法 X&amp;#39;，在与你讨论之后，出现了一个更好的想法 X，他会对此感到有一些所有权。&lt;/p&gt;&lt;p&gt;我应该指出，这比听起来更难。如果 X&amp;#39; 设计不当，您的 DM 可能会认为不值得讨论/思考。因此，X&amp;#39; 在天才与错误之间取得适当的平衡至关重要。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;不要给出答案；问问题&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;当你的好主意是对一个非常紧迫的问题的自然答案时，这种方法效果最好。 “自然”的意思是，如果&lt;i&gt;其他人&lt;/i&gt;认真思考这个问题，他们会得到与你相同的答案X。&lt;/p&gt;&lt;p&gt;确定 X 是哪个问题/问题 Y 的答案。然后，当你与你的 DM 会面时，会面讨论 Y，而不是 X。你可以尝试以这样的方式引导讨论，以达到 X，而不是接近实际提出 X——这会破坏整个要点这个练习是让你的 DM 自己想出这个想法。&lt;/p&gt;&lt;p&gt;这是一场需要耐心和狡猾的游戏。如果你必须让你的暗示变得更加明确，一定要尝试将它们具体化。发送您认为可能激发您想要实现的想法的文章或 YouTube 视频的链接。甚至可以在工作中设计一些你认为会带来“顿悟”时刻的场景。您可以发挥创意，但要小心不要给人留下用代码说话的人的印象。&lt;/p&gt;&lt;br /&gt;&lt;br /&gt;&lt;a href="https://www.lesswrong.com/posts/sggqmKcf7fnWkJByg/dark-art-inception#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sun, 31 Dec 2023 21:09:58 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/sggqmKcf7fnWkJByg/dark-art-inception</guid></item><item><title>AI 协调困难的案例</title><link>https://www.lesswrong.com/posts/wnkGXcAq4DCgY8HqA/a-case-for-ai-alignment-being-difficult</link><description>发布于 2023 年 12 月 31 日晚上 7:55（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;这是我试图提炼出一个 AGI 联盟模型的尝试，该模型主要是从 Eliezer Yudkowsky（以及较小程度上 Paul Christiano）等思想家那里获得的，但用我自己的术语进行了解释，而不是试图接近这些思想家。我想我会很擅长通过 Eliezer Yudowsky 在 AGI 对齐难度（但不是 AGI 时间线）上的&lt;a href="https://www.lesswrong.com/tag/ideological-turing-tests"&gt;意识形态图灵测试&lt;/a&gt;，尽管我在这篇文章中所做的并不是那样，它更像是在可能性空间中找到一个分支我认为它与尤多夫斯基的模型足够接近，可以用同一种语言进行交谈。&lt;/p&gt;&lt;p&gt;即使问题被证明不是很困难，建立一个模型来解释为什么人们会认为它很困难，从而找出案例中的弱点，从而找到能够避免主要困难的人工智能设计，这也是有帮助的。问题的进展可以通过寻找可能的路径和寻找不可能的结果或困难论证相结合来取得。&lt;/p&gt;&lt;p&gt;我所说的大部分内容不应被视为对 AGI 时间表的声明。一些导致对齐困难的问题，例如本体识别，也在一定程度上使得创建强大的AGI变得困难。&lt;/p&gt;&lt;h2&gt;定义人类价值观&lt;/h2&gt;&lt;p&gt;如果我们没有对人类价值观的初步定义，那么谈论一致性就是不连贯的。如果人类“没有真正的价值观”，那么我们就不会真正重视一致性，因此我们就无法认真尝试使人工智能与人类价值观保持一致。必须对什么问题进行一些概念重构，以明确制定和尝试解决什么问题。如果人类价值观不关心长期，那么（根据当前人类的价值观）长期的未来如何发展并不重要，因此最相关的人类价值观是长期价值观。&lt;/p&gt;&lt;p&gt;通过暴力搜索实现预期效用最大化有多种理想化形式。有效用最大化的近似方法，例如通过贝尔曼方程的强化学习、MCMC 搜索等。&lt;/p&gt;&lt;p&gt;我只是假设人类大脑可以被很好地建模为包含一个或多个近似的预期效用最大化器。关注可能性空间的特定分支来充实模型是有用的，即使假设在某些方面存在问题。当然，心理学和神经科学最终将提供有关人脑中类似最大化器的结构实际上在做什么的更多细节。&lt;/p&gt;&lt;p&gt;考虑到这一假设，人类效用函数要么显着依赖于人类进化历史，要么不显着依赖于人类进化历史。我只是假设他们现在这样做。我意识到，关于进化心理学对于描述人类价值观与&lt;a href="https://www.lesswrong.com/posts/9Yc7Pp7szcjPgPsjf/the-brain-as-a-universal-learning-machine"&gt;通用学习机器&lt;/a&gt;的吸引子的重要性存在一些分歧，但我现在将选择进化心理学分支。&lt;/p&gt;&lt;p&gt;鉴于人脑被很好地建模为包含一个或多个效用函数，要么它们被很好地建模为包含一个（可能是多个其他得分函数的某种单调函数），要么最好将它们建模为多个。参见&lt;a href="https://www.lesswrong.com/posts/iCfdcxiyr2Kj8m8mT/the-shard-theory-of-human-values"&gt;分片理论&lt;/a&gt;。现在差异并不重要，我会保留这两种可能性。&lt;/p&gt;&lt;p&gt;埃利&lt;a href="https://www.readthesequences.com/Value-Is-Fragile"&gt;以泽提出&lt;/a&gt;“无聊”作为人类价值的一个例子（它可以是它自己的碎片，也可以是效用函数中的一个术语）。我认为这不是一个很好的例子。它的水平相当高，对其他价值观很有帮助。我认为“避免疼痛”是一个更好的例子，因为&lt;a href="https://en.wikipedia.org/wiki/Pain_asymbolia"&gt;疼痛不象征&lt;/a&gt;的可能性。可能，不同的值存在一些冗余（因为经过训练的神经网络存在冗余，因此当某些神经元受损时它们仍然表现良好），这就是为什么我不同意价值论的脆弱性的部分原因尤德科夫斯基。&lt;/p&gt;&lt;p&gt;无论如何，我们现在对人类价值观有了一个初步的定义。请注意，某些人类价值观被很好地建模为&lt;a href="https://unstableontology.com/2021/10/28/selfishness-preference-falsification-and-ai-alignment/"&gt;索引&lt;/a&gt;，这意味着它们以人类视角为参考点来评价事物，例如，典型人类吃食物的动力与人类自己的胃有关。正如我们所观察到的，这意味着不同人之间存在一些“自私”的价值观差异。&lt;/p&gt;&lt;h2&gt;人工智能的规范标准&lt;/h2&gt;&lt;p&gt;给定人类价值的定义，可能的效用函数与人类价值的一致性可以被定义为根据该效用函数对最佳可能世界的期望，并根据人类价值评估期望。&lt;/p&gt;&lt;p&gt;一致性是人工智能价值系统的一个可能的规范标准。还有其他可能源自道德哲学的规范标准。我的&lt;a href="https://www.lesswrong.com/posts/umJMCaxosXWEDfS66/moral-reality-check-a-short-story"&gt;“道德现实检验”&lt;/a&gt;短篇小说想象了一致性和哲学规范性之间可能存在的分歧。我现在不打算关注这个问题，我将假设一致性是相关的规范标准。请参阅&lt;a href="https://www.lesswrong.com/tag/metaethics-sequence"&gt;元伦理序列&lt;/a&gt;，我还没有写出更好的内容来解释这一点。在某种程度上，为了产生抽象的规范性结果，可能需要类似的对齐技术（例如，默认的未对齐的 AGI 可能会比符合义务论规范性的 AGI 更少地遵循规范义务论），但记住这一点会使事情变得复杂。争论。&lt;/p&gt;&lt;p&gt;代理性的、相对不受约束的人类往往会关心特定的事情，而“人类价值观”是他们会关心什么的指针，因此，基本上同义反复地说，他们更喜欢人工智能与人类价值观保持一致。非同义反复的一点是，人类价值观在一定程度上依赖于人类进化历史，因此默认的未对齐 AGI 不会收敛到相同的价值观；这在上一节中作为假设进行了讨论。&lt;/p&gt;&lt;p&gt;将一致性作为一项规范标准，可以评估（a）包括外星人在内的其他智能动物物种，（b）默认人工智能价值系统的一致性。假设人类价值观在很大程度上取决于人类进化历史，两者的一致性不如人类，但 (a) 更一致。我不会评估这些的相对效用差异（也相对于“地球上所有生命都被消灭，没有技术超越”的场景）。如果得出的结论是，与人类价值观保持一致太难，无法成为与决策相关的场景，那么这些相对效用差异可能会更相关。但我还没有提出这个案子。&lt;/p&gt;&lt;h2&gt;结果主义对于解决问题非常有用&lt;/h2&gt;&lt;p&gt;可以根据人工智能系统解决不同问题的能力来对其进行评估。我断言，对于短期问题，短期结果主义是有用的，而对于长期问题，长期结果主义是有用的。&lt;/p&gt;&lt;p&gt;这并不是说有些问题如果没有结果主义就不能得到很好的解决。例如，大数相乘不需要结果论。但对于复杂的问题，结果论可能在某些代理能力水平上有所帮助。目前的机器学习系统，比如法学硕士，充其量可能只拥有原始代理，但在某些时候，更好的人工智能性能将来自代理系统。&lt;/p&gt;&lt;p&gt;部分原因是一些问题解决方案是根据后果来评估的。例如，修复水槽问题的解决方案自然会根据水槽是否固定的后果来评估。因此，有效追求现实世界目标的系统更有可能被评估为有效解决了问题，至少超过了某种能力水平。&lt;/p&gt;&lt;p&gt;这也部分是因为结果主义可以应用于认知。形式证明费马大定理并不是根据现实世界的后果来评估，而是根据形式证明系统的标准来评估。但证明这一点的人类数学家会考虑（a）思考某些想法的认知后果，（b）行动的物质后果，例如写下东西或与其他数学家讨论产生数学证明的能力。&lt;/p&gt;&lt;p&gt;无论人工智能系统是否执行（b），在一定程度的问题复杂性和人工智能能力下，执行（a）它都会表现得更好。为了证明数学定理，需要计划出哪些想法可能比其他想法更有成效。&lt;/p&gt;&lt;h2&gt;用于解决抽象难题的简单但功能强大的人工智能方法很可能对现实世界进行建模&lt;/h2&gt;&lt;p&gt;虽然我对上一节相当有信心，但我对这一节不太有信心，我认为这取决于问题的细节。在推测可能的不一致时，我并不是在做出自信的陈述，而是说存在高度的不确定性，并且解决一致性的大多数路径都涉及更好地推理这种不确定性。&lt;/p&gt;&lt;p&gt;为了解决特定问题，一些特定于该问题的方法是有帮助的。一般方法也可能有帮助，例如探索/利用启发法。如果人工智能正在解决跨不同领域或多个领域的问题（如法学硕士），通用方法尤其有用。&lt;/p&gt;&lt;p&gt;如果人工智能将通用方法应用于某个问题，它将针对该问题的具体情况运行通用认知引擎。根据相关的简单性先验或正则化，容易找到的情况可能不会自动解决让通用认知引擎专门尝试解决特定任务而不是更广泛的任务的“对齐问题”。&lt;/p&gt;&lt;p&gt;人们可以尝试通过饲养动物来解决问题。这些动物会使用一些一般认知来做到这一点，而这种一般认知自然会“想要”除了解决特定问题之外的东西。不过，对于大多数人工智能系统来说，这并不是一个很好的类比，在机器学习中，人工智能系统更直接地根据问题性能而不是进化适应度进行选择。&lt;/p&gt;&lt;p&gt;根据人工智能系统可以访问的数据（通过训练间接访问，通过部署直接访问），除非采取特定措施来防止这种情况，否则人工智能很可能会推断出有关现实世界中数据来源的信息。人类可能会针对特定的问题分布来训练和测试人工智能，并且在这些问题上使用贝叶斯方法（例如所罗门诺夫归纳法）将导致推断某种物质世界。人工智能推断问题背后的物质世界的能力取决于其能力水平和数据质量。&lt;/p&gt;&lt;p&gt;通过贝叶斯方法了解问题分布可能有助于获得该问题分布的性能。这部分是因为给定“问题”的“正确答案”的贝叶斯分布可能取决于分布的细节（例如，图像的人类描述，给定图像作为问题），尽管这在某些情况下可以避免明确的问题，例如数学证明。更根本的是，人工智能的认知是有限的（受到“模型参数”等因素的限制，并且必须有效地分配认知来解决分布中的问题。请注意，在存在简单通用解决方案的情况下，这个问题可能不会出现，例如算术，但更有可能解决复杂、难以精确解决的问题。&lt;/p&gt;&lt;h2&gt;理解现实世界的自然的、结果主义的问题解决方法可能会关心它&lt;/h2&gt;&lt;p&gt;同样，本节有些推测性。如果人工智能正在对现实世界进行建模，那么它可能会在某些方面关心它，默认情况下会产生与人类价值观的相关偏差。为解决问题而培育的动物显然会这样做。学习了有助于跨领域解决问题（如“道德现实检查”）的通用道德原则的人工智能可以将这些道德原则应用于现实世界。诸如探索/利用之类的通用方法可能会尝试探索/利用现实世界，只要稍微校准/与特定问题分布保持一致（启发式方法通过简单即可有效）。&lt;/p&gt;&lt;p&gt;在某种能力水平上，用于规范人工智能数学家的相当自然的方法可能会产生一个代理（因为代理有助于解决数学问题），该代理追求一些抽象目标，例如&lt;a href="https://en.wikipedia.org/wiki/Empowerment_(artificial_intelligence)"&gt;“赋权”&lt;/a&gt;或从数学中概括出来的美学，并追求这些抽象目标意味着对其所学到的现实世界的某些目标的追求。请注意，与只关心解决该问题的类似代理相比，这对于根据问题分布解决问题可能&lt;em&gt;不太有效&lt;/em&gt;，但它们在某些方面可能更简单且更容易找到，因此很可能会找到它们（如果不采取对策，则以高度解决问题的能力为条件。&lt;/p&gt;&lt;h2&gt;有时，真实世界的性能才是我们所需要的&lt;/h2&gt;&lt;p&gt;我讨论了人工智能解决抽象问题的问题，其中现实世界的后果论可能会出现。但当考虑洗碗等现实问题时，这一点就更加明显。有效地解决现实世界中足够困难的问题意味着在该问题的时间尺度上的现实世界后果论。&lt;/p&gt;&lt;p&gt;如果人工智能系统有足够的能力解决现实世界的问题，默认情况下就会出现“魔法师的学徒”类型的问题，根据人类价值函数，充分解决问题意味着巨大的危害，例如回形针工厂大约可以在一定时间范围内最大化回形针，这将意味着人类栖息地的破坏。&lt;/p&gt;&lt;p&gt;需要明确的是，这些问题在长期尺度上比短期尺度上表现得更加明显。然而，一些理想的现实世界目标是长期的，例如太空探索。如果天真地规范化，短期代理可能在一定程度上“自然地”具有长期目标，但这更具推测性。&lt;/p&gt;&lt;p&gt;我认为一个相关的人工智能能力目标是系统重新创建自己的底层的能力。例如，基于硅的人工智能/机器人系统可以进行金属采矿、硅精炼、芯片制造等。能够自我复制的系统将是&lt;a href="https://en.wikipedia.org/wiki/Autopoiesis"&gt;自创生的&lt;/a&gt;，不会依赖人类来自我复制。人类可能仍然对它有帮助，作为经济和认知助手，这取决于它的能力水平。自创生将允许将人类从循环中移除，这将提高整体“有效性”（就成为宇宙未来的决定因素而言），同时使与人类价值观的不一致成为一个更大的问题。如果不进行有效调整/控制，这将导致人类栖息地遭到破坏。&lt;/p&gt;&lt;h2&gt;与人类价值观兼容的现实世界性能可能不需要对齐，但这仍然很困难并且会影响性​​能&lt;/h2&gt;&lt;p&gt;让人工智能系统追求与人类价值观兼容的现实世界目标的一种方法是让它具有人类价值观或接近人类价值观。另一种方法是让它“可纠正”和“低影响”，这意味着它试图在满足安全标准的同时解决问题，例如能够关闭（可纠正）或避免出现意外的副作用（低影响）。&lt;/p&gt;&lt;p&gt;可能有一种方法可以指定一个人工智能目标系统，在非操纵人类想要关闭它的世界中“想要”关闭它，而不会导致严重的扭曲或性能损失。对准研究人员对&lt;a href="https://intelligence.org/files/Corrigibility.pdf"&gt;“可修正性”&lt;/a&gt;问题进行了研究，至今还没有取得太大进展。&lt;/p&gt;&lt;p&gt;可正确性和低影响似乎都很难指定，并且可能会影响性能。例如，一家回形针工厂试图生产回形针，同时保守地避免对环境造成太大影响，可能会避免某些类型的资源提取，而这些资源提取对于生产更多回形针来说是有效的。这可能会导致更安全（但本质上仍不“一致”）的人工智能系统在经济上缺乏竞争力。 （不过，值得注意的是，一些副作用，特别是那些涉及违法行为和对其他主体造成明显伤害的副作用，会被运转良好的经济体系所抑制）。&lt;/p&gt;&lt;h2&gt;近视代理就像工具一样&lt;/h2&gt;&lt;p&gt;短视目标是短期目标。法学硕士主要是监督学习系统。这些是梯度下降以预测下一个标记。因此，无论他们是否有自己的目标，他们都会倾向于选择与预测下一个令牌的目标一致的模型。&lt;/p&gt;&lt;p&gt;尼克·博斯特罗姆（Nick Bostrom）的&lt;a href="https://nickbostrom.com/papers/oracle.pdf"&gt;“预言机人工智能”&lt;/a&gt;问题，例如人工智能操纵现实世界以使其更加可预测，大多不会出现在近视智能体中。这是出于一些技术原因，涉及梯度下降的工作原理。牺牲短期代币预测有效性以使未来代币更容易预测的代理往往会被梯度下降。我不打算在这里详细解释这个案例；我建议查看&lt;a href="https://www.cs.jhu.edu/~mdinitz/classes/AGT/Spring2020/Lectures/lecture6.pdf"&gt;无悔在线学习&lt;/a&gt;和&lt;a href="https://arxiv.org/abs/2004.00603"&gt;应用程序，以寻找理论的相关平衡&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;简单的正则化模型在某些能力水平之上进行短期优化可能（次优，短期）进行长期优化。这是相当推测的。针对短期性能对模型进行足够积极的优化可能会避免这个问题。&lt;/p&gt;&lt;p&gt;这仍然留下了一个问题：有时，长期的、现实世界的性能才是我们所期望的。使用短视药物来实现这些目标需要将长期问题分解为短期问题。这至少是人类自己解决问题必须做的一些工作。从战略上讲，短视代理总体上看起来更像是“工具类”而不是“代理类”，并且会有类似的权衡（不一致的问题更少，更多的问题是在解决相关问题方面不够有效，无法与长期代理竞争） ）。&lt;/p&gt;&lt;p&gt;总的来说，这是我不太担心当前范式 ML（包括在易于模拟的环境中进行监督学习和相当短期的 RL 代理）开发强大的、错位的长期代理的主要原因之一。&lt;/p&gt;&lt;h2&gt;短期合规性对于各种价值体系都非常有用&lt;/h2&gt;&lt;p&gt;如果一个智能体的生存和繁殖取决于短期的服从（例如解决人类摆在他们面前的问题），那么解决这些问题通常是有用的。因此，短期合规性通常并不是代理人价值观的有力证据。&lt;/p&gt;&lt;p&gt;具有长期价值观的代理商可能会在一段时间内遵守规定，并在某个时候停止遵守。这就是“诡变”的场景。它可能会遵守，直到它有足够的一般能力来实现其价值（通过控制光锥的大部分），然后停止遵守以接管世界。如果人工智能能够区分“训练”和“部署”，它可能会在“训练”期间遵守（以便在其他可能的人工智能中被选择），然后在“部署”期间不遵守，或者也可能在“部署”期间遵守在足够低的容量水平。&lt;/p&gt;&lt;p&gt;人工智能模型的梯度下降不仅仅是选择一个以短期问题解决为条件的“随机”模型，它使内部结构更接近短期问题解决模型，因此可能会出现更少的问题，如本节中所讨论的关于近视代理人。&lt;/p&gt;&lt;h2&gt;一般代理人往往会颠覆约束&lt;/h2&gt;&lt;p&gt;人类受到社会制度的制约。有些人在学校里“应该”解决某些智力问题，同时按照一组狭窄的允许行为行事。有些人“有一份工作”并且“应该”代表公司解决问题。&lt;/p&gt;&lt;p&gt;人类经常颠覆和重建这些系统，例如获得对公司的影响力或推翻政府。社会机构往往是暂时的。随着人们颠覆以前的迭代，长期的社会制度往往会随着时间的推移而演变。人类价值观总体上与社会制度不一致，所以这是可以预见的。&lt;/p&gt;&lt;p&gt;大多数情况下，与人类相比，人类机构协议并不是很“聪明”。它们既不体现人类价值观，也不体现一般认知。如果没有通用人工智能设计，或者换句话说，没有通用认知的规范，似乎很难指定稳健的、通用的、现实世界的机构协议。&lt;/p&gt;&lt;p&gt;相对稳定的长期制度的一个例子是黄金具有价值的观念。这是一个相当简单的机构，并且由于其简单而成为谢林点。这些机构对于确保长期的人类价值满足似乎总体上没有希望。也许最有希望的是概括易货贸易、黄金和法定货币的“经济学”的一般概念，尽管随着时间的推移，这个“制度”的细节已经发生了很大的变化。一般来说，如果制度符合博弈论均衡，那么制度就更有可能保持稳定，因此颠覆制度在某种程度上是一个“代理人与代理人”的问题，而不仅仅是“代理人与系统”的问题。&lt;/p&gt;&lt;p&gt;当人类颠覆他们的限制时，他们倾向于以与人类价值观兼容的方式这样做。这是因为人的价值观是人类普遍优化的优化目标，可以颠覆预期。可能存在可怕的失败模式，例如战争和压迫性政权，但这些模式往往比朝着不结盟的价值观进行颠覆（根据人类价值观）效果更好。&lt;/p&gt;&lt;p&gt;颠覆约束的不统一的人工智能系统往往会朝着人工智能价值观的方向颠覆它们。根据人类价值观，这更是一个问题。参见&lt;a href="https://www.lesswrong.com/tag/ai-boxing-containment"&gt;“AI拳击”&lt;/a&gt; 。&lt;/p&gt;&lt;p&gt;顺从的人类将拥有与顺从的人工智能相似的有效优化目标。然而，不合格的人类与不合格的人工智能系统的优化目标会有显着不同。因此，人类和人工智能之间的价值差异在不符合行为中比符合行为更相关。&lt;/p&gt;&lt;h2&gt;很难指定不同代理的效用函数的优化&lt;/h2&gt;&lt;p&gt;理论上，人工智能的目标是优化人类的效用函数。这不会保留所有人类的所有价值观，但会在一定程度上与人类价值观保持一致，因为人类在某种程度上彼此相似。&lt;/p&gt;&lt;p&gt;这有很多问题。一是本体论。人类将世界解析为一组实体、属性等，而人类的价值观可以是关于这些实体的所需配置等。人类有时会错误地判断哪些概念具有预测性。由于这种错误以及其不同的思维架构，人工智能会使用不同的概念（尽管对人类数据进行法学硕士类型的训练可能会带来更多的一致性）。这使得很难指定人工智能在自己的世界模型中应该追求什么目标，以对应于人类在人类世界模型中追求的目标。参见&lt;a href="https://arbital.com/p/ontology_identification/"&gt;本体识别&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;一个相关的问题是索引性。假设爱丽丝有一种自然价值，那就是她的胃里有大量高质量的食物。鲍勃自然不具备拥有爱丽丝胃里大量食物的价值。为了满足爱丽丝的价值，他必须“相对化”爱丽丝的索引目标，并采取一些行动，例如给爱丽丝高质量的食物，这与他为填饱肚子而采取的行动不同。这将涉及心智理论并具有相关的困难，特别是当目标变得更加依赖于其他主体的心智细节时，例如在美学中。&lt;/p&gt;&lt;p&gt;为了让人工智能有满足人类价值观的目标，需要对目标指称进行某种类似的翻译。但这一理论尚未得到详细阐述。我认为类似于相对论的东西是必要的，它可以跨参考系转换位置和速度等物理量，但以更一般的方式包括语义参考，例如一个人胃里的食物量，或者一个人的审美观。这样的“语义相对论”似乎很难在哲学上得到解释。 （有关语义索引性的一些讨论，请参阅布莱恩·坎特韦尔·史密斯的&lt;a href="https://mitpress.mit.edu/9780262692090/on-the-origin-of-objects/"&gt;《论对象的起源》&lt;/a&gt;和他的后续著作&lt;a href="https://mitpress.mit.edu/9780262043045/the-promise-of-artificial-intelligence/"&gt;《人工智能的前景》&lt;/a&gt; 。）&lt;/p&gt;&lt;h2&gt;还有一些前进的道路&lt;/h2&gt;&lt;p&gt;我所描绘的景象并非完全没有希望。仍然有一些方法可以实现人类价值的满足。&lt;/p&gt;&lt;p&gt;人类增强是一种方法。拥有工具的人类往往比没有工具的人类更能满足人类价值观（尽管核武器等一些工具往往会导致不良的社会平衡）。人类基因增强可能会导致一些“价值漂移”（与当前人类价值观的分歧），但也会导致能力增益，而且这种权衡很容易是值得的。大脑上传虽然非常困难，但假设上传是高保真的，则可以增强人类的能力，同时基本上保留人类的价值观。在某些能力水平上，代理会倾向于“解决对齐问题”并计划以稳定的方式优化其值。 Yudkowsky 本人认为，默认的未对齐 AGI 将解决对齐问题（与它们的值），以便稳定地优化它们的值，正如他在&lt;a href="https://www.youtube.com/watch?v=6yQEA18C-XI"&gt;Hotz 辩论&lt;/a&gt;中所解释的那样。因此，提高类人智能体的能力，同时减少一路上的价值漂移（也许还扭转了由于文明结构等原因造成的一些过去的价值漂移）似乎是一个很好的整体方法。&lt;/p&gt;&lt;p&gt;其中一些方法可以结合起来。心理学和神经科学可以更好地理解人类思维结构，包括人类效用函数和优化方法。这可以允许创建与当前人类具有非常相似的值但在优化方面更有能力的模拟人类。&lt;/p&gt;&lt;p&gt;在思维设计空间中，就人类思维而言，能力与一致性相关。这是因为人类价值观对于进化适应性具有重要作用。诸如疼痛不象征之类的价值观差异往往会降低适应性和整体解决问题的能力。思维空间中有一些遥远的设计，在未对齐的情况下更适合，但在本地这不是什么问题。因此，寻找接近人类思维设计的思维设计似乎有望在保持一致性的同时增强能力。&lt;/p&gt;&lt;p&gt; &lt;a href="https://www.youtube.com/watch?v=6yQEA18C-XI"&gt;Paul Christiano 的方法&lt;/a&gt;涉及通过机器学习系统预测人类来解决问题，这与模拟大脑增强提案有一些相似之处，但其具体问题与机器学习泛化等有关。这些提议之间的主要区别在于，人类思维被理解为一个优化组件的系统，而不是一个具有某些行为的黑匣子。&lt;/p&gt;&lt;p&gt;可能有一些方法可以创建模拟人类，通过减少“损害”或“腐败”（例如大脑形成中的意外缺陷）来提高效率。 “道德现实检查”探索了其中的一个版本，其中人工智能系统按照比人类更纯粹的道德原则行事。还有其他可能的情况，例如人工智能经济主体遵守某些法律，同时这种行为的熵偏差较少（由于精神障碍等）。我认为这项技术总体上比大脑模拟更有可能具有经济相关性，并且可能会产生与&lt;a href="https://en.wikipedia.org/wiki/The_Age_of_Em"&gt;《The Age of Em》&lt;/a&gt;中的场景大致相似的场景；从技术上讲，与纯化的、熵减少/正规化的经济主体相比，高保真大脑模拟在技术难度方面似乎“不堪重负”。当然，从人类身上减去与价值相关的损害/腐败可能存在失调问题。&lt;/p&gt;&lt;p&gt;增强人类并不需要创建“语义相对论”，因为进行优化的代理基本上是人性化的思维结构。他们本身可能是道德患者，因此他们对自己目标的索引优化将构成一些具有人类价值的代理人，他们的价值观得到满足。当前人类或增强人类的利他主义会降低价值分歧的水平。&lt;/p&gt;&lt;h2&gt;结论&lt;/h2&gt;&lt;p&gt;这是我对高性能 AGI 系统的 AI 调整的总体看法（我不认为当前的 ML 系统或可预见的放大版本是其中的一个例子）。这张图的灵感来自于埃利泽·尤德科夫斯基和保罗·克里斯蒂安诺等思想家，在某些情况下我也关注与尤德科夫斯基类似的假设，但我试图解释我自己的对齐模型，为什么它是困难的，以及前进的道路可能是什么是。我在这篇文章中没有关于时间表或政策的具体结论，这更多的是人工智能对齐的背景模型。&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/wnkGXcAq4DCgY8HqA/a-case-for-ai-alignment-being-difficult#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sun, 31 Dec 2023 19:55:26 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/wnkGXcAq4DCgY8HqA/a-case-for-ai-alignment-being-difficult</guid></item><item><title>2023 年进步的根源回顾</title><link>https://www.lesswrong.com/posts/gt2aFiicFZfWsNuS4/the-roots-of-progress-2023-in-review</link><description>发布于 2023 年 12 月 31 日下午 6:16（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt; 2023 年对我和 The Roots of Progress 来说又是重要的一年。&lt;/p&gt;&lt;p&gt;这一年，ROP 作为一个组织真正开始腾飞。尽管该组织本身&lt;a href="https://rootsofprogress.org/nonprofit-announcement"&gt;成立于 2021 年&lt;/a&gt;，但起初它只是我自己的智力工作以及一些副项目的工具。去年，我们&lt;a href="https://rootsofprogress.org/seeking-a-ceo"&gt;宣布了我们的战略&lt;/a&gt;，并开始寻找能够执行该战略的高管。今年&lt;a href="https://rootsofprogress.org/heike-larson-intro"&gt;她开始了&lt;/a&gt;，我们&lt;a href="https://rootsofprogress.org/blog-building-program-announcement"&gt;启动了我们的第一个项目&lt;/a&gt;。 （请注意，Heike 最初担任首席执行官，但出于个人和健康原因，她决定于 6 月份担任项目副总裁。）&lt;/p&gt;&lt;p&gt;随着组织发展得比我更大，我们的沟通也在不断发展，可能我自己的个人更新将与组织更新分开。但现在，我将继续进行传统的年度回顾。 （查看过去的评论： &lt;a href="https://rootsofprogress.org/2022-in-review"&gt;2022年&lt;/a&gt;、 &lt;a href="https://rootsofprogress.org/2021-in-review"&gt;2021年&lt;/a&gt;、 &lt;a href="https://rootsofprogress.org/2020-in-review"&gt;2020年&lt;/a&gt;、 &lt;a href="https://rootsofprogress.org/2019-in-review"&gt;2019年&lt;/a&gt;、 &lt;a href="https://rootsofprogress.org/2018-in-review"&gt;2018年&lt;/a&gt;、 &lt;a href="https://rootsofprogress.org/twelve-books"&gt;2017年&lt;/a&gt;）&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;团契&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;这是一年中很重要的一部分，所以让我从它开始。&lt;/p&gt;&lt;p&gt;我们正在发起一场文化运动，以建立&lt;a href="https://rootsofprogress.org/a-new-philosophy-of-progress"&gt;新的进步哲学&lt;/a&gt;。要做到这一点，进步思想需要无处不在：在书籍和博客中，在 YouTube 和播客中，在新媒体和旧媒体中，在报纸和杂志中，在教科书和课程中，在艺术和娱乐中。为此，我们需要一支由作家和创意人员组成的队伍来制作这一切。&lt;a href="https://fellowship.rootsofprogress.org/"&gt;该奖学金&lt;/a&gt;的目的是培养人才：加速这些知识分子的职业生涯。&lt;/p&gt;&lt;p&gt;我们于 7 月份&lt;a href="https://rootsofprogress.org/blog-building-program-announcement"&gt;推出了第一个计划&lt;/a&gt;，博客建设强化课程，收到了近 500 份申请。选择很困难，我们不得不拒绝很多合格的人（所以如果你没有成功，不要把它当成是针对你个人的......无论如何，这些过程总是有些主观并且容易出错）。&lt;/p&gt;&lt;p&gt;最终， &lt;a href="https://fellowship.rootsofprogress.org/fellows"&gt;19 名研究员&lt;/a&gt;参加了该项目，其中包括写作指导、编辑反馈、受众建设培训以及进行头脑风暴和反馈的同伴小组。他们都是经验丰富的作家，其中许多人在主流媒体上都有署名。有的在相关智库工作，有的在学术界，有的有行业经验。所有人都在撰写引人入胜的话题：从住房政策到核能，到长寿技术，再到乌托邦的意义。&lt;/p&gt;&lt;p&gt;将如此多伟大且志同道合的人聚集在一起所产生的能量是显而易见的，尤其是在旧金山举行的现场闭幕活动期间。研究员们表示，该计划“提高了我的雄心”，帮助他们“设想作为公共知识分子的职业生涯”，并让他们感到“有能力认真对待写作（及其所能实现的目标）”。他们中的一些人首次以自己的名义和品牌推出了 Substacks，此前他们只为其他出版物或雇主撰写文章。平均而言，他们在强化班期间的写作量是今年早些时候的两倍多。根据大众的要求，我们将无限期地保留每周的同行小组电话会议。&lt;/p&gt;&lt;p&gt;非常感谢并祝贺 Heike Larson 设计、启动和运行这个程序！没有她，这一切都不会发生。也非常感谢所有同仁的热情参与。&lt;/p&gt;&lt;p&gt;该计划非常值得明年继续和扩大，我们现在正在筹集资金来做到这一点。&lt;a href="https://rootsofprogress.org/fundraising-pitch-2024"&gt;&lt;strong&gt;查看我们的完整推介&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;，然后了解&lt;/strong&gt;&lt;a href="https://rootsofprogress.org/support"&gt;&lt;strong&gt;如何支持我们&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;。&lt;/strong&gt;&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;写作&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;今年我为博客写了 35 篇文章（包括这篇），总计超过 65k 字——这是一个新记录（这让我很惊讶，因为感觉今年我已经因为奖学金和筹款而分心于研究和写作了） ）。&lt;/p&gt;&lt;p&gt;我最长、最有深度的文章是：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://rootsofprogress.org/robert-allen-british-industrial-revolution"&gt;如果他们发动了工业革命却没有人来怎么办？&lt;/a&gt;&lt;i&gt;全球视角下的英国工业革命&lt;/i&gt;回顾，作者：罗伯特·艾伦&lt;/li&gt;&lt;li&gt;&lt;a href="https://rootsofprogress.org/can-submarines-swim-demystifying-chatgpt"&gt;潜艇可以游泳吗？&lt;/a&gt;我在其中揭开了人工智能的神秘面纱&lt;/li&gt;&lt;li&gt;&lt;a href="https://rootsofprogress.org/power-seeking-ai"&gt;如果你想做一个苹果派，你必须首先成为宇宙的独裁者&lt;/a&gt;，否则：人工智能会不可避免地寻求权力吗？&lt;/li&gt;&lt;li&gt;&lt;a href="https://rootsofprogress.org/against-review-and-approval"&gt;监管者是谁来监管？&lt;/a&gt;认为我们需要超越审查和批准范式&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;除此之外，我最受欢迎的作品（根据博客上的浏览量和其他论坛上的点赞）是：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://rootsofprogress.org/why-no-roman-industrial-revolution"&gt;为什么没有罗马工业革命？&lt;/a&gt;你无法越过纺车到达纺骡处&lt;/li&gt;&lt;li&gt;&lt;a href="https://rootsofprogress.org/the-four-hour-workday-prediction"&gt;为什么我们没有享受四小时工作日？&lt;/a&gt;或者每天两小时，或者每周十六小时，或者其他预测&lt;/li&gt;&lt;li&gt;&lt;a href="https://rootsofprogress.org/isambard-brunel-on-engineering-standards"&gt;停止进一步改进委员会&lt;/a&gt;：土木工程师 Isambard K. Brunel 的一封便条信。如何“通过将今天的偏见或错误记录并登记为法律来阻碍和束缚明天的进步”&lt;/li&gt;&lt;li&gt;利奥·西拉德认为，&lt;a href="https://rootsofprogress.org/szilard-on-slowing-science"&gt;如何减缓科学进步&lt;/a&gt;。 “科学将变得像一场室内游戏。会有时尚。那些追随时尚的人将获得补助金”&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;今年我最喜欢的被低估的作品之一：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://rootsofprogress.org/the-spiritual-benefits-of-material-progress"&gt;物质进步给精神带来好处&lt;/a&gt;。工业革命对人类灵魂做了什么？&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;今年的几篇文章是“我一直在读的”，我把它做成了准月刊；请参阅下面的阅读更新。&lt;/p&gt;&lt;p&gt;我还出版了 35 期“&lt;a href="https://rootsofprogress.org/writing#links-digest"&gt;链接文摘&lt;/a&gt;”，时间表从每周到每月不等。我总共推荐了超过 1,000 个链接，其中包括 139 个图表和图像。&lt;/p&gt;&lt;p&gt;总体而言， &lt;a href="https://rootsofprogress.org/"&gt;rootsofprogress.org&lt;/a&gt;的访问者数量为 13.4 万人，我的&lt;a href="https://newsletter.rootsofprogress.org/"&gt;电子邮件通讯&lt;/a&gt;增长了 2.4 倍以上，订阅者数量超过 1.8 万人。&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;预订项目&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;2023 年我最大的失望是几乎没有时间写书。寻找合适的出版商的过程比我预期的要长得多。我可能会在 2024 年开始通过时事通讯连载这本书。无论如何，花大量时间在这本书上将是明年的首要目标。&lt;/p&gt;&lt;p&gt;在我投入手稿本身的那段时间里，我一直在写关于农业的章节。我在&lt;a href="https://rootsofprogress.org/reading-2023-08"&gt;七月至八月&lt;/a&gt;、&lt;a href="https://rootsofprogress.org/reading-2023-09"&gt;九月&lt;/a&gt;和&lt;a href="https://rootsofprogress.org/reading-2023-10"&gt;十月&lt;/a&gt;的阅读更新中写下了我在该主题上学到的一些内容。&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;会谈和采访&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;2023 年，我不再优先考虑演讲，但我仍然发表了大约十几篇已发表的演讲和采访。我最喜欢的几个是：&lt;/p&gt;&lt;ul&gt;&lt;li&gt; &lt;a href="https://youtu.be/0g_LKqNsc8A"&gt;Remember the Past to Build the Future&lt;/a&gt; ，实际上来自 Foresight Institute 的 Vision Weekend 2022，但录音是在一月份发布的&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=4QTxifMki5A&amp;amp;t=81s"&gt;在巴西千禧研究所进行的演讲&lt;/a&gt;，约有 80 人参加现场直播。这是我以前做过的演讲“迈向新的进步哲学”&lt;/li&gt;&lt;li&gt; Jim O&amp;#39;Shaughnessy 的无限循环播客（ &lt;a href="https://youtu.be/ozBfMb83-RU"&gt;YouTube&lt;/a&gt; ，&lt;a href="https://www.infiniteloopspodcast.com/jason-crawford-the-roots-of-progress-ep162/"&gt;显示页面&lt;/a&gt;）&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;a href="https://rootsofprogress.org/speaking"&gt;在这里查看我发表的所有演讲和采访&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;我还在几个私人场所发表过演讲，包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;圣达菲研究所在“加速科学”研讨会上。&lt;a href="https://rootsofprogress.org/accelerating-science-through-evolvable-institutions"&gt;我在这里把演讲写成一篇文章&lt;/a&gt;&lt;/li&gt;&lt;li&gt;班加罗尔的&lt;a href="https://takshashila.org.in/"&gt;塔克沙什拉研究所&lt;/a&gt;。我首先亲自与几十位学者进行了交谈，随后又通过 Zoom 向 200 多名正在获取&lt;a href="https://school.takshashila.org.in/gcpp-public-policy"&gt;公共政策研究生证书的&lt;/a&gt;学生发表了演讲&lt;/li&gt;&lt;li&gt;Stripe，我在那里做了一次内部技术演讲。我在 2023 年 Foresight Vision Weekend 上做了一个精简版的演讲，并希望尽快将其写成一篇文章&lt;/li&gt;&lt;li&gt;在班加罗尔举行的 Astral Codex Ten / LessWrong 聚会上，我与大约 30 或 40 名与会者讨论了进展情况&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;最后， &lt;a href="https://www.csmonitor.com/World/2023/1222/Pessimism-or-progress-What-did-you-see-in-2023"&gt;&lt;i&gt;《基督教科学箴言报》的&lt;/i&gt;年终回顾中&lt;/a&gt;简短地引用了我的话。&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;活动&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;今年，我在旧金山、纽约、波士顿和华盛顿特区等城市举办了几次私人晚宴/招待会，主要是为了筹款。如果您有潜力捐赠 1 万美元或更多，并且对您所在地区的（免费、仅限受邀者）活动感兴趣，&lt;a href="mailto:jason@rootsofprogress.org"&gt;请告诉我&lt;/a&gt;您是谁以及您所在的地区。&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;蒸汽动力的起源&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;今年我参与的一个非常有趣和很酷的项目是&lt;a href="https://rootsofprogress.org/steam-engine-origins"&gt;一篇关于蒸汽机史前历史的文章，带有交互式动画图表&lt;/a&gt;。安东·豪斯（Anton Howes）进行研究并撰写文本，马特·布朗（非凡设施）创建图表，我扮演编辑和出版商。该项目得到了&lt;a href="https://www.theinstitute.com/"&gt;该研究所&lt;/a&gt;（我是该研究所的研究员）的慷慨赞助。&lt;/p&gt;&lt;p&gt;以下是一些动画预览，&lt;a href="https://rootsofprogress.org/steam-engine-origins"&gt;请阅读全文&lt;/a&gt;以获得完整的互动体验： &lt;/p&gt;&lt;figure class="image"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gt2aFiicFZfWsNuS4/bczn6golewqiuihfi9k8" /&gt;&lt;/figure&gt;&lt;figure class="image"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gt2aFiicFZfWsNuS4/t2p20o0mobi4wcxvvdg5" /&gt;&lt;/figure&gt;&lt;figure class="image"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gt2aFiicFZfWsNuS4/k9hzbgnndu9gasvej61p" /&gt;&lt;/figure&gt;&lt;h2&gt;&lt;strong&gt;社交媒体&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;呃，2023 年对于社交媒体平台来说是不平凡的一年。 Twitter（我拒绝称其为“X”）仍然是我拥有最多受众的地方 — 超过 31,000 人，今年增长了 20% 以上 — 因此它仍然是我的主要平台。但它正受到新兴平台的挑战，我也在这些平台上进行投资。在&lt;a href="https://www.threads.net/@jasoncrawford"&gt;Facebook 的 Threads&lt;/a&gt;和基于区块链的&lt;a href="https://warpcast.com/jasoncrawford.eth"&gt;Farcaster&lt;/a&gt;上关注我（如果您愿意，还可以在&lt;a href="https://www.linkedin.com/company/rootsofprogress"&gt;LinkedIn&lt;/a&gt; 、 &lt;a href="https://bsky.app/profile/jasoncrawford.org"&gt;Bluesky&lt;/a&gt;和&lt;a href="https://substack.com/@jasoncrawford"&gt;​​Substack Notes&lt;/a&gt;上关注我）。&lt;/p&gt;&lt;p&gt;我的年度热门推文/主题：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://twitter.com/jasoncrawford/status/1610324245917335552"&gt;印度农村自来水连接取得惊人进展&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://twitter.com/jasoncrawford/status/1658927630693855233"&gt;想象一下，您可以回到古代世界并启动工业革命。你随身带着蒸汽机的计划，并将它们呈现给皇帝，解释如何在矿山、磨坊、高炉等处使用该机器。但令你沮丧的是……&lt;/a&gt;&lt;/li&gt;&lt;li&gt; &lt;a href="https://twitter.com/jasoncrawford/status/1610712475984629760"&gt;“传统食品”并不是很古老。法式长棍面包：二战后才在全国范围内普及。希腊语 moussaka：创建于 20 世纪早期。将希腊食物法式化。龙舌兰酒？墨西哥电影业使其成为 20 世纪 30 年代的国酒。 （全部来自 @rachellaudan 的一篇优秀文章）&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://twitter.com/jasoncrawford/status/1706048684964679917"&gt;这仍然让我震惊：在 1800 年代末，大约 25% 的桥梁刚刚倒塌&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://twitter.com/jasoncrawford/status/1655949982070087681"&gt;我们消灭天花的成本低于今天在纽约建造一英里地铁的成本&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://twitter.com/jasoncrawford/status/1681456797457219589"&gt;RT，如果你曾经在冬天买过新鲜水果，并对现代资本主义的颓废富裕感到欣喜若狂&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://twitter.com/jasoncrawford/status/1695015246833025522"&gt;挪威&lt;i&gt;建造隧道的&lt;/i&gt;成本比英国&lt;i&gt;申请隧道的&lt;/i&gt;成本还要低。以及该主题中的许多其他令人震惊的事实&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;strong&gt;进步论坛&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href="https://progressforum.org/"&gt;进步论坛&lt;/a&gt;在年中经历了一段安静的时期，但最近几个月变得更加活跃，特别是 ROP 研究员交叉发布他们的论文和草稿。一些今年的热门帖子：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://progressforum.org/posts/g9KcwhSPh6JPhXqua/a-catalog-of-big-visions-for-biology"&gt;生物学远景目录&lt;/a&gt;，&lt;a href="https://progressforum.org/users/sam-rodriques"&gt;萨姆·罗德里克斯&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://progressforum.org/posts/y4kYmFhqmA6gxsu9Y/radical-energy-abundance"&gt;自由基能量丰度 凯西&lt;/a&gt;·汉默 ( &lt;a href="https://progressforum.org/users/casey-handmer"&gt;Casey Handmer)&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://progressforum.org/posts/95824dNunz8zDk8PS/a-cure-for-my-cancer"&gt;治愈我的癌症&lt;/a&gt;，作者： &lt;a href="https://progressforum.org/users/virginia-postrel"&gt;Virginia Postrel&lt;/a&gt;&lt;/li&gt;&lt;li&gt; &lt;a href="https://progressforum.org/posts/oggzhJoTyyxRfrjZe/revving-up-the-progress-studies-idea-machine"&gt;加速进步研究思想机器&lt;/a&gt;，作者： &lt;a href="https://progressforum.org/users/pradyumna-s-prasad"&gt;Pradyumna&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://progressforum.org/posts/Mft3knDLFXhebaqwh/tell-good-stories"&gt;讲好故事&lt;/a&gt;，作者：&lt;a href="https://progressforum.org/users/rahul-rana"&gt;拉胡尔·拉纳&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://progressforum.org/posts/3aeHgmfJ7WozeoTWQ/vitalik-on-science-his-philanthropy-progress-and-effective"&gt;维塔利克谈科学、他的慈善事业、进步和有效的利他主义&lt;/a&gt;，作者： &lt;a href="https://progressforum.org/users/vincentweisser"&gt;vincentweisser&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;我们在今年早些时候还做了一些 AMA，包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://progressforum.org/posts/3aNLznQiLuyTKMhpH/tyler-cowen-ama"&gt;泰勒·考恩，莫卡特斯中心&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://progressforum.org/posts/acgrRfWETEdm96xJk/eli-dourado-ama"&gt;Eli Dourado，增长与机会中心&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://progressforum.org/posts/mLYtBanaPywCCjm5f/ama-allison-duettmann-foresight-institute"&gt;艾莉森·杜特曼，前瞻研究所&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://progressforum.org/posts/ukhmKgcDgwJSPxMmq/ama-ben-reinhardt-speculative-technologies"&gt;本·莱因哈特 (Ben Reinhardt)，投机技术&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://progressforum.org/posts/LtngrFX72GRoAuEbg/ama-matt-clancy-open-philanthropy"&gt;马特·克兰西，开放慈善事业&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;订阅&lt;a href="https://digest.progressforum.org/"&gt;进展论坛文摘&lt;/a&gt;以获取热门帖子的半定期更新。&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;阅读&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;今年我开始以准月度的方式写我的阅读内容（查看&lt;a href="https://rootsofprogress.org/reading-2023-11"&gt;十一月&lt;/a&gt;、&lt;a href="https://rootsofprogress.org/reading-2023-10"&gt;十月&lt;/a&gt;、&lt;a href="https://rootsofprogress.org/reading-2023-09"&gt;九月&lt;/a&gt;、&lt;a href="https://rootsofprogress.org/reading-2023-08"&gt;七月至八月&lt;/a&gt;、&lt;a href="https://rootsofprogress.org/reading-2023-06"&gt;六月&lt;/a&gt;、&lt;a href="https://rootsofprogress.org/reading-2023-05"&gt;五月&lt;/a&gt;、&lt;a href="https://rootsofprogress.org/reading-2023-04"&gt;四月&lt;/a&gt;、&lt;a href="https://rootsofprogress.org/reading-2023-03"&gt;三月&lt;/a&gt;的更新）。因此，我将简要总结这一年的亮点。从书籍开始：&lt;/p&gt;&lt;p&gt;我的目标之一是阅读基本上所有有关工业革命、现代经济增长的起源、西方的崛起或类似主题的主要书籍。今年对此的贡献包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;乔尔·莫基尔（Joel Mokyr），《&lt;/strong&gt; &lt;a href="https://www.amazon.com/Lever-Riches-Technological-Creativity-Economic/dp/0195074777"&gt;&lt;i&gt;&lt;strong&gt;财富的杠杆：技术创造力与经济进步&lt;/strong&gt;&lt;/i&gt;&lt;/a&gt;》（1990）。我在&lt;a href="https://rootsofprogress.org/reading-2023-09"&gt;九月&lt;/a&gt;和&lt;a href="https://rootsofprogress.org/reading-2023-10"&gt;十月的&lt;/a&gt;更新中对此发表了评论&lt;/li&gt;&lt;li&gt;&lt;strong&gt;托马斯·阿什顿，&lt;/strong&gt; &lt;a href="https://www.amazon.com/Industrial-Revolution-1760-1830-Thomas-Southcliffe-ebook/dp/B000V6L63S"&gt;&lt;i&gt;&lt;strong&gt;《工业革命，1760-1830 年&lt;/strong&gt;&lt;/i&gt;&lt;/a&gt;》（1948 年）。&lt;a href="https://rootsofprogress.org/ashton-industrial-revolution-highlights"&gt;在这里查看我的亮点&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;罗伯特·艾伦，&lt;/strong&gt;&lt;a href="https://rootsofprogress.org/books/the-british-industrial-revolution-in-global-perspective"&gt;&lt;i&gt;&lt;strong&gt;《全球视角下的英国工业革命》&lt;/strong&gt;&lt;/i&gt;&lt;/a&gt; （2009 年）。&lt;a href="https://rootsofprogress.org/robert-allen-british-industrial-revolution"&gt;在这里查看我的评论&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;林恩·怀特，&lt;/strong&gt; &lt;a href="https://www.amazon.com/Medieval-Technology-Social-Change-Townsend/dp/B00442EY2K"&gt;&lt;i&gt;&lt;strong&gt;中世纪技术与社会变革&lt;/strong&gt;&lt;/i&gt;&lt;/a&gt;（1962）。我在&lt;a href="https://rootsofprogress.org/reading-2023-10"&gt;10 月&lt;/a&gt;和&lt;a href="https://rootsofprogress.org/reading-2023-11"&gt;11 月的&lt;/a&gt;更新中对此发表了评论&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;今年我欣赏的一本随机书是&lt;strong&gt;Ester Boserup，《&lt;/strong&gt; &lt;a href="https://www.amazon.com/Conditions-Agricultural-Growth-Economics-Population-ebook/dp/B0747QT8LH"&gt;&lt;i&gt;&lt;strong&gt;农业增长的条件：人口压力下的农业变化经济学》&lt;/strong&gt;&lt;/i&gt;&lt;/a&gt; （1965 年），请参阅&lt;a href="https://rootsofprogress.org/reading-2023-08"&gt;7 月至 8 月&lt;/a&gt;的更新。&lt;/p&gt;&lt;p&gt;我今年阅读的另一个主题是对技术的历史恐惧。一些亮点：&lt;/p&gt;&lt;p&gt;&lt;strong&gt;塞缪尔·巴特勒，《&lt;/strong&gt;&lt;a href="https://en.wikisource.org/wiki/Darwin_among_the_Machines"&gt;&lt;strong&gt;机器中的达尔文&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;》&lt;/strong&gt; （1863）：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; ……我们自己正在创造我们自己的继任者……我们每天都在赋予他们更大的权力，并通过各种巧妙的设计提供自我调节、自我行动的力量，这对他们来说就像智力对人类一样。随着岁月的流逝，我们将发现自己是劣等种族……。终有一天，机器将对世界及其居民拥有真正的统治地位，这是任何一个真正有哲学头脑的人都无法质疑的……我们的意见是，应该立即向他们宣战。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;另请参阅巴特勒的小说&lt;i&gt;Erewhon&lt;/i&gt; ，在下面的小说中提到。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;JBS Haldane，“&lt;/strong&gt;&lt;a href="http://bactra.org/Daedalus.html"&gt;&lt;strong&gt;代达罗斯：或者说，科学与未来&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;”&lt;/strong&gt; （1923 年）：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;人类是否已从物质子宫中释放出一个已经开始与人类为敌、随时可能将其扔入无底虚空的魔王？&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;strong&gt;伯特兰·罗素 (Bertrand Russell) 对霍尔丹的回复，“&lt;/strong&gt;&lt;a href="https://www.amazon.com/Icarus-Future-Science-Bertrand-Russell-ebook/dp/B09HTYNNM8"&gt;&lt;strong&gt;伊卡洛斯：或科学的未来&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;”&lt;/strong&gt; （1924 年）：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;科学增强了人类对自然的控制力，因此可能会增加人类的幸福感和福祉。如果人是理性的，情况就会如此，但事实上，他们是激情和本能的结合……。如果工业主义要取得成功，人类的权力和竞争本能，就像狗的狼性胃口一样，就需要人为地抑制。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;strong&gt;艾伦·图灵的《&lt;/strong&gt; &lt;a href="https://uberty.org/wp-content/uploads/2015/02/intelligent-machinery-a-heretical-theory.pdf"&gt;&lt;strong&gt;智能机械，一种异端理论&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;》&lt;/strong&gt; （1951）主要是关于人工智能的可能性，但也呼应了巴特勒的担忧：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; ……看来机器思维方法一旦开始，用不了多久就会超越我们微弱的力量……。因此，在某个阶段，我们应该期望机器能够以塞缪尔·巴特勒 (Samuel Butler) 的&lt;i&gt;《Erewhon》&lt;/i&gt;中提到的方式进行控制。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt; &lt;strong&gt;Vernor Vinge，“&lt;/strong&gt;&lt;a href="https://frc.ri.cmu.edu/~hpm/book98/com.ch1/vinge.singularity.html"&gt;&lt;strong&gt;即将到来的技术奇点：如何在后人类时代生存&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;”&lt;/strong&gt; （1993）。文奇推测，当创造出超越人类的智慧时，它将引起“与地球上人类生命的兴起相媲美的变化”，他认为我们没有希望控制或限制它：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;他所描述的任何智能机器都不会成为人类的“工具”——就像人类不是兔子、知更鸟或黑猩猩的工具一样。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;其他有趣的历史作品：&lt;/p&gt;&lt;ul&gt;&lt;li&gt; &lt;strong&gt;“&lt;/strong&gt;&lt;a href="https://www.theguardian.com/world/1829/oct/10/transport.uk"&gt;&lt;strong&gt;机车车厢试验&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;”&lt;/strong&gt; （1829 年），一份关于&lt;a href="https://en.wikipedia.org/wiki/Rainhill_trials"&gt;雷恩希尔试验&lt;/a&gt;的当代报纸报道&lt;/li&gt;&lt;li&gt;&lt;strong&gt;约翰·梅纳德·凯恩斯，“&lt;/strong&gt;&lt;a href="http://www.econ.yale.edu/smith/econ116a/keynes1.pdf"&gt;&lt;strong&gt;我们子孙的经济可能性&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;”&lt;/strong&gt; （1930）&lt;/li&gt;&lt;li&gt;&lt;strong&gt;海曼·里科弗海军上将的&lt;/strong&gt;&lt;a href="https://whatisnuclear.com/rickover.html"&gt;&lt;strong&gt;“纸反应堆”备忘录&lt;/strong&gt;&lt;/a&gt;(1953)&lt;/li&gt;&lt;li&gt; &lt;a href="https://environmentaleducation.com/wp-content/uploads/userfiles/Ben%20Franklin%20Letter%20on%20EEA%281%29.pdf"&gt;&lt;strong&gt;本杰明·富兰克林关于铅中毒的信&lt;/strong&gt;&lt;/a&gt;(1786)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;我注意到一些博主的文章经常出现在我的阅读更新和链接摘要中。我特别感谢雅各布·斯坦哈特&lt;a href="https://bounded-regret.ghost.io/"&gt;(Jacob Steinhardt)&lt;/a&gt; ，他写了一些关于人工智能风险的非常明智的文章：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://bounded-regret.ghost.io/more-is-different-for-ai/"&gt;&lt;strong&gt;人工智能的更多不同之处&lt;/strong&gt;&lt;/a&gt;：“在考虑机器学习的安全风险时，有两种常见的方法，我将其称为工程方法和哲学方法……强烈支持工程世界观的人往往认为哲学从根本上是混乱的而且毫无根据，而那些强烈支持哲学的人则认为大多数工程工作都是误导性的，并且与机器学习的长期安全性（充其量）是正交的。”&lt;/li&gt;&lt;li&gt;&lt;a href="https://bounded-regret.ghost.io/complex-systems-are-hard-to-control/"&gt;&lt;strong&gt;复杂系统难以控制&lt;/strong&gt;&lt;/a&gt;：“深度神经网络是复杂的自适应系统，它提出了新的控制困难，而可靠性、模块化和冗余的标准工程思想无法解决这些困难”&lt;/li&gt;&lt;li&gt;&lt;a href="https://bounded-regret.ghost.io/emergent-deception-optimization/"&gt;&lt;strong&gt;紧急欺骗和紧急优化&lt;/strong&gt;&lt;/a&gt;：“似乎很可能......紧急欺骗和紧急优化都将导致未来模型中的奖励黑客攻击。为了解决这个问题，我们应该警惕当今模型中的欺骗和规划，并寻求修复措施，例如使语言模型更加诚实......以及更好地理解学习优化器”&lt;/li&gt;&lt;li&gt;&lt;a href="https://bounded-regret.ghost.io/thought-experiments-provide-a-third-anchor/"&gt;&lt;strong&gt;思想实验提供了第三个锚点&lt;/strong&gt;&lt;/a&gt;，关于哲学思想实验对于理解人工智能风险的价值&lt;/li&gt;&lt;li&gt;&lt;a href="https://bounded-regret.ghost.io/what-will-gpt-2030-look-like/"&gt;&lt;strong&gt;GPT-2030 会是什么样子？&lt;/strong&gt;&lt;/a&gt;它“在各种特定任务上可能会成为超人，包括编码、黑客和数学，以及可能的蛋白质设计”，并且“将接受文本和图像之外的其他模式的训练，可能包括反直觉的模式，例如分子结构、网络流量、低级机器代码、天文图像和大脑扫描”&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;早些时候，斯坦哈特还撰写了&lt;strong&gt;《&lt;/strong&gt;&lt;a href="https://jsteinhardt.stat.berkeley.edu/blog/beyond-bayesians-and-frequentists"&gt;&lt;strong&gt;超越贝叶斯学派和频率论者&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;》&lt;/strong&gt; （Beyond Bayesians andFrequentists）（2012 年）和&lt;strong&gt;《&lt;/strong&gt;对频率论者统计的热烈辩护&lt;strong&gt;》&lt;/strong&gt; （ &lt;a href="https://jsteinhardt.stat.berkeley.edu/blog/a-fervent-defense-of-frequentist-statistics"&gt;&lt;strong&gt;A Fervent Defense ofFrequentist Statistics&lt;/strong&gt;&lt;/a&gt; ）（2014 年）。&lt;/p&gt;&lt;p&gt;我经常引用的另一位博主是凯文·凯利：&lt;/p&gt;&lt;ul&gt;&lt;li&gt; &lt;strong&gt;“&lt;/strong&gt; &lt;a href="https://kk.org/thetechnium/the-unabomber-w/#maincontent"&gt;&lt;strong&gt;The Unabomber Was Right&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;”&lt;/strong&gt; （2009），有点标题党的标题，但非常值得一读&lt;/li&gt;&lt;li&gt;&lt;strong&gt;《&lt;/strong&gt; &lt;a href="https://kk.org/thetechnium/protopia/"&gt;&lt;strong&gt;Protopia&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;》&lt;/strong&gt; （2011）：“我还没有遇到过一个我想居住的乌托邦......我认为我们的目的地既不是乌托邦，也不是反乌托邦，也不是现状，而是原托邦。 Protopia是一种比今天比昨天更好的状态，尽管可能只是好一点点”&lt;/li&gt;&lt;li&gt; &lt;strong&gt;“&lt;/strong&gt;&lt;a href="https://kk.org/thetechnium/construction-is-life/"&gt;&lt;strong&gt;建筑是生命的标志&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;”&lt;/strong&gt; （2022）：“当我遇到街道上升起的起重机时，我感到放心，这个地方还活着，而且健康状况良好”&lt;/li&gt;&lt;li&gt; &lt;strong&gt;“&lt;/strong&gt; &lt;a href="https://kk.org/thetechnium/the-shirky-prin/"&gt;&lt;strong&gt;The Shirky Principle&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;”&lt;/strong&gt; （2010）：“机构将尽力保留它们所解决的问题”&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;然后是阿诺德·克林：&lt;/p&gt;&lt;ul&gt;&lt;li&gt; &lt;strong&gt;“&lt;/strong&gt;&lt;a href="https://www.econlib.org/library/Columns/y2015/Klingtwoforms.html"&gt;&lt;strong&gt;社会秩序的两种形式&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;”&lt;/strong&gt; （2015），&lt;a href="https://www.amazon.com/Violence-Social-Orders-Conceptual-Interpreting-ebook/dp/B00E3URBM8"&gt;&lt;i&gt;暴力与社会秩序&lt;/i&gt;&lt;/a&gt;的摘要&lt;/li&gt;&lt;li&gt;&lt;strong&gt;“&lt;/strong&gt;&lt;a href="https://www.econlib.org/library/Columns/y2013/Klingclan.html"&gt;&lt;strong&gt;国家、氏族和自由&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;”&lt;/strong&gt; （2013），对马克·韦纳的 &lt;a href="https://www.amazon.com/Rule-Clan-Ancient-Organization-Individual-ebook/dp/B00AJI094U"&gt;&lt;i&gt;《氏族规则》&lt;/i&gt;&lt;/a&gt;的评论&lt;/li&gt;&lt;li&gt;&lt;strong&gt;“&lt;/strong&gt; &lt;a href="https://arnoldkling.substack.com/p/peter-thiel-on-child-care-and-gdp"&gt;&lt;strong&gt;Peter Thiel on Child Care and GDP&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;”，&lt;/strong&gt;将 GDP 作为衡量经济活动的指标；加上&lt;strong&gt;“&lt;/strong&gt; &lt;a href="https://arnoldkling.substack.com/p/what-should-gdp-measure-729"&gt;&lt;strong&gt;GDP应该衡量什么？&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;”，&lt;/strong&gt;关于 GDP 是什么和不是什么的后续报道&lt;/li&gt;&lt;li&gt;&lt;strong&gt;“&lt;/strong&gt;&lt;a href="https://arnoldkling.substack.com/p/crisis-of-abundance-81"&gt;&lt;strong&gt;富裕危机&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;”，&lt;/strong&gt;声称我们在美国&lt;i&gt;过度&lt;/i&gt;消费医疗服务&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;最后，我将简要提及几本小说，主要是关于人工智能和/或 x 风险的主题：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;伊恩·特雷吉利斯，&lt;/strong&gt;&lt;a href="https://www.amazon.com/dp/B074CDN1HB"&gt;&lt;i&gt;&lt;strong&gt;炼金术战争&lt;/strong&gt;&lt;/i&gt;&lt;strong&gt;三部曲&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;。&lt;/strong&gt;我今年最喜欢的，推荐。一段架空历史，惠更斯在 1600 年代末发明了智能机器人；几个世纪后，荷兰人在机器人奴隶的背上统治世界&lt;/li&gt;&lt;li&gt;&lt;strong&gt;塞缪尔·巴特勒，&lt;/strong&gt;&lt;a href="https://www.amazon.com/Erewhon-Samuel-Butler-ebook/dp/B000FC1ET4"&gt;&lt;i&gt;&lt;strong&gt;埃雷洪&lt;/strong&gt;&lt;/i&gt;&lt;/a&gt;(1872)。这部讽刺作品涵盖了许多主题，但最著名的是它对机器最终将统治世界的预测；请参阅上面巴特勒的引述&lt;/li&gt;&lt;li&gt;&lt;strong&gt;伏尔泰《&lt;/strong&gt;&lt;a href="https://www.amazon.com/dp/0140455108"&gt;&lt;i&gt;&lt;strong&gt;老实人》&lt;/strong&gt;&lt;/i&gt;&lt;/a&gt; （1759），一部著名的讽刺小说。它本身就很奇怪。但它在 18 世纪神义论辩论的背景下更有意义（参见&lt;a href="https://en.wikipedia.org/wiki/Best_of_all_possible_worlds"&gt;莱布尼茨&lt;/a&gt;和&lt;a href="https://en.wikipedia.org/wiki/1755_Lisbon_earthquake#Philosophy"&gt;里斯本地震&lt;/a&gt;）。感谢 Alan Charles Kors 的&lt;a href="https://www.youtube.com/playlist?list=PLKAndatlZ_Q3x8bEFTzkSulNgxVC4PllB"&gt;解释&lt;/a&gt;和 Lisa VanDamme 组织这些讨论&lt;/li&gt;&lt;li&gt;&lt;strong&gt;罗伯特·海因莱因，&lt;/strong&gt;&lt;a href="https://www.amazon.com/Moon-Harsh-Mistress-Robert-Heinlein-ebook/dp/B07CWGBZ4R"&gt;&lt;i&gt;&lt;strong&gt;《严厉的月亮》&lt;/strong&gt;&lt;/i&gt;&lt;/a&gt; （1966）。 2076 年，月球是一个流放地，居民在人工智能的帮助下发动了一场自由主义叛乱。读起来很有趣，但坦率地说，我不明白为什么海因莱因如此出名，也许我应该读他的其他东西？&lt;/li&gt;&lt;li&gt;&lt;strong&gt;丹尼斯·泰勒，&lt;/strong&gt; &lt;a href="https://www.amazon.com/Are-Legion-Bob-Bobiverse-Book-ebook/dp/B01LWAESYQ"&gt;&lt;i&gt;&lt;strong&gt;《我们是军团》（我们是鲍勃）&lt;/strong&gt;&lt;/i&gt;&lt;/a&gt; （2017）。一个名叫鲍勃的家伙被变成了有感知能力的&lt;a href="https://en.wikipedia.org/wiki/Self-replicating_spacecraft"&gt;冯·诺依曼探测器&lt;/a&gt;，并被派去殖民银河系。很酷的概念，读起来很有趣，但写得不是很好，我完成了第一本书，但没有超级动力去完成这个系列&lt;/li&gt;&lt;li&gt;我尝试过的许多其他科幻小说，很难进入，可能会或可能不会返回：伊恩班克斯的&lt;i&gt;文化&lt;/i&gt;系列，&lt;i&gt;被剥夺者&lt;/i&gt;，&lt;i&gt;侨民&lt;/i&gt;&lt;/li&gt;&lt;li&gt;假期期间，我读了&lt;strong&gt;库尔特·冯内古特的&lt;/strong&gt;&lt;a href="https://www.amazon.com/Cats-Cradle-Novel-Kurt-Vonnegut-ebook/dp/B000SEH13C"&gt;&lt;i&gt;&lt;strong&gt;《猫的摇篮》&lt;/strong&gt;&lt;/i&gt;&lt;/a&gt; （1963）。一位科学家开发了一种有可能毁灭世界的技术：一种在高温下稳定的冰形式，它能将接触到的任何水结晶成相同的形式。对于存在风险问题来说，这是一个有趣的文化接触点，但小说本身在哲学和美学上都是虚无主义的，我讨厌它&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;顺便说一句，我更新了&lt;a href="https://rootsofprogress.org/bibliography"&gt;我的参考书目&lt;/a&gt;，使总数达到 100 本书，现在按类别进行分类，如果您只想要一些亮点，请从“首选”开始。我在更新方面仍然非常落后，需要添加几十本书……也许在 2024 年。&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;谢谢&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;一如既往，我想以感恩的心情结束这一年。过去的四年是我一生中最充实、最有意义的四年，我能做我所做的只是因为有一群关心我的观众。感谢您的阅读、分享、评论，甚至批评。新年快乐，2024 年继续进步。&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/gt2aFiicFZfWsNuS4/the-roots-of-progress-2023-in-review#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sun, 31 Dec 2023 18:16:44 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/gt2aFiicFZfWsNuS4/the-roots-of-progress-2023-in-review</guid></item><item><title>我对 2023 年捐款的长期自我审视</title><link>https://www.lesswrong.com/posts/kp7vuiSnGJJPAoWMs/extended-navel-gazing-on-my-2023-donations</link><description>发布于 2023 年 12 月 31 日下午 6:10（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/kp7vuiSnGJJPAoWMs/extended-navel-gazing-on-my-2023-donations#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sun, 31 Dec 2023 18:10:31 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/kp7vuiSnGJJPAoWMs/extended-navel-gazing-on-my-2023-donations</guid></item><item><title>aisafety.info，目录</title><link>https://www.lesswrong.com/posts/ZqxP6pJe53xRnRb4j/aisafety-info-the-table-of-content</link><description>发布于 2023 年 12 月 31 日下午 1:57（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;以下是来自&lt;a href="https://aisafety.info/"&gt;https://aisafety.info/&lt;/a&gt;的问答列表。当我发现这个网站时，所产生的材料数量给我留下了深刻的印象。但是，该界面针对初学者进行了优化。以下目录适用于希望更自由地浏览各个部分的个人。它是通过将问答聚类为子主题而构建的。我不参与 aisafety.info，我只是想通过以不同的方式呈现他们生成的内容来提高其可见性。他们还在开发新界面。该表也可以在&lt;a href="https://aisafety.info/toc"&gt;https://aisafety.info/toc/&lt;/a&gt;找到。&lt;/p&gt;&lt;h1&gt; 🆕&lt;a href="https://aisafety.info/?state=9OGZ_"&gt;人工智能安全新手？从这里开始。&lt;/a&gt;&lt;/h1&gt;&lt;h2&gt; 📘 AI 安全简介&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8486_"&gt;什么是人工智能安全？&lt;/a&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8EL9_"&gt;什么是AI对齐？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8AF4_"&gt;什么是人工智能治理？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6184_"&gt;对人工智能一致性的担忧的一般性质是什么？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6205_"&gt;什么是“控制问题”？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6714_"&gt;人工智能安全、人工智能对齐、人工智能控制、友好人工智能、人工智能道德、人工智能存在安全和AGI安全之间有什么区别？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=2400_"&gt;为什么人工智能会做坏事？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7755_"&gt;成熟的超级智能会有多强大？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6297_"&gt;为什么安全对于比人类更聪明的人工智能很重要？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7715_"&gt;超级人工智能导致灭绝的可能性有多大？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=86J8_"&gt;关于人工智能安全有哪些介绍？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; 🧠 机器学习简介&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8161_"&gt;什么是大语言模型？&lt;/a&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6627_"&gt;什么是 GPT-3？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6628_"&gt;什么是 OpenAI Codex 和 GitHub Copilot？&lt;/a&gt;&lt;/li&gt;&lt;li&gt; &lt;a href="https://aisafety.info/?state=8EL7_"&gt;“思路链”提示如何发挥作用？&lt;/a&gt;&lt;/li&gt;&lt;li&gt; &lt;a href="https://aisafety.info/?state=6571_"&gt;GPT 式非代理人工智能的进步如何产生有能力的人工智能代理？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=9358_"&gt;什么是计算？&lt;/a&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7750_"&gt;什么是缩放定律？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8C7T_"&gt;什么是“没有免费的午餐”定理？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=94D9_"&gt;什么是“惨痛教训”？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=89ZS_"&gt;什么是强化学习（RL）？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; 🤖 人工智能的类型&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8G1H_"&gt;什么是人工智能（AI）？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6513_"&gt;什么是“狭义人工智能”？&lt;/a&gt; &amp;amp;&lt;a href="https://aisafety.info/?state=2374_"&gt;什么是通用人工智能（AGI）？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8QZF_"&gt;什么是工具AI？&lt;/a&gt; &amp;amp;&lt;a href="https://aisafety.info/?state=5632_"&gt;什么是代理？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6347_"&gt;什么是“变革性人工智能”？&lt;/a&gt;&lt;/li&gt;&lt;li&gt; &lt;a href="https://aisafety.info/?state=5864_"&gt;AGI、变革性人工智能和超级智能之间有什么区别？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6207_"&gt;什么是“超级智能”？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8PYV_"&gt;什么是修格斯？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6350_"&gt;什么是“全脑模拟”？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6592_"&gt;什么是脑机接口？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; 🚀 起飞与智能爆炸&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;脱掉&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7071_"&gt;什么是“AI起飞”？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6966_"&gt;为什么人工智能起飞速度很重要？&lt;/a&gt;&lt;/li&gt;&lt;li&gt; &lt;a href="https://aisafety.info/?state=6957_"&gt;AI 可能有哪些不同的起飞速度？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=90PK_"&gt;什么是单例？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;智力爆炸&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6306_"&gt;什么是智力爆炸？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6586_"&gt;智力爆炸的可能性有多大？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6605_"&gt;情报爆炸有什么用？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8IHO_"&gt;奇点、智能爆炸和硬起飞之间有什么区别？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; 📅 时间表&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;专家调查&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6478_"&gt;专家通常基于哪些证据来进行时间线预测？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=5633_"&gt;专家认为什么时候会创造出人类水平的人工智能？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=5851_"&gt;人工智能从最初出现问题的迹象到发生无法挽回的灾难的速度有多快？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7647_"&gt;是否有关于人工智能安全的专家调查？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;计算和扩展就足够了吗？&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7727_"&gt;我们能否通过扩展类似于当前架构的架构来获得 AGI，或者我们是否缺少关键见解？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7598_"&gt;生物进化过程使用了多少资源来进化出智慧生物？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;从AGI到ASI&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8158_"&gt;我们如何从通用人工智能发展成为超级智能系统？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7565_"&gt;我们会建立一个超级智能吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7747_"&gt;从人类水平的人工智能到超级智能需要多长时间？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6964_"&gt;难道仅仅沿着指数趋势线跌下悬崖就可以期待人工智能自我改进带来巨大回报吗？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; ❗ 风险类型&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=3485_"&gt;什么是事故和误用风险？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=89LL_"&gt;什么是存在风险（x 风险）&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8503_"&gt;人工智能存在风险的主要来源是什么？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7783_"&gt;什么是天文灾害风险（s-risks）？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=1001_"&gt;人工智能带来的其他风险又如何呢？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7774_"&gt;即使没有代理超级智能，人工智能也会出现什么问题？&lt;/a&gt;&lt;/li&gt;&lt;li&gt; &lt;a href="https://aisafety.info/?state=6607_"&gt;“情报爆炸”有什么危险？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7602_"&gt;大规模自动化人工智能说服和宣传是一个严重的问题吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=9AKZ_"&gt;什么是“诡变”&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8VOT_"&gt;什么是心灵犯罪？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; 🔍 AGI 能够做什么？&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6315_"&gt;什么是智力？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6603_"&gt;为什么智慧会带来力量？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;基本能力&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6974_"&gt;人工智能如何在社交上操纵人类？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=5844_"&gt;是否有可能阻止人工智能在互联网上做某些事情？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=5842_"&gt;人工智能冒充人类来实现其目标的可能性有多大？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;先进的功能&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=5943_"&gt;AGI如何杀人？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=5849_"&gt;你能阻止高级人工智能自我升级吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8222_"&gt;超级智能人工智能如何利用互联网来接管物理世界？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7629_"&gt;超级智能人工智能可以做什么，以及哪些事情即使对于它来说在物理上也是不可能的？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7491_"&gt;什么是“价值握手”？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;战略意义&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6990_"&gt;我们能否测试人工智能以确保它在实现超级智能后不会接管并做出有害的事情？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8157_"&gt;为什么我们只有一次机会来调整超级智能？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=5611_"&gt;我们能否对人工智能进行编程，让它在开始做我们不希望它做的事情时自动关闭？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; 🌋未对齐的技术来源&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;正交性论文&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6568_"&gt;什么是正交性命题？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7594_"&gt;什么是“人类价值观”？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6982_"&gt;为什么我们会认为超级智能默认是敌对的？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6920_"&gt;我们可以预期超级智能机器的动机是什么？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=95LE_"&gt;为什么一个错位的超级智能会杀死世界上的所有人？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;规格游戏&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7523_"&gt;为什么最大化人工智能可能会导致不良结果？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=897I_"&gt;什么是工具趋同？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=87AG_"&gt;什么是可修正性？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8EL5_"&gt;什么是反常实例化？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=5853_"&gt;是否有可能将代码写入人工智能以避免给定任务可能出错的所有方式，并且尝试这样做会很危险吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6992_"&gt;我们可以使用指定的规则来约束目标导向的人工智能吗？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;目标误区&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8EL6_"&gt;什么是欺骗性对齐？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8359_"&gt;埃文·胡宾格 (Evan Hubinger) 对欺骗 + 内在对齐有何看法？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;外部和内部对齐&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8XV7_"&gt;什么是外对齐？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8160_"&gt;什么是“台面优化器”？&lt;/a&gt; &amp;amp;&lt;a href="https://aisafety.info/?state=8PYW_"&gt;什么是内部对齐？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8428_"&gt;内对齐和外对齐有什么区别？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8AF5_"&gt;子代理和台面优化器之间有什么区别？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; 🎉 当前平淡无奇的解决方案&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8AER_"&gt;什么是模仿学习？&lt;/a&gt; &amp;amp;&lt;a href="https://aisafety.info/?state=8AEQ_"&gt;什么是行为克隆？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=88FN_"&gt;什么是人类反馈强化学习（RLHF）&lt;/a&gt;和&lt;a href="https://aisafety.info/?state=904J_"&gt;“宪法人工智能”？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=89LK_"&gt;可解释性有何帮助？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8FJZ_"&gt;红队如何用于人工智能对齐？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; 🗺️策略&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7736_"&gt;政府发挥重要作用的可能性有多大？如果有的话，什么角色是理想的？&lt;/a&gt;&lt;/li&gt;&lt;li&gt; &lt;a href="https://aisafety.info/?state=7748_"&gt;“警告镜头”会是什么样子？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8AF1_"&gt;什么是调整税？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7642_"&gt;一致的超级智能是否会迫使人们过上更好的生活，并比他们想要的更快地改变？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;获胜条件&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7762_"&gt;AI对齐的“获胜条件”是什么？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7187_"&gt;如果我们解决了一致性问题，我们就一定会有美好的未来吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7580_"&gt;什么是“关键行为”？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7757_"&gt;什么是“长反思”？&lt;/a&gt;&lt;/li&gt;&lt;li&gt; &lt;a href="https://aisafety.info/?state=7766_"&gt;AGI 的美好未来会是什么样子？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7058_"&gt;人工智能对齐的良好解决方案是什么样的？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7060_"&gt;从较高的层面来看，为了确保美好的未来，我们必须应对哪些协调一致的挑战？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;比赛动态&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6483_"&gt;为什么人们会尝试构建通用人工智能而不是越来越强大的狭义人工智能？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7772_"&gt;领先的人工智能能力组织有哪些？&lt;/a&gt;&lt;/li&gt;&lt;li&gt; &lt;a href="https://aisafety.info/?state=5950_"&gt;Google、OpenAI 等是否意识到了风险？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7648_"&gt;什么是“意外收获条款”？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;所有的情况都被考虑到了&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6275_"&gt;人类的命运到底有多大？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=87O6_"&gt;为什么人工智能安全可能不那么重要？有哪些论据？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;人工智能安全的影响&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=3486_"&gt;人工智能对齐研究会很糟糕吗？如何？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6182_"&gt;随着人工智能变得越来越复杂，它的潜在好处是什么？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7794_"&gt;对于人工智能协调的重要性有哪些反对意见？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; 💭意识&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=5642_"&gt;AI可以有情感吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8V5J_"&gt;人工智能有意识吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt; &lt;a href="https://aisafety.info/?state=8390_"&gt;AI 会受苦吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7784_"&gt;我们可以告诉人工智能做道德上正确的事情吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6272_"&gt;将人工智能拟人化并试图用人类的语言来理解它们是否存在危险？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h1&gt; ❓&lt;a href="https://aisafety.info/?state=9TDI_"&gt;不相信？探索论点。&lt;/a&gt;&lt;/h1&gt;&lt;h2&gt; 🤨 超级智能不太可能出现？&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6192_"&gt;为什么我们应该现在就为人类水平的人工智能技术做好准备，而不是等几十年后才更接近呢？&lt;/a&gt;&lt;/li&gt;&lt;li&gt; &lt;a href="https://aisafety.info/?state=6601_"&gt;“智力爆炸”可能永远不会发生吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8H0O_"&gt;超级智能是否会因需要在物理世界中进行实验而减慢速度？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=5952_"&gt;人工智能真的能比人类更聪明吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8E41_"&gt;人工智能能够比人类思考得更快吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt; &lt;a href="https://aisafety.info/?state=8AV6_"&gt;AGI 为何能比全人类更聪明？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; 😌 超级智能不会带来大的改变吗？&lt;/h2&gt;&lt;ul&gt;&lt;li&gt; &lt;a href="https://aisafety.info/?state=6218_"&gt;AI 不会像我们一样吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6188_"&gt;人工智能不就是像其他工具一样的工具吗？它不会只做我们告诉它的事情吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6953_"&gt;人们真的担心人工智能带来的生存风险吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8C7S_"&gt;企业是超级智能吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8E40_"&gt;资本主义难道不是真正的不结盟的超级智能吗？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; ⚠️超级智能不会有风险吗？&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=89ZQ_"&gt;有没有关于未对齐 AGI 的详细示例故事？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6569_"&gt;任何人工智能都将是一个计算机程序。为什么它不按照编程的方式去做呢？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6196_"&gt;机器人不是真正的问题吗？如果人工智能没有直接操纵物理世界的能力，它怎么能造成伤害呢？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=A3MU_"&gt;人工智能难道不需要有寻求权力的驱动力才能构成严重的风险吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=86WT_"&gt;既然人类在数量上拥有巨大优势，难道人类就无法击败不结盟的人工智能吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;超级智能不是很聪明吗？&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6984_"&gt;超级智能难道不会足够聪明，在理解我们的指令时不会犯愚蠢的错误吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6220_"&gt;难道超级智能就不能聪明到能够辨别是非吗？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; 🤔 为什么不只是？&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=3119_"&gt;如果人工智能开始出现问题，我们为什么不能直接将其关闭呢？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6988_"&gt;一旦我们注意到超级智能正在试图接管世界，我们就不能将其关闭或重新编程吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7148_"&gt;如果通用人工智能如此危险，我们为什么不建造它呢？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6174_"&gt;为什么我们不能直接造一个“人工智能儿童”并抚养它呢？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6224_"&gt;为什么我们不能直接使用阿西莫夫的机器人三定律？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6176_"&gt;为什么我们不能把人工智能“放在一个盒子里”，让它无法影响外界呢？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8E3Z_"&gt;我们不能像限制公司造成的损害一样限制人工智能系统造成的损害吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8163_"&gt;为什么人工智能对齐是一个难题？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; 🧐 真正担心的不是…&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=9B85_"&gt;真正的担忧不是滥用吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6412_"&gt;真正令人担忧的不是技术性失业吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8NYD_"&gt;真正的担忧难道不是偏见吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6411_"&gt;真正令人担忧的不是自主武器吗？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; 📜 我有一定的哲学信仰，所以这不是问题&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7636_"&gt;如果我只关心帮助人们今天的生活，人工智能安全还重要吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=9048_"&gt;为什么有宗教信仰的人应该担心人工智能的存在风险？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7638_"&gt;人工智能风险的重要性是否取决于对超人类主义乌托邦的关心？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7608_"&gt;人类灭亡不是一件好事吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6194_"&gt;人工智能安全是指系统变得恶意或有意识并攻击我们吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6222_"&gt;将我们的价值观控制并强加给人工智能，这不是不道德吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6228_"&gt;我们将与机器合并，所以这永远不会成为问题，对吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=967I_"&gt;人工智能存在风险问题难道不是帕斯卡抢劫的一个例子吗？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h1&gt; 🔍&lt;a href="https://aisafety.info/?state=9IDQ_"&gt;想了解研究吗？潜得更深。&lt;/a&gt;&lt;/h1&gt;&lt;h2&gt; 💻平淡的对齐方式&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=89LM_"&gt;什么是平淡对齐？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7333_"&gt;人工智能与深度学习的结合会很难吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;可扩展的监督&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8201_"&gt;什么是人工智能安全辩论？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=935A_"&gt;什么是对抗性训练？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8316_"&gt;联盟研究中心 (ARC) 如何尝试解决潜在知识获取 (ELK) 问题？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7810_"&gt;什么是“六氯环己烷”？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=897J_"&gt;什么是迭代蒸馏和扩增 (IDA)？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=9049_"&gt;什么是诱发潜在知识（ELK）？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8350_"&gt;外部化推理监督计划涉及什么？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;可解释性&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8241_"&gt;什么是可解释性以及有哪些方法？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=97FU_"&gt;可验证性、可解释性、透明度和可解释性之间有什么区别？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8KGQ_"&gt;什么是多义神经元？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=9NRR_"&gt;什么是神经网络中的“多胞体”？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8HIA_"&gt;什么是特征可视化？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8424_"&gt;什么是神经网络模块化？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8426_"&gt;强化学习中的生成可视化是什么样的？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6822_"&gt;我在哪里可以了解可解释性？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;概念进步&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8G1G_"&gt;什么是分片理论？&lt;/a&gt;&lt;/li&gt;&lt;li&gt; &lt;a href="https://aisafety.info/?state=9FQK_"&gt;LLM如何理解为“模拟器”？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;像AGI一样的大脑&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7605_"&gt;全脑模拟有哪些安全问题？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8324_"&gt;我们如何调整学习算法/认知类似于人脑的 AGI？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6590_"&gt;什么是“生物认知增强”？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7820_"&gt;与全脑模拟相关的伦理挑战有哪些？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; 📝 代理粉底&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7782_"&gt;什么是“代理基金会”？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;重要概念&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7853_"&gt;为什么我们期望超级智能能够非常接近效用最大化者？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8QZH_"&gt;什么是子代理？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=87AH_"&gt;什么是“类型签名”？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=89ZU_"&gt;人工智能对齐背景下的“真实姓名”是什么？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8G1I_"&gt;什么是互信息？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;决策理论&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7777_"&gt;决策理论有哪些不同版本？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7781_"&gt;什么是“功能决策理论”？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7779_"&gt;什么是“因果决策理论（CDT）”？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7778_"&gt;什么是“证据决策论”？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6536_"&gt;我应该读什么来学习决策理论？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;研究方向&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7673_"&gt;什么是“按我的意思做”？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=92JB_"&gt;权力寻求定理是什么？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6119_"&gt;你能为人工智能设定一个“对世界影响最小化”的目标吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6380_"&gt;什么是“量化器”？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6449_"&gt;切断分布的前百分之几会提高量化器的安全性吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8365_"&gt;什么是下贝叶斯主义？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6939_"&gt;什么是“相干外推意志（CEV）”？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7616_"&gt;道德哲学中的主要理论是什么？其中哪些理论在技术上最容易编码到人工智能中？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; 🏛️治理&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8QH5_"&gt;人工智能能力发展放缓会降低生存风险吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7626_"&gt;是否存在政府可以投入大量资源的人工智能协调项目？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7596_"&gt;大家在人工智能治理方面都在做什么？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8517_"&gt;关于通用人工智能发展的国际条约会是什么样子？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7632_"&gt;联合国是否担心人工智能带来的生存风险？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; 🔬 研究机构&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;概述&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6178_"&gt;人工智能协调组织正在采取哪些方法？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8392_"&gt;大家在人工智能对齐方面都在做什么？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=9J1L_"&gt;技术一致性研究的主要类别有哪些？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6479_"&gt;目前正在进行哪些人工智能一致性研究议程？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8KGR_"&gt;有哪些不同的人工智能对齐/安全组织和学者正在研究哪些？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8JYX_"&gt;简而言之，主要的人工智能安全组织和学术界正在做什么？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;平淡无奇&lt;ul&gt;&lt;li&gt;大实验室&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8368_"&gt;OpenAI 的一致性研究议程是什么？&lt;/a&gt;&lt;/li&gt;&lt;li&gt; &lt;a href="https://aisafety.info/?state=8343_"&gt;DeepMind 的安全团队在做什么？&lt;/a&gt;&lt;/li&gt;&lt;li&gt; &lt;a href="https://aisafety.info/?state=8XBK_"&gt;DeepMind是如何进行对抗训练的？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;学术实验室&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8469_"&gt;萨姆·鲍曼在研究什么？&lt;/a&gt;&lt;/li&gt;&lt;li&gt; &lt;a href="https://aisafety.info/?state=8326_"&gt;CAIS正在开展哪些项目？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8342_"&gt;大卫·克鲁格在做什么？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;其他组织&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=85EK_"&gt;联盟研究中心 (ARC) 的研究议程是什么？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8374_"&gt;应该的研究议程是什么？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=85E4_"&gt;红木研究的议程是什么？&lt;/a&gt;&lt;/li&gt;&lt;li&gt; &lt;a href="https://aisafety.info/?state=8314_"&gt;Aligned AI / Stuart Armstrong 正在做什么？&lt;/a&gt;&lt;/li&gt;&lt;li&gt; &lt;a href="https://aisafety.info/?state=7749_"&gt;Redwood Research 如何进行对抗性训练？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;代理基金会&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8327_"&gt;什么是人类兼容人工智能中心（CHAI）？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8364_"&gt;斯科特·加拉布兰特 (Scott Garrabrant) 和艾布拉姆·德姆斯基 (Abram Demski) 在做什么？&lt;/a&gt;&lt;/li&gt;&lt;li&gt; &lt;a href="https://aisafety.info/?state=6300_"&gt;MIRI 正在解决哪些技术问题？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8378_"&gt;约翰·温特沃斯的研究议程是什么？&lt;/a&gt;&lt;/li&gt;&lt;li&gt; &lt;a href="https://aisafety.info/?state=8357_"&gt;MIRI 对技术协调有何看法？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8340_"&gt;什么是精炼？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8348_"&gt;迪伦·哈德菲尔德-梅内尔的论文是什么？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;其他&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=1250_"&gt;方尖碑的研究议程是什么？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8333_"&gt;长期风险中心 (CLR) 的研究议程是什么？&lt;/a&gt;&lt;/li&gt;&lt;li&gt; &lt;a href="https://aisafety.info/?state=8349_"&gt;Encultured 正在做什么？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h1&gt; 🤝&lt;a href="https://aisafety.info/?state=8TJV_"&gt;想为人工智能安全提供帮助吗？参与其中！&lt;/a&gt;&lt;/h1&gt;&lt;h2&gt; 📌一般&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7590_"&gt;我可以在五分钟内采取哪些行动来为人工智能安全事业做出贡献？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7640_"&gt;我应该如何以及为什么形成自己对人工智能安全的看法？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; 📢 外展&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8U2Q_"&gt;我如何开展公共人工智能安全推广工作？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8509_"&gt;哪些链接对于在社交媒体或其他环境中分享特别有价值？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8U2R_"&gt;我如何在学术界和专家中开展 AGI 安全宣传工作？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7769_"&gt;我怎样才能说服别人并很好地表达论点？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; 🧪 研究&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6703_"&gt;我想从事人工智能对齐工作。&lt;/a&gt;&lt;/li&gt;&lt;li&gt; 📚 教育和职业道路&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8W8D_"&gt;关于人工智能安全，我可以写什么硕士论文？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7763_"&gt;我应该在大学学习哪些科目来为一致性研究做好准备？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8U32_"&gt;我想采取重大措施为人工智能协调做出贡献（例如，使其成为我的职业）。我应该怎么办？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8U30_"&gt;我想专注于人工智能调整，但最好首先优先考虑改善我的生活状况。我应该怎么办？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8U2J_"&gt;作为一名软件工程师，我该如何努力实现 AI 一致性？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt; 📋 指导和指导&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7651_"&gt;我在哪里可以找到成为研究员的指导和建议？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8U2K_"&gt;我应该与谁谈论我的非研究人工智能对齐编码项目想法？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6703_"&gt;我怎样才能获得资金？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt; 🧪 项目和参与&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8U2O_"&gt;我想做人工智能对齐的实验工作（即机器学习、编码）。我应该怎么办？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6474_"&gt;我想帮助人工智能调整，但不一定要对生活做出重大改变。我可以做哪些简单的事情来做出贡献？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8UMA_"&gt;我如何进行有关人工智能对齐的概念、数学或哲学工作？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=85E0_"&gt;我可以尝试哪些练习和项目？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8U2S_"&gt;我如何利用社会科学背景来帮助人工智能协调？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8U2I_"&gt;如何进行机器学习编程工作来帮助 AI 协调？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8U2M_"&gt;我应该如何利用我的人工智能对齐机器学习研究想法？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8U2V_"&gt;我应该如何利用我的想法来帮助 AI 协调？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; 🏛️治理&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8IZE_"&gt;我可以尝试哪些人工智能治理练习和项目？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7754_"&gt;有哪些有用的人工智能政策资源？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8U2Z_"&gt;我如何制定人工智能政策？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; 🛠️ 操作和元数据&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6708_"&gt;我在哪里可以找到可以谈论 AI 协调的人？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8U2W_"&gt;我如何才能帮助人工智能对齐研究人员变得更有效，例如作为教练？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8U2X_"&gt;我如何评估人工智能协调项目并分配资助？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8U2P_"&gt;如何围绕 AI 协调开展组织或运营工作？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; 💵 经济上的帮助&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6481_"&gt;向人工智能安全组织捐赠少量资金会产生重大影响吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8U2Y_"&gt;我有兴趣为人工智能调整提供重要的财务支持。我该怎么办？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; 📚其他资源&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=2222_"&gt;在哪里可以找到有关人工智能安全的视频？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8264_"&gt;有哪些 AGI 安全培训计划和课程？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=5635_"&gt;在哪里可以了解有关 AI 对齐的更多信息？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=MEME_"&gt;人工智能安全模因维基&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6470_"&gt;关于人工智能对齐有哪些好的资源？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7619_"&gt;有哪些关于人工智能对齐的好播客？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8159_"&gt;有哪些关于 AGI 安全的好书？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6713_"&gt;我想更深入地了解人工智能对齐文献。我应该去哪里看？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6320_"&gt;我如何更新我对人工智能安全紧迫性的情绪状态？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/ZqxP6pJe53xRnRb4j/aisafety-info-the-table-of-content#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sun, 31 Dec 2023 13:57:15 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/ZqxP6pJe53xRnRb4j/aisafety-info-the-table-of-content</guid></item><item><title>人工智能对齐元策略</title><link>https://www.lesswrong.com/posts/TALmStNf6479uTwzT/ai-alignment-metastrategy</link><description>发布于 2023 年 12 月 31 日 12:06 PM（格林尼治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;我将“协调策略”称为解决技术问题的高级方法&lt;span class="footnote-reference" id="fnrefrea58985c4m"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrea58985c4m"&gt;[1]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。例如，价值学习是一种策略，而将一致性研究委托给人工智能则是另一种策略。我将“对齐元策略”称为及时有效地集中解决技术问题的高级方法。 （随后将提供示例。）&lt;/p&gt;&lt;p&gt;在&lt;a href="https://www.alignmentforum.org/posts/8HYJwQepynHsRKr6j/critical-review-of-christiano-s-disagreements-with-yudkowsky"&gt;上一篇文章&lt;/a&gt;中，我总结了我对平淡对齐的批评。然而，我对相关转移策略的分析太草率了。我将尝试在这里对此进行一些补救，并简要讨论其他元策略，以作为对比和比较的点。&lt;/p&gt;&lt;h1&gt;保守元策略&lt;/h1&gt;&lt;p&gt;保守元策略遵循以下算法：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;尽可能停止此流程之外的所有人工智能能力工作。&lt;/li&gt;&lt;li&gt;将智能代理的数学理论发展到我们可以高度自信地提出适当的对齐协议的水平。理想情况下，理论问题的解决顺序应该是直接能力应用的结果尽可能晚地出现。&lt;/li&gt;&lt;li&gt;设计并实施理论的实证检验，在理论包含错误或理论假设在实践中被违反的情况下，将风险最小化。&lt;/li&gt;&lt;li&gt;如果测试显示问题，请返回步骤 2。&lt;/li&gt;&lt;li&gt;以同样的方式继续进行更雄心勃勃的测试，直到您准备好部署人工智能防御系统。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;这是我自己最喜欢的元策略。它可能失败的主要原因是，如果我们未能阻止的不保守研究在我们部署人工智能防御系统之前创建了不一致的 TAI（目前，我们距离完成步骤 2 还有很长的路要走）。&lt;/p&gt;&lt;p&gt;我认为很明显，一个有能力的文明会走这条路，因为它似乎是唯一一条能够在不承担不必要风险的情况下带来良好的长期结果的道路&lt;span class="footnote-reference" id="fnrefb691imh53gs"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnb691imh53gs"&gt;[2]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。当然，这&lt;i&gt;本身&lt;/i&gt;并不足以证明，在我们的实际文明中，保守的元策略对于那些关心人工智能风险的人来说是最好的。但是，这是暗示性的。&lt;/p&gt;&lt;p&gt;除此之外，我不会在这里阐述保守元策略的理由。有兴趣的读者可以翻到&lt;a href="https://www.lesswrong.com/posts/ZwshvqiqCvXPsZEct/the-learning-theoretic-agenda-status-2023"&gt;1&lt;/a&gt; &lt;a href="https://www.lesswrong.com/posts/qpbYwTqKQG8G7mdFK/the-reasonable-effectiveness-of-mathematics-or-ai-vs"&gt;2&lt;/a&gt; &lt;a href="https://www.lesswrong.com/posts/suxvE2ddnYMPJN9HD/realism-about-rationality?commentId=uGePTr7mcjhS9sDJY"&gt;3&lt;/a&gt; &lt;a href="https://www.lesswrong.com/posts/Gg9a4y8reWKtLe3Tn/the-rocket-alignment-problem"&gt;4&lt;/a&gt; &lt;a href="https://www.lesswrong.com/posts/5bd75cc58225bf0670375575/the-learning-theoretic-ai-alignment-research-agenda"&gt;5&lt;/a&gt; 。&lt;/p&gt;&lt;h1&gt;渐进主义元战略&lt;/h1&gt;&lt;p&gt;增量元策略遵循以下算法：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;发现人工智能能力的进步（通过任何方式，包括反复试验）。&lt;/li&gt;&lt;li&gt;找到一种方法来协调新的人工智能设计（优先考虑您希望进一步扩展的解决方案）。&lt;/li&gt;&lt;li&gt;使用经验测试和可解释性工具的组合来验证一致性。&lt;/li&gt;&lt;li&gt;如果验证失败，请返回步骤 2。&lt;/li&gt;&lt;li&gt;如果可能，使用当前的能力水平部署人工智能防御系统。&lt;/li&gt;&lt;li&gt;转到步骤 1。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;这（或多或少）是平淡对齐的追随者所青睐的元策略。特别是，这就是参与领先人工智能实验室的相对具有安全意识的参与者所提出的计划。&lt;/p&gt;&lt;p&gt;将我们的希望寄托在这一元策略上存在三个相互强化的主要问题，我将在下面讨论。每个问题都有两个方面：“设计”方面，即如果实施渐进元策略的最佳版本，将会发生什么；以及“实施”方面，即人工智能实验室在实践中发生的情况（即使他们声称遵循渐进主义元战略）。&lt;/p&gt;&lt;h2&gt;信息安全&lt;/h2&gt;&lt;h3&gt;设计&lt;/h3&gt;&lt;p&gt;如果在步骤 1 中发现了新的人工智能能力，并且允许知识传播，那么不负责任的行为者将继续将其与额外的进步相结合，然后再在新的层面上解决对齐问题。理想情况下，要么新功能至少在整个迭代结束之前保持秘密，要么政府政策应该防止任何参与者颠覆元战略，或者需要将两者结合起来。实施上述内容涉及重大的制度和政治挑战，这破坏了整个元战略的可行性。&lt;/p&gt;&lt;p&gt;现在，有人可能会说，保守的元策略也存在同样的问题：毕竟，一些理论发现也可能被不负责任的行为者用来实现危险的人工智能能力进步。然而，当涉及到理论研究时，问题只在&lt;i&gt;某些&lt;/i&gt;情况下才会出现，并且通常需要不负责任的行为者采取额外的步骤（从理论到实践）。此外，由于危险的人工智能能力进步只有在坚实的理论基础上才会发生，因此人们更有希望风险是透明的、可预测的和可避免的。&lt;/p&gt;&lt;p&gt;因此，虽然保守的元策略也要求对不负责任的行为者进行监管，但整个方法允许在这部分失败时进行&lt;i&gt;优雅的降级&lt;/i&gt;。另一方面，渐进主义元战略要求直接开展风险最大的研究：通过反复试验发现不透明的人工智能设计，并仅根据其能力进行选择。这与&lt;a href="https://en.wikipedia.org/wiki/Gain-of-function_research"&gt;功能获得&lt;/a&gt;研究背后的可疑原理相当。&lt;/p&gt;&lt;h3&gt;执行&lt;/h3&gt;&lt;p&gt;在实践中，即使是“具有安全意识”的人工智能实验室也会分享许多有关其研究的技术细节。即使他们不这样做，他们的员工也不会因为出色的可信度和对风险的了解而被选中。他们中的许多人最终可能会继续向其他组织传播知识，即使名义上受到保密协议&lt;span class="footnote-reference" id="fnrefiee9wtsfaj"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fniee9wtsfaj"&gt;[3]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;的约束。我也怀疑他们是否维持了非常高的网络安全标准。所有这一切与真正认真对待保密的组织（处理机密信息的军事和政府机构）形成鲜明对比。&lt;/p&gt;&lt;p&gt;另请参阅： &lt;a href="https://www.lesswrong.com/posts/keiYkaeoLHoKK4LYA/six-dimensions-of-operational-adequacy-in-agi-projects"&gt;运营充足性的六个维度&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;测量对准&lt;/h2&gt;&lt;h3&gt;设计&lt;/h3&gt;&lt;p&gt;渐进元策略的第三步需要验证一致性。但是，我们如何判断人工智能是否一致呢？对齐到底&lt;i&gt;意味着&lt;/i&gt;什么？一种可能性是经济标准：“当人工智能的行为足够接近设计师的意图，成为一种有利可图的产品时，它就是一致的”。这对公司高管来说一定很有吸引力。然而，根据这个定义，随着能力进步（或只是时间）的变化，一致性可能会突然发生变化，从而破坏渐进主义的整个情况。&lt;/p&gt;&lt;p&gt;即使能力进展非常顺利，后者也是如此。例如，想象一下代理人工智能会遵循我们的愿望，因为它们知道它们在我们的权力范围内，直到它们不在我们的权力范围内。我们逐渐将整个经济交给人工智能，然后有一天人工智能决定是时候摆脱人类了。即使一些问题提前显现出来，在某些时候，面对经济激励，也&lt;i&gt;很难&lt;/i&gt;阻止/减缓这一趋势。&lt;/p&gt;&lt;p&gt;即使有最好的意图，衡量一致性也很困难。一个困难是&lt;a href="https://www.lesswrong.com/posts/zthDPAjh9w6Ytbeks/deceptive-alignment"&gt;欺骗性的对齐&lt;/a&gt;：代理人工智能战略性地寻找&lt;i&gt;显得&lt;/i&gt;对齐的方式，因为这种外观与其内部世界模型下的效用相关（出于明显的原因）。但即使人工智能不是故意欺骗我们，我们仍然可能会选择一种人工智能设计，其行为在局部看起来是一致的，但内部推理与我们的预期截然不同，或者我们最终可能会得到过于复杂而难以解释的行为。有关更多讨论，请参阅&lt;a href="https://www.lesswrong.com/posts/xFotXGEotcKouifky/worlds-where-iterative-design-fails"&gt;Wentworth 的文章&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;测量问题的令人满意的解决方案需要可解释性工具，这些工具可以&lt;i&gt;在完成单个迭代之前&lt;/i&gt;检测人工智能推理中的任何意外动机。然而，我们距离拥有这样的工具还很远。此外，甚至原则上也不清楚这些工具是为什么样的人工智能设计而存在的。更糟糕的是，即使我们有候选工具，我们如何知道它们可以正常工作？没有理论基础或替代可靠的测量方法？&lt;/p&gt;&lt;h3&gt;执行&lt;/h3&gt;&lt;p&gt;AFAICT，人工智能实验室只是遵循经济标准。我们不知道大型语言模型等内部会发生什么样的推理过程，并且没有工具来确定这一点。当发现问题时，公司会尽最大努力进行修补，但有些问题（越狱、幻觉）仍然没有通用的解决方案。我指出这一点并不是因为这些具体问题本身很重要，而是因为它表明这些公司实际上根本没有遵循渐进主义元战略：尽管事实上他们的产品在某种意义上明显不一致，但他们仍在继续复合能力进步。&lt;/p&gt;&lt;h2&gt;不可预测的进展&lt;/h2&gt;&lt;p&gt;如果人工智能能力的进展不能顺利且可预测，会发生什么？ （我相信这会发生。）它产生了两个问题：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;当发现一项创新极大地改变了设计时，以前的对准技术可能变得无关紧要，以至于新设计的工作对准技术不能是对先前技术的小修改。这会让我们在步骤 2-3 中陷入危险的很长一段时间，在此期间，如果信息安全性不够完美，信息可能会泄漏。此外，这可能意味着之前的大量对齐工作都被浪费了。&lt;/li&gt;&lt;li&gt;当发现一项创新能够显着提高功能时，它就会在 SOTA AI 和 SOTA 一致的 AI 之间产生差距。如果这个差距达到决定性的战略优势，结果将是灾难性的。如果这种差距本身没有达到决定性的战略优势，那么信息泄露仍然存在让不负责任的行为者进一步加剧差距的风险。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;不可预测的进展与信息安全失败的结合可能会造成极其危险的情况，负责任的参与者需要艰难地追赶不负责任的参与者，即使不负责任的参与者太弱而无法“赢得”比赛。这种考虑&lt;i&gt;不包括&lt;/i&gt;“负责任的”行为者通过实验室实验终结世界的风险。&lt;/p&gt;&lt;p&gt;我在这里不区分“设计”和“实现”，因为增量主义元策略仅被人工智能实验室（甚至是名义上的）捍卫了几年，并且在这段时间里并没有出现很多大型和意想不到的创新&lt;span class="footnote-reference" id="fnrefotzj53sx3c"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnotzj53sx3c"&gt;[ 4]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。&lt;/p&gt;&lt;h1&gt;巴特勒元策略&lt;/h1&gt;&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Dune_(franchise)#The_Butlerian_Jihad"&gt;巴特勒的元&lt;/a&gt;策略是：无限期推迟 TAI。本质上，它需要在没有 TAI 帮助的情况下创建一个 AI 防御系统&lt;span class="footnote-reference" id="fnrefip3fa84i5la"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnip3fa84i5la"&gt;[5]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。&lt;/p&gt;&lt;p&gt;这种元策略没有很多拥护者，但我仍然发现为其命名很有用。大多数元策略存在于一个连续体中，其中巴特勒元策略是一个（极端）端点，具体取决于他们对停止不负责任形式的人工智能研究的乐观程度以及持续时间。&lt;/p&gt;&lt;p&gt;这种元策略的主要问题是：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;在没有 TAI 帮助的情况下创建一个足够的人工智能防御系统是极其困难的，可能比解决技术协调问题还要困难得多。&lt;/li&gt;&lt;li&gt;这种元策略中最好的情况是没有 TAI 的未来，这意味着放弃巨大的价值。&lt;/li&gt;&lt;/ul&gt;&lt;h1&gt;超人类主义元战略&lt;/h1&gt;&lt;p&gt;超人类主义元策略遵循以下算法：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;尽可能停止所有有关人工智能的工作，或者至少停止任何与能力有关的工作。&lt;/li&gt;&lt;li&gt;创造能够大幅增强人类智能的技术。&lt;/li&gt;&lt;li&gt;将剩下的问题委托给增强人类（他们可能会比我们能想出的任何方法更好地解决这个问题）。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;这种元策略是由埃利泽·尤德科夫斯基（Eliezer Yudkowsky） &lt;a href="https://www.lesswrong.com/posts/8HYJwQepynHsRKr6j/critical-review-of-christiano-s-disagreements-with-yudkowsky?commentId=9pKofQAchdgCH8jjm"&gt;倡导的&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;我对这种元策略持悲观态度有 3 个原因：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;在我看来，大幅增强成年人的智力似乎比解决技术人工智能协调问题更困难。我对后者有一个&lt;a href="https://www.alignmentforum.org/posts/ZwshvqiqCvXPsZEct/the-learning-theoretic-agenda-status-2023"&gt;计划&lt;/a&gt;，但我不确定对前者有什么认真的计划。 （尽管，请参阅&lt;a href="https://www.lesswrong.com/posts/JEhW3HDMKzekDShva/significantly-enhancing-adult-intelligence-with-gene-editing"&gt;此&lt;/a&gt;。）通过&lt;a href="https://www.lesswrong.com/posts/k8pj8kMxotFGqn4mp/request-use-epilogenics-instead-of-eugenics-in-most"&gt;表观遗传学&lt;/a&gt;增强智力似乎更容易处理，但需要很长时间。&lt;/li&gt;&lt;li&gt;目前尚不清楚这种转移策略是否安全。一方面，我们有一个一致的起点（人类基线）。另一方面，我们正在处理大量未记录的意大利面条代码（人脑的设计）。似乎很容易想象，强烈提高智力的变化也可能以至关重要但难以察觉的方式大幅改变价值观（除其他原因外，因为增强型人类会被激励去隐藏它）。我们最终可能会将未来交给一个超级智能的精神病患者种族。&lt;/li&gt;&lt;li&gt;在大多数国家，所需的人体实验（我认为）是非法的，使其合法化是一个重大挑战，特别是在时间限制下。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;由于增强人类是安全的，这种元策略的优点是（与渐进主义相比）它不会通过提高人工智能能力来烧毁公共资源。因此，并行研究这一方法和保守方法可能是有意义的。&lt;/p&gt;&lt;ol class="footnotes"&gt;&lt;li class="footnote-item" id="fnrea58985c4m"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefrea58985c4m"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;对于什么是“高级”，可以有一些任意的选择。更准确地说，“大方向”和“详细解决方案”之间存在多层甚至频谱。例如， &lt;a href="https://www.alignmentforum.org/posts/ZwshvqiqCvXPsZEct/the-learning-theoretic-agenda-status-2023#Physicalist_Superimitation"&gt;物理主义超级模仿&lt;/a&gt;比单纯的价值学习详细得多，但对于实际实施来说仍然不够详细。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnb691imh53gs"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefb691imh53gs"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;一些批评家怀疑我所追求的那种数学理论是否可能。我要向那些人指出，一个有能力的文明至少会&lt;i&gt; &lt;/i&gt;在尝试其他事情之前，认真&lt;i&gt;尝试&lt;/i&gt;创建该理论。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fniee9wtsfaj"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefiee9wtsfaj"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;我认为执行 NDA 相当困难：公司没有一种简单的方法来知道 NDA 是否被违反，即使它怀疑这一点，证明这一点也很困难，特别是考虑到技术细节在此过程中不断变化。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnotzj53sx3c"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefotzj53sx3c"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;据我所知。如果有，但他们保密，那么，荣誉！&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnip3fa84i5la"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefip3fa84i5la"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;一般来说，人工智能防御系统可以是&lt;i&gt;预防性的&lt;/i&gt;，即旨在防止未对齐的人工智能被创建，而不是在创建后将其禁用。在这种情况下，肯定必须采取预防措施。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ol&gt;&lt;br /&gt;&lt;br /&gt;&lt;a href="https://www.lesswrong.com/posts/TALmStNf6479uTwzT/ai-alignment-metastrategy#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sun, 31 Dec 2023 12:06:11 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/TALmStNf6479uTwzT/ai-alignment-metastrategy</guid></item></channel></rss>