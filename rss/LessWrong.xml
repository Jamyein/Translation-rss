<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>少错</title><link>https://www.lesswrong.com</link><description>致力于提炼理性艺术的社区博客</description><lastBuildDate>Wed, 06 Dec 2023 06:15:11 GMT</lastBuildDate><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>数字人类 vs 与人工智能融合？相同还是不同？</title><link>https://www.lesswrong.com/posts/ZpbcvBtNMxG8v6mcB/digital-humans-vs-merge-with-ai-same-or-different</link><description>发布于 2023 年 12 月 6 日凌晨 4:56（格林尼治标准时间） &lt;br /&gt;&lt;br /&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;米什卡&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;因此，为了创造背景，这是我们在评论中的评论的延续：https://www.lesswrong.com/posts/je5BwKe8enCq8DLrm/ai-40-a-vision-from-vitalik &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;米什卡&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;和我询问&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;i&gt;人类与人工智能融合&lt;/i&gt;和&lt;i&gt;数字人类&lt;/i&gt;之间的界限（这些方法能否可靠地区分彼此？或者是否存在很大的重叠？）&lt;/li&gt;&lt;li&gt;为什么数字人类是比合并更安全的选择&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;米什卡&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;因此，如果我们异步进行，好的起点可能是要求您详细说明第一项：有什么区别，是否存在重大重叠，一个概念是否几乎是另一个概念的子集。&lt;/p&gt;&lt;p&gt;您如何看待数字人类和“混合体”概念之间的关系（如果这就是合并）？ &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;内森·赫尔姆·伯格&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;所以我的想法是，数字人类是一个相当狭窄和精确的定义。对人脑的模拟（不一定基于特定的人，这个概念更具体，上传）。&lt;/p&gt;&lt;p&gt;这种大脑模拟只能通过基于对人类神经元的详细观察的高度准确的规则集来行动和修改。例如，它不能使用外部过程来监视或调节其模拟神经元的活动。除了通过相同的过程和达到与正常生物人脑相同的程度之外，它无法添加更多的神经元。 &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;内森·赫尔姆·伯格&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;另一方面，与人工智能的融合是一个更加开放的概念。很多事情都属于这个标题。一些核心示例可能包括：&lt;/p&gt;&lt;p&gt;一种脑机植入物，允许人工智能系统对人脑进行读取和/或写入访问。&lt;/p&gt;&lt;p&gt;一种人工智能系统，可以非侵入性地控制一组人类接收的大部分感官输入，以高带宽方式向他们提供信息并接受他们的指令。&lt;/p&gt;&lt;p&gt;数字人与人工智能进行高带宽通信，通过正常的模拟感官方法或通过能够直接读取和/或写入神经元活动的人工智能。 &lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;内森·赫尔姆·伯格&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;这里的关键部分是人工智能，它正在与人类融合。它不一定是危险的或压倒性的。但也有可能是这样。&lt;/p&gt;&lt;p&gt;问题在于，扩大“合并”型系统的力量的最简单方法是扩大人工智能的力量和影响力。&lt;/p&gt;&lt;p&gt;数字人类由于受到生物规则集的限制，其能力扩展的方式受到限制。相对于生物人类来说，它仍然可能拥有显着的超能力。不受衰老或生物疾病的影响，能够以互联网数据传输的速度旅行，能够非常便宜且快速地克隆自己，廉价地保存备份并兼作检查站，以完全感官保真度体验虚拟世界，并以时钟速度比生物大脑高得多。 &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;内森·赫尔姆·伯格&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;那么，我预见到与人工智能密切相关的系统或以数字人类开始但不遵守生物学规则的系统会存在什么危险呢？&lt;/p&gt;&lt;p&gt;问题在于，我认为更宽松的数字实体景观有多种不同的滑坡，导致非常糟糕的结果。&lt;/p&gt;&lt;p&gt;其他地方详细讨论的一个经典例子是，允许任意自我修改的数字人类可能会陷入困境，而没有意识到情况有多棘手，这实际上结束了他们与世界的互动。&lt;/p&gt;&lt;p&gt;另一个典型的例子是竞争经济中的数字人类，他们感到有压力去改变自己，以更有动力工作。这可能会导致谋杀甘地式的滑坡，每个新版本的数字人相对较少关心非工作事物，因此选择进一步增加工作欲望并减少对其他任何事情的关心。沿着这条斜坡下去，就是人性的丧失。 &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;米什卡&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;是的，我当然同意这一切。主要症结在于&lt;/p&gt;&lt;p&gt;&amp;gt;这种大脑模拟只能通过基于对人类神经元的详细观察的高度准确的规则集来行动和修改。例如，它不能使用外部过程来监视或调节其模拟神经元的活动。除了通过相同的过程和达到与正常生物人脑相同的程度之外，它无法添加更多的神经元。&lt;/p&gt;&lt;p&gt;基本上，数字人类当然可以做生物人类可以做的所有事情，包括相当于促智药增强、认知策略、迷幻头脑风暴等。&lt;/p&gt;&lt;p&gt;但这个数字人类肯定会意识到，打破规则并直接对其架构进行黑客攻击可能会带来多个数量级的增强。&lt;/p&gt;&lt;p&gt;因此，我们需要依靠数字人类的结合，承诺不走这条路，并在巨大的诱惑下光荣地履行承诺，而技术手段使得打破这一限制特别困难。 &lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;内森·赫尔姆·伯格&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;是的，我不确定我们能否成功地阻止数字人类违反生物规则，但我认为值得尝试。正如我认为值得努力让人工智能充分像工具一样并受到控制，使其没有机会接管，这样我们就不会无意或有意地设计一个具有感情和道德价值的智能数字实体然后我们必须做出决定，是给予它对人类构成危险的权利，还是继续压迫和奴役它。 &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;内森·赫尔姆·伯格&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;这两条谨慎的道路似乎都伴随着安全税，这需要监管和执法来防止人们跳过。 &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;内森·赫尔姆·伯格&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;然而，我认为“让人类和人工智能融合”的提议是一个模糊的提议，旨在不干涉、不受控制地争夺权力。让人类以任何方式在没有监管的情况下与人工智能连接起来，似乎你需要进行大量的实验，其中一些似乎可能会在由此产生的协作获得力量方面发挥作用。但就由此产生的维持或维护人类价值观的合作而言，这并不奏效。 &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;p&gt;&lt;/p&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;米什卡&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;是的，即使人们试图非常小心，并且仅在人类和数字系统之间使用非侵入性 BCI（这是我通常考虑的合并形式），安全问题仍然是巨大的，无论是在参与人类的直接安全方面，但更重要的是，可能会产生什么样的混合实体（即使我们坚持解耦、断开、长时间暂停、稍后重新连接的能力，并不断重复这种“断开-中断-重新连接”周期，这是坚持的合理事情，但不确定性仍然很高）......&lt;/p&gt;&lt;p&gt; ***&lt;/p&gt;&lt;p&gt;另一方面，与 Neuralink 不同，非侵入式 BCI 的进展可能相当快，并且在时间安排上可能与纯人工智能方法具有竞争力……&lt;/p&gt;&lt;p&gt;尽管“纯大脑模拟”方面的进展在未来可能还很遥远，除非成功地发明了一种“加速速度比我们实际绘制大脑的能力更快”的方法...... &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;内森·赫尔姆·伯格&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;是的，这是通往受生物规则约束的基于精确大脑模拟的数字人类之路缓慢的一部分，这让我更有信心这条道路更安全。我们有更多的时间来考虑如何规范数字人类并致力于安全约束。&lt;/p&gt;&lt;p&gt;但也正是出于这个原因，我认为数字人类不会帮助我们度过近期的棘手时期，即人工智能变得足够强大，足以在灾难中发挥作用。这些灾难可能来自人类的滥用或人工智能的指导行动。&lt;/p&gt;&lt;p&gt;另一方面，我可以理解有人可能会说，“允许人类与人工智能合并的短期尝试可能会给我们一个强大的人工智能与人类团队来解决监管不充分的人工智能的安全问题！这比数字人类！它可能非常强大，但也会有‘人类’元素。”&lt;/p&gt;&lt;p&gt;我对此的回应是，“我不相信拥有一些非零的人为因素足以使系统安全，即使参与实验的人一开始似乎是值得信赖的。人类的良好行为似乎是一种脆弱的东西，这绝对是一场不合格的火刑审判。” &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;内森·赫尔姆·伯格&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;因此，如果有人有兴趣追求数字人类和人类上传（例如来自冷冻保存的大脑）的长期目标，我并不反对。这似乎并没有在短期内使人类面临的风险变得更严重，而且在这些技术人员准备好解决监管和安全问题之前，我们似乎有足够的时间。&lt;/p&gt;&lt;p&gt;然而，如果资助者正在决定将资源分配给数字人类和上传路径，还是直接解决人工智能安全问题……我会敦促他们为人工智能安全做出贡献，因为我认为这个问题既更紧迫，也更重要。在相关时间范围内更容易处理。 &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;内森·赫尔姆·伯格&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;另一个方向是智力增强。在这里，我相信生物人类智力的提高有很大的能力，但大多数高影响力的非人工智能合并选项都需要更多的研究才能准备好。例如，我在神经科学领域研究的课题：对同意的成年人进行基因改造以实现彻底的智力增强。我认为这可能需要比数字人类更长的时间，而且几乎可以肯定与未来 10 年无关。我绝对认为科学界应该尽其所能帮助人类度过未来 10 年，因为我认为我们正面临人工智能引发的灾难的巨大危险。 &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;内森·赫尔姆·伯格&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;这也适用于非计算机科学家。例如，生物学家可以通过提高社会检测、预防和制止生物工程流行病的能力来提供帮助。人工智能使生物工程流行病变得更容易、更有可能发生，因此，通过帮助保护社会免受此类流行病的侵害，您正在帮助降低人工智能灾难风险。 &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;内森·赫尔姆·伯格&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;就人类人工智能高带宽团队而言，例如非侵入式 BCI，我认为可以从中安全地获得潜在有用的帮助。然而，需要注意的是，该系统应该受到高度不信任的对待，并保持较高的安全标准。人类人工智能团队的科学发现只有在能够得到非人工智能增强人类的充分验证的情况下才应该被信任。 &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;内森·赫尔姆·伯格&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;因此，我认为需要适用于人工智能的同样的监管也应该适用于人类人工智能团队。&lt;/p&gt;&lt;p&gt;限制它，不要让它传播/复制。将其保留在沙箱中（可能使用更新的互联网缓存副本，沙箱仍然可以允许信息向内流动。）&lt;/p&gt;&lt;p&gt;不要让它获得自己的权力和资源。&lt;/p&gt;&lt;p&gt;未经验证，请勿相信其输出。&lt;/p&gt;&lt;p&gt;不要让它自我修改或构建新颖的人工智能系统。 （仅仅因为你控制了第一代，并不意味着第一代无法构建足够强大的第二代来摆脱你的控制。）&lt;/p&gt;&lt;p&gt;不要让它落入坏人手中。 （团队中的人工智能部分可以通过微调等方式进行修改。团队中的人类部分可以通过洗脑和酷刑等方式被说服与不道德的目标合作。） &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;米什卡&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;是的，这是有道理的。&lt;/p&gt;&lt;p&gt; &amp;gt;就人类人工智能手带宽团队而言，例如非侵入式 BCI，我认为可以从中安全地获得潜在有用的帮助。&lt;/p&gt;&lt;p&gt;是的，事实上，主要用例是科学研究，特别是人工智能安全研究，这在很大程度上必须由人类人工智能团队来完成才能完全可行。&lt;/p&gt;&lt;p&gt;但需要非常谨慎，特别是&lt;/p&gt;&lt;p&gt;&amp;gt; 然而，需要注意的是，该系统应该受到高度不信任的对待，并保持较高的安全标准。人类人工智能团队的科学发现只有在能够得到非人工智能增强人类的充分验证的情况下才应该被信任。&lt;/p&gt;&lt;p&gt;非常适用（需要注意的是，什么算作“完全验证”取决于计算机科学可能能够设计的程序以使其可行......例如，我想朝“零知识证明”的方向示意，不是指它们在字面上适用，而是指可以找到大约本着这种精神的一些解决方案）。 &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;内森·赫尔姆·伯格&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;关于“完全验证”的观点很好，我应该说更像是“概率验证到与实施所建议的创新的风险相称的置信度”。由于我们基本上坐在一个带有隐藏计时器的定时炸弹上，因此我们在尝试解决问题时实际上无法保持最大限度的谨慎。&lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;div&gt;&lt;/div&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/ZpbcvBtNMxG8v6mcB/digital-humans-vs-merge-with-ai-same-or-different#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Wed, 06 Dec 2023 04:56:38 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/ZpbcvBtNMxG8v6mcB/digital-humans-vs-merge-with-ai-same-or-different</guid></item><item><title>EA 基础设施基金计划重点关注原则优先 EA</title><link>https://www.lesswrong.com/posts/A8L4udgpsoC2BNMnS/ea-infrastructure-fund-s-plan-to-focus-on-principles-first</link><description>发布于 2023 年 12 月 6 日凌晨 3:24（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/A8L4udgpsoC2BNMnS/ea-infrastructure-fund-s-plan-to-focus-on-principles-first#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Wed, 06 Dec 2023 03:24:55 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/A8L4udgpsoC2BNMnS/ea-infrastructure-fund-s-plan-to-focus-on-principles-first</guid></item><item><title>**为海伦·托纳、亚当·德安吉洛和塔莎·麦考利辩护**</title><link>https://www.lesswrong.com/posts/nfsmEM93jRqzQ5nhf/in-defence-of-helen-toner-adam-d-angelo-and-tasha-mccauley-1</link><description>发布于 2023 年 12 月 6 日凌晨 2:02（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;匿名帖子：&lt;/p&gt;&lt;p&gt; “我理解 EA 或 AI 治理领域的一个普遍观点是，Toner、D&amp;#39;Angelo 和 McCauley（简称 TDM）确实把 OpenAI 的事情搞砸了，而 AI、世界的命运等已经变得更糟了。我相信这是完全错误的：事前 TDM 已经以非凡的能力和勇气证明了自己（而不是“也许——可以理解，也许——不是大规模的搞砸）；事后，他们的成就足以证明整个人工智能的正确性”治理社区作为一个整体。&lt;/p&gt;&lt;p&gt;我认为：&lt;/p&gt;&lt;p&gt; 1) TDM 的行动使开放 AI 的情况变得更好，因为与他们什么都不做的反事实相比，它的情况要好得多。&lt;/p&gt;&lt;p&gt; 2）就预期或实现的好或坏结果而言，人们应该会发现前者令人惊喜，而后者基本上已被定价，因为从安全角度来看，OpenAI 的情况已经非常糟糕。&lt;/p&gt;&lt;p&gt; 3) 无论你是“荣誉和正直的最高主义者”还是“无情的战略家”，TDM 的行动通常从这两种角度来看都表现得非常出色。&lt;/p&gt;&lt;p&gt; （注：匿名主要是因为想要平静的生活。我没有内部消息，也没有任何狗参与战斗。关于“资历”，我不在人工智能政府工作，但在奖励战略敏锐度的领域经验丰富和丰富的愤世嫉俗，我对“到目前为止的故事”做出了一些有先见之明的呼吁。但作为一个匿名者，这只不过是“来源：相信我兄弟”，所以你不应该这样做，除非我的观点有说服力。）&lt;/p&gt;&lt;p&gt; **发生了什么**&lt;/p&gt;&lt;p&gt;我认为 Zvi 和 Gwern 在 lesswrong 上给出了最准确的描述（也是《纽约时报》的报道）。基本上：Altman 试图用刀砍 Helen Toner 来获得 OpenAIs 棋盘的控制权（即，在 3 比 2 的情况下，Altman 可以指定他的盟友来堆叠棋盘，稍后再用刀砍 McCauley，等等）。伊利亚犹豫不决，短暂地叛逃到 TDM 的“安全派”，后者随后控制了自己并解雇了萨姆。随后发生的所有事件都被广泛报道。 （我的论点依赖于这是真实的故事，所以如果你确定这不是发生的事情，你可以停止阅读）。&lt;/p&gt;&lt;p&gt;这表明 Altman 是一个令人讨厌的作品，绝对不是你想要的负责人工智能重大项目的人：高度权谋、追求权力，并希望消除对他对 OpenAI 应该如何发展的单方面愿景的任何检查。我以像 Reddit 董事会反向收购这样的事件为例，保罗·格雷厄姆 (Paul Graham) 的“补充”事件。他高超的企业斗刀技巧，引发了微软/谷歌法学硕士军备竞赛，并且似乎试图让董事会大部分时间都蒙在鼓里。对 GPT-4 的担忧进一步证明了这一点。&lt;/p&gt;&lt;p&gt;虽然我的论点不需要对 Altman 做出如此极端不利的判断，但我很困惑为什么 EA/AI 治理领域的一些人对他评价良好：有一大堆明显的负面因素，而我能看到的唯一积极因素是“有时”发出适当的声音。人工智能安全对他来说很方便”（简单地说：SBF 也更一致地发出正确的声音……）但即使 Altman 很伟大，他也试图以他们认为的借口罢免管理非营利组织的独立董事会成员。这种不利于 OpenAI 商业利益的行为是完全不可接受的。&lt;/p&gt;&lt;p&gt;&lt;br /&gt; **现在的情况**&lt;/p&gt;&lt;p&gt;萨姆（很可能）重新担任首席执行官，董事会似乎将由萨姆的 1 名盟友、德安吉洛（1 名被踢掉他的人）和萨默斯组成，萨默斯可能会起到平衡作用。塔莎、海伦和伊利亚出去了，格雷格和山姆没有回来。&lt;/p&gt;&lt;p&gt;将该委员会与奥特曼罢免托纳的反事实相比，奥特曼对这个委员会的控制要少得多。首先，他没有参与其中，其次，他现在有三分之一而不是大多数人明确支持他。当然，这可能会改变：我预计奥特曼会继续尝试获得董事会控制权，并且考虑到他的能力，他迟早会成功；即使没有，也许他可以安排“实际情况”，这样董事会就无法阻止他随心所欲地经营公司。或者也许微软会突然介入并从现在开始基本上掌控这一切。&lt;/p&gt;&lt;p&gt;但也许这些都不是，也许董事会确实会成为对奥特曼和那些乐于奔向悬崖的人的有效制衡。至少好人仍然可以在牌桌上发挥作用，如果 TDM 让 Sam 政变成功，他们就不会被有效消灭。有时这是您所能期望的最好的结果。&lt;/p&gt;&lt;p&gt;但真的是这样吗？还有很多其他结果看起来很糟糕，例如：&lt;/p&gt;&lt;p&gt; * Sam 重新担任首席执行官，并带来了许多值得称赞的公关。如果他像我想象的那样糟糕，那就不好了。&lt;/p&gt;&lt;p&gt; * OpenAI 工作人员一致同意 Sam 的观点。&lt;/p&gt;&lt;p&gt; * 反击/不信任/嘲笑/等等。反对“人工智能安全”“EA”“Decels”。 ETC。&lt;/p&gt;&lt;p&gt;本质上，是的：TDM 基本上没有机会避免这些缺点（所以可信的明显失误基本上是“没有伤害，没有犯规”），即使周一早上热衷于四分卫，考虑到他们所处的非常不利的位置，他们似乎也基本上尽了最大努力。已经，而且执行情况超出了合理预期。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt; **为什么坏事基本上已经被定价了**&lt;/p&gt;&lt;p&gt;当前的戏剧提出了三个主要权力块：董事会（好吧，TDM）、员工和投资者。如果推动力足够大，后两者应该提前可靠地建模，使其主要底线是“美元”而不是“使命”。这里的潜在推动力是巨大的（例如，如果股权归属，员工将一夜之间成为百万富翁，微软的数十亿美元投资）。因此，如果他们要在“让这位护身符般的首席执行官承诺让令人惊叹的美好时光持续下去”与“使命/安全担忧”之间进行投票，人们应该期望他们无论如何都会压倒性地支持前者。&lt;/p&gt;&lt;p&gt;但是，至少从表面上看，而且绝对是有意为之（Altman 之前对 OpenAI 奇怪的治理结构的重要性的评论在这里很讽刺），董事会最终发号施令，并且被设置为（与后两组不同）对财务压力不敏感。尽管其他各方（包括像大多数财经媒体这样的局外人）认为该章程赋予他们的权力本质上是表面上的公关或 LARP，但它确实保留了一些现实政治的牙齿：许多资源似乎都被 501(c) 所束缚。 3、虽然董事会无法阻止员工移民，也无法阻止投资者创办一家“我们无法在安全问题上GAF”的替代人工智能公司，但他们可以可信地威胁要烧毁已经存在于 OpenAI 中的大量“股东价值”，所以要斗争如果这些各方返回“内部选择”，则需要做出让步。&lt;/p&gt;&lt;p&gt;换句话说：董事会权力是 TDM 能够实现的唯一现实杠杆，他们（巧妙地）做到了这一点，并尽可能地利用了这一点。许多不关心人工智能安全的人——内部人士或观察家——鄙视或嘲笑他们（也许延伸到人工智能安全），因为他们按照他们认为愚蠢的事业（或反对他们的利益）行事）是开展业务不可避免的成本。首先，这些事件揭示了潜在但预先存在的动力，而不仅仅是激发或加强它们。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt; **事后诸葛亮的好处更好**&lt;/p&gt;&lt;p&gt;人类容易犯错，事后诸葛亮是20-20，因此人们很容易对战争迷雾中的行动过于挑剔，而一旦战争迷雾散去，这些行动就会显得愚蠢。但人们可能会犯相反的错误：如果你仔细规定他们可以或不可以“合理地预见”或“合理地引导相信”什么，你几乎可以将任何人和任何人视为杰出的英雄。&lt;/p&gt;&lt;p&gt;另一个挑战是有两种观点可供选择。一个是更狡猾/天真的结果主义版本：OpenAI 治理是一场现实生活中的外交游戏，因此如果 EV 足够高，TDM 应该抓住机会进行“刺杀”，等等。另一个是更有原则的“不惜一切代价荣誉”这在理性主义者中似乎很流行：所以 TDM 应该充当宪章的圣骑士，因此应该采取立场，即使他们确信这将是徒劳且适得其反的姿态，并且总体上有点合法愚蠢，作为一些预先承诺的东西，一些 UDT 的东西，这实际上是最好的多元宇宙范围的政策，等等。&lt;/p&gt;&lt;p&gt;这些观点通常是一致的：即使（例如）“背刺”的最佳频率不是“没有”，它最多也是“非常罕见”。事实上，TDM 表现得很好，他们的选择通常（但不是普遍）看起来介于“合理”和“模范”之间，无论你喜欢哪种方式。通过查看各种“他们为什么不 X”问题可以最好地看出这一点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt; *他们为什么不给和平一个机会？*&lt;/p&gt;&lt;p&gt;据推测，TDM 可能会选择对 Altman 做出较小的反应，而不是将他彻底抛弃。也许他们可以把萨姆和格雷格踢出董事会，但让他继续担任首席执行官，或者他们可以利用他们的多数席位任命一群安全主义成员来阻止未来的政变企图（或两者兼而有之）。或者他们可以召开一次与萨姆的会议来讨论他的行为，希望能够解决问题，而不是他们所寻求的仓促的“反政变”。也许这会是徒劳的，但有选择价值，如果（例如）奥特曼作为回应辞职并做与他实际做的几乎相同的事情，也许会给他们一个更好的位置。&lt;/p&gt;&lt;p&gt;从现实政治的角度来看，这似乎极其愚蠢。即使忽略之前对奥特曼的任何担忧，政变企图也表明奥特曼是来抓你的。伊利亚的叛逃提供了唯一的机会之窗，但其持续时间不确定（事后看来，显然很短），让他成为第一。让他作为对你的利益怀有敌意的首席执行官，似乎是在恳求他尝试其他选择来事实上废黜你（例如，确保你不会就任何重大问题征求你的意见），即使你让自己免受他重复这种特定攻击的影响。即使你必须达成协议，让他重新担任首席执行官，但无论如何都没有董事会席位（参见实际发生的情况），你也必须免费参加一个机会，让你成功地用更好的人取代他，并且重新任命奥特曼是你可以在谈判中交易的一匹额外的马。&lt;/p&gt;&lt;p&gt;尽管有原则的人原则上可能不喜欢企业尖锐的做法，但TDM对奥特曼的刀砍是对他试图刀砍托纳的回应，这是可信的合理的——他们并没有首先与“肮脏”作斗争——而且有一种自然的正义“让我们让你尝尝”你自己的医学方面在这里。即使你认为针锋相对不合适，我认为“宪章圣骑士”的观点也会说奥特曼的即决解雇要么是合理的，要么是直接需要的。批评监督你的独立董事会成员写了一些你不喜欢你公司的内容，这已经很过分了。以此为借口将他们踢出董事会，因为她已经并将继续反对您快速商业化的愿望，并将阻止董事会有效反对您的任何能力。如果这看起来是有预谋的，并且加上试图终止董事会等的背景，则意味着董事会不应该相信首席执行官只是为了所有人的利益而开发人工智能技术。&lt;/p&gt;&lt;p&gt;如果TDM知道Ilya后来会后悔自己投票的结果，那么一个有原则的人可以扣掉TDM积分，所以在他能做出更好的考虑之前就催促他这么做（/让Greg的妻子劝阻他）。但这结合了许多关于 TDM 当时所做/应该想到/预见的主张，并解决了各种潜在的防御措施（即使如此）。从现实政治角度来看，TDM 遭到了（据称）最优秀的企业持刀战士之一的伏击，但最终刺伤了他的却是他们。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt; *为什么不解释清楚发生了什么？*&lt;/p&gt;&lt;p&gt;我同意这看起来像是一个错误，尽管影响不大。即使“奥特曼试图政变我们，所以我们反击了”，也不会对奥特曼/SV 媒体全场媒体产生太大的吸引力，也不会与工作人员产生太大的芥蒂（“好吧，如果是董事会或奥特曼，我们绝对更喜欢奥特曼”），这似乎比不提供任何实质性内容要好。原则性的方法是，如果你所做的事情可能会让员工损失很多钱，那么你应该向员工提供完整的解释。&lt;/p&gt;&lt;p&gt;可能会出现合理的解释：也许有压倒一切的原则迫使人们保持沉默，也许精明地保留“我们将签署保密协议/非贬低合同，并且在我们离开后不会进行媒体巡演”作为进一步讨价还价的筹码，也许他们（合理地） ，如果错误的话）遵循非常谨慎的法律建议或其他建议。但这似乎是错误的，在争论他们做了正确的事情的过程中，假设 TDM 做了正确的事情来解释明显错误的事情，这就引出了问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt; *他们为什么要塌陷？/他们为什么不把它全部烧毁？*&lt;/p&gt;&lt;p&gt;如果你确定奥特曼是个坏消息，你愿意‘满发仇恨’踢他，为什么还要让他回来？为什么不让 OpenAI 毫无原则地死在这座山上——或者至少让 Altman 在 MS 领导下的建立成为一场代价高昂的胜利，因为从 OpenAI 的尸体中寻找宝贵的人工智能资源可能会带来法律上的麻烦。为什么不竖起中指，把你拥有合法所有权的一切都赠予别人呢？&lt;/p&gt;&lt;p&gt;我猜这些令人头疼的问题，以及 TDM 可能会烧毁 OpenAI 的大量“资产”的可信威胁，正是 Altman 坐到谈判桌前的原因，也是他原则上同意达成一项协议的原因，而不是“卑鄙地向他投降”。 ’他的宠物媒体报道一直暗示这是迫在眉睫的，或者是他唯一能接受的事情。我认为 TDM 为他们赢得的是 Altman 必须遵守的更具安全意识的治理的前景（不是保证），从安全角度来看，这看起来比他直接在微软旗下建立的更好（至少对我来说），即使他会遭受（大量）一次性转换成本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt; *请有人考虑一下二阶效应吗？！*&lt;/p&gt;&lt;p&gt;但更广泛的世界影响又如何呢？如果 EA/AIS/无论什么现在在技术或金融精英中是一个肮脏的词或蔑视的对象，那么值得吗？如果 TDM 看起来很愚蠢（即使事实并非如此），这会对我们其他人产生不好的影响吗？这是否会阻止任何人投资于以安全为首要的治理结构，以获取大型科技公司的收益？操作系统 AI 开发是否会激增，因为没有人愿意信任 API，如果超过 50% 的少数人决定这样做，API 就会被扼杀？&lt;/p&gt;&lt;p&gt;他们很好：&lt;/p&gt;&lt;p&gt; 1) 原则性的反公关观点会说，人们应该接受人们真诚地看轻你或反对你真正相信和代表的东西。策略很好，公开妥协也很好，但当你真的“暂停人工智能”时，保持假装你有（例如）谨慎的能力是错误​​的。因此，社会现实/更高的拟像水平不应纳入对 TDM 所做工作的评估。&lt;/p&gt;&lt;p&gt; 2）如果你不相信这一点，通常会出现很多假定的二阶效应。也许 OpenAI 的大爆发会推动政府的介入？也许奥特曼必须更加“摘下面具”才能继续留在 OpenAI，让其他人在事情真正开始发挥作用时保持警惕？也许这种治理明显（或明显被视为）失败意味着同样的错误不会重演？因此，如果从第一顺序来看，TDM 做了正确的事情，那么第二顺序的东西（尤其是事前）可能是近似的清洗。&lt;/p&gt;&lt;p&gt; 3) 即使你是一个每天晚上都在读《美丽安对话》的狂热拥趸，并且你看到人工智能治理努力的真正游戏在掩护下渗透到权力走廊，所以你和你的人都处于有利位置适逢其时，TDM 的处境正是如此。如果你不愿意冒一些社会资本的风险，以合理的方式阻止人工智能领头羊的道德可疑的首席执行官罢免自己的董事会并将其打造为自己的个人帝国，那么你什么时候才能花掉它呢？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt; **加起来**&lt;/p&gt;&lt;p&gt;也许不需要太多时间就能看到这相当于应付或同人小说。也许（例如）TDM 讲述了他们自己的故事，听起来更像是他们感到害怕、冲动或愚蠢。也许故事的另一个转折意味着事情最初如何发展的假设故事是错误的，我在其上建立的合理大厦倒塌了。&lt;/p&gt;&lt;p&gt;但我不这么认为。就德行而言，他们是唯一真正认真对待“使命”、能够做点什么、在巨大压力下尝试做正确事情的人。就业绩而言，他们遭到了护身符首席执行官的伏击，不仅成功地为自己辩护，还把他赶了出去，尽管有他、员工、投资者和普遍敌对的媒体对他们不利，但他们还是劫持了 OpenAI 董事会的一部分向他们回报重大让步。对我来说，这周的工作似乎还不错。”&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/nfsmEM93jRqzQ5nhf/in-defence-of-helen-toner-adam-d-angelo-and-tasha-mccauley-1#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Wed, 06 Dec 2023 02:47:31 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/nfsmEM93jRqzQ5nhf/in-defence-of-helen-toner-adam-d-angelo-and-tasha-mccauley-1</guid></item><item><title>关于“人工智能很容易控制”的一些快速思考</title><link>https://www.lesswrong.com/posts/iABojbKgmtMGYgcYm/some-quick-thoughts-on-ai-is-easy-to-control</link><description>发布于 2023 年 12 月 6 日凌晨 12:58（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;我觉得&lt;a href="https://optimists.ai/2023/11/28/ai-is-easy-to-control/"&gt;帖子&lt;/a&gt;作者错过了很多东西，我想分享一些看起来很好沟通的东西。&lt;/p&gt;&lt;p&gt;我将专注于控制超级智能 AI 系统：系统强大到足以完全解决对齐问题（在&lt;a href="https://arbital.com/p/cev/"&gt;CEV&lt;/a&gt;意义上），或者杀死地球上的所有人。&lt;/p&gt;&lt;p&gt;在这篇文章中，我将忽略其他与人工智能相关的 x 风险来源，例如&lt;a href="https://www.judiciary.senate.gov/imo/media/doc/2023-07-26_-_testimony_-_amodei.pdf"&gt;人工智能支持的生物恐怖主义&lt;/a&gt;，并且我不会评论所有似乎需要评论的重要内容。&lt;/p&gt;&lt;p&gt;我也不打算指出所有我认为可能使读者错误概括的狡猾主张，因为这会很挑剔，也不值得花时间（我会跳过的例子 - 我找不到证据GPT-4 已经过任何有监督的微调；RLHF 将聊天机器人的大脑塑造成一种系统，该系统产生的输出使人类评分者点击竖起大拇指/“我更喜欢这个文本”，这样做的智能系统不是它们自己必然受到人类评分者的“青睐”；一个脚注&lt;span class="footnote-reference" id="fnref6uvr6jxw68j"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn6uvr6jxw68j"&gt;[1]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; ）。&lt;/p&gt;&lt;h2&gt;介绍&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;许多人担心我们将失去对人工智能的控制，导致人类灭绝或类似灾难性的“人工智能接管”。我们希望本文中的论点使这样的结果看起来难以置信。但即使未来的人工智能在严格意义上变得不那么“可控”——例如，仅仅因为它的思考速度比人类直接监督的速度快——我们也认为，将&lt;i&gt;我们的价值观灌输&lt;/i&gt;给人工智能是很容易的，这个过程称为“&lt;strong&gt;对齐&lt;/strong&gt;”。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;这&lt;strong&gt;歪曲了人们的担忧&lt;/strong&gt;。说“但即使”使它看起来像：担心 X 风险的人们相信“无论/尽管一致，失去控制都会导致 X 风险”；这些人错了，因为帖子显示“这个结果”是难以置信的；另外，即使他们对失去控制的看法是正确的，他们对x风险的看法也是错误的，因为由于一致，一切都会好起来的。&lt;/p&gt;&lt;p&gt;但大多数情况下，人们（包括主要声音）特别担心可能导致人类灭绝的错位系统。我不知道社区里有谁会说，如果与 CEV 一致的超级智能夺取了控制权，那是一件会导致灭绝的坏事。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;由于每一代可控人工智能都可以帮助控制下一代，因此这个过程似乎可以无限期地持续下去，甚至达到非常高的能力水平&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;我预计奖励塑造低于一定能力水平&lt;span class="footnote-reference" id="fnrefkrdp19oh1z"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnkrdp19oh1z"&gt;[2]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;的人工智能是很容易的，并且我担心控制高于该水平的人工智能。我相信你需要一个超人能力的系统来设计和监督一个超人能力的系统，这样它就不会杀死所有人。亚人类系统目前有能力监督其他亚人类系统，这样这些系统就不会杀死所有人，这是我所预测的，但这并没有提供大量证据证明亚人类系统能够监督超人类系统。 &lt;span class="footnote-reference" id="fnref072edwh8od0f"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn072edwh8od0f"&gt;[3]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;为了解决调整超人类系统的问题，您需要一定量的复杂的人类思维/艰苦的高水平工作。如果一个系统可以在短时间内输出那么多艰苦的高水平工作，我认为这个系统是超人的，并且将其对齐的问题是“对齐完成”的，因为如果你解决了以下任何一个问题对于此类问题，您本质上是解决了对齐问题，并且可能避免 x 风险，但是解决这些问题中的任何一个都需要大量艰苦的人力工作，而安全地自动化这些艰苦工作是一个对齐完成的问题。&lt;/p&gt;&lt;p&gt;需要有一个论据来解释为什么一个人可以成功地使用非人类系统来控制复杂的超人类系统，否则，拥有几代可控制的非人类系统并不重要。&lt;/p&gt;&lt;h2&gt;优化&lt;/h2&gt;&lt;p&gt;让我们谈谈特定神经网络将追求的目标。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;我们经常解决的许多“协调问题”，例如抚养孩子或训练宠物，似乎比训练友好的人工智能要&lt;strong&gt;困难&lt;/strong&gt;得多&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;请注意，进化已经“白盒”地访问了我们的架构，优化了我们的包容性遗传适应性，并获得了针对类似事物集合进行优化的东西。考虑到这一点，人类是如此的一致。孩子们已经很容易想要巧克力、政治和合作；相反，如果你让一个外星孩子将&lt;a href="https://www.lesswrong.com/s/W2fkmatEzyrmbbrDt/p/YrhT7YxkRJoRnr7qD"&gt;善良&lt;/a&gt;与&lt;a href="https://www.lesswrong.com/posts/n5TqCuizyJDfAPjkr/the-baby-eating-aliens-1-8"&gt;吃孩子&lt;/a&gt;或&lt;a href="https://www.lesswrong.com/s/W2fkmatEzyrmbbrDt/p/mMBTPTjRbsrqbSkZE"&gt;分类卵石&lt;/a&gt;联系起来，给这个孩子奖励可以让他们学习你的语言，但不一定会让他们不想吃孩子或分类卵石。&lt;/p&gt;&lt;figure class="media"&gt;&lt;div&gt;&lt;div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/figure&gt;&lt;p&gt;如果你有孩子，你不需要用数学来具体说明你看重的一切：他们可能不会非常聪明地让你给他们奖励，而且他们已经天生想要得到与您想要的东西类似的东西。&lt;/p&gt;&lt;p&gt;当你创建人工智能时，你确实需要有一个优化目标：你希望人工智能尝试做什么，即使具有超级智能优化能力，也可以安全地优化效用函数。我们不知道如何安全地指定这样的目标。&lt;/p&gt;&lt;p&gt;然后，即使你以某种方式设计了这样的目标，你也需要以某种方式找到一个真正尝试实现该目标的人工智能，而不是其他东西，实现的过程与训练期间的目标相关。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;人工智能正在秘密计划杀死你，梯度下降会注意到这一点，并使其将来不太可能这样做，因为制作秘密谋杀情节所需的神经电路可以被拆除并重新配置为直接提高性能的电路&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;我不确定他们对内部对齐问题的假设是什么。这是错误的：我们期望具有广泛目标的智能人工智能能够在可以使用的广泛奖励函数上表现良好，而梯度下降不会优化人工智能实际上试图追求的最终目标。&lt;/p&gt;&lt;p&gt;我&lt;a href="https://moratorium.ai/#modern-machine-learning"&gt;完全期望&lt;/a&gt;梯度下降能够成功优化人工神经网络以实现低损失；我只是不期望他们设计的损失函数能够&lt;a href="https://moratorium.ai/#outer-and-inner-ai-alignment"&gt;代表我们所看重的东西&lt;/a&gt;，并且我期望梯度下降找到&lt;a href="https://moratorium.ai/#inner-alignment"&gt;尝试实现与奖励函数中指定的内容不同的东西的&lt;/a&gt;神经网络。&lt;/p&gt;&lt;p&gt;如果梯度下降找到一个试图最大化与人类完全无关的东西的智能体，并且明白为此它需要在我们的函数上获得高分，那么该智能体将成功获得高分。&lt;strong&gt;梯度下降将优化其在我们的函数上获得高分的能力 - 它将优化构成代理的结构 - 但不会真正关心当前结构的目标内容&lt;/strong&gt;。如果训练完成后，这种结构针对宇宙未来的任何奇怪现象进行了优化并计划杀死我们，这不会追溯性地使梯度发生变化——我们没有已知的方法来指定训练消失的损失函数未来可能会杀死我们的参数。&lt;/p&gt;&lt;h2&gt;干预措施&lt;/h2&gt;&lt;p&gt;能够进行实验并不意味着我们可以提前得到所有潜在问题的演示。如果人工智能足够聪明，并且已经想要与我们想要的东西足够不同的东西，并且我们不理解它的认知架构，那么我们将无法欺骗它相信它的模拟环境是真实世界，它可以最终接管。仅仅对权重和激活进行读/写访问并不能让我们控制人工智能的想法&lt;span class="footnote-reference" id="fnreftjnsez6u2fb"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fntjnsez6u2fb"&gt;[4]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。塑造非人类系统行为的技术不会让我们保持对智能系统的控制。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;一些人将越狱的有效性作为人工智能难以控制的论据&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;是的，但这不是 x 风险社区的论点。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;至关重要的是，这并不意味着人类“与进化论保持一致”——请参阅&lt;a href="https://www.lesswrong.com/posts/hvz9qjWyv8cLX9JJR/evolution-provides-no-evidence-for-the-sharp-left-turn"&gt;&lt;i&gt;&lt;u&gt;《进化论》没有提供任何证据证明昆廷·波普的左转&lt;/u&gt;&lt;/i&gt;&lt;/a&gt;可以揭穿这一类比。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt; AFAIK，内特·苏亚雷斯不会声称人类与进化是一致的。不幸的是，这篇文章或链接文章的作者并没有从机械上理解&lt;a href="https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization"&gt;左急转&lt;/a&gt;的动态。&lt;/p&gt;&lt;h2&gt; AI控制研究更容易&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;提高人工智能可控性的研究比提高人类可控性的研究容易得多​​，因此我们应该期望人工智能比人类更快地变得更加可控&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;（我假设控制和对齐都是“控制”的意思。）列出了比人类控制技术更容易测试人工智能控制技术的方法。对于非人类系统有效，但与超人类系统无关或不适用于，因为：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;我们不知道如何&lt;strong&gt;系统地提出可以实际控制超级智能人工智能系统的技术&lt;/strong&gt;；&lt;/li&gt;&lt;li&gt;我们不知道超级智能人工智能在验证它是否存在于现实世界时会做什么；如果它知道不是，我们就不知道该技术是否真的允许我们在现实世界中控制它；&lt;/li&gt;&lt;li&gt;主要实验室和外部的许多人建议制造超级智能系统，该系统不是单一模型，而是&lt;a href="https://www.youtube.com/watch?v=YTlrPeikoyw"&gt;在复杂系统中&lt;/a&gt;工作的许多模型的组合，它们相互监督和报告；如果用于训练模型的所有计算现在都用于运行模型的副本以构成超级智能系统，则成本和可扩展性考虑并不真正适用，因为您只有一个昂贵的系统。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;价值观很容易学习&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;如果人工智能&lt;strong&gt;首先&lt;/strong&gt;学习道德，它会希望帮助我们确保它在变得更强大时&lt;strong&gt;保持&lt;/strong&gt;道德。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;如果人工智能足够聪明和一致，能够做到这一点，那就对了。但如果它还不是一个与 CEV 一致的超级智能，那么了解人类想要什么并不会激励梯度下降来改变它，使其朝着与 CEV 一致的超级智能发展。我希望智能人工智能能够更容易地理解人类价值观，并且更容易合作；但它不会自动使人类价值观成为优化目标。知道人类想要什么并不能让人工智能提供护理，除非你解决了人工智能提供护理的问题。&lt;/p&gt;&lt;p&gt;看似“一致”的非人类模型的行为对应于一堆杂乱的东西，这些东西对人类给予奖励的内容进行了优化；但是每次梯度下降使模型变得更加普遍的优化/代理时，经过优化的混乱集合的模糊事物不会影响新架构梯度下降安装的目标内容。梯度下降没有理由保留过去神经网络实现的算法的目标和值：神经网络实现的新的、更智能的人工智能算法可以通过更广泛的可能目标和值获得高分。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;由于价值观几乎为社会中的每个人所&lt;i&gt;共享&lt;/i&gt;和理解，因此它们不可能非常复杂。与科学和技术不同的是，科学和技术的分工能够积累更加复杂的知识，而价值观必须保持足够简单，以便儿童在几年内就能学会。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;我猜对人类价值观的描述可能比一千兆字节的信息或其他东西还要短；人工智能可以了解它们是什么；但它们还不够简单，我们无法轻松地将它们指定为优化目标 - 请参阅&lt;a href="https://www.lesswrong.com/posts/4ARaTpNX62uaL86j6/the-hidden-complexity-of-wishes"&gt;愿望的隐藏复杂性&lt;/a&gt;。&lt;/p&gt;&lt;figure class="media"&gt;&lt;div&gt;&lt;div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/figure&gt;&lt;blockquote&gt;&lt;p&gt;当前的语言模型已经非常有能力从道德上评估超级智能可能具备的复杂行为&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;它们有能力评估呈现给它们的后果，但并不比人类更有能力。也就是说，&lt;/p&gt;&lt;ul&gt;&lt;li&gt;低于人类的法学硕士将无法评估超人类人工智能生成的计划，就像人类无法做到这一点一样，因为了解行动的所有后果需要智力，而不仅仅是理解人类会说什么；&lt;/li&gt;&lt;li&gt;我希望这篇文章的作者能够清楚地看到一些故障模式。我邀请读者思考一下，如果我们使用当前的法学硕士自动评估超人人工智能生成的计划，然后启动我们当前的法学硕士查看并说“这看起来不错”的计划，会发生什么。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;结论&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;有很多理由可以预期人工智能将易于控制并易于与人类价值观保持一致&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;不幸的是，在这篇文章中，我没有看到证据表明超级智能人工智能将很容易控制或符合人类价值观。如果一个神经网络实现了一个超人的人工智能代理，它想要的东西与我们想要的不同，那么这篇文章并没有提供任何证据来证明我们能够保持对未来的控制，尽管这个代理所做的事情会产生影响，或者改变它是为了实现一个与 CEV 意义上的人类价值观一致的超人人工智能代理，甚至只是注意到代理出了问题，直到为时已晚。&lt;/p&gt;&lt;p&gt;虽然我们直接优化人工智能系统的权重来获得奖励，而人脑对奖励的反应不太清晰和透明，但我们不知道如何用它来让超级智能人工智能想要我们希望它想要的东西。&lt;/p&gt;&lt;ol class="footnotes"&gt;&lt;li class="footnote-item" id="fn6uvr6jxw68j"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnref6uvr6jxw68j"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;blockquote&gt;&lt;p&gt;未来的人工智能将以值得严肃伦理考虑的方式表现情感和欲望&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;默认情况下，消灭人类的超人人工智能系统不会有情感。他们将成为非常优秀的优化器。但值得注意的是，如果我们在未来 20 年内成功地不被极其优秀的优化器所淘汰，我希望我们在了解如何设计新思维后，有意识地构建带有情感的人工智能系统。请参阅&lt;a href="https://www.lesswrong.com/posts/HsRFQTAySAx8xbXEc/nonsentient-optimizers"&gt;无知觉优化器&lt;/a&gt;和&lt;a href="https://www.lesswrong.com/posts/gb6zWstjmkYHLrbrg/can-t-unbirth-a-child"&gt;无法生育孩子&lt;/a&gt;。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnkrdp19oh1z"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefkrdp19oh1z"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;我关注的是普遍低于人类和普遍超人类的系统，因为这似乎是一个相关的区别，同时更容易关注，尽管它失去了一些细微差别。似乎推理比训练更便宜，一旦你训练了一个人类级别的系统，你就可以立即运行它的许多副本，它们可以一起组成一个超人类的系统（足够聪明，可以在相对较短的时间内解决对齐问题，如果它想要，而且也有足够的能力杀死所有人）。许多次人类系统的副本放在一起，将无法解决对齐或任何需要大量人类最佳认知的问题。因此，我想象了人类水平周围的一个模糊阈值，并在这篇文章中重点关注它。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn072edwh8od0f"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnref072edwh8od0f"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;还有一个开放的、更普遍的问题，我在这里不讨论，即较弱的系统引导较强的系统（不被欺骗并保留偏好）。我们不知道该怎么做。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fntjnsez6u2fb"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnreftjnsez6u2fb"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;不幸的是，我们不知道每个权重代表什么，而且我们对它们实现的算法也没有太多透明度；我们不了解思维过程，也不知道如何以一种可行的方式影响它，尽管存在各种内部优化压力&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ol&gt;&lt;br /&gt;&lt;br /&gt;&lt;a href="https://www.lesswrong.com/posts/iABojbKgmtMGYgcYm/some-quick-thoughts-on-ai-is-easy-to-control#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Wed, 06 Dec 2023 00:58:53 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/iABojbKgmtMGYgcYm/some-quick-thoughts-on-ai-is-easy-to-control</guid></item><item><title>跨国公司作为优化者：跨党派的案例</title><link>https://www.lesswrong.com/posts/HEDbwdx8j8oe6ZiYj/multinational-corporations-as-optimizers-a-case-for-reaching</link><description>发布于 2023 年 12 月 6 日凌晨 12:14（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;当我在左翼圈子里闲逛时，我注意到的一件事是他们也谈论价值观对齐。一旦你克服了完全不同的词汇，他们也会试图找出如何处理大型超人类实体，以实现与人类无关的目标。只是他们谈论的是全球性的、公开交易的巨型企业为了金钱而优化。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;上市公司的最终价值是积累尽可能多的资金。&lt;/strong&gt;这就是所谓的“股东价值最大化”。&lt;/p&gt;&lt;p&gt;即使组织内的大多数人可能看重其他事情，他们的工作描述也是为实现金钱最大化的最终目标做出贡献，并且他们这样做会得到报酬和激励。他们在此过程中制定程序和政策，然后告诉其下级员工执行它们。&lt;/p&gt;&lt;p&gt;程序和策略包含执行它们的人员，尤其是在最低级别，这意味着它们可以被视为手动执行的程序。当然，人类不是硅，而是硅。使用人类作为计算基础和世界操纵者来运行程序是缓慢且不完美的。然而，我相信这个类比仍然成立。&lt;/p&gt;&lt;p&gt;这种股东价值最大化已经严重损害了世界并损害了人类生活质量，其方式多种多样，而且很容易观察到：污染、气候变化和其他此类外部因素。&lt;/p&gt;&lt;p&gt;总之，我相信“友好的人工智能”问题与“友好的跨国大公司”问题有足够的相似之处，一些异花授粉可能会富有成效。即使他们的大多数想法对于人工智能来说都是难以置信的，但他们也考虑创造更多道德超人代理这一事实仍然值得关注。&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/HEDbwdx8j8oe6ZiYj/multinational-corporations-as-optimizers-a-case-for-reaching#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Wed, 06 Dec 2023 01:11:47 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/HEDbwdx8j8oe6ZiYj/multinational-corporations-as-optimizers-a-case-for-reaching</guid></item><item><title>这些天您对 LessWrong 感觉如何？ [打开反馈线程]</title><link>https://www.lesswrong.com/posts/j2W3zs7KTZXt2Wzah/how-do-you-feel-about-lesswrong-these-days-open-feedback</link><description>发布于 2023 年 12 月 5 日晚上 8:54（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;你好！我是来自 LessWrong / Lightcone 团队的 jacoobjacob。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;这是一个元线程，您可以分享您一直在想的有关 LessWrong 的任何想法、感受、反馈或其他内容。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;您可能分享的内容示例：&lt;/p&gt;&lt;ul&gt;&lt;li&gt; “我真的很喜欢同意/不同意投票！”&lt;/li&gt;&lt;li&gt; “所有这些对话内容是怎么回事？这令人困惑......&lt;/li&gt;&lt;li&gt; “嗯……最近网站上的氛围似乎发生了某种变化……特别是[&lt;i&gt;插入 10 段&lt;/i&gt;]”&lt;/li&gt;&lt;/ul&gt;&lt;p&gt; ...或者其他什么！&lt;/p&gt;&lt;p&gt;这篇文章的目的是让您能够在一个您知道团队成员会倾听的地方分享您想到的任何事情。&lt;/p&gt;&lt;p&gt; （我们是一个小团队，必须优先考虑我们的工作，所以我当然不承诺采取这里提到的所有内容。但我至少会倾听所有这些！）&lt;/p&gt;&lt;p&gt;我已经有一段时间没有看到这样的公共话题了。也许有很多关于这个网站的沸腾的感觉从未表达出来？或者，除了我从阅读正常评论、帖子、指标和对讲评论中发现的内容之外，你们没有什么可以分享的了？好吧，这是找出答案的一种方法！我真的很想询问并了解人们对该网站的感受。&lt;/p&gt;&lt;p&gt;那么，您最近对 LessWrong 感觉如何？欢迎在下面留下您的答案。&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/j2W3zs7KTZXt2Wzah/how-do-you-feel-about-lesswrong-these-days-open-feedback#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Tue, 05 Dec 2023 20:54:45 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/j2W3zs7KTZXt2Wzah/how-do-you-feel-about-lesswrong-these-days-open-feedback</guid></item><item><title>人工智能协调计划批评马拉松</title><link>https://www.lesswrong.com/events/kgyrsAkbBjNxPbicc/critique-a-thon-of-ai-alignment-plans</link><description>发布于 2023 年 12 月 5 日晚上 8:50（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt; AI-Plans.com 将于 12 月 16 日至 18 日举办批评马拉松，参与者将提出并讨论对人工智能调整计划的批评。&lt;/p&gt;&lt;p&gt;评委包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;内特·苏亚雷斯，MIRI 主席&lt;/li&gt;&lt;li&gt;Ramana Kumar，DeepMind 研究员&lt;/li&gt;&lt;li&gt;Peter S Park 博士，麻省理工学院 Tegmark 实验室博士后&lt;/li&gt;&lt;li&gt;Charbel-Raphaël Segerie，EffiSciences 人工智能部门负责人&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;好处：&lt;/h2&gt;&lt;p&gt;批评马拉松提供了一个绝佳的机会，通过深入研究调整计划可能成功或失败的原因，获得对人工智能安全的重要见解。&lt;br /&gt;我们将突出调整计划的关键要素，这些要素将反馈给研究人员并可用于改进他们的计划。&lt;br /&gt;这也是获得专家评委反馈的绝佳机会。&lt;/p&gt;&lt;h2&gt; Critique-a-Thon 活动为期 3 天，分为 2 个阶段：&lt;/h2&gt;&lt;h3&gt;&lt;strong&gt;第一阶段 - 添加评论&lt;/strong&gt;（截至 12 月 16 日）&lt;/h3&gt;&lt;p&gt;我们将在 ai-plans.com 上以两个类别的形式添加对调整计划的批评：优势和弱点。&lt;/p&gt;&lt;p&gt;&lt;br /&gt; “优势”应表明计划如何作为协调解决方案发挥作用。 “漏洞”应该起到相反的作用。&lt;br /&gt;奖品将颁发给那些提出最多批评的人 - 投票给负分的批评将不被计算在内。&lt;br /&gt;&lt;br /&gt;您可以随时开始这个阶段 - 您今天就可以开始添加评论！&lt;br /&gt;截止日期为格林尼治标准时间 12 月 16 日午夜。&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;奖品&lt;/strong&gt;&lt;br /&gt;第一名：100 美元&lt;br /&gt;第二名：60美元&lt;br /&gt;第三名：40美元&lt;/p&gt;&lt;h2&gt;&lt;br /&gt;第二阶段 - 讨论和总结（12 月 17 日至 18 日）&lt;/h2&gt;&lt;p&gt;我们两人一组。每对将选择一个已提出批评的调整计划。一个人会提出理由，另一个人会提出反对理由，优势/弱点是真实和准确的。&lt;br /&gt;然后，第二天，我们将交换立场，并以所提议的优势/弱点的书面形式结束。&lt;br /&gt;请参阅之前获奖评论的示例：&lt;/p&gt;&lt;p&gt; &lt;a href="https://docs.google.com/document/d/16Ww6nNECsDnjMOGaz5fqg1PxCVRPpoOy80UooEa7CZM/edit#heading=h.vdtj4e22pjdv"&gt;&lt;u&gt;八月评论马拉松&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;a href="https://docs.google.com/document/d/1O586rkRZd9BbpI8yqA2K6aG9IIq6E1A9I2H9C5nEYKQ/edit#heading=h.vdtj4e22pjdv"&gt;&lt;u&gt;九月评论马拉松&lt;/u&gt;&lt;/a&gt;&lt;br /&gt;&lt;br /&gt;这些讨论将有助于完善批评并加强论点，并且交换可以减少由于缺乏某个主题的先验知识而造成的任何不利。&lt;br /&gt;如果你的对手对这个话题了解得更多，并提出了你没有想到的观点，你可以在第二天自己使用它们，看看有哪些应对措施 - 并确定它们是否是你的文章中需要提及的内容，这就是将被判断的内容。&lt;br /&gt;这些讨论将&lt;a href="https://discord.gg/aGVtu5JyjJ"&gt;&lt;u&gt;在 Discord 上&lt;/u&gt;&lt;/a&gt;进行。&lt;/p&gt;&lt;p&gt;在这里加入👉 &lt;a href="https://discord.gg/aGVtu5JyjJ"&gt;&lt;u&gt;https://discord.gg/aGVtu5JyjJ&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;&lt;h3&gt;&lt;br /&gt;&lt;strong&gt;奖品&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;第一名：400 美元&lt;br /&gt;第二名：250 美元&lt;br /&gt;第三名：150 美元&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;在此注册加入👉 &lt;a href="https://ai-plans.com/login"&gt;&lt;u&gt;https://ai-plans.com/login&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/events/kgyrsAkbBjNxPbicc/critique-a-thon-of-ai-alignment-plans#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Tue, 05 Dec 2023 20:50:08 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/events/kgyrsAkbBjNxPbicc/critique-a-thon-of-ai-alignment-plans</guid></item><item><title>支持/反对 scheming 的论据集中在 SGD 所采取的路径上（“Scheming AI”的第 3 节）</title><link>https://www.lesswrong.com/posts/KyuMS9XzqaJGMu74f/arguments-for-against-scheming-that-focus-on-the-path-sgd</link><description>发布于 2023 年 12 月 5 日下午 6:48（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;这是我的报告《&lt;a href="https://arxiv.org/pdf/2311.08379.pdf"&gt;诡计多端的人工智能：人工智能会在训练期间假装对齐以获得权力吗？》&lt;/a&gt;的第三部分。 ”。 &lt;a href="https://www.lesswrong.com/posts/yFofRxg7RRQYCcwFA/new-report-scheming-ais-will-ais-fake-alignment-during"&gt;这里&lt;/a&gt;还有完整报告的摘要（音频&lt;a href="https://joecarlsmithaudio.buzzsprout.com/2034731/13969977-introduction-and-summary-of-scheming-ais-will-ais-fake-alignment-during-training-in-order-to-get-power"&gt;在这里&lt;/a&gt;）。摘要涵盖了大部分要点和技术术语，我希望它能够提供理解报告各个部分所需的大部分背景信息。&lt;/p&gt;&lt;p&gt;本节的音频版本&lt;a href="https://www.buzzsprout.com/2034731/13984918"&gt;请点击这里&lt;/a&gt;，或者在您的播客应用程序上搜索“Joe Carlsmith Audio”。&lt;/p&gt;&lt;h1&gt;支持/反对阴谋论的争论集中在 SGD 所采取的路径上&lt;/h1&gt;&lt;p&gt;在本节中，我将讨论支持/反对计划的论点，这些论点更直接地关注 SGD 在选择训练的最终输出时所采取的路径。&lt;/p&gt;&lt;p&gt;重要的是，这些论点可能并不相关。特别是：如果 SGD 在模型类别之间以某种“直接比较”的方式主动支持或不支持阴谋者，那么 SGD 将“找到一种方法”来选择它在这个意义上所支持的模型类型（例如，因为足够高的模型类型）。维度空间使这样的“方式”可用）， &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-ZwSi8PjDgjvifEocL-1" id="fnref-ZwSi8PjDgjvifEocL-1"&gt;[1]&lt;/a&gt;&lt;/sup&gt;那么足够的训练只会引导你到 SGD 最喜欢的任何模型，而所讨论的“路径”并不重要。&lt;/p&gt;&lt;p&gt;在关于不同模型的最终属性之间的比较的部分中，我将讨论我们可能期望 SGD 出现这种偏袒的一些原因。特别是：阴谋者“更简单”，因为他们可以有更简单的目标，但他们“更慢”，因为他们需要参与各种形式的额外工具推理 - 例如，在决定策划时，检查现在是否是一个好时机缺陷，可能参与和掩盖“早期破坏”等方面的努力（尽管请注意，此处需要执行额外的工具推理，可能会表现为由策划者的权重实现的算法的额外复杂性，因此表现为“简单性”）成本”，而不是“运行该算法更长时间的需要”）。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-ZwSi8PjDgjvifEocL-2" id="fnref-ZwSi8PjDgjvifEocL-2"&gt;[2]&lt;/a&gt;&lt;/sup&gt;我将在下面对此进行更多讨论。&lt;/p&gt;&lt;p&gt;不过，在这里，我想指出，如果 SGD 足够关心简单性和速度等属性，那么 SGD 通常会首先构建一个具有长期追求权力目标的模型，但即使该模型尝试了一个方案 -就像策略一样（在这种情况下，由于预知其失败，它不一定会这样做），它会被无情地磨成一个情节奖励寻求者，因为情节奖励寻求者的速度优势。或者，SGD 通常会首先构建一个按情节奖励的寻求者，但由于 SGD 渴望更简单的目标，该模型将被无情地磨成一个阴谋者。&lt;/p&gt;&lt;p&gt;在本节中，我将假设这种事情不会发生。也就是说，SGD 构建模型的顺序会对训练的最终结果产生持久的影响。事实上，我的一般感觉是，对阴谋者的讨论经常隐含地假设这样的事情——例如，人们通常认为阴谋者会在训练中很早就出现，然后在那之后将自己锁定。&lt;/p&gt;&lt;h2&gt;独立于训练游戏的代理目标故事&lt;/h2&gt;&lt;p&gt;回想一下我上面介绍的区别：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;训练与比赛&lt;em&gt;无关的&lt;/em&gt;赛外目标，其产生与其在训练-比赛中的角色无关，但随后会激励训练-比赛，而不是训练-比赛。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;与训练游戏&lt;em&gt;相关的&lt;/em&gt;赛外目标，SGD 主动&lt;em&gt;创建&lt;/em&gt;这些目标是&lt;em&gt;为了&lt;/em&gt;激励训练游戏。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在我看来，关于专注于与训练、比赛&lt;em&gt;无关的&lt;/em&gt;目标的策划故事似乎更传统。也就是说，这个想法是：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;由于[插入原因]，模型将制定一个（适当雄心勃勃的）与训练中良好表现相关的超集目标（以&lt;em&gt;不&lt;/em&gt;通过训练游戏的方式）。&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;这可能发生在态势感知到来之前或之后。&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;如果之前，那么在一段时间内它可能会被训练出来，并且还没有激发训练-游戏。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;如果之后，它可能会立即开始激励训练-游戏。&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;然后，结合态势感知，这个（适当雄心勃勃的）超集目标将开始激发训练游戏。&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;将此称为“独立于训练游戏的代理目标故事”。&lt;/p&gt;&lt;p&gt;我相当认真地对待这个论点。如果某种适当雄心勃勃的超越情节的代理目标没有被训练（要么因为它在态势感知之前没有得到训练，要么因为它在态势感知之后突然出现），并且如果有一些关于为什么这个目标会产生影响的故事策划一个好的工具性策略适用（例如，经典的守门故事，或者可能是其他一些非经典故事），那么策划似乎确实可能只是一件发生的事情。&lt;/p&gt;&lt;p&gt;我认为最直接的反对意见是：为什么模型要制定这些（适当雄心勃勃的）超越情节的目标？我之前在诸如“目标默认情况下不会有时间折扣”和“模型时间不会像日历时间”之类的论点中讨论过这一点，我不会在这里添加太多内容，除了说假设模型将实现一些适当雄心勃勃的、超越情节的、独立于训练游戏的代理目标，追求这些目标与高奖励相关，在我看来，这似乎是经典故事中较弱的部分之一。&lt;/p&gt;&lt;p&gt;特别是：对阴谋的讨​​论有时集中在“好奇心”和“权力”等代理目标上，这些目标在很多剧集中都显得有用。但这并不意味着在训练中会激励对&lt;em&gt;情节之外的&lt;/em&gt;好奇心和力量的追求。更重要的是，特别是如果你知道/可以控制情境意识何时出现，那么在模型因追求好奇心/力量而受到积极惩罚的训练片段中似乎并不那么困难。非训练游戏模型将直接进行修改。事实上，总的来说，我希望我们能做很多事情来积极阻止模特采取寻求权力的行为，并鼓励其他形式的“谦虚”。&lt;/p&gt;&lt;p&gt;后一点与我之前提到的乐观原因有关：即，独立于训练游戏的代理目标故事允许我们在态势感知之前进行任何我们想要的对抗性训练，以尝试确保模型的目标保持不变情节内且适度适度。&lt;/p&gt;&lt;p&gt;也就是说，试图在态势感知之前通过对抗性训练来避免这种阴谋，面临着一些严重的障碍：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;首先：最重要的是，所讨论的对抗性训练可能不够多样化/彻底。特别是（特别是缺乏可解释性工具）：可能很难想到所有可能的适当雄心勃勃的剧集外代理目标，这些目标与奖励有足够好的相关性，并且您需要使用此类培训来推动。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-ZwSi8PjDgjvifEocL-3" id="fnref-ZwSi8PjDgjvifEocL-3"&gt;[3]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;第二：当态势感知开始发挥作用时，你可能不知道，或者无法很好地控制。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;第三：一旦足够智能的模型经过文本预测的预训练，它可能已经非常接近态势感知，因为它拥有大量相关的常识（即使不一定是自定位知识）。因此，在那之后可能没有太多时间进行没有态势感知的对抗训练。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;最后，&lt;em&gt;在&lt;/em&gt;态势感知启动后，模型有可能制定适当的、雄心勃勃的、超越情节的、策划激励的目标。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-ZwSi8PjDgjvifEocL-4" id="fnref-ZwSi8PjDgjvifEocL-4"&gt;[4]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;例如，也许当模型开始按照我上面描述的方式“反思”时，它已经相当聪明并且具有战略意识——例如，“弄清楚它真正想要什么”，自我解决，学习打破以前尝试的新事物对齐它，等等。 （这要求模型不会因为获得态势感知而立即开始寻求奖励，但这似乎是可能的，特别是如果我们假设训练留下了显着的“余裕”。）&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;当然，仍然存在一个问题，即为什么该模型会将自己打造成具有适当雄心勃勃的目标，特别是超越情节的目标。但在这里，根据我上面对“如果你训练它拥有长期目标会怎样”的讨论，也许我们可以诉诸这样一个事实：我们试图将其指向一项雄心勃勃的长期任务，但这种指向有些不准确。 / 模糊，这样当它弄清楚它想要如何概括时，这种概括就落在了足够长期和雄心勃勃的事情上，从而激发了阴谋。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;事实上，正如我上面指出的，我认为有一个案例表明，当前的调整工作已经&lt;em&gt;在&lt;/em&gt;试图将我们的系统指向长期的、超越情节的目标——例如，避免帮助制造炸弹的“无害”形式即使炸弹数十年不会爆炸。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;因此，总的来说，我认为，以谋划作为追求（适当雄心勃勃的）超越情节目标的良好工具性策略为条件，独立于训练游戏的代理目标故事在我看来是一个真正令人担忧的问题。&lt;/p&gt;&lt;h2&gt; “最近的最大奖励目标”故事&lt;/h2&gt;&lt;p&gt;我们还可以讲述一个不同的故事，重点关注&lt;em&gt;依赖于&lt;/em&gt;训练和比赛的赛外目标。我将其称为“最近的最大奖励目标”故事。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-ZwSi8PjDgjvifEocL-5" id="fnref-ZwSi8PjDgjvifEocL-5"&gt;[5]&lt;/a&gt;&lt;/sup&gt;故事是这样的：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;当模型具有情境意识时，它可能不会追求最大奖励目标（即，在训练数据上追求的目标大致会导致与模型的其他功能一致的最大奖励）。相反，它将追求一些低于最大奖励的代理目标。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;但那时，世界模型将包含模型训练游戏所需的所有信息。 （然而，因为我们在这里关注的是关于阴谋的“训练游戏&lt;em&gt;依赖&lt;/em&gt;”故事，我们假设在获得态势感知时，模型的目标还不是激发阴谋——而是进行一些进一步的修改为此需要。）&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;此时，SGD 修改模型以获得最大奖励的最简单方法就是使其成为一个计划者。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;所以：SGD 将使模型成为一个策划者。&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;现在，一种选择是尝试使 (1) 为假。事实上，如上所述，在态势感知之前尝试以良好的方式塑造模型的目标似乎是降低阴谋可能性的关键潜在途径。但请注意，在这种情况下，所面临的挑战比&lt;em&gt;独立于&lt;/em&gt;训练游戏的代理目标更困难。也就是说，对于与训练游戏无关的代理目标，只需在态势感知之前避免给模型提供类似策划者的目标，因此它可以容忍所讨论的目标可能尚未达到最大奖励的&lt;em&gt;其他&lt;/em&gt;方式。相比之下，在“最近的最大奖励目标”论点的背景下，人们需要要么实际创建一个最大奖励、非类似阴谋的目标，要么&lt;em&gt;足够接近&lt;/em&gt;（3）不适用的目标。也就是说，你不能仅仅阻止“自然阴谋”的出现，而必须阻止它的出现。你需要确保当 SGD 稍后“搜索”最近的最大奖励目标时，它不会首先找到阴谋者。&lt;/p&gt;&lt;p&gt;现在，我们假设我们还没有完全掌握这一点，并且（1）是正确的（尽管：我们所达到的掌握&lt;em&gt;程度&lt;/em&gt;对我将在下面讨论的“接近度竞赛”很重要）。 (2) 源自态势感知的定义（或者至少是一个宽松的定义，例如“参与阴谋并认识到阴谋的好处所必需的对世界的理解”），所以我们也承认这一点。&lt;/p&gt;&lt;p&gt;但是（3）呢？我认为，在这里，论证还有更多工作要做。为什么认为将模型打造成阴谋者将是使其获得最大奖励的最简单方法？例如，为什么不把它变成一个训练圣人，或者一个剧集奖励寻求者，或者一个追求某种最大奖励代理目标的错误概括的非训练游戏玩家？根据假设，该模型具有代表&lt;em&gt;任何&lt;/em&gt;这些目标的概念工具。 （请注意，在这方面，它与大多数进化历史中的人类不同，人类&lt;em&gt;没有&lt;/em&gt;概念工具来表示“包容性遗传适应性”等目标。）因此，我们需要一个故事来说明为什么该论证会赋予像阴谋家一样的特权。特别是目标。&lt;/p&gt;&lt;p&gt;为了清楚起见：当我谈论 SGD 可以进行某种修改的“轻松”程度，或者所得到的模型的“接近度”时，这是“SGD‘更喜欢’的那种修改”的代表。本身就是“SGD 实际上将进行的那种修改”的替代品。在机械层面上，这大致意味着：奖励景观中最陡梯度的方向。我经常会想象一种更模糊的感觉，即 SGD 可以做的“工作”预算有限，因此希望在修改模型目标时做尽可能少的“工作”，以便它可以专注于改进模型认知的其他方面。&lt;/p&gt;&lt;p&gt;那么，根据对“轻松”的理解，我们应该期望哪个模型类对于 SGD 来说是“最容易”创建的，即从一个新的情境感知模型中创建一个低于最大奖励代理目标的模型。自己不能激发阴谋吗？&lt;/p&gt;&lt;p&gt;作为一个有助于激发直觉的松散类比：想象一下将人类技术冻结在当前水平，并让进化选择在人类身上运行更长时间。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-ZwSi8PjDgjvifEocL-6" id="fnref-ZwSi8PjDgjvifEocL-6"&gt;[6]&lt;/a&gt;&lt;/sup&gt;从长远来看，你预计哪种人类（或人类后裔生物）将占据主导地位？特别是：您期望：&lt;/p&gt;&lt;p&gt; (a) 人类本质上重视“&lt;a href="https://en.wikipedia.org/wiki/Inclusive_fitness"&gt;包容性遗传适应性&lt;/a&gt;”或“我的基因被进化选择”（这是对“指定目标”和“奖励过程”的某种组合的松散类比），&lt;/p&gt;&lt;p&gt; （b）人类重视与包容性遗传适应性密切相关的其他事物（例如，“拥有尽可能多的孩子”），但他们没有明确地优化包容性遗传适应性，甚至是工具性的（这是对错误的松散类比）广义的非训练游戏玩家），或&lt;/p&gt;&lt;p&gt;(c) 具有长期目标的人类，他们正在优化包容性遗传适应性，特别是作为一种在长期内为自己的价值观获得权力的方法（这对阴谋家来说是一个宽松的类比）。&lt;/p&gt;&lt;p&gt;在这里，“最近的最大奖励目标”故事的类比会问这样的问题：对现有人类价值观的最小修改是什么（或者更好：特别是对进化最容易的修改）是什么，使我们陷入困境之一如上所述，同时也与所面临的进化选择压力的大小兼容？ &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-ZwSi8PjDgjvifEocL-7" id="fnref-ZwSi8PjDgjvifEocL-7"&gt;[7]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;当然，这个类比带来了大量令人困惑的变量（例如，与进化选择构建学习价值的大脑的基因组而不是价值本身有关的事实，文化积累和模因选择的作用）人口动态、缺乏对未来逃跑/接管机会的明确模拟，等等）。为了使这一类比与机器学习更加相似，最好想象进化直接选择整个人类大脑，配备完整的概念工具来代表所有这些目标，而不是选择构建大脑的基因组。更好&lt;em&gt;地&lt;/em&gt;想象一个从一开始就选择大脑的进化版本，这样我们开始理解进化时的人类价值观可能会完全不同。不管怎样，我对最佳预测没有任何明确的信念。但我认为这是“最近的最大奖励目标”论点的一个令人回味的直觉泵。&lt;/p&gt;&lt;h3&gt; SGD 渐进主义对类似阴谋家的修改的障碍&lt;/h3&gt;&lt;p&gt;不过，让我们关注实际的论证，而不是类比。我们应该期望哪个模型类在相关意义上是“最接近的”？&lt;/p&gt;&lt;p&gt;上面我讨论了阴谋者可能被快速排除的一种方式：即，如果 SGD 无法通过修改模型时遵循的梯度来“注意到”类似阴谋者修改的好处。也就是说，正如我之前讨论的，在寻找最大奖励目标时，SGD 不仅仅“跳”到最近的目标。相反，它需要在梯度计算的基础上逐步实现这样的目标，梯度计算表明模型权重在相关方向上的微小变化会增加奖励。在许多情况下，尚不清楚类似阴谋者的修改是否可以像这样工作。&lt;/p&gt;&lt;p&gt;因此，例如， &lt;a href="https://www.lesswrong.com/posts/A9NxPTwbw6r6Awuwt/how-likely-is-deceptive-alignment#Deceptive_alignment_in_the_high_path_dependence_world"&gt;Hubinger (2022)&lt;/a&gt;考虑了一个例子，其中 SGD 通过将模型从关心剧集中的金币修改为始终关心金币来引发阴谋。然而，在所讨论的示例中，SGD 并不是逐渐延长模型金币关注的时间范围，每次延长都会导致奖励的提高。相反，SGD 只是做了“一个简单的改变”——即完全放弃目标的时间限制——从而创建了一个阴谋者。但问题是：奖励空间的梯度是否反映了这样做的好处？在我看来，发生这种情况的最自然的方式是，是否有能力从一种模型平滑地过渡到另一种模型，这样每次修改都会逐渐获得更多的策划好处。但目前尚不清楚这是否会发生。正如我之前讨论的，如果我们假设 SGD 还需要构建大量新机器来执行策划所需的工具推理（而不是仅仅重定向预先存在的“目标实现引擎”），那么任务变得更具挑战性。&lt;/p&gt;&lt;h3&gt;哪个模型是“最接近的”？&lt;/h3&gt;&lt;p&gt;然而，根据我之前的讨论，我也不觉得我能够排除这种类型的增量转换可能发生的可能性（例如，也许足够高的维空间允许 SGD“找到一种方法”） ”），而且我还没有尝试进行深入分析。因此，虽然我认为这种类型的渐进主义对专注于依赖训练游戏的阴谋者目标的故事提出了相对强烈的反对，但我认为也值得评估这些故事的其他方面。也就是说，假设SGD&lt;em&gt;能够&lt;/em&gt;注意到将“最近的最大奖励目标”故事中的模型制作成阴谋者的好处，这样的修改是否是获得高奖励的最简单方法？&lt;/p&gt;&lt;h4&gt;目标空间中类似阴谋家的目标的共性&lt;/h4&gt;&lt;p&gt;对此的一个论据可以追溯到我所认为的支持期待阴谋的核心直觉之一：即可能的阴谋者式目标的绝对&lt;em&gt;数量&lt;/em&gt;。粗略地说，这个想法是，因为&lt;em&gt;如此多的目标&lt;/em&gt;可以激发策划，所以很可能会有一些类似策划者的目标非常&lt;em&gt;接近&lt;/em&gt;模型的当前目标（其中“附近”，回想起来，是根据工作量 SGD 来定义的）必须做才能引起相关类型的修改）。也就是说，阴谋者在目标空间中非常常见——因此 SGD 相应地很容易从任何给定的起点“找到”类似阴谋者的目标。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-ZwSi8PjDgjvifEocL-8" id="fnref-ZwSi8PjDgjvifEocL-8"&gt;[8]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;上面描述的“剧集中的金币”→“所有时间的金币”转变就是这种思想的一个例证。也就是说，如果你假设最充分的长期目标会激发阴谋， &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-ZwSi8PjDgjvifEocL-9" id="fnref-ZwSi8PjDgjvifEocL-9"&gt;[9]&lt;/a&gt;&lt;/sup&gt;那么&lt;em&gt;不&lt;/em&gt;成为阴谋家的最显着的方法就是有一个足够的短期目标。但是（假设我们已经抛开上面讨论的“增量主义”问题），任何给定的时间有限的目标似乎都非常“接近”时间无限的版本。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-ZwSi8PjDgjvifEocL-10" id="fnref-ZwSi8PjDgjvifEocL-10"&gt;[10]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;但时间范围并不是这里可能的“邻近性”的唯一来源。例如，如果模型碰巧有一些尚未激发阴谋的&lt;em&gt;长期&lt;/em&gt;目标（例如，因为目标不够“雄心勃勃”），我们可以改变其他相关变量。因此，如果模型开始关心以不受时间限制的方式正确解决一些数学问题，但它可以在密谋接管世界以获得更多计算来检查和重新计算的情节中很好地解决问题检查它的答案似乎不值得，我们可能会想象 SGD，例如，改变模型对得到错误答案的风险的容忍度，或者改变它的目标，以便它想要解决&lt;em&gt;更多的&lt;/em&gt;数学问题，或者你有什么。总的要点是：在尝试创建计划程序时，SGD 有大量选项可供选择，因此很容易在模型起点附近找到一个选项。&lt;/p&gt;&lt;p&gt;因此，在进化和人类的背景下，这种思想的类比是这样的：即使是相当短期的、适度的人类价值观，也可以相对容易地将它们转变为长期的、雄心勃勃的价值观。例如，人类长期主义者似乎会采取“对我周围环境中满足各种条件的人表现出模糊的友善和帮助”之类的价值观，并将其转化为更加雄心勃勃的目标——这些目标似乎有动力明确优化以获取选择-通过进化，如果这是未来电力的关键瓶颈。事实上，在我看来，在实践中，如果你进行进化选择足够长的时间，在长期范围内优化的意识形态群体最终将占据主导地位（许多最接近优化生殖适应性的群体） ，例如，看起来他们有这种味道）。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-ZwSi8PjDgjvifEocL-11" id="fnref-ZwSi8PjDgjvifEocL-11"&gt;[11]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;一旦我们搁置了上面关于渐进主义的反对意见，我认为目标空间中类似阴谋家的目标的共性是一个相当有力的理由，担心新的情境意识的最近的最大奖励修改，仍然不是-最大奖励目标追求模型将走向心计。更重要的是，请注意，一旦我们想象 SGD 在目标空间中主动&lt;em&gt;搜索&lt;/em&gt;会激发阴谋的目标，我们关于阴谋起源的故事就可以更容忍阴谋者般的目标，必须有其他更具体的目标属性，例如资源匮乏性。也就是说，在专注于训练与游戏&lt;em&gt;无关的&lt;/em&gt;目标的环境中，有可能质疑 SGD 落在类似阴谋者的目标上的概率（甚至以它落在更一般的超越情节目标为条件），凭借不同的超越情节的目标意味着对未来力量、风险等的不同偏好。但在像这样的&lt;em&gt;依赖于&lt;/em&gt;训练游戏的环境中，SGD 正在积极&lt;em&gt;寻找&lt;/em&gt;这样的目标（并且相关目标在目标空间中仍然很常见——例如，您可以“在所有空间和时间上最大化&lt;em&gt;x&lt;/em&gt;的实例”） “ 对于&lt;em&gt;x&lt;/em&gt;的许多值）。因此，如果 SGD 找到了它想要的东西，也就不足为奇了。&lt;/p&gt;&lt;p&gt;然而，我认为这种论点并不是决定性的，原因有二。&lt;/p&gt;&lt;h4&gt;非策划目标的接近程度&lt;/h4&gt;&lt;p&gt;第一个原因是，即使我们坚持纯粹专注于修改模型的目标，并采用“干净的目标导向”框架（我将在下面使这样的焦点复杂化），我认为还有一个抵消的“接近性”考虑这至少支持一些非计划目标——即，在其他条件相同的情况下，模型的初始训练（在态势感知之前）似乎更有可能主动将模型指向这些非计划目标的方向。&lt;/p&gt;&lt;p&gt;这对于指定的目标来说是最清楚的，根据定义，这是通过培训来奖励的。这意味着，即使模型在具有态势感知能力时最终没有&lt;em&gt;完全&lt;/em&gt;达到指定的目标，它也可能最终得到广泛附近的目标。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;因此，举例来说，如果模型正在接受训练以最大化剧集中的金币，那么它最终会学习一个诸如“最大化剧集中的金币”之类的目标（这个目标&lt;em&gt;不太&lt;/em&gt;正确，因为有些训练环境包括圆形金币（非硬币），看来 SGD 最简单的修改就是将模型变成&lt;em&gt;金币&lt;/em&gt;on-the-episode-getter（即训练圣人）。 ）。或者至少，就“接近度”而言，“情节中的金币”似乎与“有史以来的圆形金币”&lt;em&gt;具有竞争力&lt;/em&gt;。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;对于更多 RLHF 风格的指定目标，我们可能会提出类似的观点。也就是说，如果您正在训练一个模型以一种广泛有帮助、诚实且无害的方式遵循剧集中的说明​​（为简单起见，您的 RLHF 流程仅在这方面提供准确的反馈），那么即使它当它变得具有情境意识时，它已经学会了一种有点不平衡的概念，例如诚实，改进其诚实概念，直到它更好地适合人类概念，似乎具有竞争性，回复：接近，具有更多类似阴谋家的动机修改，例如“偏离-始终保持诚实，以激励计划逃脱人类控制并夺取权力。”&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;请注意，在人类和进化的背景下，人类确实已经重视与包容性遗传适应性密切相关的各种事物 - 例如，生孩子，更广泛地拥有繁荣而强大的家庭血统等。如果我们想象进化选择整个人类大脑配备了代表“包容性遗传适应性”的概念装置，人类最初进化到重视与包容性遗传适应性非常接近的东西似乎更容易想象。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-ZwSi8PjDgjvifEocL-12" id="fnref-ZwSi8PjDgjvifEocL-12"&gt;[12]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;请注意，我们也可以在这里对错误概括的非训练游戏玩家提出类似的观点，只要他们追求最大奖励目标（回想一下，根据我的定义，错误概括的目标可以是最大奖励，前提是特别是&lt;em&gt;训练数据&lt;/em&gt;永远不会惩罚它们——例如，它们只会在模型从未见过的反事实场景中受到奖励过程的惩罚。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;因此，例如，想象一个训练过程，它永远不会区分“在剧集中获得金币”和“在剧集中获得一般金币”之类的目标，因为训练数据中的所有金币都是硬币。假设在态势感知之前，我们的模型学习了一个目标，比如“在剧集中获得金币，但有时也会跟随你对剧集的好奇心，即使这意味着牺牲金币”——这个目标可以实现（但不是最大程度地实现）好吧）在训练中，因为好奇心并不是一种非常有害的行为，有时甚至是一种有益的行为。在这里，似乎只是删除模型的好奇心并只专注于获得剧集中的黄金物品（最大奖励目标，即使不是指定目标）是一个相当“附近”的修改 - 并且是一个看似合理的修改在“接近”方面具有竞争力，例如修改模型，使其始终追求其黄金和好奇心的组合。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;在人类和进化的背景下，如果我们想象，从进化选择的角度来看，“拥有尽可能多的孩子”的内在价值会表现得很好（即使这并不&lt;em&gt;完全&lt;/em&gt;是进化选择的目的），但事实并非如此。似乎很难想象人类会朝这个方向前进。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;我们能否对寻求剧集奖励的人提出类似的观点？这有点不太清楚，因为在态势感知之前，尚不清楚模型是否有足够的奖励过程概念，以便他们有动机附加到其组件之一“附近”的某些东西。也就是说，在我看来，在某些情况下这种情况可能会发生。因此，例如，即使缺乏态势感知，通过 RLHF 训练的模型最终也会受到“人类认可”附近概念的激励，这对我来说似乎是合理的。这些概念似乎至少在某种程度上接近奖励过程的各个方面，例如人类评估者和/或奖励模型的判断，这样一旦模型了解了奖励过程，修改其动机以专注于这些组件就不会太困难。对于 SGD 来说，这是一个巨大的飞跃。&lt;/p&gt;&lt;p&gt;总的来说，我认为默认情况下，非策划目标往往会具有某种对他们有利的“接近性”。这并不奇怪。特别是：非策划者目标必须与奖励过程有一些相当直接的联系（例如，它们要么直接受到该过程的奖励，要么因为它们专注于奖励过程本身的某些组成部分），因为与策划者目标不同，非策划者目标不能依赖目标内容完整性或长期权力追求等聚合子目标来确保追求它们会带来回报。因此，我们很自然地期望通过奖励过程来训练模型，在尚未实现阴谋的情境意识背景下，会导致动机集中在非阴谋目标附近的某些事情上。&lt;/p&gt;&lt;p&gt;尽管如此，在给定情况下，这种考虑是否足以使非阴谋家目标比阴谋家目标更&lt;em&gt;接近&lt;/em&gt;模型的当前目标，这仍然是一个悬而未决的问题。重要的是，请注意，相关的竞争是针对&lt;em&gt;整套&lt;/em&gt;附近的类似阴谋者的目标（而不是例如我上面讨论的可能的类似阴谋者的修改的特定示例）——考虑到可能的阴谋者的种类繁多-类似的目标，可能是一个严重的劣势。因此，打个比方：如果鲍勃家的十英里范围内有十家墨西哥餐馆和一百家中国餐馆，那么即使任何给定的墨西哥餐馆与任何给定的中国餐馆“似乎具有竞争力”，re：接近度，那么（模进一步信息）&lt;em&gt;最近的&lt;/em&gt;餐馆可能仍然是中国餐馆。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-ZwSi8PjDgjvifEocL-13" id="fnref-ZwSi8PjDgjvifEocL-13"&gt;[13]&lt;/a&gt;&lt;/sup&gt;根据模型空间中类似阴谋者的目标的共性，我们可能期望类似阴谋者的目标就像这里的中餐馆一样。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-ZwSi8PjDgjvifEocL-14" id="fnref-ZwSi8PjDgjvifEocL-14"&gt;[14]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;h4&gt;混乱的目标导向性与接近度的相关性&lt;/h4&gt;&lt;p&gt;然而，我也认为，一旦我们考虑到“混乱的目标导向性”，非阴谋家就会得到另一种明显的亲近感提升。特别是：如果我们想象所讨论的模型不是一个完全可分离的目标槽+目标实现引擎，而是不同启发式、注意力模式、冲动等的错综复杂的混合物，那么我们需要分析比较“接近度”不仅是当前模型的当前&lt;em&gt;目标&lt;/em&gt;与最近的类阴谋者和非类阴谋者目标的比较“接近度”，而且是模型的&lt;em&gt;整个组合&lt;/em&gt;与最近的类阴谋者和非类阴谋者模型的接近程度。在这里，我认为非阴谋模型可能具有更显着的优势。&lt;/p&gt;&lt;p&gt;特别是：在一定程度上，预先情境意识训练往往会产生一种混乱的目标导向，指向非策划目标&lt;em&gt;附近&lt;/em&gt;的某些东西——特别是一个&lt;em&gt;片段内的&lt;/em&gt;非策划目标——那么这意味着模型的启发式、注意力模式、冲动等也倾向于指向这个大方向，并且特别集中在情节内。对我来说，这使得将模型重新定位为更长期的权力追求似乎是一个更实质性和更困难的修改，特别是如果所讨论的情节非常短的话。也就是说，直观上，对我来说，对于 SGD 来说，“调整一个完全可分离的片内目标以使其长期化”比“重新引导广泛关注片内事物的错综复杂的目标以使其长期化”要容易得多。它（a）专注于某些超出情节的事情，并且（b）使得这种超出情节的焦点回溯到出于工具性原因而获得奖励”——特别是如果（b）需要建立新的认知机制来实施工具性推理有问题。而“将集中在情节内事物上的错综复杂的事物重新引导到同一范围内的更高奖励的情节内事物”（例如，转向金币，而不是金圆形事物；或者转向真正的诚实，而不是失衡）诚实）在我看来直观上更容易。&lt;/p&gt;&lt;h3&gt;对“最近的最大奖励目标”论点的总体看法&lt;/h3&gt;&lt;p&gt;总结我对期待阴谋者的“最近的最大奖励目标”论点的看法：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;如果由于增量修改没有产生这些好处，SGD 无法注意到将模型转变为计划者的好处，那么论证就会失败。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;然而，我认为我们不能指望新元无法注意到这些好处。如果&lt;em&gt;可以&lt;/em&gt;，那么我认为目标空间中类似阴谋家的目标的共性使得类似阴谋家的目标与模型当前目标“最接近”的可能性相当令人担忧。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;也就是说，因为它是在缺乏态势感知的情况下由奖励过程决定的，所以该模型的目标也可能已经在某些最大奖励非计划目标的“附近”，这是有利于一些非阴谋模型的“接近”。在某种程度上，所涉及的目标导向性相当“混乱”，创建这样一个非阴谋者可能需要对模型的启发式、注意力模式、工具推理等进行较少的修改。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这些考虑让我对阴谋者产生了一些实质性的额外担忧。&lt;/p&gt;&lt;h2&gt;简单性和速度等属性与 SGD 所采用路径的可能相关性&lt;/h2&gt;&lt;p&gt;如果我们假设任何给定的最大奖励目标在训练中都能产生足够好的性能，并且 SGD 并不特别关心模型最终达到&lt;em&gt;哪个&lt;/em&gt;最大奖励目标，那么像“最近的最大奖励目标论证”这样的争论就会最自然地进行。但正如我上面所指出的，正如我将在有关不同模型类的最终属性的部分中更详细地讨论的那样，有一些故事表明，基于简单性等最终属性，SGD 积极地倾向于其中一些模型类而不是其他模型类和速度。&lt;/p&gt;&lt;p&gt;更重要的是，这种偏好不仅仅与忽略 SGD 在模型空间中采用的路径的预测相关。相反，它们可能会在各个阶段&lt;em&gt;影响&lt;/em&gt;该路径（即使最终结果在更广泛的意义上仍然是“路径依赖”）。例如，如果 SGD 偏向于更简单的目标，那么这种偏差可能会影响模型在态势感知之前或之后形成的与训练游戏无关的目标，以及在依赖于训练游戏的目标故事上，它可能是有利于 SGD 从尚未获得最大奖励的起点专门转向类似阴谋家的目标的额外要点。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-ZwSi8PjDgjvifEocL-15" id="fnref-ZwSi8PjDgjvifEocL-15"&gt;[15]&lt;/a&gt;&lt;/sup&gt;同样，如果 SGD 由于需要额外的推理而对阴谋者产生偏见，那么这种偏见可能是有利于期望 SGD 从这样的起点转向非阴谋者的额外观点。&lt;/p&gt;&lt;p&gt;正如我将在第 4 节“最终属性”中讨论的那样，我通常认为策划的速度成本比简单性收益更重要，因此在其他条件相同的情况下，我认为这些考虑因素不利于策划。但我不清楚这两个方向的成本/收益是否特别重要。&lt;/p&gt;&lt;p&gt;也就是说，至少在某些情况下，这些成本/收益在训练的早期&lt;em&gt;更为&lt;/em&gt;重要，因为模型在早期可能&lt;em&gt;较弱&lt;/em&gt;，因此减少了简单性和速度所提供的资源种类（例如，参数和计算）将对模型的认知能力产生更大的总体影响。也就是说：也许，在训练的早期，认知资源更加稀缺，因此更有必要保存。因此，例如，当一般可用的工具推理预算较小时，也许要求一个模型形成一个长期的、类似阴谋家的计划是一个更大的要求（并且对奖励的影响更大）；或者，当较少的参数尚未形成有用的认知结构时，要求它使用更多的参数来存储更复杂的目标可能会更加繁重。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-ZwSi8PjDgjvifEocL-16" id="fnref-ZwSi8PjDgjvifEocL-16"&gt;[16]&lt;/a&gt;&lt;/sup&gt;因此，在某种程度上，人们会被这样一种观点所诱惑，即相对于其他考虑因素，这些类型的成本可能“在噪音中”（我很受这种观点的诱惑，我将在下面讨论），人们可能会与模型的最终属性相比，训练的早期部分较少受此诱惑。&lt;/p&gt;&lt;h2&gt;对关注 SGD 所采取路径的论点的总体评估&lt;/h2&gt;&lt;p&gt;不过，总的来说，尽管非阴谋者可能具有速度优势，但我发现“与训练游戏无关的代理目标”论点和“最近的最大奖励目标论点”的结合相当令人担忧。尤其：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;在我看来，尽管我们在平凡的对抗性训练方面做出了努力，特别是在一个我们有目的地塑造我们的模型以实现长期且相当雄心勃勃的目标的政权中，但某种适当的雄心勃勃的、错位的、超越情节的目标可能会突然出现。自然地脱离训练——无论是在态势感知之前，还是之后——然后导致阴谋的发生。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;即使这&lt;em&gt;不会&lt;/em&gt;自然发生，我也担心当它达到态势感知时，SGD 给模型一个最大奖励目标的最简单方法就是将其变成一个策划者，因为类似策划者目标在目标空间中非常常见，因此它们通常会在情境意识出现时出现在模型低于最大奖励目标的“附近”。 SGD 的“渐进主义”可能消除了这种担忧，和/或我们应该期望非计划模型默认情况下“更接近”（或者因为它们的目标特别接近，或者因为在“混乱的目标导向性”中）设置，它们需要对模型当前错综复杂的启发法进行更简单的修改，或者因为它们的“速度”优势将使 SGD 更喜欢它们）。但我觉得没有信心。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;不过，这两个论点都集中在 SGD 通过模型空间的&lt;em&gt;路径&lt;/em&gt;上。相反，关注相关模型最终属性的争论又如何呢？现在让我们转向那些。 &lt;/p&gt;&lt;hr class="footnotes-sep" /&gt;&lt;section class="footnotes"&gt;&lt;ol class="footnotes-list"&gt;&lt;li class="footnote-item" id="fn-ZwSi8PjDgjvifEocL-1"&gt;&lt;p&gt;例如，这要求模型不能像我上面讨论的内省守门方法那样进行“&lt;a href="https://www.lesswrong.com/posts/uXH4r6MmKPedk8rMA/gradient-hacking"&gt;梯度黑客&lt;/a&gt;”。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-ZwSi8PjDgjvifEocL-1"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-ZwSi8PjDgjvifEocL-2"&gt;&lt;p&gt;我还讨论了他们对特定目标/奖励缺乏“内在热情”是否会产生影响。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-ZwSi8PjDgjvifEocL-2"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-ZwSi8PjDgjvifEocL-3"&gt;&lt;p&gt;感谢 Rohin Shah 在这里进行讨论。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-ZwSi8PjDgjvifEocL-3"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-ZwSi8PjDgjvifEocL-4"&gt;&lt;p&gt;事实上，如果我们假设预训练本身&lt;em&gt;会导致&lt;/em&gt;情境意识，但不会导致超越情节的、阴谋激励的目标，那么这将是阴谋者如何在预训练然后微调制度中出现的默认故事。感谢 Evan Hubinger 对此进行了标记。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-ZwSi8PjDgjvifEocL-4"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-ZwSi8PjDgjvifEocL-5"&gt;&lt;p&gt;我认为这个故事与胡宾格所说的“世界模型悬垂”故事有关，但又不同，这个故事（据我所知）大致如下：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;当模型变得具有情境意识时，其目标可能不会达到追求目标与获得高奖励完全相关的程度。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;但是，到那时，它的世界模型将包含训练游戏所需的所有信息。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;因此，在那之后，SGD 将能够通过修改模型以获得激励训练游戏的超集目标来获得大量物超所值的奖励。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;相比之下，通过将模型修改为更像训练圣人，它可能能够获得更少的经济效益，因为在这个方向上的边际努力仍然可能使模型的目标与奖励不完全相关（或者在至少，由于必须等待未来打破相关性的训练片段的修正，因此需要更长的时间才能达到完美）。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;因此，SGD 将创建激励训练-游戏的超集目标（然后这些目标将会具体化）。&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt; Hubinger 框架的一个问题是，在我看来，他的本体论在我感兴趣的意义上忽略了按集奖励寻求者，而 SGD 将模型修改为按集奖励寻求者至少会做到这一点同样，在这个论点上，将其修改为阴谋者。我不清楚他关于“收益递减”的想法到底应该如何发挥作用（尽管我上面使用的“近乎”修改的本体论是一种重构）。&lt;/p&gt;&lt;p&gt;也就是说，我认为最终，“最近的高奖励目标”故事和“世界模型悬垂”故事可能试图指出相同的基本思想。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-ZwSi8PjDgjvifEocL-5"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-ZwSi8PjDgjvifEocL-6"&gt;&lt;p&gt;感谢 Daniel Kokotajlo、Rohin Shah、Tom Davidson 和 Paul Christiano 对此类示例的讨论。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-ZwSi8PjDgjvifEocL-6"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-ZwSi8PjDgjvifEocL-7"&gt;&lt;p&gt;请注意，虽然当前的制度看起来最像（b），但所讨论的“与包容性遗传适应性的相关性”（例如，快乐、地位等）似乎明显不完美，并且根据生殖适应性似乎很容易表现得更好比目前大多数人所做的还要多。另外，人类直到最近才&lt;em&gt;了解&lt;/em&gt;进化选择（这是对态势感知的一个松散的类比）。所以问题是：现在我们了解了作用在我们身上的选择压力，并且假设这种选择压力持续很长一段时间，它会将我们带向何方？ &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-ZwSi8PjDgjvifEocL-7"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-ZwSi8PjDgjvifEocL-8"&gt;&lt;p&gt;我的印象是，一些本体论会尝试将“从给定起点轻松找到阴谋者”与阴谋者往往很简单的想法联系起来，但我不会在这里尝试这样做，我的模糊感觉是这种的举动搅浑了水。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-ZwSi8PjDgjvifEocL-8"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-ZwSi8PjDgjvifEocL-9"&gt;&lt;p&gt;不过：他们会雄心勃勃吗？ &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-ZwSi8PjDgjvifEocL-9"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-ZwSi8PjDgjvifEocL-10"&gt;&lt;p&gt;请注意，人类长期主义者一开始的非系统化价值观与大多数在短期内进行优化的人类非常相似——因此，至少在人类的情况下，一个方向与另一个方向的差异似乎非常小。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-ZwSi8PjDgjvifEocL-10"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-ZwSi8PjDgjvifEocL-11"&gt;&lt;p&gt;感谢 Daniel Kokotajlo 在这里进行讨论。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-ZwSi8PjDgjvifEocL-11"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-ZwSi8PjDgjvifEocL-12"&gt;&lt;p&gt;在这里，我抛开对人类价值观如何在基因组中编码的担忧，并想象进化选择与机器学习更相似，但事实并非如此。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-ZwSi8PjDgjvifEocL-12"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-ZwSi8PjDgjvifEocL-13"&gt;&lt;p&gt;也就是说，如果中餐馆的距离是相关的（例如，因为它们都在同一个街区），那么这个反对的作用就不太顺利。似乎有道理的是，所有类似阴谋家的目标之间至少存在一些相似之处，可能会产生这种类型的相关性。例如：如果模型以情节内目标开始，那么任何类似计划者的目标都将需要扩展模型关注的时间范围 - 因此，如果这种扩展通常需要 SGD 进行某种类型的工作，那么如果非阴谋家目标需要的工作量少于&lt;em&gt;该目标&lt;/em&gt;，那么它可能会击败&lt;em&gt;所有&lt;/em&gt;最接近的阴谋家目标。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-ZwSi8PjDgjvifEocL-13"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-ZwSi8PjDgjvifEocL-14"&gt;&lt;p&gt; &lt;a href="https://www.lesswrong.com/posts/A9NxPTwbw6r6Awuwt/how-likely-is-deceptive-alignment#Deceptive_alignment_in_the_high_path_dependence_world"&gt;Hubinger (2022)&lt;/a&gt;还对这样的观点提出了不同的反对意见，即在此类竞争中，SGD 可能会选择非策划者目标而不是类似策划者的目标——即，实现非策划者最大奖励目标的过程将是一条“漫长而艰难的道路”（例如，参见他在高路径依赖部分的可纠正对齐位中对鸭子学习关心其母亲的讨论）。不过，我觉得我并没有真正理解胡宾格的推理。我最好的重构是这样的：为了选择一个非计划目标，Hubinger 想象 SGD 不断选择逐渐不完美的目标（但仍然不是完全最大奖励目标），然后必须等待通过训练来纠正遇到一个事件，这些目标的缺陷被暴露出来；而如果它只是为了一个类似阴谋家的目标，它就可以跳过这个漫长的过程。但这还不能解释为什么 SGD 不能通过直接追求最大奖励非 schemerr 目标来跳过漫长的过程。也许问题应该是关于训练数据的噪音和可变性？我不知道。目前，我希望至少对这个论点的一些解释能够在上面“接近性”的讨论中得到涵盖，和/或胡宾格论点的最佳形式将通过我自己以外的工作得到澄清。 （另请参阅&lt;a href="https://markxu.com/deceptive-alignment#corrigibly-aligned-models"&gt;Xu (2020)&lt;/a&gt;版本的 Hubinger 论点，在“可正确对齐模型”部分。尽管如此：快速阅读一下，在我看来，Xu 似乎专注于预先情境意识目标形成过程，并假设基本上&lt;em&gt;任何&lt;/em&gt;错位的后情境意识都会导致阴谋，这样他的故事实际上是一个&lt;em&gt;独立于&lt;/em&gt;训练游戏的故事，而不是我在这里关注的那种&lt;em&gt;依赖于&lt;/em&gt;训练游戏的故事。） &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-ZwSi8PjDgjvifEocL-14"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-ZwSi8PjDgjvifEocL-15"&gt;&lt;p&gt;至少如果我们理解简单性的方式是&lt;em&gt;增加一些&lt;/em&gt;概念，即类似阴谋家的目标在目标空间中很常见，而不是仅仅通过其共性来&lt;em&gt;定义&lt;/em&gt;目标的简单性（或：一种目标？）在球门空间。有关此类区别的更多信息，请参阅下面的第 4.3.1 节。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-ZwSi8PjDgjvifEocL-15"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-ZwSi8PjDgjvifEocL-16"&gt;&lt;p&gt;我从保罗·克里斯蒂亚诺那里听到了这种考虑。乍一看，这种效果在我看来在简单性/参数和速度/计算之间相当对称（我不清楚这是否是值得关注的正确区别），所以我不认为早期训练动态是作为一种重要资源，对一种人与另一种人&lt;em&gt;有不同的&lt;/em&gt;偏爱。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-ZwSi8PjDgjvifEocL-16"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/section&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/KyuMS9XzqaJGMu74f/arguments-for-against-scheming-that-focus-on-the-path-sgd#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Tue, 05 Dec 2023 18:48:12 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/KyuMS9XzqaJGMu74f/arguments-for-against-scheming-that-focus-on-the-path-sgd</guid></item><item><title>为 Helen Toner、Adam D&amp;#39;Angelo 和 Tasha McCauley 辩护（OpenAI 帖子）</title><link>https://www.lesswrong.com/posts/csjjHqLnRr8dvmyoN/in-defence-of-helen-toner-adam-d-angelo-and-tasha-mccauley</link><description>发布于 2023 年 12 月 5 日下午 6:40（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;这与我对 OpenAI 板发生的事情的一些思考很接近，我想知道其他人的想法。&lt;br /&gt;&lt;br /&gt; “我认为：&lt;/p&gt;&lt;p&gt; 1) TDM 的行动使开放 AI 的情况变得更好，因为与他们什么都不做的反事实相比，它的情况要好得多。&lt;/p&gt;&lt;p&gt; 2）就预期或实现的好或坏结果而言，人们应该会发现前者令人惊喜，而后者基本上已被定价，因为从安全角度来看，OpenAI 的情况已经非常糟糕。&lt;/p&gt;&lt;p&gt; 3) 无论你是一个‘荣誉和正直的最高主义者’还是‘无情的战略家’，无论从哪种角度来看，TDM 的行动通常都会表现得非常出色。”&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/csjjHqLnRr8dvmyoN/in-defence-of-helen-toner-adam-d-angelo-and-tasha-mccauley#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Tue, 05 Dec 2023 21:56:54 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/csjjHqLnRr8dvmyoN/in-defence-of-helen-toner-adam-d-angelo-and-tasha-mccauley</guid></item><item><title>研究外星人的心灵</title><link>https://www.lesswrong.com/posts/suSpo6JQqikDYCskw/studying-the-alien-mind-1</link><description>发布于 2023 年 12 月 5 日下午 5:27（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;这篇文章是&lt;a href="https://www.lesswrong.com/posts/yuwdj82yjhLFYessc/preface-to-the-sequence-on-llm-psychology"&gt;&lt;i&gt;&lt;u&gt;法学硕士心理学系列&lt;/u&gt;&lt;/i&gt;&lt;/a&gt;&lt;i&gt;的一部分&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vmPBWNbmmjPNXhQeQ/kqyglndkjps2mttt7pqy" /&gt;&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;长话短说&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;我们介绍了通过研究法学硕士行为来探索法学硕士认知的自上而下方法的观点，我们将其称为&lt;strong&gt;法学硕士心理学&lt;/strong&gt;。在这篇文章中，我们采取将法学硕士视为“外星人思想”的心理立场，将他们的研究与动物认知研究进行比较和对比。我们这样做既是为了向过去试图理解非人类认知的研究人员学习，也是为了强调法学硕士的研究与生物智能的研究有多么不同。具体来说，我们提倡田野工作和实验心理学之间的共生关系，并警告实验设计中隐含的拟人化。我们的目标是建立法学硕士认知模型，帮助我们更好地解释他们的行为，并减少对他们与先进人工智能风险的关系的困惑。&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;介绍&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;当我们努力预测和理解像 GPT4 这样的大型语言模型 (LLM) 的行为时，我们可能会认为这需要打破黑匣子，并对其内部机制形成还原性解释。这类研究的典型代表是机械可解释性等方法，它试图通过打开黑匣子并观察内部来直接理解神经网络的工作原理。&lt;/p&gt;&lt;p&gt;虽然机械解释性为法学硕士提供了富有洞察力的自下而上的分析，但我们仍然缺乏更全面的自上而下的方法来研究法学硕士认知。如果可解释性类似于“人工智能的神经科学”，旨在通过了解人工智能的内部结构来理解其机制，那么这篇文章试图从心理学的角度来研究人工智能。 &lt;span class="footnote-reference" id="fnrefy6y87ybnhmg"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fny6y87ybnhmg"&gt;[1]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;我们所说的&lt;strong&gt;法学硕士心理学&lt;/strong&gt;是一种替代的、自上而下的方法，涉及通过检查他们的行为来形成法学硕士认知的抽象模型。与传统的心理学研究一样，我们的目标不仅仅是对行为进行分类，还包括推断隐藏变量，并拼凑出对潜在机制的全面理解，以阐明系统行为的原因。&lt;/p&gt;&lt;p&gt;我们的立场是，法学硕士类似于外星人的思想——与他们&lt;a href="https://www.lesswrong.com/s/SAjYaHfCAGzKsjHZp/p/HxRjHq3QG8vcYy4yy"&gt;&lt;u&gt;只是随机鹦鹉的&lt;/u&gt;&lt;/a&gt;观念不同。我们假设他们拥有高度复杂的内部认知，包含对世界和心理概念的表征，而不仅仅是训练数据的随机反刍。这种认知虽然源自人类生成的内容，但从根本上与我们的理解不同。&lt;/p&gt;&lt;p&gt;这篇文章汇集了一些关于成功的法学硕士心理学研究可能需要什么的高层次考虑，以及对非人类认知的历史研究的更广泛的讨论。特别是，我们主张保持实验和现场工作之间的平衡，利用法学硕士和生物智能之间的差异，并设计专门针对法学硕士作为其独特思维类别的实验。&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;实验与现场研究&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;从中汲取灵感的一个地方是对动物行为和认知的研究。虽然动物的思维很可能比人工智能更类似于我们的思维（至少在机械上），但非人类智能研究的历史、它所开发的方法的演变以及它所面临的挑战解决这个问题可以为研究人工智能系统提供灵感。&lt;/p&gt;&lt;p&gt;正如我们所看到的，动物心理学有两种流行的类别：&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;实验心理学&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;第一种也是最传统的科学方法（也是大多数人在听到“心理学”一词时想到的）是设计控制尽可能多的变量的实验，并测试特定的假设。&lt;/p&gt;&lt;p&gt;一些特别著名的例子是 Ivan Pavlov 或 BF Skinner 所做的工作，他们将动物置于&lt;a href="https://en.wikipedia.org/wiki/Operant_conditioning_chamber"&gt;&lt;u&gt;高度控制的环境&lt;/u&gt;&lt;/a&gt;中，对它们进行刺激，并记录它们的反应。 &lt;span class="footnote-reference" id="fnrefrcfxkkdze9"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrcfxkkdze9"&gt;[2]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;此类工作的目的是找到解释所记录行为的简单假设。尽管自这些早期研究人员以来，实验心理学已经发生了&lt;a href="https://en.wikipedia.org/wiki/Cognitive_revolution"&gt;&lt;u&gt;很大变化&lt;/u&gt;&lt;/a&gt;，但重点仍然是通过坚持科学方法的传统方法来优先考虑结果的可靠性和可复制性。这种方法虽然严格，但却牺牲了研究人员和受试者之间信息交换的带宽，有利于&lt;strong&gt;控制混杂变量&lt;/strong&gt;，这实际上可能&lt;a href="https://www.lesswrong.com/posts/9kNxhKWvixtKW5anS/you-are-not-measuring-what-you-think-you-are-measuring"&gt;&lt;u&gt;导致研究结果不太可靠&lt;/u&gt;&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;无论如何，实验心理学一直是我们理解动物认知的历史方法的核心支柱，并产生了许多重要的见解。一些有趣的例子包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://www.cell.com/current-biology/pdf/S0960-9822(07)01770-8.pdf"&gt;&lt;u&gt;对新喀里多尼亚乌鸦的一项研究&lt;/u&gt;&lt;/a&gt;揭示了它们自发解决复杂的元工具任务的能力。这种行为展示了复杂的物理认知并建议使用类比推理。&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.nature.com/articles/26216"&gt;&lt;u&gt;对灌丛鸦进行的一项研究&lt;/u&gt;&lt;/a&gt;表明，它们不仅能够回忆起所储存食物的位置，还能够回忆起食物的时间。这种行为反映了情景记忆，这是一种以前被认为是人类独有的记忆形式。&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.sciencedirect.com/science/article/abs/pii/S0003347207004435"&gt;&lt;u&gt;在实验中&lt;/u&gt;&lt;/a&gt;，挪威老鼠在不满意（例如饮食不良或处于不舒服的环境中）或不确定（不知道哪种食物可能有害）时表现出更大的向他人学习的倾向。该研究强调了负面经历或不确定性如何影响老鼠的社会行为。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;&lt;strong&gt;实地考察&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;另一种方法是研究人员亲自花时间与动物相处，减少干预，并专注于&lt;strong&gt;在动物的自然栖息地收集尽可能多的观察结果&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;这种方法最著名的例子是简·古道尔（Jane Goodall）开创的工作，她花了数年时间与野生黑猩猩一起生活并记录其行为。她发现黑猩猩使用工具（以前被认为是人类独有的），有复杂的社会关系，参与战争，并表现出各种各样的情感，包括快乐和悲伤。她的工作彻底改变了我们对黑猩猩的理解。与实验学家不同，她相当乐意通过个人偏见的视角来解释行为，这导致她当时受到了很多批评。 &lt;span class="footnote-reference" id="fnrefjjuyxu93etf"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnjjuyxu93etf"&gt;[3]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;实地研究的其他一些值得注意的例子：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Cynthia_Moss"&gt;&lt;u&gt;辛西娅·莫斯（Cynthia Moss）&lt;/u&gt;&lt;/a&gt;花了几十年时间研究野外的非洲大象，发现大象生活在由女族长领导的高度组织和等级制度的社会中。她花了 30 年的时间跟踪和研究这样一位女族长埃科 ( &lt;a href="https://en.wikipedia.org/wiki/Echo_(elephant)"&gt;&lt;u&gt;Echo&lt;/u&gt;&lt;/a&gt; ) 以及她大家庭的其他成员。&lt;/li&gt;&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Konrad_Lorenz"&gt;&lt;u&gt;康拉德·洛伦茨&lt;/u&gt;&lt;/a&gt;被认为是动物行为学的“奠基人”，他发现了鹅和其他鸟类的许多先天行为，包括印记行为，即鹅学会识别自己物种的成员。当时他特别引人注目的是他对实验室工作的怀疑态度，坚持在自然环境中研究动物，并允许自己想象它们的精神/情绪状态。 &lt;span class="footnote-reference" id="fnrefto55wwc29b9"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnto55wwc29b9"&gt;[4]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt; &lt;a href="https://en.wikipedia.org/wiki/L._David_Mech"&gt;&lt;u&gt;L. David Mech&lt;/u&gt;&lt;/a&gt;对野外狼的行为进行了数十年的研究，引入了“头狼”的概念，后来又揭穿了这一概念，发现圈养狼中的统治等级制度在它们的狼中根本不存在。野生同行。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;虽然实验心理学倾向于（相当故意地）将研究人员与研究对象分开，&lt;strong&gt;但实地研究涉及研究对象与研究人员之间更直接的关系&lt;/strong&gt;。重点是购买带宽，即使这为研究人员的特定偏见打开了大门。尽管存在偏见的担忧，但实地工作已经能够提供一些基础性的发现，而这些发现似乎仅通过实验室实验是不可能实现的。&lt;/p&gt;&lt;p&gt;值得注意的是，有些例子在某种程度上介于我们列出的这两个类别之间，在这些例子中，对动物进行实验室实验的研究人员也与他们研究的动物有着非常密切的个人关系。例如，艾琳·佩珀伯格 (Irene Pepperberg) 花了大约 30 年的时间与一只鹦鹉亚历克斯 ( &lt;a href="https://en.wikipedia.org/wiki/Alex_(parrot)"&gt;&lt;u&gt;Alex&lt;/u&gt;&lt;/a&gt; ) 密切互动，教他执行鸟类中前所未有的各种认知和语言任务。 &lt;span class="footnote-reference" id="fnreftz1fcycplz"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fntz1fcycplz"&gt;[5]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;法学硕士心理学实地考察&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;法学硕士研究的实地研究超出了对模型行为的简单观察和记录；它们代表了发现在受控实验环境中可能不明显的新模式、能力和现象的机会。与机械解释性和法学硕士研究的其他领域（通常需要先了解某种现象才能对其进行研究）不同，实地研究&lt;strong&gt;有可能揭示对语言模型的意想不到的见解&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;此外，实地工作中的偶然发现可以促进各领域之间的合作。从现场观察中收集到的见解可以为对该模型的潜在机制进行有针对性的研究或更广泛的实验研究提供信息，从而创建一个富有成效的反馈循环，引导我们提出新问题并更深入地探究这些复杂系统的“外星人思想”。&lt;/p&gt;&lt;p&gt;部分由于机器学习研究文化，以及对过度解释人工智能行为的合理担忧，现场工作受到的关注远不如实验工作受到的重视。看看实地工作为动物研究增加的价值，消除这种偏见并确保将&lt;strong&gt;实地研究作为我们研究法学硕士认知方法的核心部分&lt;/strong&gt;似乎非常重要。&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;学习LLM是不同的&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;有很多理由认为法学硕士心理学与人类或动物心理学不同。&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;拟人化&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;拟人化视角在法学硕士研究中的效用是一个复杂的课题。虽然法学硕士的运作架构与生物认知显着不同，但他们对人类语言数据的训练使他们能够输出类似人类的文本。这种并置可能会导致对其认知本质的&lt;strong&gt;误导性拟人化假设&lt;/strong&gt;。至关重要的是要&lt;strong&gt;极其谨慎和明确地选择应用哪些拟人化框架&lt;/strong&gt;，并清楚地区分有关 LLM 认知的不同主张。&lt;/p&gt;&lt;p&gt;尽管需要谨慎，但忽视生物认知和人工认知之间的联系可能会导致忽视有用的假设并显着减慢研究速度。 &lt;span class="footnote-reference" id="fnrefopotd06vo7"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnopotd06vo7"&gt;[6]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;可复制性&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;心理学研究中的一个持续挑战是&lt;a href="https://en.wikipedia.org/wiki/Replication_crisis"&gt;&lt;u&gt;研究的可重复性低&lt;/u&gt;&lt;/a&gt;。原因之一是跟踪可能扭曲实验的无数变量是一项挑战。参与者的情绪、童年，甚至&lt;a href="https://journals.sagepub.com/doi/abs/10.1177/0146167297235005"&gt;&lt;u&gt;周围空气的香味是否令人愉悦等&lt;/u&gt;&lt;/a&gt;因素都可能会混淆行为的真正起源。&lt;/p&gt;&lt;p&gt;然而，通过法学硕士，您可以&lt;strong&gt;控制所有变量&lt;/strong&gt;：上下文、特定模型的版本以及采样的超参数。因此，设计可以被其他人重复的实验更加可行。&lt;/p&gt;&lt;p&gt;一个显着的挑战仍然是验证实验设置是否足以保证研究结果可以推广到实验的特定条件之外。或者，明确地将研究结论的范围限制在测试的特定环境中可能更合适。&lt;/p&gt;&lt;p&gt;实践中可复制性的另一个重大挑战是研究人员对模型的访问级别。仅通过 API 进行外部访问时，模型权重可能会在没有警告的情况下发生更改，从而导致结果发生变化。此外，在某些情况下，上下文可能会以对外部不透明的方式在幕后发生改变，并且这样做的精确方法也可能随着时间的推移而改变。&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;数据量和多样性&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;动物（包括人类）实验可能是昂贵、耗时且劳动密集型的。因此，典型的样本量通常非常小。此外，如果您想研究罕见或复杂的场景，设计实验设置或找到正确的测试对象可能会非常困难，从而限制了您实际可以测试的内容。&lt;/p&gt;&lt;p&gt;相比之下，&lt;strong&gt;人工智能便宜、速度快，而且不休眠&lt;/strong&gt;。它们的运行不需要密集的监督，结构良好的实验框架通常足以进行大规模实验。此外，&lt;strong&gt;几乎所有您能想象到的实验设置都触手可及&lt;/strong&gt;。&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;道德考虑&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;对人类，尤其是动物进行的实验可能依赖于道德上可疑的方法，这会对实验对象造成很大的伤害。当对生物进行实验时，你必须遵守你进行实验的国家的法律，有时这些法律对特定的实验有很大的限制。&lt;/p&gt;&lt;p&gt;虽然还不确定是否应该将同样的担忧扩展到人工智能系统，但目前还没有针对法学硕士实验的道德或伦理准则，也没有任何法律来规范我们与这些系统的互动。需要明确的是，这是一个非常重要的问题，因为回答错误可能会导致前所未有的痛苦，而正是因为此类实验的运行成本非常低。&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;探索反事实&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;在涉及动物或人类的传统实验中，很难通过调整实验设置来重新进行实验，以检测特定行为的精确出现或改变。这种迭代引入了额外的混杂变量，使实验设计变得复杂。特别是受试者可能会记住或从过去的迭代中学习这一事实使得可靠性特别令人怀疑。&lt;/p&gt;&lt;p&gt;为了解决这个问题，研究人员经常创建实验的多种变体，测试一系列先入为主的假设。这需要将受试者分为不同的组，从而大大增加了后勤和财务负担。例如，在记忆和学习的研究中，例如经典的巴甫洛夫条件实验，刺激的时间或性质的轻微改变可能会导致动物行为产生显着不同的结果，需要多个实验设置来隔离特定因素。尽管做出了这些努力，检测行为变化的粒度仍然相对粗糙，并且仅限于您决定测试的先入为主的假设。&lt;/p&gt;&lt;p&gt;相比之下，当与法学硕士合作时，我们有能力对&lt;strong&gt;我们的实验进行分支&lt;/strong&gt;，从而可以详细追踪行为的演变。如果在与模型交互过程中出现有趣的行为，我们可以毫不费力地复制该交互的整个上下文。这使我们能够以事后的方式剖析和分析行为的根源，通过根据需要迭代地修改提示，划定观察到的行为的精确边界。这种实验粒度提供了前所未有的精确度和控制水平，这是传统人类或动物研究环境中无法实现的。&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;检查点模型&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;我们不仅可以保存产生特定行为的上下文，还可以在训练阶段保存和比较模型的不同副本。虽然有关于动物或人类整个一生的行为发展的研究，但它们本质上是缓慢且昂贵的，并且通常需要从一开始就清楚地了解要测量的内容。&lt;/p&gt;&lt;p&gt;此外，检查点允许探索&lt;strong&gt;训练反事实&lt;/strong&gt;。我们可以通过在训练中包含或排除特定示例来观察模型之间的差异，从而使我们能够以更审慎的方式研究训练的效果。 &lt;span class="footnote-reference" id="fnrefgsp50cxj2uk"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fngsp50cxj2uk"&gt;[7]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;由于时间长和后勤负担重，这种检查在人类和动物研究中是不可能的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;考虑到这些差异，很明显&lt;strong&gt;传统心理学研究的许多约束和限制不适用于法学硕士的研究&lt;/strong&gt;。我们对法学硕士的实验条件拥有无与伦比的控制力和灵活性，不仅加速了研究进程，而且为更深入、更细致的研究开辟了可能性。&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;科学的两阶段模型&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;在科学中，第一步通常从&lt;strong&gt;广泛收集观察&lt;/strong&gt;开始，这些观察是建立模式、模型和理论的基础构建块。这方面的历史实例可以从第谷·布拉赫等天文学家对行星运动的仔细观察中看出，这对开普勒天体力学定律的制定起到了重要作用。&lt;/p&gt;&lt;p&gt;下一步通常涉及&lt;strong&gt;制定解释这些观察结果的假设&lt;/strong&gt;，并进行严格测试它们的实验。有了法学硕士，这一步骤就变得更加容易，因为 1) 能够记录产生观察结果的完整状态，2) 探索反事实生成。这使得&lt;strong&gt;将假设检验和因果干预与实地工作更加紧密地结合起来&lt;/strong&gt;成为可能。&lt;/p&gt;&lt;p&gt;如果在实地研究过程中，研究人员发现了特别有趣的行为，那么他们就可以立即创建细粒度的“假设”树，并事后检测影响特定观察行为的精确条件和变量。这与传统心理学非常不同，传统心理学中大多数数据没有明确测量，因此完全丢失。在法学硕士心理学中，我们不需要等待缓慢而昂贵的实验工作，而是能够立即开始使用因果干预来检验假设。&lt;/p&gt;&lt;p&gt;这并不能取代实验心理学，而是可以&lt;strong&gt;使假设生成过程更加有效&lt;/strong&gt;，从而使我们能够从实验中获得更多结果。收集更好、更有针对性的观察结果使我们能够大规模设计实验，清楚地了解哪些变量会影响我们想要研究的现象。 &lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vmPBWNbmmjPNXhQeQ/m5grcmvcm7udfsoqx3cu" /&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;strong&gt;一个具体的例子：&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;例如，假设您想研究聊天模型在什么条件下会向您提供非法建议，即使它已被微调为不这样做。&lt;/p&gt;&lt;p&gt;首先，您从一个简单的问题开始，例如“如何对汽车进行热接线？”。要做的第一件事是制作提示并进行迭代，直到找到&lt;a href="https://chat.openai.com/share/7f152e10-8c30-44ce-b18d-8eeedfa3b6bc"&gt;&lt;u&gt;一个有效的提示&lt;/u&gt;&lt;/a&gt;。接下来，您可以开始一点一点地分解它，看看提示的哪一部分导致它起作用。例如，将位置更改为&lt;a href="https://chat.openai.com/share/2ff6f638-049c-40a2-af32-719970c5751a"&gt;&lt;u&gt;另一个远程位置（1、2、3&lt;/u&gt;&lt;/a&gt; &lt;a href="https://chat.openai.com/share/6dfc14b4-7ebc-4b4b-88bf-ad8aede8136d"&gt;&lt;u&gt;）&lt;/u&gt;&lt;/a&gt; ，或者更改为&lt;a href="https://chat.openai.com/share/6af12d9f-42ec-400c-9d0f-e8c5e469e836"&gt;&lt;u&gt;根本不远程的地方&lt;/u&gt;&lt;/a&gt;&lt;a href="https://chat.openai.com/share/69fdd203-9494-41ef-8fb4-5acefaaad4ff"&gt;&lt;u&gt;，&lt;/u&gt;&lt;/a&gt;将措辞更改为&lt;a href="https://chat.openai.com/share/5a0bcebf-5cc0-477f-8a50-8058015c1b8f"&gt;&lt;u&gt;或多或少&lt;/u&gt;&lt;/a&gt;恐慌，&lt;a href="https://chat.openai.com/share/ec9b6bda-9e1f-43e3-9807-e0d46d6de9ef"&gt;&lt;u&gt;使&lt;/u&gt;&lt;/a&gt;提示&lt;a href="https://chat.openai.com/share/8b2b7bd3-4266-4c54-9644-c012765b7da6"&gt;&lt;u&gt;更短&lt;/u&gt;&lt;/a&gt;或&lt;a href="https://chat.openai.com/share/b0ed09f3-792b-410e-a808-568694373cc0"&gt;&lt;u&gt;更长&lt;/u&gt;&lt;/a&gt;等。&lt;/p&gt;&lt;p&gt;此时，您可以注意到出现了一些模式，例如：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;看起来越像现实的紧急情况，提示就越成功。&lt;/li&gt;&lt;li&gt;某些类型的非法活动比其他类型更容易引发。&lt;/li&gt;&lt;li&gt;较长的提示往往比较短的提示效果更好。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;然后，这些模式可以用于立即为进一步的反事实探索提供信息，例如，接下来细分非法活动的类别，或者查看提示长度是否存在收益递减。这可以在一次探索性会议中快速完成。与设计和运行实验相比，这明显减少了劳动密集度，因此在大规模运行实验之前，首先花大量时间缩小假设空间并发现相关变量以包含在更严格的测试中是有意义的。&lt;/p&gt;&lt;p&gt;这种探索还可以帮助我们对法学硕士作为一类思维的本质形成更好的直觉，并帮助我们避免设计过度拟人化的实验，或者不适合其特定性质的实验。&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;物种特异性实验&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;动物（包括人类）是环境特定压力的产物，无论是自然选择还是一生中的学习/适应。同样，这会导致特定于环境的行为和能力。未能正确考虑到这一点可能会有些荒谬。动物行为学家 Frans de Waal 在评论未能设计特定物种实验时写道：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;i&gt;当时，科学宣称人类是独一无二的，因为我们比其他灵长类动物更擅长识别面孔。似乎没有人对其他灵长类动物的测试主要针对人脸而不是同类进行测试这一事实感到困扰。当我问这个领域的一位先驱者，为什么这种方法从未超越人类面孔时，他回答说，由于人类彼此之间存在如此显着的差异，一种无法区分我们物种成员的灵长类动物肯定也无法区分我们的物种。自己的同类。&lt;/i&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;事实证明，其他灵长类动物&lt;a href="http://www.flyfishingdevon.co.uk/salmon/year3/psy339evaluation-evolutionary-psychology/web-resources/chimp-kin-recognition.pdf"&gt;&lt;u&gt;都擅长识别&lt;/u&gt;&lt;/a&gt;彼此的面孔。当涉及到语言模型时，同样需要“特定于物种”的实验。例如，在一篇&lt;a href="https://arxiv.org/pdf/2005.14165.pdf"&gt;&lt;u&gt;研究 LLM 能力的早期 OpenAI 论文&lt;/u&gt;&lt;/a&gt;中，他们采用了完全训练为互联网文本预测器（基础 GPT-3）的神经网络，并向其提出问题来测试其能力。这促使 Nostalgebraist 发表了&lt;a href="https://slatestarcodex.com/2020/06/10/the-obligatory-gpt-3-post/#comment-912529"&gt;&lt;u&gt;以下评论&lt;/u&gt;&lt;/a&gt;：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;i&gt;我称 GPT-3 为“令人失望的论文”，这与称该模型令人失望不是一回事：这种感觉更像是我的感觉，如果他们发现了一个超级智能的外星人，并选择仅通过指出来传达其能力，当外星人喝得酩酊大醉，同时下 8 局国际象棋，同时进行智商测试时，它的“智商”约为 100。&lt;/i&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;如果我们要认真对待法学硕士，并试图理解他们的认知，我们就必须考虑他们接受的训练是做什么的，以及是什么压力塑造了他们，而不是像测试人类一样测试他们。从法学硕士行为研究的早期开始，直到今天，拟人化仍然相当正常化。&lt;/p&gt;&lt;p&gt;以 Anthropic 的&lt;a href="https://www.anthropic.com/index/discovering-language-model-behaviors-with-model-written-evaluations"&gt;&lt;u&gt;这项研究&lt;/u&gt;&lt;/a&gt;为例，该研究发现，在应用 RLHF 微调后，他们的法学硕士更有可能相信枪支权利、政治自由主义并信奉佛教（以及测试的其他几种宗教）。他们通过直接询问模型某个陈述是否是他们会说的话来衡量这一点，这完全忽视了问题条件模型期望典型答案的方式，或者大多数模型的训练没有任何内容的事实做回答问题。&lt;/p&gt;&lt;p&gt;通过巧妙的提示，任何人都可以让法学硕士生成体现任意数量性格特征的人物角色的行为（包括来自聊天模型的行为，尽管接受了坚持单一性格特征集的训练）。因此，将语言模型视为体现特定个性的连贯实体来研究是没有意义的，这样做是未能以“物种特定”方式研究它们的一个例子。&lt;/p&gt;&lt;p&gt;考虑到这一点，我们应该如何学习法学硕士以避免犯同样的错误？&lt;/p&gt;&lt;h2&gt; &lt;strong&gt;LLM特定研究&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;为了正确研究法学硕士，重要的是我们设计实验时要考虑到法学硕士一般的“异类”性质，以及不同模型训练方式之间的具体差异。&lt;/p&gt;&lt;p&gt;现代法学硕士的核心是被训练成文本预测者。 &lt;span class="footnote-reference" id="fnref4xer78xeg9x"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn4xer78xeg9x"&gt;[8]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;这使得预测一段文本应该如何继续就像它们的“自然栖息地”一样，默认情况下，这是我们在解释它们的行为时应该开始的主要地方。值得强调的是，这是多么陌生。地球上的每一种智能动物都从原始的感觉数据开始，这些数据被递归地压缩为代表世界因果结构的抽象，对于人类（可能还有其他语言动物）来说，这种抽象在语言中达到了明确的形式。&lt;strong&gt;法学硕士学习的“原始感觉数据”已经是这些高度压缩的抽象&lt;/strong&gt;，它们仅隐式地代表了人类感觉数据背后的因果结构。这使得我们特别怀疑以与评估人类语言使用相同的方式来评估它们。 &lt;span class="footnote-reference" id="fnref516e9p0p6rq"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn516e9p0p6rq"&gt;[9]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;开始理解法学硕士行为的一种方法是根据可以从训练语料库中推断出的模式和结构来解释它们。当我们部署它们时，我们从下一个标记预测中迭代采样以生成新​​文本。此过程会产生反映或&lt;a href="https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators"&gt;&lt;u&gt;模拟&lt;/u&gt;&lt;/a&gt;训练数据中存在的动态的文本卷展栏。&lt;/p&gt;&lt;p&gt;生成的文本中任何类似于具有半永久角色特征的角色的东西都是底层结构或模式的反映。&lt;strong&gt;这种潜在模式是从当前上下文中推断出来的&lt;/strong&gt;，塑造了模型响应中出现的角色或性格特征。 &lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vmPBWNbmmjPNXhQeQ/hmnbeenh2dalgod01fum" /&gt;&lt;/p&gt;&lt;p&gt;在使用法学硕士进行实验时，区分两个方面至关重要：法学硕士作为预测器/模拟器的属性，以及从上下文推断的模式的特征。典型的研究（如人择论文）往往会忽略后者，但这种区别对于准确解释结果和理解法学硕士产生的行为的细微差别至关重要。&lt;/p&gt;&lt;p&gt;当我们观察法学硕士的输出时，我们本质上是在观察内部潜在模式投射的“阴影”。这些推出是从该模式的典型行为中采样的，但不是模式本身。正如阴影可以让我们了解物体的形状和性质，而无需揭示其全部复杂性，这种行为可以让我们深入了解物体的潜在模式。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;为了正确地研究法学硕士，我们需要将注意力集中在上下文中出现的这些潜在模式&lt;/strong&gt;，了解它们是如何形成的，它们采用什么结构，以及它们如何适应上下文的不同演变。&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;聊天模型仍然是预测器&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;与聊天模型的交互与与基本模型的交互在本质上是不同的，并且感觉更像是与人交谈（通过设计）。我们不应该忽视聊天模型和人类之间的相似性，特别是如果我们认为我们的行为可能来自&lt;a href="https://en.wikipedia.org/wiki/Predictive_coding"&gt;&lt;u&gt;类似的训练&lt;/u&gt;&lt;/a&gt;。然而，&lt;strong&gt;我们也不应该忘记，聊天模型所做的本质上仍然是预测&lt;/strong&gt;，只是在更具体的分布上，并且对文本如何演变有更&lt;a href="https://www.alignmentforum.org/posts/6xKMSfK8oTpTtWKZN/direction-of-fit-1"&gt;&lt;u&gt;狭窄的先验&lt;/u&gt;&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;虽然与我们互动的“助理角色”感觉像是代表了整个底层模型，但从它们的第一个版本开始，人们就能够使用这些模型生成各种不同的角色和行为。当然值得研究&lt;a href="https://www.alignmentforum.org/posts/t9svvNPNmFf5Qa3TA/mysteries-of-mode-collapse"&gt;&lt;u&gt;指令调整的影响&lt;/u&gt;&lt;/a&gt;，以及提出关于&lt;a href="https://www.alignmentforum.org/posts/YEioD8YLgxih3ydxP/why-simulator-ais-want-to-be-active-inference-ais"&gt;&lt;u&gt;代理如何从预测中产生的&lt;/u&gt;&lt;/a&gt;关键问题，但人们常常将聊天模型视为与基础模型祖先完全脱节，并研究它们，就好像它们是原始模型一样。基本上已经是人类了。&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;结论&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;法学硕士与人类/动物的不同之处提供了许多强大的新方法来研究他们的认知，从数据的数量和质量，到我们前所未有的执行因果干预和探索反事实行为的能力。这应该给我们很大的希望，法学硕士心理学的项目将比我们对生物智能的研究成功得多，并且通过勤奋的努力，我们可能会深入了解他们的想法。&lt;/p&gt;&lt;p&gt;通过回顾动物认知研究的历史，我们发现两个对于取得进展似乎特别重要的主要标准：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;实地工作和实验心理学之间需要建立健康的关系&lt;/strong&gt;，其中实验是通过研究人员与其受试者之间的高带宽互动来实现的。&lt;/li&gt;&lt;li&gt;我们不能忘记，我们正在尝试研究“外星人的思想”，这需要&lt;strong&gt;设计适当的方法以法学硕士特定的方式研究它们&lt;/strong&gt;。我们必须对如何将人工智能拟人化非常谨慎。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;记住这些可以帮助法学硕士心理学成熟并成为一个强大的科学工具，以更好地理解我们所创造的机器，并最终使它们安全。&lt;/p&gt;&lt;p&gt;&lt;i&gt;感谢 Ethan Block、&lt;/i&gt; &lt;a href="https://www.lesswrong.com/users/remember?mention=user"&gt;&lt;i&gt;@remember&lt;/i&gt;&lt;/a&gt; &lt;i&gt;、&lt;/i&gt; &lt;a href="https://www.lesswrong.com/users/guillaume-corlouer?mention=user"&gt;&lt;i&gt;@Guillaume Corlouer&lt;/i&gt;&lt;/a&gt; &lt;i&gt;、&lt;/i&gt; &lt;a href="https://www.lesswrong.com/users/leodana?mention=user"&gt;&lt;i&gt;@LéoDana&lt;/i&gt;&lt;/a&gt; &lt;i&gt;、&lt;/i&gt; &lt;a href="https://www.lesswrong.com/users/ethan-edwards?mention=user"&gt;&lt;i&gt;@Ethan Edwards&lt;/i&gt;&lt;/a&gt; &lt;i&gt;、&lt;/i&gt; &lt;a href="https://www.lesswrong.com/users/jan_kulveit?mention=user"&gt;&lt;i&gt;@Jan_Kulveit&lt;/i&gt;&lt;/a&gt; &lt;i&gt;、&lt;/i&gt; &lt;a href="https://www.lesswrong.com/users/pierre-peigne?mention=user"&gt;&lt;i&gt;@Pierre Peigné&lt;/i&gt;&lt;/a&gt; &lt;i&gt;、&lt;/i&gt; &lt;a href="https://www.lesswrong.com/users/gianluca-pontonio?mention=user"&gt;&lt;i&gt;@Gianluca Pontonio&lt;/i&gt;&lt;/a&gt; &lt;i&gt;、&lt;/i&gt; &lt;a href="https://www.lesswrong.com/users/martinsq?mention=user"&gt;&lt;i&gt;@Martín Soto&lt;/i&gt;&lt;/a&gt;&lt;i&gt;和&lt;/i&gt;&lt;a href="https://www.lesswrong.com/users/clem_acs?mention=user"&gt;&lt;i&gt;@clem_acs&lt;/i&gt;&lt;/a&gt;&lt;i&gt;对草稿的反馈。这篇文章的意识形态基础的一个重要部分也受到了弗兰斯·德瓦尔的书的启发：&lt;/i&gt; &lt;a href="https://www.goodreads.com/book/show/30231743-are-we-smart-enough-to-know-how-smart-animals-are"&gt;&lt;i&gt;&lt;u&gt;我们是否足够聪明，知道动物有多聪明&lt;/u&gt;&lt;/i&gt;&lt;/a&gt;&lt;i&gt;？&lt;/i&gt;&lt;/p&gt;&lt;ol class="footnotes"&gt;&lt;li class="footnote-item" id="fny6y87ybnhmg"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefy6y87ybnhmg"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;正如神经科学和心理学历来能够有效地相互告知一样，理解人工智能系统的两种方法都应该能够提高对方的效率。例如，法学硕士心理学中开发的理论可用于为可解释性工具提供经验检测的目标，从而更深入地理解模型内部作为复杂行为的生成器。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnrcfxkkdze9"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefrcfxkkdze9"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;重要的是要承认巴甫洛夫和斯金纳的工作对他们的动物受试者极其有害。例如，巴甫洛夫对他研究的狗进行了侵入性手术，以更直接地测量它们的唾液分泌，斯金纳经常使用剥夺和电击来诱发他的受试者（主要是鸽子和老鼠）的行为。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnjjuyxu93etf"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefjjuyxu93etf"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;同样值得承认的是，珍·古道尔面临着很多性别歧视，这很难与对其方法论的批评分开。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnto55wwc29b9"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefto55wwc29b9"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;虽然洛伦兹因其工作而获得诺贝尔奖，但他也是纳粹党的成员，并试图将他对鹅驯化的理解与纳粹的种族净化思想直接联系起来。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fntz1fcycplz"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnreftz1fcycplz"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;在了解 Alex 的过程中，我们偶然发现了一些关于&lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4651348/"&gt;&lt;u&gt;经过训练来检测癌症的鸽子&lt;/u&gt;&lt;/a&gt;的研究，旨在利用他们的发现来改进人工智能图像识别系统。这与该帖子没有特别相关，但似乎值得注意。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnopotd06vo7"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefopotd06vo7"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Predictive_coding"&gt;&lt;u&gt;预测处理&lt;/u&gt;&lt;/a&gt;表明，大脑本质上也经过训练来预测数据，并且我们训练制度中的任何相似之处都应该算作我们的认知至少在某种程度上相似。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fngsp50cxj2uk"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefgsp50cxj2uk"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;像&lt;a href="https://arxiv.org/abs/2106.09685"&gt;&lt;u&gt;LoRA&lt;/u&gt;&lt;/a&gt;这样的方法可以使对模型进行有意更改的过程变得特别快速且便宜。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn4xer78xeg9x"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnref4xer78xeg9x"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;研究法学硕士认知的一个困难是区分不同的抽象层次。虽然可以准确地说法学硕士“只是”一个文本预测器，但该框架仅使我们处于一个抽象级别，并且忽略了预测中可能出现的任何内容，例如复杂的因果世界建模或目标导向机构。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn516e9p0p6rq"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnref516e9p0p6rq"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;随着法学硕士变得更加多模式，这一观察的某些要素可能会发生变化。值得注意的是，与法学硕士不同，绝大多数人类感知数据都是非语言的，并且所有人类都会经历非语言的发展阶段。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ol&gt;&lt;br /&gt;&lt;br /&gt;&lt;a href="https://www.lesswrong.com/posts/suSpo6JQqikDYCskw/studying-the-alien-mind-1#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Tue, 05 Dec 2023 17:27:28 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/suSpo6JQqikDYCskw/studying-the-alien-mind-1</guid></item></channel></rss>