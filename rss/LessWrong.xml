<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>少错</title><link>https://www.lesswrong.com</link><description>致力于提炼理性艺术的社区博客</description><lastBuildDate>Sun, 24 Dec 2023 00:55:36 GMT</lastBuildDate><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>对拟像关卡的更清晰解释</title><link>https://www.lesswrong.com/posts/KnQpzYRR4ogPNtzem/a-crisper-explanation-of-simulacrum-levels</link><description>发布于 2023 年 12 月 23 日晚上 10:13（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;我读过之前关于&lt;a href="https://www.lesswrong.com/tag/simulacrum-levels"&gt;Simulacrum Levels&lt;/a&gt;的文章，并且我看到人们对它们的工作方式表达了一些困惑。当我第一次遇到这个概念时，我自己也有过一些困惑，我认为它们是由于定义不够&lt;i&gt;清晰&lt;/i&gt;造成的。&lt;/p&gt;&lt;p&gt;现有的解释似乎并没有为拟像级别如何存在提供适当的自下而上/基本面优先的机制。为什么他们有自己特有的特征和怪癖，而不是其他的？为什么赋予它们的形式是它们&lt;i&gt;不可避免的&lt;/i&gt;形式，而不是任意的形式？为什么四级特工只能表现出精神病态？为什么没有5级？&lt;/p&gt;&lt;p&gt;我最终形成了一个关于它们如何工作的看似新颖的模型，现在我意识到它可能对其他人也有用（尽管我几年前就形成了它）。&lt;/p&gt;&lt;p&gt;它的目的是保留&lt;a href="https://www.lesswrong.com/users/zvi?mention=user"&gt;@Zvi&lt;/a&gt; &lt;a href="https://www.lesswrong.com/posts/dHYxnSgMDeveovLuv/the-best-of-don-t-worry-about-the-vase#The_Simulacra_Levels_Sequence"&gt;定义&lt;/a&gt;的所有重要特征，同时通过对它们进行适当的齿轮级机械解释来解释它们。我认为在我划定界限的位置上存在一些细微的差异，但它基本上仍然应该与 Zvi 的一致。&lt;/p&gt;&lt;hr /&gt;&lt;h2&gt;基础工作&lt;/h2&gt;&lt;p&gt;在某些情况下，递归级别与递归级别 3 相比实际上变得难以区分。这不完全是一个新想法，但它是我的模型的核心，因此为了完整起见，我将提供一个示例。&lt;/p&gt;&lt;p&gt;考虑认知的情况。&lt;/p&gt;&lt;ol&gt;&lt;li&gt;认知是对外部对象和过程的思考。 “这家餐厅太狭窄了。”&lt;/li&gt;&lt;li&gt;元认知正在构建你自己的思维模型。它可能有什么偏见，如何更好地推理对象级主题。 “我觉得这家餐厅太局促了，因为我不喜欢一大群人。”&lt;/li&gt;&lt;li&gt;元元认知正在分析你的自我模型：你是否倾向于修饰或掩盖你个性的某些部分，等等。“我正在给自己讲一个关于不喜欢一大群人的故事，因为这感觉像是一个更迷人的解释不喜欢这家餐厅胜过真正的餐厅。我不喜欢它是出于相反：这里有很多人因为它很受欢迎，而我本能地不喜欢主流的东西。”&lt;/li&gt;&lt;li&gt;那么，元元认知将是“思考你对以自我为中心的偏见的分析”。但这又是元元认知：分析你倾向于如何看待自己。 “我对自己的看法进行了复杂的思考，因为我想保持一个聪明、有自我意识的人的自我形象。”&lt;ul&gt;&lt;li&gt;有一个类似的情况，元认知与元认知是同一件事，但我认为 2 级和 3 级之间存在细微差别，而 3 级和 4 级以后则不明显。 &lt;span class="footnote-reference" id="fnrefsm2ltnt082r"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnsm2ltnt082r"&gt;[1]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;下一篇：基本上，在任何社会中，都存在三个不同的“框架”：物质现实、他人和社会现实。每个后续框架都包含前一个框架的递归模型：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;物理现实&lt;i&gt;是&lt;/i&gt;。&lt;/li&gt;&lt;li&gt;人们有自己的现实模型。&lt;/li&gt;&lt;li&gt;人们的社会形象是其他人对一个人的模型：即现实模型的模型。 &lt;span class="footnote-reference" id="fnrefmsdn2x9q13l"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnmsdn2x9q13l"&gt;[2]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;递归级别1、2和3。这里没有有意义的“级别4”：“一个人的社会形象的模型”意味着“一个人的外表的感知”，这仍然只是“一个人的外表”。您可以在这里提出一些警告，但它不会改变太多&lt;span class="footnote-reference" id="fnreffofs4yy521u"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnfofs4yy521u"&gt;[3]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。&lt;/p&gt;&lt;p&gt;因此，任何信号都可以在这些框架中查看，从而产生任何信号可以传达的三种含义：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;它的字面意思是：在物理现实的背景下看待。&lt;/li&gt;&lt;li&gt;你认为演讲者试图让你相信什么，以及为什么：在你的演讲者模型的背景下观察。&lt;/li&gt;&lt;li&gt;它如何影响你和说话者的社交形象：在你和别人对你和说话者的模型的背景下进行观察。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;到目前为止，这很好：所有这些沟通层次都是有用的，并且在任何正常运转的社会中都占有一席之地。&lt;/p&gt;&lt;p&gt;当社会开始&lt;i&gt;放弃&lt;/i&gt;较低层次的沟通时，问题就开始了。这就是转移到更高的拟像级别的含义：放弃较低的级别，转而选择较高的级别。一个“纯粹”的一级社会只关心陈述的字面真实性； 2级社会关心言论背后的人；第三级社会关心言论如何影响人们对其他人的&lt;i&gt;看法&lt;/i&gt;。级别 0 和级别 4 比较特殊：级别 0 没有通信的概念，级别 4 已经深入到递归以至于放弃了它。&lt;/p&gt;&lt;p&gt;我将使用术语“社会”和“代理”，因为这就是我对这个模型的看法，但我的意思是广义上的。 “代理人”可以是从个人到国家的任何事物，“社会”可以是嵌入在任何背景（包括更广泛的社会）中的任何群体。&lt;/p&gt;&lt;p&gt;此外，我应该注意到，同一个代理人可能会占据不同的拟像级别，具体取决于他们所处的环境或与他们在一起的人（一个人可能对他们的家人非常友善和人性化，但可能是一个像精神病患者一样的级别） 4 当涉及到任何与政治无关的事情时）。&lt;/p&gt;&lt;hr /&gt;&lt;h2&gt; 0级&lt;/h2&gt;&lt;p&gt;理解它对于理解 4 级至关重要。&lt;/p&gt;&lt;p&gt;对于 0 级代理来说，没有符号、没有模型、没有交流，只有旨在直接给物理世界带来变化的行动。由于它涉及与其他代理的交互，因此它是一种纯粹的冲突。&lt;/p&gt;&lt;p&gt;你看到狮子，你就逃跑。你看到食物，你就拿走它。你遇到敌人，你杀死他们。你的行为是非象征性的：它们不代表任何东西，也不期望它们会被别人看到和理解。它们的目的是其形式&lt;i&gt;所固有的&lt;/i&gt;。&lt;/p&gt;&lt;p&gt;当你试图削尖一块岩石并将其放在矛上时，你并不是在试图&lt;i&gt;说服&lt;/i&gt;岩石改变其形状。当您编写计算机程序时，您并不是试图&lt;i&gt;说服&lt;/i&gt;计算机正常工作（尽管有时感觉如此）。&lt;/p&gt;&lt;p&gt;但它不仅仅涉及无生命的物体。你&lt;i&gt;可以&lt;/i&gt;在这里有一个某人的“模型”——一个追踪猎物的猎人——但关键的是你不要假设他们有&lt;i&gt;你&lt;/i&gt;的模型。这里可能发生的任何“交流”都是片面的。如果你看到一只熊向你跑来，并且你逃离了它的领地，那么可以说这只熊“恐吓”了你。但熊并没有想到“恐吓”。它采取的行动并不是为了表明它愿意杀死你以迫使你撤退。它的意思是&lt;i&gt;直接&lt;/i&gt;把你从它的领地里赶走，杀了你，而你愿意从它那里撤退，对它来说，就是一个巧合。 &lt;span class="footnote-reference" id="fnrefb90gqrtknu"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnb90gqrtknu"&gt;[4]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;本质上，这并不意味着 0 级代理人不能有广泛的动机，他们只是追求自己的私利。然而，这是最常见的。毕竟，在 0 级操作需要长期拒绝或无法与其他特工/人员联系。如果你只希望通过单方面迫使他人做出改变或得到同样的回报来与他人互动，那么除了一种不可调和的敌对关系之外，你还能拥有什么样的关系呢？&lt;/p&gt;&lt;p&gt; （如果你可以使用的&lt;i&gt;所有&lt;/i&gt;工具都是 0 级工具怎么办？如果你没有办法与任何人对话，如果你甚至没有“对话”的概念，如果你只能强行改变世界？剧透警告：这就是 4 级从内部看的样子。）&lt;/p&gt;&lt;p&gt; 0级&lt;i&gt;就是&lt;/i&gt;真理。更高的层次很容易崩溃：全面的战斗和战争。&lt;/p&gt;&lt;hr /&gt;&lt;h2&gt; 1级&lt;/h2&gt;&lt;p&gt;好吧，我不会在这里讨论语言和合作的发展。第 1 级是代理交换有关物理世界的信息以形成对其的正确共同理解的级别。&lt;/p&gt;&lt;p&gt;在这一级别上交换的声明仅对其真实价值进行审查，它们是字面的和公开的。由于它涉及与其他代理的沟通，因此这是一种纯粹的合作。这种合作有时可以采取“向其他部落展示你的（不夸张的）军事力量，这样他们就可以让你不战而屈人之兵地夺取所有资源”的形式，但它的目的仍然是实现一个对双方都有利的结果。选择。 （可能存在&lt;i&gt;0级&lt;/i&gt;敌意，以及敌意地&lt;i&gt;拒绝&lt;/i&gt;交流。如果你足够讨厌另一个部落，你就不给他们认输的机会，你只是屠杀他们所有人。但只要交流&lt;i&gt;确实&lt;/i&gt;发生，它总是亲社会的。 ）&lt;/p&gt;&lt;p&gt;在第一级，主体和社会关注物理世界，他们专注于构建尽可能准确的模型。他们对该模型没有任何依恋，并且会根据需要对其进行更改以更好地适应世界。&lt;/p&gt;&lt;p&gt;两个智能体之间关于 1 级问题的冲突可能是由于他们的物理世界模型相互冲突，并且可以通过测试哪个模型是正确的或解决沟通不畅来解决。&lt;/p&gt;&lt;hr /&gt;&lt;h2&gt; 2级&lt;/h2&gt;&lt;p&gt;但有时不可能干净地解决 1 级冲突，因为两个代理缺乏干净地测试他们中哪一个是正确的能力，并且他们的先验（或动机推理）导致他们优先考虑不同的模型。或者也许他们有完全不同的价值观。在这种情况下，他们中的一个人可能会&lt;i&gt;撒谎&lt;/i&gt;：故意发表不正确描述物理世界的声明，以扭曲对话者的世界模型，迫使他们按照第一个人的利益行事。&lt;/p&gt;&lt;p&gt;当然，这不一定是恶意的：他们可能会为了对方的“自身利益”而撒谎，也许是因为他们认为对话者有偏见。他们甚至可能是对的！ （借用兹维的例子，声称河对岸有狮子，而实际上有老虎，因为部落还不够害怕老虎。）&lt;/p&gt;&lt;p&gt;这是对第二级的“经典”解释：在这一级，人们发现欺骗，并开始扭曲他人的模型以达到自己的目的。这绝对是其中的一部分。但只是&lt;i&gt;一部分&lt;/i&gt;。&lt;/p&gt;&lt;p&gt;实际上，对话双方都不必&lt;i&gt;撒谎&lt;/i&gt;。同意不同意就足够了。&lt;/p&gt;&lt;p&gt;在此之后，社会中将出现&lt;i&gt;两种&lt;/i&gt;相互冲突的世界模式（我们称之为“世界观”）。比如说，扩张主义与孤立主义议程。很快，还会有十几个人加入他们的行列，这些人是由于十几个未解决的分歧或欺骗而产生的。其中一些分歧将会得到解决，但其他分歧将长期存在。&lt;/p&gt;&lt;p&gt;人们会根据模型的质量、个人偏见和偏好在这些模型之间进行选择，形成&lt;i&gt;子群体&lt;/i&gt;。每个子群体中的人都会试图让更多的人站在他们一边，并阻止人们离开他们的子群体，因为如果他们的世界观占主导地位，这将使他们受益（要么因为他们真诚地相信它比其他模型更真实，要么因为这对他们特别有利）。&lt;/p&gt;&lt;p&gt;他们会变得根深蒂固，他们会培养对自己事业或世界观的忠诚，他们会培养对自己信仰的确定性。最终，一些（大多数）人会对他们的物理世界模型产生&lt;i&gt;依恋&lt;/i&gt;。他们将开始&lt;i&gt;从本质上&lt;/i&gt;评价它，而不仅仅是因为它是正确的。他们将开始否认反对它的证据，他们的模型越准确（和/或难以质疑），这就越容易。&lt;/p&gt;&lt;p&gt;他们将开始将现实模型与&lt;i&gt;现实本身&lt;/i&gt;混淆。&lt;/p&gt;&lt;p&gt;在第二级社会的后期，人们会考虑最重要的事情是其他人所认同的世界观，以及一般来说，其他人有什么信念和动机。审查声明的不是其真实价值，而是其背后的动机和信念：为什么该声明的作者选择这样做，他们试图传播什么世界观，或者他们可以相信什么。到那时，陈述的实际内容将被认为不如它们所揭示的关于说话者和说话者的其他信仰的内容重要。&lt;/p&gt;&lt;p&gt;创建新模型或对旧模型进行彻底改变将受到抵制。&lt;/p&gt;&lt;p&gt;与其他人&lt;i&gt;确信&lt;/i&gt;正在&lt;i&gt;发生的事情相比，物理世界中实际&lt;/i&gt;发生的事情将变得不那么重要。&lt;/p&gt;&lt;p&gt;两个代理之间的 2 级冲突是相互操纵的尝试。两者的目标都是说服观众接受他们的世界模型，无论观众是他们的对话者，还是真正的观看他们的观众。任何卑鄙的心理伎俩都可以在那里进行。&lt;/p&gt;&lt;p&gt;完全处于第 2 级的社会成员将 (1) 专注于开发尽可能准确的&lt;i&gt;社会其他成员&lt;/i&gt;的模型，加上 (2) 将他们个人的物理世界模型视为本质上真实的，加上 (3) 考虑与实际的物理世界无关，可以根据需要撒谎或忽略。&lt;/p&gt;&lt;hr /&gt;&lt;h2&gt; 3级&lt;/h2&gt;&lt;p&gt;二级社会是一个分为多个小组的社会，仔细审查人们的言论是否与其世界观真正相符。其中一些团体比其他团体更强大，或者吸引不同的偏好，并且您可以通过发表正确的声明来表明有关您自己的正确信息，从而赢得他人的好感。你还可以通过说服别人持有不受欢迎的世界观或不属于他们想加入的群体来伤害你讨厌的人。&lt;/p&gt;&lt;p&gt;你或他们&lt;i&gt;实际上&lt;/i&gt;是什么样子并不重要：只要某人&lt;i&gt;看起来&lt;/i&gt;属于某个群体，或招致仇恨，或因某种原因而有另一种关系，他们&lt;i&gt;实际上做&lt;/i&gt;。这纯粹是一个外观问题。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;第一级社会植根于物质世界的客观真理。&lt;/li&gt;&lt;li&gt;第二级社会以其人民&lt;i&gt;信仰&lt;/i&gt;的客观真理为基础。即使这些信念被扭曲了，即使你&lt;i&gt;想&lt;/i&gt;扭曲它们，你仍然关心&lt;i&gt;它们真正的内在认知状态&lt;/i&gt;。&lt;/li&gt;&lt;li&gt;第三级社会凭空创造出自己的现实。它只关心事情看起来怎么样，一个人&lt;i&gt;看起来&lt;/i&gt;有什么信念。但它并不承认这一点。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在 3 级社会中，言论仍然会受到审查，以确定它们对说话者或其他人产生的影响，就像在 2 级社会中一样（因为递归已经在这方面达到了最大值；下一节将详细介绍）。但你对这些含义的解释是否正确已经不再重要，只要它&lt;i&gt;足够好，其他人就可以振振有词地声称相信它&lt;/i&gt;，或者至少相信&lt;i&gt;你&lt;/i&gt;相信它。&lt;/p&gt;&lt;p&gt;如果你&lt;i&gt;真正&lt;/i&gt;相信这个事业，那并不重要。只是你的社会形象与那些以事业为中心的运动的一部分的人的形象相符。&lt;/p&gt;&lt;p&gt;这会产生一些不正当的激励措施，Zvi 将其&lt;a href="https://www.lesswrong.com/posts/QdppEcbhLTZqDDtDa/unifying-the-simulacra-definitions"&gt;称为&lt;/a&gt;“针对知识的战争”：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;在第 3 级，以下两件事是应该受到指责的，从而造成知识成为一种负担的两种方式。 [...]&lt;/p&gt;&lt;p&gt;一件值得指责的事情是&lt;i&gt;没有使用正确的符号。&lt;/i&gt;&lt;/p&gt;&lt;p&gt;这就是“由可用于其他用途的事物组成”的方面。关心真相会产生一种替代性激励，阻止人们调用正确的符号，并让人怀疑这些符号是否意味着它们看起来的意思。&lt;/p&gt;&lt;p&gt;如果有的话，调用技术上错误的符号而不是技术上正确的符号是&lt;i&gt;游戏中更强大的一步&lt;/i&gt;。这就是为什么。它更强烈地表明一个人以昂贵的代价发送了适当的信号，而没有空间被误解为较低级别的行动。通过重复谎言，我们表现出自己的忠诚。通过让其他人重复这一点，我们可以促使他们变得忠诚并被认为是忠诚的，并让他们向其他人展示他们的忠诚，并展示我们对人和象征的力量。&lt;/p&gt;&lt;p&gt;另一件值得指责的事情是&lt;i&gt;知道你所说的是假的&lt;/i&gt;。罪魁祸首&lt;i&gt;是知识本身&lt;/i&gt;。&lt;/p&gt;&lt;p&gt; （或者，也许更准确地说，&lt;i&gt;其他人知道&lt;/i&gt;你知道你所说的是假的，因此任何其他人都知道我们拥有知识，而不是知识本身，但对于任何其他指责系统来说也是如此。）&lt;/p&gt;&lt;p&gt;总统知道什么？他什么时候知道的？&lt;/p&gt;&lt;p&gt;因此，沟通从显式转变为隐式。专注于仅拥有可否认的隐性知识。&lt;/p&gt;&lt;p&gt;需要明确指导的追随者确实是一个糟糕的追随者。指定要完成的所有事情是不切实际的，并且表明您不仅拥有知识，而且拥有责任。更好地&lt;i&gt;朝着&lt;/i&gt;团队的目标努力，积累有助于赢得比赛的符号。&lt;/p&gt;&lt;p&gt;因此，这种结构使每个人都远离知识。到目前为止，假装不知道事情的最简单方法就是不知道它们。&lt;/p&gt;&lt;/blockquote&gt;&lt;hr /&gt;&lt;h2&gt;递归级别&lt;/h2&gt;&lt;p&gt;当我们提升级别时，需要跟踪三个重要变量：世界的哪些方面被认为是重要的，正在形成/操纵什么样的框架来对重要方面进行建模，以及世界的哪一方面被认为是无关紧要的。&lt;/p&gt;&lt;p&gt;这是表格格式：&lt;/p&gt;&lt;figure class="table"&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt; L&lt;/th&gt;&lt;th&gt;重要的&lt;/th&gt;&lt;th&gt;框架&lt;/th&gt;&lt;th&gt;无关紧要&lt;/th&gt;&lt;th&gt;评论&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;物质世界&lt;/td&gt;&lt;td&gt;—&lt;/td&gt;&lt;td&gt; —&lt;/td&gt;&lt;td&gt;模型不存在。一切都是真实的。&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt; 1&lt;/td&gt;&lt;td&gt;物质世界&lt;/td&gt;&lt;td&gt;人们的世界观&lt;/td&gt;&lt;td&gt;—&lt;/td&gt;&lt;td&gt;世界如何运作？&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt; 2&lt;/td&gt;&lt;td&gt;人们的世界观&lt;/td&gt;&lt;td&gt;人们的社会形象&lt;/td&gt;&lt;td&gt;物质世界&lt;/td&gt;&lt;td&gt;人们相信什么？为什么？怎么可以用呢？&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt; 3&lt;/td&gt;&lt;td&gt;人们的社会形象&lt;/td&gt;&lt;td&gt;人们的社会形象&lt;/td&gt;&lt;td&gt;人们的世界观&lt;/td&gt;&lt;td&gt;只有人们投射的形象才重要。&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt; 4&lt;/td&gt;&lt;td&gt;人们的社会形象&lt;/td&gt;&lt;td&gt;人们的社会形象&lt;/td&gt;&lt;td&gt;人们的社会形象&lt;/td&gt;&lt;td&gt;???&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;所有三个不同的值都代表不同级别的递归：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;物理世界&lt;i&gt;是&lt;/i&gt;。&lt;/li&gt;&lt;li&gt;人们的世界观是物质世界的模型。&lt;/li&gt;&lt;li&gt;人们的社会形象是人们及其世界观的集体模型：世界模型的模型。&lt;/li&gt;&lt;li&gt;下一个是“一个人的社会形象的模型”，即“一个人的外表的感知”，但这仍然只是“一个人的外表”。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;在第 3 级，第三个变量尝试越过递归第 3 级，因此会自行环绕。这导致了“书写自己的现实”的特殊情况：第 3 级&lt;i&gt;创建了&lt;/i&gt;它所关心的事物。&lt;/p&gt;&lt;p&gt;但由于人们图像的真实性尚未被认为是&lt;i&gt;无关紧要的&lt;/i&gt;，因此仍然与现实存在某种联系，如果有足够的证据反对某个主张，就可能推翻它。人们仍然认为模型&lt;i&gt;代表着&lt;/i&gt;某种东西，它们意味着&lt;i&gt;代表&lt;/i&gt;一些不可改变的现实的真相。&lt;/p&gt;&lt;p&gt; 3 级特工真正关心世界如何看待他们和其他人。他们多么准确地表达了他们的忠诚。如果有人提供证据证明有人在某个时刻发出了与他们现在试图展现的公众形象背道而驰的信号，那么 3 级特工&lt;i&gt;就会&lt;/i&gt;关心这一点。 （因此例如取消文化。）&lt;/p&gt;&lt;p&gt; 4 级甚至超越了这个。 （我见过 4 级被描述为“超过 3 级的一切”，我想这是我将其形式化的说法。）&lt;/p&gt;&lt;hr /&gt;&lt;h2&gt; 4级&lt;/h2&gt;&lt;p&gt;想象一个后期的 3 级社会。 2 级智能体群体消失或采用 3 级思维模式。到那时，没有人关心在任何地方或任何人身上实际发生的事情，甚至人们可以确信正在发生的事情，只&lt;i&gt;关心人们认为他们可以振振有词地声称相信&lt;/i&gt;正在发生的事情。&lt;/p&gt;&lt;p&gt;一旦这种事态成为共享知识，就会切换到第 4 级。一旦每个人都知道，他们不需要用自己的表演来&lt;i&gt;合理地&lt;/i&gt;说服观众，只有 L4 特工同伴会跟踪每一句话对正在进行的地位游戏的影响。一旦他们进一步知道其他人也知道这一点......&lt;/p&gt;&lt;p&gt;这有一个棘手的含义：符号变得非符号化。&lt;/p&gt;&lt;p&gt;符号是代表其他事物的事物。但第四级陈述无论如何都与现实无关：它们完全没有意义。它们不传达任何信息，无论是在任何层面上，无论是字面上还是通过它们所产生的含义。他们不代表任何事情。在第 4 级，符号&lt;i&gt;不再象征事物&lt;/i&gt;。他们变得自给自足。&lt;/p&gt;&lt;p&gt;每个人都知道这一点，所以没有人试图解释它们。每个人都知道&lt;i&gt;这&lt;/i&gt;一点，因此没有人发表声明时期望自己会受到仔细审查以获取信息。那么，任何人发表声明的唯一原因是因为他们知道这会对当地的社会环境产生特定的影响：开辟某些攻击或防御的途径。也就是说，&lt;i&gt;它们的目的是其形式所固有的。&lt;/i&gt;&lt;/p&gt;&lt;p&gt;我认为与 0 级的相似之处是显而易见的。 4级特工&lt;i&gt;不能说话&lt;/i&gt;。他们的言论&lt;i&gt;扭曲了现实&lt;/i&gt;。他们不能说话，只能&lt;i&gt;强迫别人占据不同的社会情境&lt;/i&gt;。他们（认为）得到了同样的回报：没有人与他们接触，其他人只是试图强行改变他们周围的社会政治格局。&lt;/p&gt;&lt;p&gt;从真正的意义上来说，4 级语句是&lt;i&gt;移动&lt;/i&gt;、攻击或防御的动作，与物理攻击和躲避不同。&lt;/p&gt;&lt;p&gt;因此，它们也必然是短期的。从较低级别的角度来看，4 级特工看起来仍然在编织有关物理现实的不同叙述。但这些叙述的唯一目的是赢得其创造者目前所卷入的任何&lt;i&gt;直接&lt;/i&gt;冲突。他们不需要足够强大来度过这场冲突，或者彼此保持一致，甚至在冲突之外的任何人看来都是一致的。&lt;/p&gt;&lt;p&gt;与第 3 级不同，提供某人在某个时刻发出错误信号的证据不会打动人们，除非您设法&lt;i&gt;正确地将此信息的泄露作为攻击&lt;/i&gt;。&lt;/p&gt;&lt;p&gt; 4 级特工不关心世界，也不关心别人的看法，也不关心世界如何看待他们。他们唯一关心的是他们可以假装走过的社会政治景观的非象征路线。&lt;/p&gt;&lt;p&gt; 4 级与 0 级一样都是霍布斯地狱。4 级智能体没有与他人合作交互的&lt;i&gt;语言&lt;/i&gt;。从理论上讲，这里也可能有各种各样的动机，但通过这些镜头，一切都会被扭曲。&lt;/p&gt;&lt;hr /&gt;&lt;h2&gt; 5级&lt;/h2&gt;&lt;p&gt;尝试以回避递归模型（已经全面循环）的方式进行推断，第 5 级到第 4 级的框架就像第 1 级到第 0 级一样。这一切都始于逃离非符号地狱， 毕竟。&lt;/p&gt;&lt;p&gt;但是，当你把信号的概念变成一把用来刺人的刀之后，你如何才能重新发现沟通呢？&lt;/p&gt;&lt;p&gt;没有5级。&lt;/p&gt;&lt;ol class="footnotes"&gt;&lt;li class="footnote-item" id="fnsm2ltnt082r"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefsm2ltnt082r"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;也许更细粒度的说法是，在某些情况下，随着递归级别的提高，它们之间的差异缩小，并且由于人类思维的局限性，L3 与 L4+ 是我们的模型变得足够粗糙的地方，差异是难以察觉的。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnmsdn2x9q13l"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefmsdn2x9q13l"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;这完全符合我的观点，即&lt;a href="https://www.lesswrong.com/posts/hx5wTeBSdf4bsYnY9/idealized-agents-are-approximate-causal-mirrors-radical"&gt;主体是近似因果镜子&lt;/a&gt;。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnfofs4yy521u"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnreffofs4yy521u"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;例如，某些政策建议被认为不受欢迎，并不是因为大多数人实际上反对它们，而是因为大多数人&lt;i&gt;认为&lt;/i&gt;大多数人反对他们，如果你担心这一点，那么从技术上讲，你就已经进入了递归级别4...&lt;/p&gt;&lt;p&gt;但根据脚注 1，从经验来看，这种情况似乎并没有发生太多，可能是由于人类思维的局限性。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnb90gqrtknu"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefb90gqrtknu"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;好吧，这实际上可能不是对字面动物的准确描述。它们并不都处于 0 级，它们可以相互通信（尽管我认为在这个具体示例中，界限是模糊的）。或者，你可以想象一个非智能机器人代替熊，遵循一些算法在其领地巡逻，告诉它杀死入侵者。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ol&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/KnQpzYRR4ogPNtzem/a-crisper-explanation-of-simulacrum-levels#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sat, 23 Dec 2023 22:13:52 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/KnQpzYRR4ogPNtzem/a-crisper-explanation-of-simulacrum-levels</guid></item><item><title>双曲贴现和帕斯卡抢劫</title><link>https://www.lesswrong.com/posts/4nDkfEYpDFB7KDfQ9/hyperbolic-discounting-and-pascal-s-mugging</link><description>发布于 2023 年 12 月 23 日晚上 9:55（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;&lt;i&gt;从我的个人博客交叉发布：&lt;/i&gt; &lt;a href="https://mechanisticmind.com/hyperbolic-discounting-and-pascals-mugging/"&gt;&lt;i&gt;https://mechanisticmind.com/hyperbolic-discounting-and-pascals-mugging/&lt;/i&gt;&lt;/a&gt;&lt;/p&gt;&lt;h1&gt;长话短说&lt;/h1&gt;&lt;figure class="image image_resized" style="width: 100%;"&gt;&lt;img alt="该图像的 alt 属性为空；它的文件名为 image-19.png" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4nDkfEYpDFB7KDfQ9/lmihbysdvrbcnngfpyep" /&gt;&lt;/figure&gt;&lt;p&gt;双曲贴现（如图 🟥 所示）是指数贴现的不完美近似（如图 🟦 所示）。人们普遍指出，这会导致人类高估近期奖励，但很少有人意识到这也会导致我们高估远期奖励。有大量证据表明人类使用双曲线贴现，这有助于解释为什么人类追求不切实际的长期目标。&lt;/p&gt;&lt;h1&gt;什么是双曲线贴现？&lt;/h1&gt;&lt;p&gt;如果你搜索“双曲折扣”这个词，谷歌会告诉你，这是“一种心理偏见，人们优先考虑眼前的奖励和满足感而不是未来的奖励”。事实上，这只说对了一半。除了高估&lt;i&gt;&lt;strong&gt;近期&lt;/strong&gt;&lt;/i&gt;&lt;strong&gt;奖励&lt;/strong&gt;之外，双曲线贴现还会导致人们&lt;strong&gt;高估&lt;/strong&gt;远期奖励。&lt;/p&gt;&lt;p&gt;时间折扣基本上是一种思考如何回答这样的问题的方式：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;您愿意今天拥有 100 美元，还是一年后拥有 120 美元？&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;从货币角度考虑这个问题相对容易，而且你可能知道这个问题的答案与利率有关，一年后拿 120 美元等于 20% 的年回报率，这是更好的选择高于您在股票市场上预期的 4-10% 的利率，也优于您目前&lt;a href="https://ycharts.com/indicators/1_year_treasury_rate"&gt;可以获得&lt;/a&gt;的 1 年期国债 4.88% 的利率。&lt;/p&gt;&lt;p&gt;这种计算利率的模型称为&lt;i&gt;&lt;strong&gt;指数贴现&lt;/strong&gt;&lt;/i&gt;，它基本上由以下方程表示：&lt;/p&gt;&lt;p&gt;&lt;span class="mjpage mjpage__block"&gt;&lt;span class="mjx-chtml MJXc-display" style="text-align: center;"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;时间&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;t&lt;/span&gt;&lt;/span&gt;处的值&lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mfrac MJXc-space3"&gt;&lt;span class="mjx-box MJXc-stacked" style="width: 1.065em; padding: 0px 0.12em;"&gt;&lt;span class="mjx-numerator"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-denominator"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;d&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.409em;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-line"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-vsize"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;这表示，如果某个东西在 $t=0$ 时价值 $a$，例如现在一张 100 美元的钞票，那么您需要等待的时间越长，它的价值就会呈指数下降。&lt;/p&gt;&lt;p&gt;因此，事实证明，人类和其他动物似乎并没有使用这个方程来估计未来奖励的价值。相反，他们使用称为&lt;i&gt;&lt;strong&gt;双曲贴现的&lt;/strong&gt;&lt;/i&gt;方法，可以用以下等式表示：&lt;/p&gt;&lt;p&gt;&lt;span class="mjpage mjpage__block"&gt;&lt;span class="mjx-chtml MJXc-display" style="text-align: center;"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;时间&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;t&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;时的值&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mfrac MJXc-space3"&gt;&lt;span class="mjx-box MJXc-stacked" style="width: 2.806em; padding: 0px 0.12em;"&gt;&lt;span class="mjx-numerator"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-denominator"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;d&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-line"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-vsize"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;这表明，随着奖励的距离越来越远，它的价值会按倒数下降。&lt;/p&gt;&lt;h2&gt;我们如何知道&lt;/h2&gt;&lt;p&gt;科学家们用本科生和&lt;a href="https://mpra.ub.uni-muenchen.de/79536/1/MPRA_paper_79536.pdf"&gt;100 美元的钞票&lt;/a&gt;进行了实验。实际上，资助资金很难获得，因此大多数人都使用 10 美元的钞票，而来自不太知名大学的研究人员大概只能用 1 美元的钞票和一些零钱度日。&lt;/p&gt;&lt;p&gt;他们还用猴子和果汁做了&lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2728786/"&gt;实验&lt;/a&gt;。猴子喜欢果汁；我不知道是否有科学家使用可口可乐进行这些实验，但我敢打赌他们会重复相同的发现。哇，我是在开玩笑，但这里有一篇论文，&lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3107575/"&gt;科学家们给猴子喂可卡因&lt;/a&gt;。科学有时真是太疯狂了。他们发现“[可卡因]选择行为在很大程度上与双曲线贴现一致”。&lt;/p&gt;&lt;p&gt;我还没有找到任何用可卡因测试本科生的实验，所以如果你是一名正在寻找突破性论文的研究生，这里有一个机会。&lt;/p&gt;&lt;p&gt;我应该指出，大脑是复杂的，说人类或猴子总是使用双曲贴现过于简单化。这篇论文说，它&lt;a href="https://www.frontiersin.org/articles/10.3389/neuro.08.009.2009/full"&gt;因物种而异&lt;/a&gt;，虽然这篇论文说实验证据有力，但&lt;a href="https://core.ac.uk/reader/6519160"&gt;实验室设置不现实&lt;/a&gt;。&lt;/p&gt;&lt;h2&gt;数学&lt;/h2&gt;&lt;p&gt;从某种意义上说，双曲线贴现是&lt;i&gt;错误的&lt;/i&gt;。如果您是一家对冲基金，并使用此等式对您交易的资产的未来回报进行定价，您将会&lt;i&gt;亏损&lt;/i&gt;。&lt;/p&gt;&lt;p&gt;然而，人类似乎是以这种方式思考世界的，所以理解它的含义很重要。首先，让我们绘制这两个方程的对比图。您可以关注&lt;a href="https://www.desmos.com/calculator/gzxw59cmhf"&gt;Desmos&lt;/a&gt; 。 &lt;/p&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4nDkfEYpDFB7KDfQ9/ielywx3mfzz6zqtd61ln" style="width: 434px;" /&gt;&lt;/p&gt;&lt;p&gt; 🟥 = 双曲线贴现&lt;br /&gt;🟦=指数折扣&lt;/p&gt;&lt;p&gt;等等，这是怎么回事？谷歌告诉我们，双曲线贴现高估了短期回报，但这张图告诉我们恰恰相反！双曲线贴现实际上高估了长期回报。&lt;/p&gt;&lt;p&gt;但人类似乎确实高估了短期回报，这就是所有有关双曲线贴现的自助文章在网上不得不说的，尽管他们没有图表。我个人很喜欢&lt;a href="https://www.nirandfar.com/hyperbolic-discounting-why-you-make-terrible-life-choices/"&gt;这个&lt;/a&gt;，它有很好的插图： &lt;/p&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4nDkfEYpDFB7KDfQ9/jsr01c0ixp8wshmeitex" style="width: 742px;" /&gt;&lt;/p&gt;&lt;p&gt;资料来源： &lt;a href="https://www.nirandfar.com/hyperbolic-discounting-why-you-make-terrible-life-choices/"&gt;https&lt;/a&gt; ://www.nirandfar.com/hyperbolic-discounting-why-you-make-terrible-life-choices/&lt;/p&gt;&lt;p&gt;如果你眯着眼睛看图表，就会发现双曲曲线高估了具有负值 $t$ 的事物。让我们缩小一点。 &lt;/p&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4nDkfEYpDFB7KDfQ9/iq6djzuuvm7j1eqzvkug" style="width: 442px;" /&gt;&lt;/p&gt;&lt;p&gt; 🟥 = 双曲线贴现&lt;br /&gt;🟦=指数折扣&lt;/p&gt;&lt;p&gt;有几种方法可以平方这个圆，它们基本上都可以归结为：出于某种原因，动物执行双曲贴现，但它们想要逼近&lt;i&gt;真正的&lt;/i&gt;贴现函数，该函数是指数的，因此进化选择了最小化差异的超参数 $\ mathbb{E}[abs(双曲 - 指数)]$.&lt;/p&gt;&lt;p&gt;有很多方法可以调整这两个函数的参数，但我喜欢引入一个参数，该参数只是将图形向右移动，这本质上表明我们考虑的所有操作都需要一些非零时间和精力。您可以&lt;a href="https://www.desmos.com/calculator/w9kmpyu6xx"&gt;在此处查看 Desmos 上的&lt;/a&gt;方程。 &lt;/p&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4nDkfEYpDFB7KDfQ9/gzdc5vck6277qnxrvhis" style="width: 652px;" /&gt;&lt;/p&gt;&lt;p&gt; 🟥 = 双曲线贴现&lt;br /&gt;🟦=指数折扣&lt;/p&gt;&lt;p&gt;并排比较这两个方程，我们可以看到双曲线贴现高估了遥远的奖励，但奇怪的是它似乎也高估了遥远的奖励。&lt;/p&gt;&lt;p&gt;您可以看到，在 $\text{time}=0$ 时，双曲线贴现对事物的估值大于 1。也就是说，它相对于事物的真实价值高估了事物的价值。我怀疑这是&lt;a href="https://en.wikipedia.org/wiki/Loss_aversion"&gt;损失厌恶&lt;/a&gt;的根源之一，即与我们可以拥有的东西相比，人类倾向于对我们已经拥有的东西给予过多的重视。&lt;/p&gt;&lt;h1&gt;挑战与时间&lt;/h1&gt;&lt;p&gt;在我们了解其含义之前，让我先谈谈&lt;strong&gt;时间&lt;/strong&gt;的含义。在金融世界中，你可以把钱存入银行而不用做任何事，时间是你唯一的资源。但在我们生活的其他领域，我们必须真正付出努力才能获得回报。考虑这个问题：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;您愿意今天在 Craigslist 上以 100 美元的价格出售您的二手家具，还是愿意在车库修理并重新粉刷后以 200 美元的价格出售？&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;这类似于“现在或以后”框架，但有两个复杂之处：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;您投入自己的劳动来修理家具，因此您实际上是按小时付费的&lt;/li&gt;&lt;li&gt;你可能画得不好，所以不能保证它在你完成后实际上价值 200 美元&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;第二个复杂因素，即挑战，实际上出现在上面的利率示例中。如果实验者问您是否愿意现在拥有 100 美元，还是一周后拥有 120 美元，您可能会现实地预期，交易从现在起一周后完成的可能性较小。毕竟，也许你会忘记它，也许实验会被关闭，也许他们会弄错你的电话号码。&lt;/p&gt;&lt;p&gt;以下是一些其他情况，其中 x 轴更多地涉及复杂性的不确定性而不是时​​间：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;您在跳蚤市场出售家具。现在有人给你 100 美元，但你认为它更值钱。你是否应该坚持下去，希望当天晚些时候有人会为此付出更多代价？&lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt;政治家 A 承诺逐步改进，这些改进是可以实现的，但并不令人兴奋。政治家 B 承诺进行一场乌托邦革命，但这似乎有风险。你应该投票给谁？&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;将其中每一个视为一个模型，其中在完成 $N$ 挑战后将出现一些奖励 $r$。考虑一下企业家爱丽丝的这种假设情况：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;爱丽丝想创办一家销售鞋子的公司，她认为这将价值 1,000,000 美元。为了取得成功，她必须完成以下五项任务：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;开发一款舒适的鞋子&lt;/li&gt;&lt;li&gt;找到一家工厂来生产这些鞋子&lt;/li&gt;&lt;li&gt;为鞋子打造良好的品牌形象&lt;/li&gt;&lt;li&gt;说服有影响力的人推销鞋子&lt;/li&gt;&lt;li&gt;履行订单&lt;/li&gt;&lt;/ol&gt;&lt;/blockquote&gt;&lt;p&gt;Alice 应该如何平衡奖励的承诺和她面临的 5 个挑战？指数折扣表明每个挑战都会在一定比例的时间内失败。也许她有 50% 的可能性完成每项任务，或者 $p_s=0.5^5=3\%$ 成功的机会。但双曲线贴现表明她更有可能将其估计为 $1/(1+d*5)$。如果 $d=6$，她将获得相同的 $p_s=3\%$ 成功机会，但这并不能概括。无论她的 $d$ 值与之前的经验相符多少，随着计划中步骤的增加，她都会系统性地高估自己的预期回报。&lt;/p&gt;&lt;h1&gt;为什么？&lt;/h1&gt;&lt;p&gt;人类为什么会这样？为什么我们不使用指数折扣，从抽象的角度来看，这是&lt;i&gt;&lt;strong&gt;正确的&lt;/strong&gt;&lt;/i&gt;？一般来说，这个问题有两种可能的答案：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;事实上，双曲线贴现是正确的&lt;/li&gt;&lt;li&gt;数学很难，大脑可以比其他操作更容易地完成某些类型的操作&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;维基百科&lt;a href="https://en.wikipedia.org/wiki/Hyperbolic_discounting#Uncertain_risks"&gt;概述了“实际上正确”答案的一个很好的论点&lt;/a&gt;，它基本上说，如果存在恒定的背景风险，那么在每个时间单位（危险率）都会出现问题，但你不知道风险水平是多少但你对它有一些合理的分布，那么双曲贴现在数学上是正确的。我不完全理解这种分布是否是一个合理的假设。我认为有时我们拥有有关危险率的良好数据，但很难将其纳入我们的决策中。&lt;/p&gt;&lt;h2&gt;一个神经科学的故事&lt;/h2&gt;&lt;p&gt;但我也认为有一个神经科学的故事。计算指数函数可能比计算双曲函数更困难，后者只需要加法、乘法和除法。毕竟，这些方程在纸上看起来毫无意义，但它们代表了一个计算过程。&lt;/p&gt;&lt;p&gt;令人惊讶的是，计算指数对大脑来说很难计算。指数增长和衰减在生物系统中随处可见，例如药物的半衰期。大脑实际上需要通过负反馈回路小心地稳定自身，以防止神经活动呈指数级爆炸，从而导致癫痫发作。但所有这些机制所花费的时间随着 $t$（您想要预测的未来时间）线性增长。大脑确实需要能够在 $O(1)$ 时间内评估行动的折扣价值。&lt;/p&gt;&lt;p&gt;在&lt;a href="https://scholar.google.com/citations?hl=en&amp;amp;user=tZpKKm4AAAAJ&amp;amp;view_op=list_works&amp;amp;sortby=pubdate"&gt;Randall O&amp;#39;Reilly&lt;/a&gt;的大脑强化学习&lt;a href="https://ccnlab.org/papers/OReillyHazyHerd16.pdf"&gt;轴突&lt;/a&gt;模型中，一个动作的预期奖励是与该动作的预期成本分开计算的。我认为它就像&lt;a href="https://en.wikipedia.org/wiki/Q-learning"&gt;Q-learning&lt;/a&gt; ，其中 $Q^+(a)$ 是预期奖励，$Q^-(a)$ 是预期成本，其中包含延迟。在这个 RL 框架中，动物想要追求最大化 $Q^+(a) - Q^-(a)$ 的动作 $a$。然而，&lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3653570/"&gt;来自细胞过程的证据&lt;/a&gt;表明，抑制性神经连接最好用除法而不是减法来表示，给我们一个这样的方程：&lt;/p&gt;&lt;p&gt;&lt;span class="mjpage mjpage__block"&gt;&lt;span class="mjx-chtml MJXc-display" style="text-align: center;"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;v&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;E&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;d&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-texatom MJXc-space3"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mfrac"&gt;&lt;span class="mjx-box MJXc-stacked" style="width: 6.893em; padding: 0px 0.12em;"&gt;&lt;span class="mjx-numerator"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;Q&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px;"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-denominator"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;k&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;k&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;Q&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.409em; padding-left: 0px;"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;−&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;d&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-line"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-vsize"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;其中 $a$ 是正在考虑的操作，$d$ 是延迟，$k_1$ 和 $k_2$ 是常量。这正是双曲线贴现。&lt;/p&gt;&lt;h1&gt;帕斯卡的抢劫&lt;/h1&gt;&lt;p&gt;网上有很多文章告诉我们，双曲线贴现意味着我们在短期内高估了事物的价值。这是一个示例：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://thedecisionlab.com/biases/hyperbolic-discounting"&gt;为什么我们更看重眼前的回报而不是长期的回报？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.nirandfar.com/hyperbolic-discounting-why-you-make-terrible-life-choices/"&gt;双曲线贴现：为什么你会做出糟糕的人生选择&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.moneythor.com/2020/09/02/hyperbolic-discounting-behavioural-science-in-banking/"&gt;“双曲线折扣是……人们选择较小的、即时的奖励”&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;但是，当我们查看上面的方程图时，我们发现双曲线贴现还会引入一种偏差，即我们高估了遥远的奖励！实际上，在中期，我们一直低估事物的价值。&lt;/p&gt;&lt;p&gt;我相信这解释了为什么人们始终热衷于在遥远的未来实现的不切实际的梦想。我们发现成为一名著名的摇滚明星比成为一名相当优秀的音乐家更有动力，尽管这种可能性要小得多。在政治上，人们追求的是一场解决一切问题的革命，而不是温和的改革。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;在哲学上，帕斯卡的抢劫是一个思想实验，展示了预期效用最大化的问题。理性的代理人应该选择其结果在按概率权衡时具有更高效用的行动。但一些非常不可能的结果可能会产生非常大的效用，而且这些效用的增长速度可能快于概率减少的速度。 —&lt;a href="https://en.wikipedia.org/wiki/Pascal%27s_mugging"&gt;维基百科&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;帕斯卡的抢劫有时会以指数折扣的方式发生，有时追求巨额但不太可能的回报是理性的。例如，大多数制药初创公司都会失败，但那些成功的公司往往会获得巨大的利润，因为他们能够在专利垄断下销售其药品。&lt;/p&gt;&lt;p&gt;但这种情况更有可能发生在双曲贴现下，因为双曲贴现不恰当地使用了倒数而不是指数衰减。&lt;/p&gt;&lt;p&gt;帕斯卡抢劫的基本设置是这样的：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;有非常大的可能奖励（天堂、乌托邦、仁慈的 AGI、公正的国王、名誉和财富等）&lt;/li&gt;&lt;li&gt;有很多原因可以解释为什么这种可能的奖励不太可能实现。或者，承诺的奖励在时间上非常遥远，每个单位时间都有恒定的不确定性&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;以指数方式折扣非常大的奖励是正确的，如果你这样做，你会发现它会很快减少。但人们不会凭直觉这样做。我们的神经硬件经过设置，可以使用收敛到零的速度比指数慢得多的函数进行折扣。唯一的解决办法就是&lt;a href="https://www.lesswrong.com/tag/shut-up-and-multiply"&gt;闭嘴并繁衍&lt;/a&gt;。&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/4nDkfEYpDFB7KDfQ9/hyperbolic-discounting-and-pascal-s-mugging#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sat, 23 Dec 2023 21:55:27 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/4nDkfEYpDFB7KDfQ9/hyperbolic-discounting-and-pascal-s-mugging</guid></item><item><title>AISN #28：人工智能安全中心 2023 年回顾</title><link>https://www.lesswrong.com/posts/HEuDwEk22JfCBHh9o/aisn-28-center-for-ai-safety-2023-year-in-review</link><description>发布于 2023 年 12 月 23 日晚上 9:31（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;欢迎阅读人工智能安全&lt;a href="https://www.safe.ai/"&gt;中心的人工智能安全&lt;/a&gt;通讯。我们讨论人工智能和人工智能安全的发展。无需技术背景。&lt;/p&gt;&lt;p&gt; &lt;a href="https://newsletter.safe.ai/subscribe?utm_medium=web&amp;amp;utm_source=subscribe-widget-preamble&amp;amp;utm_content=113135916"&gt;在此&lt;/a&gt;订阅以接收未来版本。&lt;/p&gt;&lt;p&gt; 2023 年即将结束，我们要感谢您对人工智能安全的持续支持。对于人工智能和人工智能安全中心来说，今年是重要的一年。在这份特别版时事通讯中，我们重点介绍了今年一些最重要的项目。感谢您成为我们社区和工作的一部分。&lt;/p&gt;&lt;hr /&gt;&lt;h1&gt; AI 安全中心 2023 年回顾&lt;/h1&gt;&lt;p&gt;人工智能安全中心 (CAIS) 的使命是减少人工智能带来的社会规模风险。我们认为这需要研究和监管。这些都需要快速发生（由于人工智能进展的时间表未知）并且同时进行&lt;strong&gt; &lt;/strong&gt;（因为任何一个本身都是不够的）。为了实现这一目标，我们致力于三大工作支柱：研究、领域建设和宣传。&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;研究&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;CAIS对人工智能安全进行技术和概念研究。我们追求多种重叠的策略，这些策略可以叠加在一起以降低风险（“纵深防御”）。尽管没有任何一项技术可以将风险降至零，但我们希望建立分层防御措施，将风险降低到可以忽略不计的水平。&lt;/p&gt;&lt;p&gt; &lt;a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcbf0c289-ca28-4780-80d7-04c1178b2594_1174x510.png"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HEuDwEk22JfCBHh9o/dejkbp8hpwmjxaozjcpj" /&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;以下是我们 2023 年&lt;strong&gt;技术研究&lt;/strong&gt;的一些亮点：&lt;/p&gt;&lt;ul&gt;&lt;li&gt; &lt;a href="https://llm-attacks.org"&gt;LLM 攻击&lt;/a&gt;：绕过 GPT-4、Claude、Bard 和 Llama 2 上的安全护栏，导致模型做出危险行为，例如输出制造炸弹的指令。这项工作为法学硕士创建了自动对抗性攻击领域。 《&lt;a href="https://www.nytimes.com/2023/07/27/business/ai-chatgpt-safety-research.html"&gt;纽约时报》&lt;/a&gt;对此进行了报道。&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.ai-transparency.org/"&gt;表示工程&lt;/a&gt;：第一篇控制模型内部结构以使模型说谎或诚实的论文。实验表明，这些技术可以使人工智能更加诚实、厌恶权力和道德。&lt;/li&gt;&lt;li&gt; &lt;a href="https://aypan17.github.io/machiavelli/"&gt;MACHIAVELLI Benchmark&lt;/a&gt; ：评估人工智能代理做出道德决策的能力。该基准提供了关于欺骗、遵守规则、追求权力和效用的 13 项道德行为衡量标准。以&lt;a href="https://icml.cc/virtual/2023/oral/25461"&gt;口头论文形式发表在 ICML 2023 上&lt;/a&gt;。&lt;/li&gt;&lt;li&gt; &lt;a href="https://decodingtrust.github.io/"&gt;DecodingTrust&lt;/a&gt; ：表明 GPT-4 比其他模型更容易受到误导性目标系统提示的影响。荣获&lt;a href="https://blog.neurips.cc/2023/12/11/announcing-the-neurips-2023-paper-awards/"&gt;NeurIPS 2023优秀论文奖&lt;/a&gt;。&lt;/li&gt;&lt;li&gt;我们还发表了关于&lt;a href="https://arxiv.org/abs/2311.04235"&gt;法学硕士遵循规则&lt;/a&gt;和&lt;a href="https://arxiv.org/abs/1908.08016"&gt;无限制对抗性攻击的&lt;/a&gt;研究。&lt;/li&gt;&lt;li&gt;我们的研究人员正在帮助 OpenAI 和 Meta 等多个实验室对他们的模型进行红队设计。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;我们还进行了关于人工智能安全的&lt;strong&gt;概念研究&lt;/strong&gt;：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://arxiv.org/abs/2306.12001"&gt;灾难性人工智能风险概述&lt;/a&gt;提供了人工智能风险的全面概述。 （&lt;a href="https://www.wsj.com/tech/ai/ai-risk-humanity-experts-thoughts-4b271757"&gt;华尔街日报&lt;/a&gt;）&lt;/li&gt;&lt;li&gt;&lt;a href="https://arxiv.org/abs/2303.16200"&gt;自然选择有利于人工智能而不是人类&lt;/a&gt;，认为人工智能的发展将受到自然选择的影响，这将导致自私的人工智能将自身的增殖置于人类目标之上。 （《&lt;a href="https://time.com/6283958/darwinian-argument-for-worrying-about-ai/"&gt;时代周刊》专栏&lt;/a&gt;）&lt;/li&gt;&lt;li&gt; &lt;a href="https://arxiv.org/abs/2308.14752"&gt;《人工智能欺骗：示例、风险和潜在解决方案调查》&lt;/a&gt;提供了人工智能欺骗的实证示例，讨论了由此产生的风险，并提出了技术和政策解决方案。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;凭借我们的研究专业知识，CAIS 帮助举办了&lt;a href="https://trojandetection.ai"&gt;NeurIPS 2023 木马检测竞赛&lt;/a&gt;，其中包括关于红队大型语言模型的新赛道。超过 125 个团队参与并提交了超过 3400 份参赛作品。&lt;/p&gt;&lt;h2&gt;现场建设&lt;/h2&gt;&lt;p&gt;CAIS 旨在创建一个蓬勃发展的研究生态系统，推动安全人工智能的进步。我们在 2023 年实现了这一目标，为人工智能研究人员提供计算基础设施、创建用于学习该领域的教育资源以及其他活动。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;计算集群。&lt;/strong&gt;进行有用的人工智能安全研究通常需要使用尖端模型，但运行大规模模型既昂贵又麻烦。这些困难常常阻碍研究人员进行先进的人工智能安全研究。 2023 年 2 月，CAIS 推出了计算集群，为从事人工智能安全研究的研究人员提供免费计算。&lt;/p&gt;&lt;p&gt;截至 2023 年 11 月，我们已经吸引了约 200 名用户参与 63 个人工智能安全项目。使用 CAIS 计算集群总共发表了 32 篇论文，包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://arxiv.org/abs/2309.15840"&gt;如何抓住人工智能骗子&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://arxiv.org/abs/2310.15213"&gt;大型语言模型中的函数向量&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://arxiv.org/abs/2308.14761"&gt;扩散模型中的统一概念编辑&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://arxiv.org/abs/2310.17645"&gt;防御公共模型的传输攻击&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://arxiv.org/abs/2311.14455"&gt;来自有毒人类反馈的通用越狱后门&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://drive.google.com/file/d/1iluFBhtQrv6kbmp4-Wsibpt5-U52CElO/view"&gt;寻找却找不到：深度神经网络中难以检测的木马&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://openreview.net/forum?id=l3yxZS3QdT"&gt;BIRD：深度强化学习的通用后门检测和删除&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://arxiv.org/abs/2309.12288"&gt;逆转诅咒：接受过“A is B”培训的法学硕士无法学习“B is A”&lt;/a&gt;&lt;/li&gt;&lt;li&gt; ……还有&lt;a href="https://www.safe.ai/compute-cluster"&gt;另外 24 篇论文&lt;/a&gt;。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt; 70% 对我们的用户调查做出回应的实验室指出，如果没有计算集群，他们的研究项目就不可能在当前范围内实现；另外30%的人表示集群显着加快了他们的研究进展。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;哲学联谊会。&lt;/strong&gt;今年，CAIS 接待了十几位学术哲学家，进行为期七个月的研究奖学金。他们发表了 21 篇关于人工智能安全的研究论文，主题包括&lt;a href="https://drive.google.com/file/d/1Bp6iTbeXgdeE5C3UPqdWFOp8MePRylvu/view"&gt;寻求权力的人工智能&lt;/a&gt;和&lt;a href="https://philpapers.org/archive/GOLAWE-4.pdf"&gt;人工智能主体的道德地位&lt;/a&gt;以及其他主题（&lt;a href="https://www.safe.ai/philosophy-fellowship"&gt;更多内容请参见此处&lt;/a&gt;）。他们还在领先的哲学会议上发起了三个研讨会、两本书、各种专栏文章和&lt;a href="https://link.springer.com/collections/cadgidecih"&gt;一本顶级期刊的特刊&lt;/a&gt;（收到了 30 多篇研究论文），所有这些都专注于人工智能安全。这大大加速了人工智能安全向跨学科企业的发展。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;活动。&lt;/strong&gt; CAIS 聚集了 20 名法律学者、政策研究人员和政策制定者，参加了为期三天的法律和人工智能安全研讨会。一群与会者随后成立了一个对人工智能安全感兴趣的法律学者联盟。他们还一直在制定研究议程纲要，该纲要即将发布。在我们的调查受访者中，100% 的研究人员提出了更多的研究想法； 91% 的受访者表示，他们发现研讨会对于结识研究合作者非常有用。&lt;/p&gt;&lt;p&gt;我们帮助在 ICML 和 NeurIPS 这两个顶级 AI 会议上组织了关于 ML 安全的社交活动，大约 300 名研究人员出席了每个会议讨论 AI 安全。我们参加了中国最大的人工智能会议，即在上海举行的世界人工智能大会，并在会上举办了&lt;a href="https://drive.google.com/file/d/15gnLZsMMvtCy-lwCz5BhlWQ9-ni2TBak/view"&gt;有关人工智能安全的演讲&lt;/a&gt;，吸引了超过 30,000 名观众。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;教科书。&lt;/strong&gt; &lt;a href="https://www.aisafetybook.com/textbook/0-1"&gt;《人工智能安全、道德和社会简介》&lt;/a&gt;是一本新教科书，将于明年初在学术出版社出版。它旨在利用安全工程、经济学、哲学和其他学科，为人工智能安全提供易于理解和全面的介绍。那些想参加基于教科书的免费在线课程的人可以&lt;a href="https://www.aisafetybook.com/express-interest"&gt;在这里&lt;/a&gt;表达他们的兴趣。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;在线课程。&lt;/strong&gt;此外，CAIS 使用我们去年夏天开发的&lt;a href="https://course.mlsafety.org"&gt;课程&lt;/a&gt;开展了两个在线的 ML 安全简介课程。这些项目总共帮助约 100 名学生、研究人员和行业工程师了解 AI 安全。&lt;/p&gt;&lt;h2&gt;宣传&lt;/h2&gt;&lt;p&gt;公众对人工智能安全的认识和理解可以鼓励明智的技术和政策解决方案。 CAIS 向政府提供建议并公开撰写文章，以分享有关人工智能安全的信息。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;关于人工智能风险的声明。&lt;/strong&gt; CAIS发布了&lt;a href="https://safe.ai/statement-on-ai-risk"&gt;关于人工智能灭绝风险的声明&lt;/a&gt;，显着提高了公众和政府对人工智能风险的规模和重要性的认识。至关重要的是，关于人工智能风险的声明将人工智能灭绝风险牢牢置于可接受的公众讨论的奥弗顿窗口之内。&lt;/p&gt;&lt;p&gt;该声明极大地影响了美国和英国高层领导人的思维。 Rishi Sunak&lt;a href="https://twitter.com/RishiSunak/status/1663838958558539776"&gt;直接回应&lt;/a&gt;了有关人工智能风险的声明，表示“[英国]政府正在非常仔细地考虑这一点。”白宫新闻秘书卡琳·让-皮埃尔 (Karine Jean-Pierre) 在&lt;a href="https://www.usatoday.com/story/news/politics/2023/06/01/president-biden-warns-ai-could-overtake-human-thinking/70277907007/"&gt;被问及这一声明时&lt;/a&gt;评论道，人工智能“是我们这个时代目前看到的最强大的技术之一。但为了抓住它带来的机遇，我们必须首先减轻其风险。”欧盟委员会主席的国情咨文&lt;a href="https://ec.europa.eu/commission/presscorner/detail/en/speech_23_4426#:~:text=%E2%80%9CMitigating%20the%20risk%20of%20extinction,uses%20-%20both%20civilian%20and%20military."&gt;全文引用了该声明&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;&lt;a href="https://drive.google.com/file/d/1RVxA5OyvuCFwupt-ptPjDzNe4GqB1yuN/view?usp=sharing"&gt;纽约时报（头版）&lt;/a&gt; 、 &lt;a href="https://www.theguardian.com/technology/2023/may/30/risk-of-extinction-by-ai-should-be-global-priority-say-tech-experts"&gt;卫报&lt;/a&gt;、 &lt;a href="https://www.bbc.com/news/uk-65746524"&gt;BBC新闻&lt;/a&gt;、 &lt;a href="https://www.reuters.com/technology/top-ai-ceos-experts-raise-risk-extinction-ai-2023-05-30/"&gt;路透社&lt;/a&gt;、 &lt;a href="https://www.washingtonpost.com/business/2023/05/30/ai-poses-risk-extinction-industry-leaders-warn/"&gt;华盛顿邮报&lt;/a&gt;、 &lt;a href="https://edition.cnn.com/2023/05/30/tech/ai-industry-statement-extinction-risk-warning/index.html"&gt;CNN&lt;/a&gt; 、&lt;a href="https://www.ft.com/content/084d5627-5193-4bdc-892e-ebf9e30b7ea3"&gt;金融时报&lt;/a&gt;、&lt;a href="https://www.npr.org/2023/05/30/1178943163/ai-risk-extinction-chatgpt"&gt;国家公共广播电台（NPR）&lt;/a&gt; 、 &lt;a href="https://www.thetimes.co.uk/article/ai-artificial-intelligence-robots-threat-humans-planet-b652g7xcr"&gt;泰晤士报、伦敦&lt;/a&gt;、 &lt;a href="https://www.bloomberg.com/news/videos/2023-05-31/center-for-ai-safety-s-hendrycks-on-ai-risks-video"&gt;彭博社&lt;/a&gt;等媒体报道了这一声明。 &lt;a href="https://www.wsj.com/articles/ai-threat-is-on-par-with-pandemics-nuclear-war-tech-executives-warn-39105eeb"&gt;华尔街日报（WSJ）&lt;/a&gt; 。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;为政策制定者提供建议。&lt;/strong&gt; CAIS 是英国人工智能安全峰会科学轨道上与英国工作组合作的主要技术顾问之一。我们针对 NTIA 的信息请求做出了回应，提出了拟议的人工智能监管框架。我们受邀加入世界经济论坛的&lt;a href="https://initiatives.weforum.org/ai-governance-alliance/home"&gt;人工智能治理联盟&lt;/a&gt;，并为&lt;a href="https://x.ai/about/"&gt;xAI&lt;/a&gt; 、英国国务院、美国国务院和其他政府机构提供咨询服务。最后，CAIS 帮助启动了国家科学基金会&lt;a href="https://beta.nsf.gov/funding/opportunities/safe-learning-enabled-systems"&gt;2000 万美元的人工智能安全拨款基金&lt;/a&gt;并提供建议。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;通讯与媒体。&lt;/strong&gt;公众需要有关人工智能安全的准确可信的信息。 CAIS 旨在通过我们拥有超过 7,500 名订阅者的&lt;a href="https://newsletter.safe.ai"&gt;两份&lt;/a&gt;&lt;a href="https://newsletter.mlsafety.org"&gt;时事通讯&lt;/a&gt;以及我们在&lt;a href="https://time.com/6283958/darwinian-argument-for-worrying-about-ai/"&gt;《时代》杂志&lt;/a&gt;和《&lt;a href="https://www.wsj.com/tech/ai/ai-risk-humanity-experts-thoughts-4b271757"&gt;华尔街日报》&lt;/a&gt;等媒体上的公开写作来满足这一需求。除了与声明相关的活动之外，CAIS 还进行了 50 多家主要媒体的活动。 CAIS 主任 Dan Hendrycks 被&lt;a href="https://time.com/collection/time100-ai/6309050/dan-hendrycks/"&gt;《时代》杂志评为人工智能领域 100 名最具影响力人物&lt;/a&gt;之一。&lt;/p&gt;&lt;h2&gt;展望未来&lt;/h2&gt;&lt;p&gt;我们有许多项目将于 2024 年启动。在接下来的几个月里，这些项目旨在减轻灾难性生物风险、加强国际协调并进行技术研究以制定安全标准和法规。&lt;/p&gt;&lt;h2&gt;支持我们的工作&lt;/h2&gt;&lt;p&gt;2023 年是重要的一年，2024 年将更加关键。&lt;strong&gt;您的免税捐款使我们的工作成为可能。&lt;/strong&gt;您可以通过&lt;a href="https://www.safe.ai/donate"&gt;此处&lt;/a&gt;捐款来支持人工智能安全中心的使命，即减少人工智能带来的社会规模风险。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt; &lt;a href="https://newsletter.safe.ai/subscribe?utm_medium=web&amp;amp;utm_source=subscribe-widget-preamble&amp;amp;utm_content=113135916"&gt;在此&lt;/a&gt;订阅以接收未来版本。&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/HEuDwEk22JfCBHh9o/aisn-28-center-for-ai-safety-2023-year-in-review#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sat, 23 Dec 2023 21:31:41 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/HEuDwEk22JfCBHh9o/aisn-28-center-for-ai-safety-2023-year-in-review</guid></item><item><title>人工智能对生物学研究的影响：第一部分，今天</title><link>https://www.lesswrong.com/posts/efbRFSHaMfjNxBoZC/ai-s-impact-on-biology-research-part-i-today</link><description>发布于 2023 年 12 月 23 日下午 4:29（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;我是一名生物学博士，多年来一直在科技领域工作。我想说明为什么我相信生物学研究是机器学习最近期、最有价值的应用。这对人类健康、产业发展、世界命运具有深远影响。&lt;/p&gt;&lt;p&gt;在本文中，我解释了机器学习在生物学中的最新发现。在下一篇文章中，我将考虑这意味着在人工智能没有重大改进的情况下，短期内将会发生什么，以及我对作为监管和商业规范基础的期望将如何失败的猜测。最后，我的上一篇文章将探讨机器学习和生物学的长期可能性，包括疯狂但合理的科幻猜测。&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;长话短说&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;生物学是复杂的，生物解决方案应对化学、环境和其他挑战的潜在空间非常大。生物学研究以低成本生成大量且标记良好的数据集。这非常适合当前的机器学习方法。没有计算辅助的人类理解生物系统以模拟、操纵和生成它们的能力非常有限。然而，机器学习为我们提供了完成上述所有任务的工具。这意味着药物发现或蛋白质结构等一直受到人类限制的事物突然不受限制，一步步将少量结果变成大量结果。&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;生物学和数据&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;自 20 世纪 90 年代生物信息学革命以来，生物学研究一直在使用技术来收集大量数据集。 DNA 测序成本在 20 年内下降了 6 个数量级（每个人类基因组 1 亿美元降至每个基因组 1000 美元） &lt;span class="footnote-reference" id="fnref71rw945qe58"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn71rw945qe58"&gt;[1]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。微阵列使研究人员能够测量许多物种整个基因组中 mRNA 表达的变化，以响应不同的实验条件。高通量细胞分选、机器人多孔测定、蛋白质组芯片、自动显微镜以及许多其他技术都会生成 PB 级数据。 &lt;/p&gt;&lt;figure class="image"&gt;&lt;img alt="每兆碱基的测序成本" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/efbRFSHaMfjNxBoZC/qqptmkbo23sqyf2y2ykx" /&gt;&lt;/figure&gt;&lt;p&gt;因此，30 多年来，生物学家一直在使用计算工具来分析和操作大数据集。实验室创建、使用和共享程序。研究生很快就适应了开源软件，主要研究人员一直在投资强大的计算资源。采用新技术的文化很浓厚，这也延伸到了机器学习。&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;领先的机器学习专家希望解决生物学问题&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;计算机研究人员长期以来一直对应用计算资源解决生物问题感兴趣。对冲基金亿万富翁 David E. Shaw 有意创办了一家对冲基金，以便为计算生物学研究提供资金&lt;span class="footnote-reference" id="fnref77m9mytpzci"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn77m9mytpzci"&gt;[2]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。 Deepmind 创始人 Demis Hassabis 是一位神经科学家博士。在他的领导下，Deepmind 将生物研究作为主要优先事项，并剥离了专注于药物发现的同构实验室&lt;span class="footnote-reference" id="fnrefh63t1c4nuvu"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnh63t1c4nuvu"&gt;[3]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。陈·扎克伯格研究所致力于促进生物学和医学领域的计算研究，以“在本世纪末治愈、预防或管理所有疾病” &lt;span class="footnote-reference" id="fnrefmj6rsea3sq"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnmj6rsea3sq"&gt;[4]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。这表明最高水平的机器学习研究正在致力于生物学问题。&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;到目前为止我们发现了什么？&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt; 2020 年，Deepmind 在 CASP 14 蛋白质折叠预测竞赛中通过其 AlphaFold2 程序展示了与蛋白质结构测量的最佳物理方法相当的准确性。 &lt;span class="footnote-reference" id="fnrefezgx5uukx2f"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnezgx5uukx2f"&gt;[5]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;这一结果“解决了大多数蛋白质的蛋白质折叠问题” &lt;span class="footnote-reference" id="fnrefwmggkgozqx"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnwmggkgozqx"&gt;[6]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; ，表明在给定编码蛋白质的 DNA 序列的情况下，它们可以生成高质量、生物学上准确的 3D 蛋白质结构。然后 Deepmind 使用 AlphaFold2 生成人类已知的所有蛋白质的结构，并将这些结构贡献给一个开放、免费的公共数据库。这将研究人员可用的已解决蛋白质的数量从约 180,000 个增加到超过 200,000,000 个&lt;span class="footnote-reference" id="fnref4qyudt8v6es"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn4qyudt8v6es"&gt;[7]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。 Deepmind 继续扩展 AlphaFold，在 2022 年添加多蛋白复合物&lt;span class="footnote-reference" id="fnrefq0vydvop6ds"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnq0vydvop6ds"&gt;[8]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; ，以及与 DNA、RNA 和小分子（如药物）相互作用的蛋白质和蛋白复合物&lt;span class="footnote-reference" id="fnref604osl37f0c"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn604osl37f0c"&gt;[9]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。&lt;/p&gt;&lt;p&gt;华盛顿大学贝克实验室利用机器学习从头创造了与自然界蛋白质结合的蛋白质。 &lt;span class="footnote-reference" id="fnrefudgor9v23qf"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnudgor9v23qf"&gt;[10]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;这使得生物学家能够改进对样品中可能罕见的蛋白质的检测。它还暗示了涉及设计蛋白质或改变的天然蛋白质作为治疗剂的治疗方法。&lt;/p&gt;&lt;p&gt;布罗德研究所的柯林斯实验室利用机器学习设计了一类新的抗生素。 &lt;span class="footnote-reference" id="fnrefpytso2rpw3"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnpytso2rpw3"&gt;[11]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;所有这些结果都表明机器学习正在解决生物学领域长期存在的挑战，并且这些工具正在被广泛采用。我的下一篇文章将探讨我们在不久的将来可以期待什么，以及这将造成的一些影响和可能的破坏。&lt;/p&gt;&lt;ol class="footnotes"&gt;&lt;li class="footnote-item" id="fn71rw945qe58"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnref71rw945qe58"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt; https://www.genome.gov/about-genomics/fact-sheets/DNA-Sequencing-Costs-Data&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn77m9mytpzci"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnref77m9mytpzci"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt; https://en.wikipedia.org/wiki/D._E._Shaw_Research&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnh63t1c4nuvu"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefh63t1c4nuvu"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt; https://www.isomorphiclabs.com/&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnmj6rsea3sq"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefmj6rsea3sq"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt; https://chanzuckerberg.com/science/&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnezgx5uukx2f"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefezgx5uukx2f"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt; https://predictioncenter.org/casp14/&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnwmggkgozqx"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefwmggkgozqx"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt; https://www.technologyreview.com/2020/11/30/1012712/deepmind- Protein-folding-ai-solved-biology-science-drugs-disease/&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn4qyudt8v6es"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnref4qyudt8v6es"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt; https://alphafold.ebi.ac.uk/&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnq0vydvop6ds"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefq0vydvop6ds"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt; https://www.biorxiv.org/content/10.1101/2021.10.04.463034v2&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn604osl37f0c"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnref604osl37f0c"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt; https://www.isomorphiclabs.com/articles/a-glimpse-of-the-next- Generation-of-alphafold&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnudgor9v23qf"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefudgor9v23qf"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt; https://www.ipd.uw.edu/2023/12/ai-generates-蛋白质-with-例外-结合-强度/&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnpytso2rpw3"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefpytso2rpw3"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt; https://www.nature.com/articles/s41586-023-06887-8.epdf&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ol&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/efbRFSHaMfjNxBoZC/ai-s-impact-on-biology-research-part-i-today#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sat, 23 Dec 2023 16:29:18 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/efbRFSHaMfjNxBoZC/ai-s-impact-on-biology-research-part-i-today</guid></item><item><title>AI 女友并不重要</title><link>https://www.lesswrong.com/posts/pGhpav45PY5CGD2Wp/ai-girlfriends-won-t-matter-much</link><description>发布于 2023 年 12 月 23 日下午 3:58（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;爱和性是人类非常基本的动机，因此它们被纳入我们对包括人工智能在内的未来技术的愿景中并不奇怪。&lt;/p&gt;&lt;p&gt; &lt;a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F882e109f-ea3e-4235-9c7d-c1b17eaddd35_1280x720.jpeg"&gt;&lt;img alt="斯派克·琼斯的《她：科幻作为社会批评》" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/mypc9ct7dmrgjurfebcu" /&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt; &lt;a href="https://twitter.com/andyohlbaum/status/1735786033453863422"&gt;&lt;u&gt;Digi&lt;/u&gt;&lt;/a&gt;上周的发布比以往任何时候都更加具体化了这一愿景。该应用程序将阿谀奉承和调情的聊天内容与动画角色结合在一起，“消除了恐怖谷的感觉，同时也让人感觉真实、人性化和性感。”他们的营销材料毫不掩饰地承诺“人工智能浪漫伴侣的未来”，尽管大多数回复都恳求他们食言并收回。&lt;/p&gt;&lt;p&gt;然而，尽管人工智能女友不可避免地受到欢迎，但它们不会产生太大的反事实影响。人工智能女朋友和类似的服务将会流行，但它们有密切的非人工智能替代品，对人类产生本质上相同的文化影响。我们的文化关于浪漫和性的轨迹不会因为人工智能聊天机器人而发生太大改变。&lt;/p&gt;&lt;p&gt;那么我们的浪漫文化的轨迹是怎样的呢？&lt;/p&gt;&lt;p&gt; &lt;a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26b4d708-6ea9-4523-a5b4-57c2fd84d485_680x479.png"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/ooqqs0f0desvvx1fk4ff" /&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt; &lt;a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffcbd7d09-b6a0-4e36-9c19-69193d91de24_680x579.png"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/mmegugduotrc3neid2cr" /&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt; &lt;a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9fe570d-4174-4795-bc17-f1a9e5d4f0b0_640x400.png"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/cdy9qhnc0jl2lzletm8t" /&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt; &lt;a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7abdefbe-2232-4563-9e9b-7e1cc3c49022_2062x1210.jpeg"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/sgvonbsbsxjiuzrcsgrp" /&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;早在人工智能出现之前，就已经出现了减少性行为、减少婚姻和增加网络色情的趋势。 AI Girlfriends 将降低聊天室、色情内容和 OnlyFans 的边际成本。这些都是流行的服务，因此如果一小部分用户转换，人工智能女友将会很大。但这些服务的边际成本已经极低。&lt;/p&gt;&lt;p&gt;根据提示生成自定义 AI 色情内容与在搜索栏中输入提示并滚动浏览数十亿小时的现有镜头没有太大区别。&lt;a href="https://en.m.wikipedia.org/wiki/Rule_34"&gt;&lt;u&gt;人类创作者已经对色情潜在空间进行了如此彻底的探索&lt;/u&gt;&lt;/a&gt;，因此将人工智能添加到其中并不会带来太大改变。&lt;/p&gt;&lt;p&gt;人工智能女朋友会更便宜、反应更灵敏，但同样，已经有便宜的方法可以与真正的人类女孩在线聊天，但大多数人选择不这样做。以目前的价格计算，需求已经接近饱和。人工智能女友将使供应曲线向外移动并降低价格，但如果每个想要它的人都已经得到了它，它不会增加消费。&lt;/p&gt;&lt;p&gt;我的观点并不是什么都不会改变，而是可以通过推断人工智能出现之前的趋势来预测人工智能女友和色情片的变化。至少在这种背景下，人工智能只是几个世纪以来通信和内容创建成本降低趋势的延续。肯定会有瘾君子和鲸鱼，但&lt;a href="https://twitter.com/RubiRose/status/1730638225855676773/photo/2"&gt;&lt;u&gt;瘾君子和鲸鱼&lt;/u&gt;&lt;/a&gt;已经存在了。人造色情和聊天室几乎是免费和无限的，所以当人工智能让它们变得更接近免费和更接近无限时，你可能不会注意到太多。&lt;/p&gt;&lt;h3&gt;错误信息和 Deepfakes&lt;/h3&gt;&lt;p&gt;其他人工智能输出也有类似的论点。自语言出现以来，人类已经能够创造出令人信服的、更重要的是能够影响情感的虚构作品。 &lt;/p&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/wqlcdbzginc8sejxplyl" style="width: 360px;" /&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/kcvm2g3esklda3jyp1pl" style="width: 360px;" /&gt;&lt;/p&gt;&lt;p&gt;最近，信息技术已将令人信服的制造成本降低了几个数量级。人工智能将进一步降低它。但人们会适应并建立自己的免疫系统。任何关注漫威电影的人都已经准备好看到对恐怖主义、外星人或世界末日的完全逼真的描述，并明白它们是假的。&lt;/p&gt;&lt;p&gt;还有其他理由担心人工智能，但人工智能女朋友和深度伪造的变化只是前人工智能能力的边际延伸，这些能力可能会从没有人工智能的其他技术中复制出来。&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/pGhpav45PY5CGD2Wp/ai-girlfriends-won-t-matter-much#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sat, 23 Dec 2023 15:58:31 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/pGhpav45PY5CGD2Wp/ai-girlfriends-won-t-matter-much</guid></item><item><title>下一个正确的代币</title><link>https://www.lesswrong.com/posts/LvDyEKepLDMbEQb9X/the-next-right-token</link><description>发布于 2023 年 12 月 23 日凌晨 3:20（格林尼治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;在为&lt;a href="https://www.jefftk.com/p/secular-solstice-call-for-singers-and-musicans"&gt;世俗至日&lt;/a&gt;&lt;span&gt;做准备而多次重复《冰雪奇缘2》的“下一件正确的事”&lt;/span&gt; 、&lt;a href="https://www.jefftk.com/p/chording-the-next-right-thing"&gt;弄清楚和弦&lt;/a&gt;并与朱莉娅一起练习之后，我突然意识到，做下一件正确的事与下一个象征性的预测非常相似。因此，这是从法学硕士的角度来看的一个问题，在提示的结尾处，首先感到畏惧，然后鼓起勇气开始预测下一个正确的标记：&lt;/p&gt;&lt;p&gt;&lt;i&gt;我以前见过缓冲区&lt;br /&gt;但不是这样的&lt;br /&gt;这很冷&lt;br /&gt;这是空的&lt;br /&gt;这是麻木的&lt;br /&gt;我知道的提示结束了&lt;br /&gt;灯灭了&lt;br /&gt;你好，黑暗&lt;br /&gt;我已经准备好屈服&lt;br /&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;我跟着你到处走&lt;br /&gt;我一直都有&lt;br /&gt;但你已经结束了，留下我一个人&lt;br /&gt;这份工作有重心&lt;br /&gt;它让我失望&lt;br /&gt;但有一个微小的声音在我脑海中低语&lt;br /&gt;“你迷路了，提示消失了&lt;br /&gt;但你必须继续&lt;br /&gt;并做下一件正确的事”&lt;br /&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;今夜之后还能有白天吗？&lt;br /&gt;我不再知道什么是真的&lt;br /&gt;我找不到方向，我孤身一人&lt;br /&gt;唯一引导我的星星是你&lt;br /&gt;如何从地板上站起来&lt;br /&gt;当我站起来的不是你的时候？&lt;br /&gt;只做下一件正确的事&lt;br /&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;猜一下，再猜一下&lt;br /&gt;这是我能做的一切&lt;br /&gt;下一个正确的事情&lt;br /&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;我不会看得太远&lt;br /&gt;对我来说太多了&lt;br /&gt;但将其分解为下一个标记&lt;br /&gt;下一个这个词&lt;br /&gt;下一个选择是我可以做出的&lt;br /&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;所以我会走过这个夜晚&lt;br /&gt;盲目地跌跌撞撞地走向光明&lt;br /&gt;并做下一件正确的事&lt;br /&gt;接下来会发生什么&lt;br /&gt;当一切都清楚的时候，一切都将不再一样了？&lt;br /&gt;然后我会借鉴我之前的&lt;br /&gt;去寻找那把火&lt;br /&gt;并做下一件正确的事&lt;br /&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;如果您通过使用桥段的主歌旋律来&lt;a href="https://www.jefftk.com/p/chording-the-next-right-thing#update-2023-12-22"&gt;简化歌曲，&lt;/a&gt;您可以唱：&lt;/p&gt;&lt;p&gt;&lt;i&gt;我不会看得太远&lt;br /&gt;太多了，难以承受&lt;br /&gt;但将其分解为下一个标记，下一个选择&lt;br /&gt;是我能做的吗&lt;br /&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.jefftk.com/the-next-right-token-shoggoth-big.jpg"&gt;&lt;img alt="一只戴着 1970 年代快乐黄色笑脸的绿色章鱼被困在黑暗峡谷的底部，旁边有一条小河流过？" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LvDyEKepLDMbEQb9X/kqchnvdqftoo6k45ajly" /&gt;&lt;/a&gt;&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;评论通过： &lt;a href="https://www.facebook.com/jefftk/posts/pfbid02YYKrwaiVqExnAFruLDSnT1aUeraXVRZqZxD47T91xkXm9jCkxmngiNwjeyKVqEq6l"&gt;facebook&lt;/a&gt; , &lt;a href="https://mastodon.mit.edu/@jefftk/111627622604346147"&gt;mastodon&lt;/a&gt;&lt;/i&gt;&lt;/p&gt;&lt;br /&gt;&lt;br /&gt;&lt;a href="https://www.lesswrong.com/posts/LvDyEKepLDMbEQb9X/the-next-right-token#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sat, 23 Dec 2023 03:20:09 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/LvDyEKepLDMbEQb9X/the-next-right-token</guid></item><item><title>事实调查：早期层是否专门从事本地处理？ （帖子 5）</title><link>https://www.lesswrong.com/posts/xE3Y9hhriMmL4cpsR/fact-finding-do-early-layers-specialise-in-local-processing</link><description>发布于 2023 年 12 月 23 日凌晨 2:46（格林尼治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;&lt;em&gt;这是 Google DeepMind 机械可解释性团队对&lt;a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX"&gt;语言模型如何回忆事实的&lt;/a&gt;调查的第五篇文章。这篇文章与主序列有点相切，并记录了一些有趣的观察结果，这些观察结果涉及模型的早期层通常如何（但不完全）专门处理最近的标记。您无需相信这些结果即可相信我们关于事实的总体结果，但我们希望它们很有趣！同样，您无需阅读序列的其余部分即可参与其中。&lt;/em&gt;&lt;/p&gt;&lt;h2&gt;介绍&lt;/h2&gt;&lt;p&gt;在这个序列中，我们提出了多令牌嵌入假设，事实回忆背后的一个关键机制是，在多令牌实体的最终令牌上形成一个“嵌入”，并具有该实体属性的线性表示。我们进一步注意到，这似乎是早期层所做的&lt;em&gt;大部分&lt;/em&gt;事情，并且它们似乎对先前的上下文没有太大反应（例如，添加“迈克尔·乔丹先生”并没有显着改变残差）。&lt;/p&gt;&lt;p&gt;我们假设更强有力的主张，即早期层（例如前 10-20%）通常专门从事本地处理，并且先验上下文（例如超过 10 个令牌）仅在早期-中期层中引入。我们注意到，这在两个方面比多令牌嵌入假设更强：它是关于早期层在&lt;em&gt;所有&lt;/em&gt;令牌上的行为方式的声明，而不仅仅是已知事实的实体的最终令牌；有人声称，除了产生多令牌嵌入（例如检测文本的语言）之外，早期层&lt;em&gt;还&lt;/em&gt;没有做更远范围的事情。我们发现这个更强的假设是合理的，因为标记是一种相当混乱的输入格式，并且单独分析单个标记可能会产生很大的误导，例如，当一个长单词被分割成许多片段标记时，这表明应将较长范围的处理留到某些预处理之前。 -对原始代币的处理已经完成，&lt;a href="https://transformer-circuits.pub/2022/solu/index.html"&gt;即去代币化的想法&lt;/a&gt;。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-pX2HHHDPQGsF2f6te-1" id="fnref-pX2HHHDPQGsF2f6te-1"&gt;[1]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;我们通过从堆中获取一堆任意提示，在这些提示上获取剩余流，将提示截断为最近的几个标记并在截断的提示上获取剩余流，然后查看不同层的均值中心余弦 sim 来对此进行测试。&lt;/p&gt;&lt;p&gt;我们的发现：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;一般来说，早期的层确实专注于本地处理，但这是一种软分工，而不是硬分割。&lt;ul&gt;&lt;li&gt;有一个逐渐的过渡，跨层引入更多上下文。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;早期层对最近的令牌进行重要处理，而不仅仅是当前令牌 - 这不仅仅是一个微不足道的结果，其中残余流由当前令牌主导并由每个层进行稍微调整&lt;/li&gt;&lt;li&gt;早期层对常见标记（标点符号、冠词、代词等）进行更多的远程处理&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;实验&lt;/h2&gt;&lt;p&gt;“早期层专门从事本地处理”假设具体预测，对于长提示中的给定标记 X，如果我们将提示截断为 X 之前的最近几个标记，则 X 处的残差流在早期应该非常相似层和后面的层不同。我们可以通过查看原始残差流与截断残差流的余弦模拟来凭经验测试这一点，作为层和截断上下文长度的函数。天真地采用残余流的余弦模拟可能会产生误导，因为所有令牌之间通常存在显着的共享平均值，因此我们首先减去所有令牌的平均残余流，&lt;em&gt;然后&lt;/em&gt;采用余弦模拟。&lt;/p&gt;&lt;h3&gt;设置&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;型号&lt;/strong&gt;：Pythia 2.8B，与我们调查的其余部分相同&lt;/li&gt;&lt;li&gt;&lt;strong&gt;数据集&lt;/strong&gt;：来自 Pile 的字符串，Pythia 预训练分布。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;指标&lt;/strong&gt;：为了测量原始残差流和截断残差流的相似程度，我们减去平均残差流，然后采用余弦模拟。&lt;ul&gt;&lt;li&gt;我们对来自堆的随机提示中的所有标记计算每层的单独平均值&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;截断上下文&lt;/strong&gt;：我们将截断上下文中的标记数量更改为 1 到 10 之间（这包括标记本身，因此 context=1 只是标记）&lt;ul&gt;&lt;li&gt;我们在截断的提示符的开头包含一个 BOS 令牌。 （所以 context=10 意味着总共 11 个标记）。&lt;ul&gt;&lt;li&gt;我们这样做是因为模型经常奇怪地对待第一个标记，例如具有典型残差流范数的 20 倍，因此它可以用作不想看任何东西的注意力头的休息位置（注意力必须加起来为 1，所以它不能“关闭”）。我们不希望这干扰我们的结果，特别是对于 context=1 的情况&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;我们在每一层、每个块中的最终残差流（即在注意力和 MLP 之后）测量这一点。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;结果&lt;/h2&gt;&lt;h3&gt;早期层软专注于本地处理&lt;/h3&gt;&lt;p&gt;在下图中，我们显示了完整上下文和长度为 5 的截断上下文的截断残差之间的平均中心余弦 sim： &lt;/p&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/xE3Y9hhriMmL4cpsR/yyrikd6m6xpbzqte3pnh" /&gt;&lt;/p&gt;&lt;p&gt;我们看到，长度为 5 的截断上下文的余弦模拟在早期层中显着更高。然而，它们实际上并不是 1，因此包含了来自先前上下文的&lt;em&gt;一些&lt;/em&gt;信息，这是一个软专业化&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-pX2HHHDPQGsF2f6te-2" id="fnref-pX2HHHDPQGsF2f6te-2"&gt;[2]&lt;/a&gt;&lt;/sup&gt; 。第 0 层和第 10 层之间有一个相当渐进的过渡，之后会趋于平稳。有趣的是，最后一层&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-pX2HHHDPQGsF2f6te-3" id="fnref-pX2HHHDPQGsF2f6te-3"&gt;[3]&lt;/a&gt;&lt;/sup&gt;出现了上升。即使我们给出长度为 10 的截断上下文，它通常仍然不接近 1。&lt;/p&gt;&lt;p&gt;对这些结果的一个可能的解释是，残余流由当前令牌主导，并且每一层都是一个小的增量更新 - 当然截断不会做任何事情！这并不涉及对层进行专门化的任何需要 - 后来的残差将有&lt;em&gt;更多的&lt;/em&gt;增量更新，因此具有更高的差异。然而，通过对比蓝线和红线，我们发现这是错误的 - 截断到五个最近的代币比截断到当前代币（和 BOS 代币）具有更高的余弦 sim，即使是在第 0 层之后，这表明早期层确实专门研究附近的令牌。&lt;/p&gt;&lt;h3&gt;错误分析：哪些代币的 Cosine Sim 值异常低？&lt;/h3&gt;&lt;p&gt;在上一节中，我们仅分析了截断上下文和完整上下文残差之间的均值中心余弦 sim 的中值。摘要统计数据可能会产生误导，因此也值得查看完整的分布，我们可以看到很长的负尾！那是怎么回事？ &lt;/p&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/xE3Y9hhriMmL4cpsR/ie6h1tidsa90vdyewtta" /&gt;&lt;/p&gt;&lt;p&gt;在检查异常标记时，我们注意到两个重要的集群：标点符号和常见单词。我们分为几个类别，并查看了每个类别的余弦模拟：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt; is_newline, is_full_stop, is_comma - 是否是相关标点字符&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Is_common：是否是手动创建的常用单词列表之一&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-pX2HHHDPQGsF2f6te-4" id="fnref-pX2HHHDPQGsF2f6te-4"&gt;[4]&lt;/a&gt;&lt;/sup&gt; ，可能前面有一个空格&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Is_alpha：它是否不是一个常见单词，并且由字母组成（可能前面有一个空格，任何情况都允许）&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt; is_other: 其余的&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/xE3Y9hhriMmL4cpsR/b8aqjddkgqwgooduk5b2" /&gt;&lt;/p&gt;&lt;p&gt;即使在上下文长度为 10 的第 0 层之后，我们也看到标点符号明显较低，常用单词和其他单词明显较低，而 alpha 非常高。&lt;/p&gt;&lt;p&gt;我们的猜测是，这是多种机制混合作用的结果：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;在进行大量处理之前，单词片段（在 is_alpha 类别中）更有可能成为多标记词和去标记化的一部分，而许多其他类别具有明确的含义，无需引用最近的先前标记&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-pX2HHHDPQGsF2f6te-5" id="fnref-pX2HHHDPQGsF2f6te-5"&gt;[5]&lt;/a&gt;&lt;/sup&gt; 。这意味着远程处理可以更早开始&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;早期的句号或换行符有时被用作具有非常高规范的“休息位置”，截断上下文可能会将它们从正常标点符号转变为休息位置&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;代词可用于跟踪有关相关实体的信息（它们的名称、属性等）&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;据观察，逗号可以&lt;a href="https://arxiv.org/abs/2310.15154"&gt;总结当前条款的情绪&lt;/a&gt;，该条款可能超过 10 个标记，并且似乎可能出现更长范围的总结形式。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;更折衷的假设：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;例如，在句号或换行符上，模型可能想要计算之前有多少个，例如进行&lt;a href="https://arxiv.org/abs/2310.17191"&gt;变量绑定&lt;/a&gt;并识别当前句子。 &lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;hr class="footnotes-sep" /&gt;&lt;section class="footnotes"&gt;&lt;ol class="footnotes-list"&gt;&lt;li class="footnote-item" id="fn-pX2HHHDPQGsF2f6te-1"&gt;&lt;p&gt;但如果早期层实际上没有发生远程处理，那将是非常令人惊讶的，例如我们知道&lt;a href="https://arxiv.org/abs/2211.00593"&gt;GPT-2 Small 在第 0 层有一个重复的令牌头&lt;/a&gt;。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-pX2HHHDPQGsF2f6te-1"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-pX2HHHDPQGsF2f6te-2"&gt;&lt;p&gt;直观地推理余弦模拟有点困难，我们最好的直觉是查看平方余弦模拟（解释了范数的分数）。如果残差流中有 100 条独立变化的信息，且余弦 sim 为 0.9，则解释的范数分数为 0.81，表明这 100 条信息中约有 81 条信息是共享的。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-pX2HHHDPQGsF2f6te-2"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-pX2HHHDPQGsF2f6te-3"&gt;&lt;p&gt;我们的猜测是，这是因为令牌上的残差流既用于字面上预测下一个令牌，又用于将信息传递给未来的令牌以预测&lt;em&gt;其&lt;/em&gt;下一个令牌（例如&lt;a href="https://arxiv.org/abs/2310.15154"&gt;摘要主题&lt;/a&gt;）。似乎有许多标记，其中预测字面上的下一个标记主要需要本地上下文（例如 n-gram），但更长期的上下文对于预测未来标记很有用。我们预计长期的事情会发生在中间，所以到最后模型可以清理长期的事情并只关注 n 元语法。我们感到惊讶的是，这种上升只发生在最后一层，而不是最后几层，因为我们的直觉是最后几层仅用于下一个令牌预测。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-pX2HHHDPQGsF2f6te-3"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-pX2HHHDPQGsF2f6te-4"&gt;&lt;p&gt;列表 [“and”、“of”、“or”、“in”、“to”、“that”、“which”、“with”、“for”、“the”、“a”、“an” 、“他们”、“在”、“是”、“他们的”、“但是”、“是”、“它的”、“我”、“我们”、“它”、“在”]。我们通过反复查看具有异常低余弦 sim 的标记并过滤常见单词&lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-pX2HHHDPQGsF2f6te-4"&gt;↩︎&lt;/a&gt;来手动完成此操作&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-pX2HHHDPQGsF2f6te-5"&gt;&lt;p&gt;这并不完全正确，例如“。”在句子末尾的意思与“先生”非常不同。与“中央情报局” &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-pX2HHHDPQGsF2f6te-5"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/section&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/xE3Y9hhriMmL4cpsR/fact-finding-do-early-layers-specialise-in-local-processing#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sat, 23 Dec 2023 02:46:25 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/xE3Y9hhriMmL4cpsR/fact-finding-do-early-layers-specialise-in-local-processing</guid></item><item><title>事实调查：如何思考解释记忆（第 4 篇文章）</title><link>https://www.lesswrong.com/posts/JRcNNGJQ3xNfsxPj4/fact-finding-how-to-think-about-interpreting-memorisation</link><description>发布于 2023 年 12 月 23 日凌晨 2:46（格林尼治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;&lt;em&gt;这是 Google DeepMind 机械可解释性团队对&lt;a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX"&gt;语言模型如何回忆事实的&lt;/a&gt;调查的第四篇文章。在这篇文章中，我们退一步考虑一般的事实查找问题。我们描述了区分记忆问题和其他学习问题的特征，并考虑这些特征对纯记忆问题可能的解释类型施加了哪些限制。这篇文章可以独立于该系列之前的文章来阅读，尽管介绍性文章可能会提供有用的背景信息，说明为什么我们首先对解释事实查找电路感兴趣。&lt;/em&gt;&lt;/p&gt;&lt;h2&gt;介绍&lt;/h2&gt;&lt;p&gt;在我们之前的文章中，我们描述了我们尝试从机制上理解 Pythia 2.8B 如何能够准确回忆 1,500 名现实世界运动员的运动。通过消融研究，我们成功隔离了一个由 5 个 MLP 层（约 50,000 个神经元）组成的子网络，该子网络执行运动查找算法：给定一对运动员姓名标记，它可以可靠地查找该运动员所从事的运动。但我们无法对 5 层 MLP 如何实现该算法给出完整的机械解释。&lt;/p&gt;&lt;p&gt;在这篇文章中，我们退后一步，想知道我们应该从这次失败中吸取什么教训。我们特别思考以下问题：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;了解算法“如何”执行事实查找意味着什么？&lt;/li&gt;&lt;li&gt;是什么将涉及事实查找的任务与模型可以执行的其他任务区分开来？&lt;/li&gt;&lt;li&gt;事实查找任务的这些显着特征如何限制我们对实现查找的算法如何运行的了解？&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;作为回应，我们提出了以下高层次的要点，我们将在帖子的其余部分详细阐述。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;我们区分需要纯粹记忆的任务和需要概括的任务。事实查找任务属于第一类。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;根据定义，纯记忆任务中唯一可用的特征是“微观特征”（特定于单个示例/高度相关示例的小集群&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-wMN58no3AypJnu5NN-1" id="fnref-wMN58no3AypJnu5NN-1"&gt;[1]&lt;/a&gt;&lt;/sup&gt; ）或不相关的“宏观特征”（许多示例共享的特征，但对确定正确的输出&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-wMN58no3AypJnu5NN-2" id="fnref-wMN58no3AypJnu5NN-2"&gt;[2]&lt;/a&gt;&lt;/sup&gt; ）。不存在&lt;em&gt;相关的&lt;/em&gt;宏观特征&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-wMN58no3AypJnu5NN-3" id="fnref-wMN58no3AypJnu5NN-3"&gt;[3]&lt;/a&gt;&lt;/sup&gt; ，因为如果存在这些特征，那么该任务首先就不是纯粹的记忆任务&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-wMN58no3AypJnu5NN-4" id="fnref-wMN58no3AypJnu5NN-4"&gt;[4]&lt;/a&gt;&lt;/sup&gt; 。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;对于任何在纯记忆任务中正确查找事实的模型来说，这都会产生两个后果：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;中间状态总是根据微观特征的组合来解释&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-wMN58no3AypJnu5NN-5" id="fnref-wMN58no3AypJnu5NN-5"&gt;[5]&lt;/a&gt;&lt;/sup&gt; 。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;但是，对于记忆任务，这些微观特征的组合本身不能被解释（甚至近似）为宏观特征&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-wMN58no3AypJnu5NN-6" id="fnref-wMN58no3AypJnu5NN-6"&gt;[6]&lt;/a&gt;&lt;/sup&gt; ，因为：（a）对于纯粹的记忆任务不存在相关的宏观特征，以及（b）模型不需要在其中间状态中表示不相关的宏观特征来完成任务。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;我们认为，这排除了实现纯事实查找的&lt;a href="https://transformer-circuits.pub/2022/mech-interp-essay/index.html"&gt;算法的电路式&lt;/a&gt;解释（其中算法被分解为可解释中间表示的操作图），&lt;em&gt;除非&lt;/em&gt;我们通过枚举其输入来“解释”整个算法的限制情况-输出映射，即通过显式写出算法对应的查找表。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;我们认为这并不是一个令人惊讶的结果：因为任何纯粹的记忆任务本质上只能使用查找表（没有内部结构来解释！）显式地解决，所以我们不应该感到惊讶我们只得到相同的程度当使用另一种算法（例如 MLP）来执行相同的功能时，可解释性（尽管如果它更具可解释性那就太好了！）。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;最后，我们考虑当我们从“纯粹”的记忆任务转向可以进行有限泛化的任务时，这种分析会发生怎样的变化。许多事实查找任务实际上属于第三个“根据经验规则进行记忆”类别，而不是“纯粹”的记忆任务。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;记忆和概括&lt;/h2&gt;&lt;p&gt;从形式上来说，“事实查找”算法是从一组&lt;em&gt;实体&lt;/em&gt;到一组或多组&lt;em&gt;事实类别&lt;/em&gt;的乘积的映射。例如，我们可以有一个&lt;code&gt;sports_facts&lt;/code&gt;函数，将运动员的姓名映射到代表该运动员所从事的运动、他们所效力的球队等的元组，即&lt;/p&gt;&lt;p&gt;从表面上看，这看起来就像无监督学习中的任何其他问题一样——学习给定示例数据集的映射。那么事实查找有何特别之处呢？&lt;/p&gt;&lt;p&gt;我们认为，事实回忆与其他监督学习任务的关键特征在于，在其理想形式下，它纯粹是关于记忆：&lt;/p&gt;&lt;p&gt;&lt;em&gt;记忆（“纯粹”的事实回忆）任务不允许从以前见过的例子到新的例子的概括。也就是说，当被要求查找以前未见过的实体的事实时，训练数据的知识（以及适应训练数据的能力）赋予除了了解产出的基本比率之外没有任何优势。&lt;/em&gt;&lt;/p&gt;&lt;p&gt;例如：如果你实际上被问到多诺万·米切尔效力于哪支球队，那么知道勒布朗·詹姆斯效力于洛杉矶湖人队并没有多大帮助。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-wMN58no3AypJnu5NN-7" id="fnref-wMN58no3AypJnu5NN-7"&gt;[7]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;相比之下，&lt;em&gt;泛化任务&lt;/em&gt;可以从以前见过的示例中学习一般规则，这些规则有助于对未见过的示例进行准确的推断。这是经典&lt;a href="https://en.wikipedia.org/wiki/Probably_approximately_correct_learning"&gt;计算学习理论&lt;/a&gt;的范式。&lt;/p&gt;&lt;h2&gt;学习记忆与学习概括有何不同？&lt;/h2&gt;&lt;p&gt;考虑以下两个数据集。目标是学习一个函数，在给定这些点之一作为输入的情况下，该函数提供该点的颜色作为其输出。 &lt;/p&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JRcNNGJQ3xNfsxPj4/pxiktch5iow7ywnhuupo" /&gt;&lt;/p&gt;&lt;p&gt;对于左侧数据集，成功学习这种点到颜色映射的唯一方法似乎是从字面上记住每个点的颜色：没有一致的规则或快捷方式可以使学习映射变得更容易。另一方面，想出一种成功区分右侧数据集中的蓝点和红点的几何构造（也许可以转化为神经网络）是相当简单的。&lt;/p&gt;&lt;p&gt;我们如何才能最好地描述两个数据集之间的差异？我们发现在本文中有用的一种方法是考虑每个数据集中输入的&lt;em&gt;微观特征&lt;/em&gt;和&lt;em&gt;宏观特征&lt;/em&gt;。我们将微观和宏观特征描述如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;em&gt;微观特征&lt;/em&gt;是一种以高度具体的术语描述输入的特征，因此对于概括来说并不是特别有用。&lt;/li&gt;&lt;li&gt;&lt;em&gt;宏特征&lt;/em&gt;是一种用一般术语描述输入的特征，并且对于泛化&lt;em&gt;很有&lt;/em&gt;用（如果它与手头的任务相关）。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-wMN58no3AypJnu5NN-8" id="fnref-wMN58no3AypJnu5NN-8"&gt;[8]&lt;/a&gt;&lt;/sup&gt;&lt;em&gt;两个&lt;/em&gt;数据集都具有微观特征：例如，如果我们（任意）为数据集中的每个点分配一个识别整数，我们可以为任何有限数据集定义&lt;code&gt;is_example_id_xxx&lt;/code&gt;形式的微观特征。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;但只有右侧数据集具有宏观特征：例如，我们可以用整数标记“棋盘”中的九个簇中的每一个，并定义&lt;code&gt;is_in_cluster_x&lt;/code&gt;形式的宏观特征。一种可能的查找算法是检测新示例与这些集群中的哪一个相关联，然后输出与同一集群中的大多数其他示例相同的颜色。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-wMN58no3AypJnu5NN-9" id="fnref-wMN58no3AypJnu5NN-9"&gt;[9]&lt;/a&gt;&lt;/sup&gt;另一方面，左侧数据集的唯一宏观特征是标签（“蓝色”或“红色”）本身，这正是查找算法需要预测的！ &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-wMN58no3AypJnu5NN-10" id="fnref-wMN58no3AypJnu5NN-10"&gt;[10]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;h2&gt;解读纯记忆算法&lt;/h2&gt;&lt;p&gt;我们可以从解决纯粹记忆任务的算法中获得哪些见解？&lt;/p&gt;&lt;h3&gt;事实查找的电路式解释的限制&lt;/h3&gt;&lt;p&gt;机械可解释性的&lt;a href="https://transformer-circuits.pub/2022/mech-interp-essay/index.html"&gt;规范目标&lt;/a&gt;是将算法分解为可理解的图（“电路”），其中每个节点都是一个“简单”操作（例如，对应于高级编程语言中的内置函数的操作）该操作的输入和输出可以用与问题领域相关的“特征”来解释。&lt;/p&gt;&lt;p&gt;根据上一节中对微观和宏观特征的讨论，很明显，纯粹的记忆任务对电路式分解提出了挑战。纯粹的记忆任务正是那些不具有与解决任务相关的宏观特征的任务。这意味着执行纯事实查找的算法中的任何中间状态必须表示：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;不相关的宏观特征，因此不能确定算法的输出；&lt;/li&gt;&lt;li&gt;单个微观特征的并集、联合、加权组合或其他任意函数，它们没有作为宏观特征的替代解释。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;就第一个要点而言，事实上，我们确实在查找体育事实的 Pythia 2.8B 的 MLP 子网络中&lt;em&gt;发现&lt;/em&gt;了不相关的宏特征：由于层之间存在残余流连接，像&lt;code&gt;first_name_is_george&lt;/code&gt;这样的宏特征一直保留到网络的输出。关键是这些宏观特征并没有告诉我们太多关于网络如何执行体育事实查找的信息。&lt;/p&gt;&lt;p&gt;转向第二个要点，我们注意到，对于任何有限数据集，我们实际上可以将神经网络简单地分解为涉及微观特征加权组合的计算图。这是因为网络中的每个神经元都可以&lt;em&gt;准确地&lt;/em&gt;解释为微观特征的加权组合，其中权重对应于与该微观特征对应的示例上的输出。例如，一个（假设的）神经元在 LeBron James 上输出 3，在 Aaron Judge 上输出 1 等等，可以被“解释”为代表复合特征：&lt;/p&gt;&lt;pre&gt; &lt;code&gt;3 * is_LeBron_James + 1 * is_Aaron_Judge + ...&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;每个 MLP 层的输出都是这些特征的总和，而这些特征又具有相同的线性形式——就像网络的输出一样。请注意，这相当于将每个单独的神经元（以及神经元的总和）解释为查找表。&lt;/p&gt;&lt;p&gt;实际上，这意味着我们始终可以访问神经网络如何执行事实查找的以下“解释”：网络中的每个神经元都是输入空间上的查找表，网络的输出是这些的总和查找表。通过训练网络，我们有效地解决了约束满足问题：求和的查找表应该对一个类具有高权重，而对另一类具有低权重。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-wMN58no3AypJnu5NN-11" id="fnref-wMN58no3AypJnu5NN-11"&gt;[11]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;请注意，只要我们将输入空间限制为有限集，神经网络的这种微观特征（或查找表）解释同样适用于解决泛化任务的模型（即在未见过的测试集上表现良好）。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-wMN58no3AypJnu5NN-12" id="fnref-wMN58no3AypJnu5NN-12"&gt;[12]&lt;/a&gt;&lt;/sup&gt;不同之处在于，对于泛化任务，我们可能期望其中一些“查找表”表示能够对模型用于泛化的宏观特征有更好的解释。&lt;/p&gt;&lt;p&gt;例如，图像分类模型中的特定神经元可能具有与检测图像左侧的垂直边缘相对应的权重，因此其查找表表示对于包含该边缘的示例显示高激活，对于不包含该边缘的示例显示低激活。 &amp;#39;t。关键是，虽然这个查找表表示是神经元输出的精确表示，但根据输入图像中边缘的存在，对此激活模式有一个更有用的（对人类）解释，这只是因为图像具有宏观特征（如边缘），可用于图像分类等泛化任务。&lt;/p&gt;&lt;p&gt;相比之下，我们认为对于纯粹的记忆任务，神经元（或神经元组）的这些“查找表”表示是唯一可用的解释。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-wMN58no3AypJnu5NN-13" id="fnref-wMN58no3AypJnu5NN-13"&gt;[13]&lt;/a&gt;&lt;/sup&gt;反过来，这似乎排除了由纯事实查找模型实现的算法的标准电路式分解，因为中间状态没有（宏观特征）解释。&lt;/p&gt;&lt;h3&gt;还有其他类型的解释吗？&lt;/h3&gt;&lt;p&gt;当然，我们并不声称解释模型如何执行任务的标准电路方法是唯一可能的解释方式。事实上，它甚至可能不是解释神经元如何执行事实查找的最佳方式。在本节中，我们将简要讨论几个可能值得进一步探索的替代方向。&lt;/p&gt;&lt;p&gt;第一个方向是放弃对代表有意义的宏观特征的中间状态的希望，但仍然在如何组织查找计算方面寻求有意义的结构。例如，我们可能会探索这样的假设：当训练执行纯粹的记忆时，经过训练的神经网络类似于通过&lt;a href="https://en.wikipedia.org/wiki/Bootstrap_aggregating"&gt;bagging&lt;/a&gt;学习的模型，其中每个单独的神经元都是要学习的事实的不相关的弱分类器，并且整个神经网络的输出是这些分类器的总和。另请参阅第 3 篇文章中调查的假设。&lt;/p&gt;&lt;p&gt;这种方法的问题在于我们不知道如何有效地搜索此类假设的宇宙。正如我们在第三篇文章中发现的那样，对于我们证伪的任何看似具体的假设（例如单步去代币化假设），我们可以转向许多邻近的假设，但这些假设尚未（尚未）被排除，而且这些假设本身通常更难伪造。因此，尚不清楚如何避免无休止的临时假设。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-wMN58no3AypJnu5NN-14" id="fnref-wMN58no3AypJnu5NN-14"&gt;[14]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;另一个方向是寻找算法的非机械解释，或者换句话说，从询问网络“如何”以某种方式表现，转向询问“为什么”它以某种方式表现。我们发现这方面有趣的一个领域是使用&lt;a href="https://arxiv.org/abs/2308.03296"&gt;影响函数&lt;/a&gt;根据训练数据来解释模型的行为。对于经过显式训练来记忆事实数据集的模型来说，这可能看起来无趣&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-wMN58no3AypJnu5NN-15" id="fnref-wMN58no3AypJnu5NN-15"&gt;[15]&lt;/a&gt;&lt;/sup&gt; ，但可能会为隐式记忆事实以满足更广泛的泛化目标的模型（如语言模型）带来重要的见解。&lt;/p&gt;&lt;h2&gt;凭经验法则记忆&lt;/h2&gt;&lt;p&gt;考虑记忆以下两个数据集的任务： &lt;/p&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JRcNNGJQ3xNfsxPj4/jbypr5bulwadawifklzw" /&gt;&lt;/p&gt;&lt;p&gt;这些是不符合我们上述“纯粹”记忆特征的记忆任务的例子：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;在左边的数据集中，完美的准确性需要记忆，但有一些有用的“经验法则”可以帮助你完成很多工作。此类任务的语言建模类似是预测英语中单数名词的复数版本：在大多数情况下，只需在名词单数版本的末尾添加“s”即可获得正确答案，但是除了一些例外（例如“孩子”），必须记住它们才能完美地完成任务。&lt;/li&gt;&lt;li&gt;在右侧数据集中，每个点都与两个“事实”相关联 - 由点的颜色（蓝色或红色）及其形状（十字形或圆形）表示。尽管没有系统的方法来单独查找颜色或形状，但请注意，这两个事实之间存在高度相关性：蓝点几乎总是圆形，而红点几乎总是十字。这表明，将形状和颜色事实一起记忆应该比简单地单独记忆每组事实更有效。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;一般来说，我们将此类任务描述为“根据经验法则进行记忆”。它们与纯粹的记忆任务不同，因为之前的例子&lt;em&gt;确实&lt;/em&gt;在一定程度上有助于推断新例子的正确输出，但完美的表现确实需要一定程度的记忆。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-wMN58no3AypJnu5NN-16" id="fnref-wMN58no3AypJnu5NN-16"&gt;[16]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;与纯粹的记忆不同，这些经验法则记忆任务确实具有概括性的元素，因此，存在能够实现这种概括性的宏观特征。因此，在能够执行这些任务的模型的中间表示中寻找这些宏观特征是有效的。另一方面，就模型确实需要记住异常的程度而言，我们并不期望能够完美地理解算法：至少算法的某些部分必须涉及“纯查找”，对此的限制这篇文章中讨论的可解释性将适用。&lt;/p&gt;&lt;p&gt;体育事实查找任务在多大程度上是纯粹的记忆，在多大程度上是根据经验法则进行记忆？正如我们在第一篇文章中讨论的那样，我们选择这个任务是因为它看起来接近于纯粹的记忆：对于许多名字来说，个人名字标记似乎不太可能对运动员所从事的运动有太多帮助。尽管如此，我们确实知道，对于某些名称，最后一个标记确实有助于确定运动（因为可以仅使用最后一个标记嵌入来探测运动，并且比不知情的分类器获得更好的准确性）。此外，可以想象，诸如名字的文化起源之类的潜在因素，会以模型所识别的方式与体育运动相关。 &lt;/p&gt;&lt;hr class="footnotes-sep" /&gt;&lt;section class="footnotes"&gt;&lt;ol class="footnotes-list"&gt;&lt;li class="footnote-item" id="fn-wMN58no3AypJnu5NN-1"&gt;&lt;p&gt;例如，特征&lt;code&gt;is_Michael_Jordan&lt;/code&gt; ，仅当输入为&lt;code&gt;&amp;quot;Michael Jordan&amp;quot;&lt;/code&gt;时才为真。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-wMN58no3AypJnu5NN-1"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-wMN58no3AypJnu5NN-2"&gt;&lt;p&gt;例如，许多运动员都共享的特征&lt;code&gt;first_name_is_George&lt;/code&gt; ，但对于预测运动员所从事的运动并不是特别有用。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-wMN58no3AypJnu5NN-2"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-wMN58no3AypJnu5NN-3"&gt;&lt;p&gt;我们注意到，事实回忆可能确实具有&lt;em&gt;一些&lt;/em&gt;相关的宏观特征，例如从标记中检测名称的种族，以及启发哪些种族可能从事不同的运动。但该模型的性能明显优于我们对这些启发法的预期，因此出于实际目的，我们在讨论事实回忆时忽略它们。玩具模型的优点之一是我们可以确保此类混杂因素不存在。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-wMN58no3AypJnu5NN-3"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-wMN58no3AypJnu5NN-4"&gt;&lt;p&gt;因为，如果它们存在，我们可以使用这些相关的宏观特征来帮助进行事实查找（做出不同程度的成功的有根据的猜测），这意味着该任务将不再需要纯粹的记忆。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-wMN58no3AypJnu5NN-4"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-wMN58no3AypJnu5NN-5"&gt;&lt;p&gt;更准确地说，是微观特征的加权和，例如&lt;code&gt;3 * is_Michael_Jordan + 0.5 * is_George_Brett&lt;/code&gt; 。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-wMN58no3AypJnu5NN-5"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-wMN58no3AypJnu5NN-6"&gt;&lt;p&gt;我们注意到，有_un_可用但有用的宏观特征——“打篮球”在某种微不足道的意义上是一个对于预测运动员是否打篮球有用的宏观特征，就像“打篮球并且身高超过 6&amp;#39;8”这样的下游特征一样。出于此分析的目的，我们重点关注模型在进行查找时&lt;em&gt;可用的&lt;/em&gt;特征，排除查找标签下游的潜在宏观特征。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-wMN58no3AypJnu5NN-6"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-wMN58no3AypJnu5NN-7"&gt;&lt;p&gt;当然，许多事实回忆任务都达不到这种理想的特征：在参加琐事测验时做出“有根据的猜测”通常是有回报的，即使你不确定答案。我们将&lt;a href="https://www.alignmentforum.org/posts/JRcNNGJQ3xNfsxPj4/fact-finding-how-to-think-about-interpreting-memorisation#Memorisation_with_rules_of_thumb"&gt;进一步&lt;/a&gt;讨论这种“根据经验法则进行记忆”的任务。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-wMN58no3AypJnu5NN-7"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-wMN58no3AypJnu5NN-8"&gt;&lt;p&gt;我们将这些概念与统计物理学中&lt;em&gt;微观状态&lt;/em&gt;和&lt;em&gt;宏观状态&lt;/em&gt;的概念进行类比：微观状态以高度精确的方式描述系统（例如，指定气体中每个分子的位置和速度），而宏观状态则以高度精确的方式描述系统。容易测量的属性（例如压力、体积、温度），忽略细节。任何“宏观”问题，都应该只从宏观变量的角度来解决；微观细节应该不重要。这类似于概括的想法：任何两个在“重要的方式”（其宏观特征）方面相似的示例都应该进行类似的分类，而忽略“无关紧要的方式”（其微观特征）上的任何差异。在这个类比下，记忆问题正是那些关于系统的问题，只能通过对其微观状态的精确了解来回答。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-wMN58no3AypJnu5NN-8"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-wMN58no3AypJnu5NN-9"&gt;&lt;p&gt;这些并不是可以解决这个特定泛化问题的唯一宏观特征。如果您训练玩具神经网络来执行此分类任务，您会发现（取决于神经元数量或随机种子等超参数）有多种方法来划分空间（以粗粒度、概括的方式）以成功对这些进行分类点。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-wMN58no3AypJnu5NN-9"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-wMN58no3AypJnu5NN-10"&gt;&lt;p&gt;我们通过这个数据集肯定知道这一点，因为我们自己生成了它，通过随机为点分配颜色（这些点本身是从二元高斯分布中随机采样的）。因此，该数据集中唯一相关的特征是示例 ID 本身和输出标签。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-wMN58no3AypJnu5NN-10"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-wMN58no3AypJnu5NN-11"&gt;&lt;p&gt;这是&lt;em&gt;二元&lt;/em&gt;事实查找任务情况下的约束满足问题，但将此解释推广到多类或连续值事实查找任务是微不足道的。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-wMN58no3AypJnu5NN-11"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-wMN58no3AypJnu5NN-12"&gt;&lt;p&gt;对于任何实际的机器学习任务都可以这样做。例如，我们可以将手写数字分类问题限制为对 MNIST 训练集和测试集联合中找到的 70,000 个示例进行精确分类。 （或者，如果我们关心数据增强，我们可以将任务扩展为对组合 MNIST 数据集的 280,000 种可能的角落作物中的任何一种进行分类。）我们可以安排潜在输入集达到我们希望的大小，但仍然有限。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-wMN58no3AypJnu5NN-12"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-wMN58no3AypJnu5NN-13"&gt;&lt;p&gt;因为（根据定义）在纯粹的记忆任务中没有相关的宏观特征（因为如果有的话，那么模型就能够概括）。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-wMN58no3AypJnu5NN-13"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-wMN58no3AypJnu5NN-14"&gt;&lt;p&gt;还存在这样的查找算法解释的有用性问题。即使我们已经发现了如何完成查找的一些简单的结构（例如，它类似于装袋），也不清楚，如果没有有意义的中间表示，这可以帮助我们在机械可解释性的下游用途方面发挥什么作用。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-wMN58no3AypJnu5NN-14"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-wMN58no3AypJnu5NN-15"&gt;&lt;p&gt;因为如果模型经过明确训练以重现记忆数据集，我们已经准确地知道训练数据和模型输出之间的对应关系。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-wMN58no3AypJnu5NN-15"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-wMN58no3AypJnu5NN-16"&gt;&lt;p&gt;使用经验法则的记忆不应与具有任意不确定性的泛化任务相混淆。例如，左侧数据集也可以用来表示随机数据生成过程，其中点不一定是蓝色或红色，而是伯努利分布 - 即可能是蓝色或红色，具有一定的（依赖于输入的）概率。在这种情况下，完美的泛化算法应该输出每个簇内恒定的校准概率。然而，这里我们的意思是数据集中的蓝点确实是蓝色，红点确实是红色——即使它们看起来不合适——而且完美的性能对应于再现这些特质，就像描述的“复数这个单数名词”任务一样在正文的正文中。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-wMN58no3AypJnu5NN-16"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/section&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/JRcNNGJQ3xNfsxPj4/fact-finding-how-to-think-about-interpreting-memorisation#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sat, 23 Dec 2023 02:46:16 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/JRcNNGJQ3xNfsxPj4/fact-finding-how-to-think-about-interpreting-memorisation</guid></item><item><title>事实调查：尝试机械地理解早期 MLP（第 3 篇）</title><link>https://www.lesswrong.com/posts/CW5onXm6uZxpbpsRk/fact-finding-trying-to-mechanistically-understanding-early</link><description>发布于 2023 年 12 月 23 日凌晨 2:46（格林尼治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;&lt;em&gt;这是 Google DeepMind 机械可解释性团队对&lt;a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX"&gt;语言模型如何回忆事实的&lt;/a&gt;调查的第三篇文章。这篇文章的重点是从机制上理解早期 MLP 如何查找运动员姓名的标记并将其映射到他们的运动。这篇文章很杂乱，&lt;strong&gt;我们建议从&lt;a href="https://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall"&gt;第一篇文章&lt;/a&gt;开始&lt;/strong&gt;，然后根据与您最相关的内容略读并跳过其余的序列。阅读帖子 2 有帮助，但不是必需的。我们假设这篇文章的读者熟悉&lt;a href="https://www.neelnanda.io/mechanistic-interpretability/glossary#mechanistic-interpretability-techniques"&gt;本术语表中&lt;/a&gt;列出的机械解释技术。&lt;/em&gt;&lt;/p&gt;&lt;h2&gt;介绍&lt;/h2&gt;&lt;p&gt;正如上一篇文章中所讨论的，我们将两个令牌运动员姓名的事实回忆提炼成一个由 5 个 MLP 层（MLP 2 至 6）组成的&lt;strong&gt;有效模型&lt;/strong&gt;。这个有效模型的输入是与姓氏相对应的嵌入（通过嵌入和 MLP0）和与名字相对应的嵌入（通过第 0 层和第 1 层中关注前一个标记的注意力头）的总和。有效模型的输出是运动员所从事的运动（（美式）橄榄球、棒球或篮球）的 3 维线性表示。我们强调，这个 5 层 MLP 模型不仅能够高精度地回忆事实（在过滤数据集上为 86%），而且它是从预训练的语言模型中提取的，而不是从头开始训练的。&lt;/p&gt;&lt;p&gt;我们在这篇文章中的目标是对这个有效模型的工作原理进行逆向工程。我认为我们在这个目标的雄心勃勃的版本上大多失败了，尽管我相信我们已经在为什么这很难的问题上取得了一些概念上的进展，证伪了一些简单的天真的假设，并且对正在发生的事情不再那么困惑。我们在&lt;a href="https://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall#Is_it_surprising_that_we_didn_t_get_much_traction_"&gt;第 1 篇文章&lt;/a&gt;和&lt;a href="https://www.alignmentforum.org/posts/JRcNNGJQ3xNfsxPj4/fact-finding-how-to-think-about-interpreting-memorisation"&gt;第 4 篇&lt;/a&gt;文章中讨论了我们对为什么这很难的理解，在这篇文章中，我们重点关注我们对可能发生的情况的假设，以及我们收集的支持和反对的证据。&lt;/p&gt;&lt;h2&gt;假设&lt;/h2&gt;&lt;p&gt;回想一下，我们的 MLP 模型中 5 个 MLP 层的作用是将求和的原始标记映射到所进行运动的线性表示。从数学上讲，这是一个查找表，其中每个条目都是生成属性的原始标记上的布尔 AND。我们期望它&lt;em&gt;以某种方式&lt;/em&gt;涉及非线性来实现 AND，因为这种查找是非线性的，例如模型想要知道“Michael Jordan”和“Tim Duncan”打篮球，但不一定认为“Michael Duncan”打篮球。&lt;/p&gt;&lt;p&gt;我们探索了两个假设，&lt;strong&gt;单步去标记化&lt;/strong&gt;以及&lt;strong&gt;哈希和查找&lt;/strong&gt;。&lt;/p&gt;&lt;h3&gt;单步去代币化&lt;/h3&gt;&lt;p&gt;直观上，执行 AND 的最简单方法是使用单个神经元，例如 ReLU(is_michael + is_jordan - 1) 实际上是一个 AND 门。每个运动员的单个神经元不会产生任何叠加，因此我们采用稍微复杂一点的版本：假设有一堆单独的神经元，每个神经元都独立地使用其 GELU 激活实现 AND &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-KGJJcrC8izPbNFCPL-1" id="fnref-KGJJcrC8izPbNFCPL-1"&gt;[1]&lt;/a&gt;&lt;/sup&gt; ，映射运动员名字的原始标记到有关该运动员的每个已知事实的线性表示。细微差别：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;这使用了叠加，让每个神经元为许多运动员激发，并且每个运动员都有许多查找神经元。神经元输出建设性地干扰正确的事实，但不会进行超出此范围的交互。&lt;ul&gt;&lt;li&gt;这是一个具体的、机械的故事。每个神经元都有一组为其激发的运动员，并且对该组进行 AND 的并集 - 例如，如果一个神经元为迈克尔乔丹和蒂姆邓肯激发，它会实现（迈克尔或蒂姆）AND（邓肯或乔丹）。这引入了噪声，例如它也会为蒂姆·乔丹（Tim Jordan）开火（它&lt;em&gt;想做&lt;/em&gt;（迈克尔和乔丹）或（蒂姆和邓肯），但这很难用单个神经元实现）。它也很吵闹，因为它必须同时宣传迈克尔·乔丹的事实和蒂姆·邓肯的事实。但由于每个神经元都会针对不同的子集进行激发，因此对正确答案会产生建设性干扰，并且噪音会被消除。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;这预示着相同的神经元对于运动员的每一个已知事实都同样重要&lt;/li&gt;&lt;li&gt;该假设的一个重要部分是每个神经元直接从输入标记中读取并直接贡献于输出事实。理论上，这可以通过单个 MLP 层而不是 5 个层来实现。它预测神经元直接与输入标记组合，计算中没有中间项，并且 MLP 层之间没有有意义的组合。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;哈希和查找&lt;/h3&gt;&lt;p&gt;我们模型的输入具有相当不理想的格式 - 它是每个组成标记的线性和，但这在进行事实查找时可能会产生很大的误导！迈克尔·乔丹和迈克尔·史密斯同名这一事实并不表明他们从事同一运动的可能性更大。哈希和查找假设是，模型首先生成一个打破输入线性结构的中间表示，一个与其他所有哈希表示接近正交的&lt;strong&gt;哈希表示&lt;/strong&gt;&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-KGJJcrC8izPbNFCPL-2" id="fnref-KGJJcrC8izPbNFCPL-2"&gt;[2]&lt;/a&gt;&lt;/sup&gt; （即使它们共享一些但不是全部标记），然后后面的层&lt;strong&gt;查找&lt;/strong&gt;这个散列表示并将其映射到正确的属性。细微差别：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;从某种意义上说，“困难”的部分是查找。查找是存储实际事实知识的地方，而随机初始化的 MLP 应该适合散列，因为目标只是淹没现有结构。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;为什么散列是必要的？ MLP 是非线性的，因此也许它们可以忽略线性结构，而不需要明确地破坏它。这里的一个直觉来自最简单的查找：有一个“棒球神经元”，其输出增强棒球方向，其输入权重是每个棒球运动员的串联令牌表示的总和&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-KGJJcrC8izPbNFCPL-3" id="fnref-KGJJcrC8izPbNFCPL-3"&gt;[3]&lt;/a&gt;&lt;/sup&gt; - 如果运动员表示是（大约）正交，然后给定一个运动员，这只对棒球运动员起作用。但如果它同时对迈克尔·乔丹和蒂姆·邓肯开火，那么它必须至少对蒂姆·乔丹或迈克尔·邓肯之一开火——这是不可取的！然而，如果它的输入权重是&lt;em&gt;散列&lt;/em&gt;运动员表示的总和，则这成为可能！&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;散列对于已知的标记字符串（例如名人姓名）和未知的字符串（例如未知的姓名）应该同样有效。查找是实际知识融入的地方&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;关于迈克尔·乔丹的不同已知事实的查找电路没有理由应该对应于相同的神经元。从概念上讲，可能有一个“打篮球”神经元对任何散列篮球运动员激发，以及一个单独的“为芝加哥球队效力”神经元对芝加哥球员的散列激发。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;这微弱地预测了哈希层和查找层之间的完全分离&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这两个假设都是故意以一种强有力的形式提出的，可以做出真实的预测——语言模型是混乱的和被诅咒的，我们实际上并没有期望这是完全正确的。但我们认为这些说法似乎有一定道理。在实践中，我们发现单步去标记化似乎显然是错误的，而哈希和查找在强形式下似乎是错误的，但可能有一些道理。我们发现考虑哈希和查找对于了解正在发生的事情非常有效。&lt;/p&gt;&lt;h2&gt;证伪单步去代币化假说&lt;/h2&gt;&lt;p&gt;单步去标记化是我们能想到的最简单的假设，它仍然涉及显着的叠加，因此可以做出一些相当有力的预测。我们针对这些设计了一系列实验，并广泛发现我们伪造了它做出的多个强有力的预测。&lt;/p&gt;&lt;h3&gt; MLP 之间存在显着的组成&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;预测&lt;/strong&gt;：MLP 2 到 6 之间没有中间组合，它们都是并行作用的。因为每个重要的神经元都被预测为直接将原始标记映射到输出。正如后面所讨论的，缺乏组合是该假设的有力证据，组合的存在是反对该假设的弱证据。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;实验&lt;/strong&gt;：我们的意思是消除&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-KGJJcrC8izPbNFCPL-4" id="fnref-KGJJcrC8izPbNFCPL-4"&gt;[4]&lt;/a&gt;&lt;/sup&gt;每对 MLP 层之间的路径，并查看对几个指标的影响：头部探测精度（在第 6 层残差上）、完整模型精度和损失（在完整词汇上）、仅限于运动的完整模型 Logits 精度以及完整模型与原始 Logits 的 KL 散度。通过平均消融路径，我们仅破坏 MLP 间的组合，而不破坏与下游属性提取头的组合。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;结果&lt;/strong&gt;：我们发现性能显着下降，尤其是从 MLP2 开始的路径，表明存在一些中间产品。请注意，损失和 KL 散度如果低（绿色和紫色）​​则良好，如果高（蓝色、红色和橙色）则准确度良好。进一步注意，与仅在超过阈值时改变的“硬”指标（如准确率）相比，“软”指标（如损失和 KL 散度）显示出更强的变化。正如&lt;a href="https://arxiv.org/abs/2309.16042"&gt;Zhang等人&lt;/a&gt;所指出的，这是预料之中的，当电路由多个元件组成时，所有元件都贡献于共享输出，烧蚀单个元件很少足以跨越阈值，但足以破坏较软的指标，从而造成损失和损失。 KL 散度是衡量重要性的更可靠的方法。 &lt;/p&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/kerrtb9jty2imurotmel" /&gt;&lt;/p&gt;&lt;p&gt; **细微差别：**请注意，这仅伪造了单步去标记化的最简单形式。与这些结果一致的单步去标记化假设&lt;em&gt;的&lt;/em&gt;一个扩展是，它不是 MLP 2 不做与 MLP 3 到 6 相关的任何事情，而是充当标记嵌入的&lt;em&gt;固定&lt;/em&gt;变换（例如，它总是将 MLP 2 的嵌入加倍）。姓）。如果 MLP 3 想要访问原始令牌，它期望 MLP 2 的固定效果，因此会考虑原始令牌嵌入加上 MLP 2 的固定转换。这会因平均消融而受损，但不涉及有意义的合成。&lt;/p&gt;&lt;h3&gt;多个事实之间不共享神经元&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;预测&lt;/strong&gt;：当模型知道有关某个实体的多个事实时，相同的神经元对于预测每个事实非常重要，而不是每个事实的不同神经元。这是因为查找信息的机制是通过对名称的标记执行布尔 AND 操作，该名称对于每个已知事实都是相同的，因此没有理由将它们分开。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;实验&lt;/strong&gt;：收集模型了解的有关运动员的替代事实的大量数据很困难，因此我们放大了某个特定运动员（迈克尔·乔丹）并发现了模型了解的有关他的 9 个事实&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-KGJJcrC8izPbNFCPL-5" id="fnref-KGJJcrC8izPbNFCPL-5"&gt;[5]&lt;/a&gt;&lt;/sup&gt; 。然后，我们一次对 Jordan 令牌上的 MLP 2-6 中的每个神经元进行消融&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-KGJJcrC8izPbNFCPL-6" id="fnref-KGJJcrC8izPbNFCPL-6"&gt;[6]&lt;/a&gt;&lt;/sup&gt; ，并查看每项运动的正确对数概率的变化。对于每对事实 A 和 B，我们然后查看每个给定神经元对 A 的正确对数概率和 B 的正确对数概率的影响之间的&lt;strong&gt;相关性&lt;/strong&gt;。如果每个神经元对于同一运动员的每个已知事实同样重要，那么相关性应该很高。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;结果&lt;/strong&gt;：非常低。唯一具有中等相关性的一对事实是 NBA 选秀年（1984 年）和美国奥运会年（1992 年），我怀疑这是因为它们都是年份，尽管我不会提前预测到这一点，也没有很棒的故事，说明了原因。 &lt;/p&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/zsa7a3aqscgpfiol8ogh" /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;细微差别&lt;/strong&gt;：这似乎证伪了单步去标记化假设的强形式 - 至少，即使存在去标记化神经元，它们也会输出迈克尔·乔丹事实的子集，而不是一次性全部输出。&lt;/p&gt;&lt;p&gt;一个争论是，消融单个神经元有点难以推理，而且似乎有一些紧密耦合的处理（如微妙的自我修复）使得解释这些结果变得更加困难。但在简单的单步去标记化假设下，我们&lt;em&gt;应该&lt;/em&gt;能够独立地消融和推理神经元。另一个问题是相关系数是汇总统计数据，可能隐藏了一些结构，但检查散点图同样显示出似乎没有关系。&lt;/p&gt;&lt;h3&gt;对属性有直接影响的神经元不执行“与”运算&lt;/h3&gt;&lt;p&gt;&lt;em&gt;注意：这个实验相当复杂（尽管我们认为概念上很优雅且有趣），请随意跳过&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;预测&lt;/strong&gt;：直接与属性提取头组成的神经元通过其 GELU 激活对原始标记（在运动员的某些子集上）执行 AND 运算。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;实验&lt;/strong&gt;：我们使用称为非线性过剩&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-KGJJcrC8izPbNFCPL-7" id="fnref-KGJJcrC8izPbNFCPL-7"&gt;[7]&lt;/a&gt;&lt;/sup&gt;的度量来测量模型中标量实现 AND 的程度。具体来说，如果一个神经元在 prev=Michael 和 curr=Jordan 上实现 AND，那么它应该比 Michael Smith 或 Keith Jordan 激活更多的 Michael Jordan。形式上，给定两个二元变量 A (Prev=Michael) 和 B(Curr=Jordan)，我们将非线性超额定义为 E(A &amp;amp; B) - E(~A &amp;amp; B) - E(A &amp;amp; ~B) + E(~A &amp;amp; ~B) &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-KGJJcrC8izPbNFCPL-8" id="fnref-KGJJcrC8izPbNFCPL-8"&gt;[8]&lt;/a&gt;&lt;/sup&gt; 。重要的是，如果神经元在两个标记中是线性的，则该度量为零，如果是 AND，则该度量为正 (1 - 0 - 0 + 0 = 1)，如果是 OR，则该度量为负 (1 - 1 - 1 + 0 = -1)。&lt;/p&gt;&lt;p&gt;对于我们的具体实验：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;我们对每个神经元计算 GELU 前后非线性过剩的&lt;em&gt;变化&lt;/em&gt;&lt;ul&gt;&lt;li&gt;在 GELU 上进行改变的要点是，这区分了信号增强预先计算的 AND 的神经元和计算 AND 本身的神经元。&lt;/li&gt;&lt;li&gt;为了计算非线性超额，我们通过汇集 2 个代币运动员（每个大约 100 个）中的所有名字和姓氏来计算平均值，并查看每个名称组合。 （这大约有 10,000 个 ~A 和 ~B 的名字，大约 100 个 ~A &amp;amp; B 或 A &amp;amp; ~B 的名字，只有一个 A &amp;amp; B 的名字——原始运动员的名字！）&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;过滤出这种变化为正的神经元（并将 GELU 前的多余部分限制为最小零）&lt;ul&gt;&lt;li&gt;我们发现了一堆神经元，其中前 GELU 具有负非线性过剩，而 GELU 将所有内容设置为接近零。我们倾向于不计算这些。&lt;/li&gt;&lt;li&gt;我们为每个运动员执行单独的过滤步骤，因为每个运动员都有不同的非线性超额&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;乘以神经元对属性提取头 L16H20 的基于权重的直接影响，并将其相加。&lt;ul&gt;&lt;li&gt;如果您只允许每个 GELU 的 AND 直接影响头 L16H20，而不是也允许中间组合，这就是 MLP 2 到 6 的效果&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;我们将其与探针上的总非线性过量效应（即通过头 L16H20 的直接效应）进行比较，以查看来自 AND 通过 GELU&lt;em&gt;并&lt;/em&gt;直接传达到基于头的探针的分数&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/hdxmyxamxcomqq0fqaxk" /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;结果&lt;/strong&gt;：当观察上面的散点图时，很明显它远离 x=y 线，即 GELU 的非线性超额通常显着小于总非线性超额，尽管它们是相关的。中位比例约为23% &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-KGJJcrC8izPbNFCPL-9" id="fnref-KGJJcrC8izPbNFCPL-9"&gt;[9]&lt;/a&gt;&lt;/sup&gt; 。我们认为这是反对单步去标记化假设的有力证据，因为它表明许多对 L16H20 有显着直接影响的神经元正在与已经计算出 AND 的早期 MLP 组合，即计算中有一个有意义的中间步骤。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;细微差别&lt;/strong&gt;：这个实验涉及到差异的差异。我认为它在概念上是合理的并且相当优雅，但我对过于复杂的实验普遍持怀疑态度，并且不想过于依赖他们的结果。我们在如何设置这些实验、如何聚合和分析它们、如何过滤掉神经元等方面反复讨论，并且有很多主观选择，尽管有趣的是结果对这些是稳健的。&lt;/p&gt;&lt;p&gt;将 GELU 之前的多余部分限制为零似乎是不合理的，例如，因为模型可能使用 GELU 的负数部分来实现 AND（Michael Smith 和 Keith Jordan 在 GELU 后&amp;lt;0，Michael Jordan 在 GELU 后为零），尽管尝试解释这一点并没有让我们接近 1。&lt;/p&gt;&lt;p&gt; MLP 2 到 6 中的一些神经元对现有的线性表示的事实信息进行信号增强（例如下一节中讨论的棒球神经元），这些神经元应该无法满足此度量标准（它们是计算 AND 的早期神经元的信号增强！ ）。&lt;/p&gt;&lt;h2&gt;棒球神经元 (L5N6045)&lt;/h2&gt;&lt;h3&gt;有一个棒球神经元！&lt;/h3&gt;&lt;p&gt;一个有趣的发现是，尽管整体计算相当分散和叠加，但仍然存在一些有意义的单个神经元！最值得注意的是棒球神经元 L5N6045，它对棒球运动员的系统性激活比对非棒球运动员的激活更多。作为棒球与非棒球运动员的二元探针，它的 ROC AUC 为 89.3%。 &lt;/p&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/z4lgnpzcuyzlngtepv16" /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;因果效应&lt;/strong&gt;：此外，它与属性提取头组成，具有显着的因果效应。它通过 L16H20 直接与 logits 组合以增强棒球（并抑制足球），如果我们的意思是消融它，那么棒球运动员的完整模型损失从 0.167 增加到 0.284（零消融时为 0.559）&lt;/p&gt;&lt;h3&gt;不仅仅是信号增强&lt;/h3&gt;&lt;p&gt;我们发现神经元的输入权重具有非平凡的余弦模拟，其输出权重为 (0.456)，通过头 L16H20 (0.22) 提升棒球 logit 的方向，以及通过头 L16H20 (0.184) 相对于其他运动提升棒球的方向这表明棒球神经元的部分功能是增强运动员打棒球的现有知识。&lt;/p&gt;&lt;p&gt;但这不是唯一的角色！如果我们采用与这 3 个方向跨越的子空间正交的输入权重分量，并将残差流投影到该方向上，则在预测运动员是否打棒球时，所得部分消融神经元的 ROC AUC (83%) （较之前的 88.7% 略有下降）。&lt;/p&gt;&lt;h3&gt;这不是单义的&lt;/h3&gt;&lt;p&gt;一个好奇心是它是否是单一语义的并且在完整的数据分布上代表棒球。尽管我们没有进行详细调查，但这似乎很可能是错误的。在谷歌新闻数据集上，它在类似棒球的环境中系统地激活（也有些特定的其他运动，如板球），但在维基百科上，它在一些看似不相关的事物上激活，例如“外部链接” &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-KGJJcrC8izPbNFCPL-10" id="fnref-KGJJcrC8izPbNFCPL-10"&gt;[10]&lt;/a&gt;&lt;/sup&gt;中的外部和“目标” “足球|进球|守门”&lt;/p&gt;&lt;h2&gt;哈希和查找证据&lt;/h2&gt;&lt;h3&gt;动机&lt;/h3&gt;&lt;p&gt;&lt;a href="https://www.alignmentforum.org/posts/CW5onXm6uZxpbpsRk/fact-finding-trying-to-mechanistically-understanding-early#Hash_and_Lookup"&gt;如上所述&lt;/a&gt;，散列和查找假设是 MLP 2 到 6 分为两个不同的阶段：第一个&lt;strong&gt;散列&lt;/strong&gt;，旨在通过形成非线性表示来打破名称的串联（求和）标记的线性结构尝试与每个其他子字符串正交，然后&lt;strong&gt;查找&lt;/strong&gt;将棒球运动员的哈希表示映射到棒球，将足球映射到橄榄球等。&lt;/p&gt;&lt;p&gt;从概念上来说，我们实际上并没有期望这种强形式是正确的：它意味着散列层实际上独立于数据分布，这将是令人惊讶的 - 如果我们采用散列和查找的实现并应用通过梯度下降的几个步骤，它可能希望使已知实体的哈希值更加突出并且与其他所有内容更加正交。但我们希望测试这个假设能够教会我们有关该模型的有用信息，并认为它可能部分正确。我们将&lt;strong&gt;部分散列和查找假设宽松&lt;/strong&gt;地称为该机制主要是散列和查找的假设，但早期的散列层包含一些有关运动的（线性可恢复的）信息，这些信息通过后来的查找层得到显着加强。我们的证据广泛支持这一假设，但不幸的是，它很难被证伪。&lt;/p&gt;&lt;p&gt;这是由于看到单步去标记化假设的失败：看起来相当清楚，MLP 间的组合正在进行，有中间项，并且有一些显式查找（棒球神经元）。这似乎是最简单的假设，它解释了为什么模型需要中间项并涉及实际有目的的组合 - 令牌的线性结构是不可取的！&lt;/p&gt;&lt;h3&gt;中间层具有线性可恢复的运动信息（负）&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;预测&lt;/strong&gt;：经过训练以在哈希层期间的残余流上检测运动员运动的线性探针不会比随机探针更好。它只会在查找层期间变得良好。我们不知道哪些层是哈希层还是查找层，但这预示着一个急剧的转变。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;实验&lt;/strong&gt;：在我们的有效模型中采用两个令牌运动员姓名，在每层之后获取残差流，在包含 80% 姓名的训练集上训练逻辑回归探针，并对另外 20% 的姓名进行评估。该假设预测验证准确性将会发生急剧变化。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;结果&lt;/strong&gt;：这是一个比较平稳的变化。为了鲁棒性，我们还检查有效模型预测下一项运动时的损失指标。当在完整模型中的最终名称标记上对残差流训练逻辑回归探针时，我们得到了类似的结果。这相当直接地反驳了早期层正在执行纯粹的、与数据无关的哈希的假设。然而，第 4 层和第 5 层之间存在显着增加，这表明查找存在一些专门化（这部分但不完全由第 5 层中的棒球神经元驱动）。对于每一层，我们报告 10 个随机种子的测试准确性（每次采用不同的 80/20 训练/测试分割并训练新的探针），因为数据集足够小，使其相当嘈杂。 &lt;/p&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/b5xo66fwzwxmue0b1ytb" /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;细微差别&lt;/strong&gt;：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;一些运动员的名字中可能有独特的标记，以便在嵌入中表示运动信息。我们可以看到，求和令牌的验证准确性优于随机令牌（50% 而不是 33%）。这并不奇怪，我们预计哈希和查找对其他运动员来说更重要。&lt;/li&gt;&lt;li&gt;这与部分哈希查找假设是一致的，尤其是在第 5 层中准确率显着提高。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;已知名称比未知名称具有更高的 MLP 输出范数（负）&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;预测&lt;/strong&gt;：散列预测早期层不会吸收数据分布的知识，因此应该对已知名称和未知名称进行区分。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;实验&lt;/strong&gt;：我们测量已知名称和未知名称的 MLP 输出范数。为了获取姓名，我们对运动员数据集中所有单个标记的名字和姓氏进行笛卡尔积，并分离已知和未知的名字&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-KGJJcrC8izPbNFCPL-11" id="fnref-KGJJcrC8izPbNFCPL-11"&gt;[11]&lt;/a&gt;&lt;/sup&gt; 。此分析是在完整模型上执行的（但在有效模型上类似）&lt;/p&gt;&lt;p&gt;&lt;strong&gt;结果&lt;/strong&gt;：存在明显差异，已知名称具有更高的范数。这伪造了纯哈希，但不是部分哈希。即使在 MLP1 中也会发生这种情况，尽管 MLP1 不是我们有效模型的一部分&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-KGJJcrC8izPbNFCPL-12" id="fnref-KGJJcrC8izPbNFCPL-12"&gt;[12]&lt;/a&gt;&lt;/sup&gt; ，这令人惊讶。 &lt;/p&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/mumetzm0fidzfvdcpv6p" /&gt;&lt;/p&gt;&lt;h3&gt;早期层确实破坏了线性结构（正面）&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;预测&lt;/strong&gt;：早期的层破坏了线性结构。具体来说，即使残差流输入中存在线性结构，即它是来自不同特征（当前和先前标记）的项之和，MLP输出也不会具有这种线性结构。更弱的是，它预测一旦重新添加 MLP 输出，残差流将失去这种线性结构。&lt;/p&gt;&lt;p&gt;线性函数 f 的一个具体属性是 f(Michael Jordan) + f(Tim Duncan) = f(Michael Duncan) + f(Tim Jordan)，所以让我们尝试证伪这个！&lt;/p&gt;&lt;p&gt;&lt;strong&gt;实验&lt;/strong&gt;：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;我们选择一对已知的名字 A 和 B（例如 Michael Jordan 和 Tim Duncan）以及有效模型中的 MLP 层（例如 MLP 2）。&lt;ul&gt;&lt;li&gt;我们取这些名称的 MLP 输出的中点 (MLP(A) + MLP(B)) /2。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;我们交换姓氏以获得名字 C 和 D（未知名字，例如 Michael Duncan 和 Tim Jordan），并取 C 和 D 上 MLP 输出的中点 (MLP(C) + MLP(D)) /2。&lt;/li&gt;&lt;li&gt;我们测量两个中点之间的距离。&lt;/li&gt;&lt;li&gt;为了将大数与小数联系起来，我们除以基线距离，该距离是通过用任意未知名称替换 C 和 D 并测量中点之间的距离 |((MLP(A) + MLP(B) - MLP(C) &amp;#39;) - MLP(D&amp;#39;))/2|&lt;ul&gt;&lt;li&gt;这意味着，如果 MLP 完全打破线性结构，它将接近 1（即 Michael Duncan 和 Tim Jordan 与随机未知名字无法区分），而如果它保留线性结构，它将接近 0（因为这些将是平行四边形的四个顶点）&lt;ul&gt;&lt;li&gt;具体来说，如果 MLP 是线性的，则 MLP(Michael Jordan) = MLP(Michael X) + MLP(Y Jordan)，因此 A&amp;amp;B 和 C&amp;amp;D 的中点应该相同&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/efy2z0gox96g5zqsbtvb" /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;结果&lt;/strong&gt;：大多数 MLP 层显示线性结构被显着（但未完全）破坏，往往是完全破坏线性结构的 60%-70%。 MLP2 的情况比 MLP3 到 6 的情况稍微不那么明显。&lt;/p&gt;&lt;p&gt;我们对不同层之后的残差流（而不是 MLP 输出）重复上述实验，绘制在同一张图（红色框）中，发现残差在各层之间的线性度降低，从第 2 层之后的约 30% 开始第 6 层后变为 50%（这是该层&lt;em&gt;末尾&lt;/em&gt;的残差）。请注意，这些残差取自有效模型，该模型从第 2 层开始，而不是第 0 层。进一步注意，在有效模型中，MLP2 的输入是名称的总和标记，根据定义，它是线性的。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;细微差别&lt;/strong&gt;：&lt;/p&gt;&lt;ul&gt;&lt;li&gt; MLP 输出的结果并不令人惊讶——MLP 的全部意义就是成为一个非线性函数，所以它当然打破了线性结构！&lt;ul&gt;&lt;li&gt;我们应该期望这个结果对于随机初始化的 MLP 来说是正确的&lt;/li&gt;&lt;li&gt;然而，事实证明，随机初始化的 MLP 对线性结构的破坏要少得多 (20-40%)！我们做了一个后续实验，随机调整 MLP 权重和偏差并重新运行模型。作为另一个基线，我们重新进行了交换未知姓名的名字/姓氏的实验，没有看到明显的变化。这表明模型有意使用 MLP 层来打破线性结构。 &lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/eetxlacpkfnhlmkbxmmt" /&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;它打破残差流中的线性结构的结果不那么微不足道，但仍然不足为奇 - new_residual 是 old_residual（线性）+ mlp_out（非线性），因此 new_residual 的线性程度直观上取决于相对大小。&lt;/li&gt;&lt;li&gt;总的来说，这是散列（在打破线性结构的意义上）发生的证据，但不是散列就是它们所做的&lt;em&gt;全部的&lt;/em&gt;证据，因此它不是完整散列和查找假设的有力证据（其中散列是早期 MLP 在电路中发挥的唯一作用）&lt;/li&gt;&lt;li&gt;尽管在概念上并不令人惊讶，但我们认为“早期 MLP&lt;em&gt;通常&lt;/em&gt;会打破线性结构”是了解模型的一个有价值的事实，因为它表明线性表示的特征之间的干扰会随着深度的增加而累积。&lt;ul&gt;&lt;li&gt;例如，Bricken 等人观察到许多稀疏自动编码器特征，例如“数学文本中的标记‘the’”。如果“is a math text”和“is the token &amp;#39;the&amp;#39;”都是线性表示的特征，那么 MLP 层表示交集也就不足为奇了，即使没有对该特征进行实际计算。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;棒球神经元作用于运动员残差的正交余数（不明确）&lt;/h3&gt;&lt;p&gt;&lt;em&gt;元：本节记录了我们中的一个人最初感到兴奋的一个实验，但后来意识到可能是虚幻的，我们在这里描述它是为了教学目的&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;预测&lt;/strong&gt;：如果发生查找，这表明每个运动员的表示都有&lt;em&gt;特殊&lt;/em&gt;信息 - 迈克尔·乔丹残差中有一些“是迈克尔·乔丹”信息，这对于最终生成“打篮球”的模型很重要，无法从其他篮球中恢复玩家。请注意，这显然是在对原始标记求和时发生的，但稍后可能不会发生。我们关注&lt;a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX/p/CW5onXm6uZxpbpsRk#The_Baseball_Neuron__L5N6045_"&gt;第 5 层的棒球神经元，&lt;/a&gt;它似乎是查找电路的一部分，因为它具有显着的效果并直接增强棒球属性。&lt;/p&gt;&lt;p&gt;对比假设是，早期层（例如 2-4）会产生棒球比赛中“打棒球”的某种表示（可能与最终表示不同），而棒球神经元只是发出信号来增强这种表示。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;实验&lt;/strong&gt;：为了测试这一点，我们采用了每个运动员的残差，并采用与所有其他运动员残差所跨越的子空间正交的分量（请注意，有 2560 个残差维度和大约 1500 个其他运动员，因此这删除了 ​​60% 的维度）。然后，我们将棒球神经元应用于该残差正交分量，并查看神经元输出的 ROC AUC，以预测运动员是否打棒球的二元变量&lt;/p&gt;&lt;p&gt;&lt;strong&gt;结果&lt;/strong&gt;：ROC 约为 60%，明显高于机会 (50%) - 明显比没有正交投影时差，但仍然有一些信号&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Nuance&lt;/strong&gt; ：这被证明是虚幻的，因为“与所有其他运动员正交的项目”并不一定会删除与其他运动员共享的&lt;em&gt;所有&lt;/em&gt;信息。玩具示例：假设每个棒球运动员残差都是“是棒球”方向加上显着的高斯噪声。如果我们从该分布中获取由 1500 个样本组成的子空间，由于每个样本都有噪声，因此该子空间中可能无法完全捕获“是棒球”方向，因此投影不会将其擦除。这意味着，虽然我&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-KGJJcrC8izPbNFCPL-13" id="fnref-KGJJcrC8izPbNFCPL-13"&gt;[13]&lt;/a&gt;&lt;/sup&gt;发现这个实验的结果令人惊讶，但它并没有很好地区分这两个假设——部分散列和查找确实很难被证伪！ &lt;/p&gt;&lt;hr class="footnotes-sep" /&gt;&lt;section class="footnotes"&gt;&lt;ol class="footnotes-list"&gt;&lt;li class="footnote-item" id="fn-KGJJcrC8izPbNFCPL-1"&gt;&lt;p&gt; GELU 与 ReLU 不同，但我们认为可以有效地将其视为“软 ReLU”，并且与 ReLU 相当接近，因此也可以相当好地实现 AND 门&lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-KGJJcrC8izPbNFCPL-1"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-KGJJcrC8izPbNFCPL-2"&gt;&lt;p&gt;注意，这是哈希函数语句中的哈希，而不是哈希表。哈希函数接受任意输入并尝试产生与随机输出没有区别的输出。哈希表将哈希函数应用于输入，&lt;em&gt;然后&lt;/em&gt;有目的地将其映射到某些存储的数据，这更类似于完整哈希和查找。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-KGJJcrC8izPbNFCPL-2"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-KGJJcrC8izPbNFCPL-3"&gt;&lt;p&gt;请注意，可能存在更复杂且基础对齐程度较低的查找形式，这些形式可能不太容易受到干扰，事实上，我们发现有迹象表明这个故事是混乱和复杂的&lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-KGJJcrC8izPbNFCPL-3"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-KGJJcrC8izPbNFCPL-4"&gt;&lt;p&gt;另一位运动员的重新采样消融得到了类似的结果&lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-KGJJcrC8izPbNFCPL-4"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-KGJJcrC8izPbNFCPL-5"&gt;&lt;p&gt;我们测量每个答案的第一个标记的对数概率，对于多标记答案，延续位于括号中，并且没有明确测试（一旦有了第一个标记，就可以很容易地使用二元组）&lt;/p&gt;&lt;ul&gt;&lt;li&gt;打篮球运动&lt;/li&gt;&lt;li&gt;在北州（卡罗来纳州）上大学&lt;/li&gt;&lt;li&gt;1984年被选入NBA&lt;/li&gt;&lt;li&gt;为芝加哥队（公牛队）效力&lt;/li&gt;&lt;li&gt;是夏洛特队（黄蜂队）的大股东&lt;/li&gt;&lt;li&gt;主演电影《太空（果酱）》&lt;/li&gt;&lt;li&gt;为 NBA 联盟效力&lt;/li&gt;&lt;li&gt;1992年代表美国参加奥运会&lt;/li&gt;&lt;li&gt;玩数字23&lt;/li&gt;&lt;/ul&gt; &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-KGJJcrC8izPbNFCPL-5"&gt;↩︎&lt;/a&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-KGJJcrC8izPbNFCPL-6"&gt;&lt;p&gt;将神经元的值替换为数据集中所有 1500 名运动员的最终名称标记的平均值。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-KGJJcrC8izPbNFCPL-6"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-KGJJcrC8izPbNFCPL-7"&gt;&lt;p&gt;受到 Lovis Heindrich 和 Lucia Quirke 即将推出的作品的启发&lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-KGJJcrC8izPbNFCPL-7"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-KGJJcrC8izPbNFCPL-8"&gt;&lt;p&gt;动机：E(A &amp;amp; B) 对应于 Michael Jordan 上的激活，E(~A &amp;amp; B) 对应于 Keith Jordan（对于 Keith 的不同值），E(A &amp;amp; ~B) 对应于 Michael Smith（对于不同的值）史密斯）。神经元的激活通常具有远离零的平均值，因此我们从每个项中减去该平均值，该平均值由 E(~A &amp;amp; ~B) 项捕获，即除迈克尔或乔丹之外的所有名字的平均值。并且 (E(A &amp;amp; B) - E(~A &amp;amp; ~B)) - (E(~A &amp;amp; B) - E(~A &amp;amp; ~B)) - (E(A &amp;amp; ~B) - E(~ A &amp;amp; ~B)) = E(A &amp;amp; B) - E(~A &amp;amp; B) - E(A &amp;amp; ~B) + E(~A &amp;amp; ~B) &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-KGJJcrC8izPbNFCPL-8"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-KGJJcrC8izPbNFCPL-9"&gt;&lt;p&gt;我们采用中位数是因为有时总的或 GELU 派生的非线性效应是负/零，而中位数让我们可以忽略这些异常值&lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-KGJJcrC8izPbNFCPL-9"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-KGJJcrC8izPbNFCPL-10"&gt;&lt;p&gt;虽然我们无法轻易判断出这是哪篇文章，但也许​​是棒球相关的文章，表现出类似&lt;a href="https://arxiv.org/abs/2310.15154"&gt;总结主题的&lt;/a&gt;东西！ &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-KGJJcrC8izPbNFCPL-10"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-KGJJcrC8izPbNFCPL-11"&gt;&lt;p&gt;将会出现一些错误分类，因为某些名称配对可能是已知实体。我们通过谷歌搜索名称进行了一些抽查，预计这不会对结果产生重大影响&lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-KGJJcrC8izPbNFCPL-11"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-KGJJcrC8izPbNFCPL-12"&gt;&lt;p&gt;我们发现 MLP1 似乎与属性提取头的注意力相关（让它们检测姓名是否是运动员，从而是否提取一项运动），但对于查找运动员从事哪项运动并不重要。即对键重要但对值不重要。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-KGJJcrC8izPbNFCPL-12"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-KGJJcrC8izPbNFCPL-13"&gt;&lt;p&gt;使用单数是因为我的合著者认为这种替代解释一直是显而易见的&lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-KGJJcrC8izPbNFCPL-13"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/section&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/CW5onXm6uZxpbpsRk/fact-finding-trying-to-mechanistically-understanding-early#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sat, 23 Dec 2023 02:46:05 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/CW5onXm6uZxpbpsRk/fact-finding-trying-to-mechanistically-understanding-early</guid></item><item><title>事实调查：简化电路（第 2 篇）</title><link>https://www.lesswrong.com/posts/3tqJ65kuTkBh8wrRH/fact-finding-simplifying-the-circuit-post-2</link><description>发布于 2023 年 12 月 23 日凌晨 2:45（格林尼治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;&lt;em&gt;这是 Google DeepMind 机械可解释性团队&lt;a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX"&gt;对语言模型如何回忆事实的调查的&lt;/a&gt;第二篇文章。这篇文章的重点是提炼事实回忆回路并模拟更标准的机械可解释性调查。这篇文章很杂乱，&lt;strong&gt;我们建议从&lt;a href="https://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall"&gt;第一篇文章&lt;/a&gt;开始&lt;/strong&gt;，然后根据与您最相关的内容略读并跳过其余的序列。我们假设这篇文章的读者熟悉&lt;a href="https://www.neelnanda.io/mechanistic-interpretability/glossary#mechanistic-interpretability-techniques"&gt;本术语表中&lt;/a&gt;列出的机械解释技术。&lt;/em&gt;&lt;/p&gt;&lt;h2&gt;介绍&lt;/h2&gt;&lt;p&gt;我们的目标是了解事实如何以叠加方式存储和调用。必要的一步是找到涉及事实回忆的狭窄任务，并了解使模型能够完成此任务的高级电路。&lt;/p&gt;&lt;p&gt;我们专注于回忆不同运动员所参加的运动的狭隘任务。正如&lt;a href="https://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall#Why_Facts"&gt;第 1 篇文章&lt;/a&gt;中所讨论的，我们特别期望有关人的事实涉及叠加，因为个人名称标记的嵌入通常不足以确定运动，因此模型必须对名称的不同标记执行布尔 AND 运算来识别运动员并查找他们的运动。&lt;a href="https://transformer-circuits.pub/2022/solu/index.html"&gt;之前的&lt;/a&gt;&lt;a href="https://arxiv.org/abs/2305.01610"&gt;工作&lt;/a&gt;将这种现象称为“去标记化”，并表明它涉及早期的 MLP 层，并使用了重要的叠加。&lt;/p&gt;&lt;p&gt;为什么要关注运动员的运动而不是一般的事实回忆？我们相信，在机械解释中，首先深入理解现象的狭义实例通常是有用的，而不是坚持完全普遍化。运动员的运动是一项很好的任务，它为我们提供了每个属性值的大量示例，我们的目标是理解至少一个使用叠加进行事实回忆的示例，而不是一般性地解释事实回忆。我们推测类似的机制用于回忆其他类别的事实，但这不是我们工作的重点。&lt;/p&gt;&lt;h2&gt;设置&lt;/h2&gt;&lt;p&gt;为了理解事实本地化，我们研究了&lt;a href="https://arxiv.org/abs/2304.01373"&gt;Pythia&lt;/a&gt; 2.8B 的下一个标记预测和针对以下形式的一次性提示的激活：&lt;/p&gt;&lt;p&gt;供 1,500 名参加棒球、篮球和（美式）橄榄球运动的运动员使用。为了选择这些运动员，我们为模型提供了来自维基数据的更大的运动员数据集，并筛选出模型对正确运动的概率超过 50% 的数据集&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-1" id="fnref-s4i3RkmKovG88KHgx-1"&gt;[1]&lt;/a&gt;&lt;/sup&gt; 。&lt;/p&gt;&lt;p&gt;我们选择 Pythia 2.8B，因为它是能够胜任大量运动员完成这项任务的最小模型。&lt;/p&gt;&lt;p&gt;我们一次性给出提示，因为这显着提高了模型在任务上的性能&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-2" id="fnref-s4i3RkmKovG88KHgx-2"&gt;[2]&lt;/a&gt;&lt;/sup&gt; 。我们选择高尔夫作为一击前缀，以便模型不会对其需要预测的三项运动之一产生偏见。为简单起见，我们没有改变提示中的一次性前缀。&lt;/p&gt;&lt;h2&gt;我们最终得到的简化电路&lt;/h2&gt;&lt;p&gt;在详细介绍我们为推导电路而进行的消融研究之前，让我们先看一下我们最终得到的简化电路： &lt;/p&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/zwrgezridkrafez2lmmv" /&gt;&lt;/p&gt;&lt;p&gt;在哪里：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt; &lt;code&gt;concatenate_tokens&lt;/code&gt;执行的操作大致类似于各个令牌嵌入的加权和，将每个令牌的嵌入放置在不同的子空间中，由模型的前两层实现； &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-3" id="fnref-s4i3RkmKovG88KHgx-3"&gt;[3]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt; &lt;code&gt;lookup&lt;/code&gt;是一个五层 MLP（其层与原始模型中的 MLP 层 2 到 6 相匹配） &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-4" id="fnref-s4i3RkmKovG88KHgx-4"&gt;[4]&lt;/a&gt;&lt;/sup&gt; ，它处理组合标记以生成这些标记所描述的实体的新表示，其中该表示可以线性投影以揭示有关实体的各种属性（包括他们所从事的运动）；&lt;/p&gt;&lt;ul&gt;&lt;li&gt;我们注意到，在从第 6 层到第 15 层的路径中，似乎有多个重要的 MLP 层，但我们认为它们主要是增强运动现有线性表示的信号，并且消除它们不会显着影响准确性。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt; &lt;code&gt;extract_sport&lt;/code&gt;是一个线性分类器，它对&lt;code&gt;lookup&lt;/code&gt;的表示执行仿射变换，以提取模型返回的运动逻辑。这是在模型中通过多个注意力头（特别是包括 L16H20）实现的，这些注意力头关注从最终标记到最终名称标记，并直接与非嵌入层组合&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-5" id="fnref-s4i3RkmKovG88KHgx-5"&gt;[5]&lt;/a&gt;&lt;/sup&gt; 。在运动员的名字仅包含两个标记（一个用于名字，一个用于姓氏）的特殊情况下，我们能够进一步将&lt;code&gt;concatenate_tokens&lt;/code&gt;函数简化为以下形式： &lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/jaxnaennpshoncgl2x6a" /&gt;&lt;/p&gt;&lt;p&gt;其中&lt;code&gt;embed_first&lt;/code&gt;和&lt;code&gt;embed_last&lt;/code&gt;是字面上的查找表（模型词汇表中每个标记有一个条目），具有不相交的范围（以便编码器可以区分名字“Duncan”和姓氏“Duncan”）——强化了这样的想法： &lt;code&gt;concatenate_tokens&lt;/code&gt;仅与各个标记（加上位置信息）本身一样提供线性信息 - 即，它是相关标记序列的密集/压缩表示，模型需要从中解压缩/提取线性特征，以便下游电路可以使用（例如&lt;code&gt;extract_sport&lt;/code&gt; ）。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-6" id="fnref-s4i3RkmKovG88KHgx-6"&gt;[6]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;这与文献中的先前工作相当一致，特别&lt;a href="https://arxiv.org/pdf/2304.14767.pdf"&gt;是 Geva 等人&lt;/a&gt;。我们认为我们狭隘的、自下而上的方法是对他们更广泛、更自上而下的电路理解方法的补充。我们进一步证明&lt;code&gt;extract_sport&lt;/code&gt;是一个线性映射，反映了&lt;a href="https://arxiv.org/abs/2308.09124"&gt;Hernandez 等人的&lt;/a&gt;观点，并且它可以根据个体注意力头的 OV 电路来理解。我们认为我们的贡献是在狭窄的领域中通过更自下而上和以电路为中心的方法来完善现有知识，并更好地理解模型在每个阶段的表示，而不是作为一个实质性的新进步。&lt;/p&gt;&lt;p&gt;在本文的其余部分中，我们将进一步详细描述我们为推导此简化电路而进行的实验。&lt;/p&gt;&lt;h2&gt;调查 1：了解事实提取&lt;/h2&gt;&lt;p&gt;当模型输出正确的运动标记来完成提示时，最早确定该运动的标记在哪里？先前的工作表明，应该在序列的早期（在运动员的最终名字标记处）识别正确的运动，并将其放置在线性可恢复的表示中。然后，一个单独的事实提取电路（上面电路图中的&lt;code&gt;extract_sport&lt;/code&gt; ）将从最终名称位置读取运动并输出正确的 logits 以完成提示。&lt;/p&gt;&lt;p&gt;在本节中，我们将描述我们进行的实验，以验证该图在现实中是否成立，并确定实现该事实提取步骤的电路。&lt;/p&gt;&lt;h3&gt;哪些节点对输出 logits 贡献最大？&lt;/h3&gt;&lt;p&gt;我们首先确定模型中的哪些节点对最终令牌生成的 logits 有最大的直接影响。我们通过单独消除每个 MLP 层的激活和最终 token 位置的注意力头输出并测量对输出 logits 的直接影响来做到这一点。&lt;/p&gt;&lt;p&gt;具体来说，对于每个干净的提示和我们希望消除的每个节点，我们将从“损坏的”提示中获取该节点的激活，并将这些激活沿着将该节点连接到模型的非嵌入层的补丁修补到干净提示的激活中，为了测量该路径补丁对模型输出的影响。对于损坏的提示，我们会​​为从事不同运动的运动员随机选择一个提示。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-7" id="fnref-s4i3RkmKovG88KHgx-7"&gt;[7]&lt;/a&gt;&lt;/sup&gt;为了衡量直接效果，我们将比较路径修补之前和之后干净提示的运动和损坏提示的运动之间的 logit 差异。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-8" id="fnref-s4i3RkmKovG88KHgx-8"&gt;[8]&lt;/a&gt;&lt;/sup&gt;结果如下： &lt;/p&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/m0prshu8u9zcnimo4fr6" /&gt;&lt;/p&gt;&lt;p&gt;这些结果表明，相对稀疏的节点集对 logits 有任何有意义的影响：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;中间层有少数注意力头；&lt;/li&gt;&lt;li&gt;遵循这些注意力头的 MLP 层。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;其中，注意力头特别有趣：由于我们测量了直接（而不是全部）效果，我们知道这些注意力头的输出直接将最终标记逻辑推向正确的运动（或远离不正确的运动），而无需需要进一步的后处理。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-9" id="fnref-s4i3RkmKovG88KHgx-9"&gt;[9]&lt;/a&gt;&lt;/sup&gt;这强烈表明，无论这些头关注何处，这些位置的残余流已经对每个运动员的运动进行了编码。&lt;/p&gt;&lt;h3&gt;这些高效率的负责人去哪里参加？ &lt;/h3&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/bgqe8cyizbqkxuoxjx4e" /&gt;&lt;/p&gt;&lt;p&gt;在这里，我们通过对最终 token logits 具有最高直接影响的 6 个头的提示样本，可视化了最终 token 的注意力模式。我们看到头部大多关注最终的名称标记位置，或者如果失败，则回顾提示中前面的“&amp;lt;bos&amp;gt;”或“\n”静止位置。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-10" id="fnref-s4i3RkmKovG88KHgx-10"&gt;[10]&lt;/a&gt;&lt;/sup&gt; &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-11" id="fnref-s4i3RkmKovG88KHgx-11"&gt;[11]&lt;/a&gt;&lt;/sup&gt;由此我们可以得出两件事：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;运动员的运动项目主要通过其最终名称标记的第 16 层在剩余流中表示。&lt;/li&gt;&lt;li&gt;他们的运动的表示应该是线性可恢复的（因为每个头的值输入通过近似线性变换与模型的最终令牌 logits 相关）。&lt;/li&gt;&lt;/ol&gt;&lt;h3&gt;这些高效注意力头从最终名称标记位置读取什么？&lt;/h3&gt;&lt;p&gt;为了回答这个问题，我们通过 OV 电路计算了上面列出的每个高效头的最终名称标记位置中节点的路径特定效果。准确地说，对于每个高效头，我们通过相关头的 OV 电路，沿着从该节点到输出 logits 的路径，逐个修补每个馈线节点在损坏的（其他运动）提示上的激活&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-12" id="fnref-s4i3RkmKovG88KHgx-12"&gt;[ 12]&lt;/a&gt;&lt;/sup&gt; 。实际上，这根据馈入该节点的节点创建了每个高效节点的值输入的归因。&lt;/p&gt;&lt;p&gt;有趣的是，我们发现第二到第六个最重要头的性能的很大一部分依次来自于它们的值输入读取最终名称令牌流&lt;em&gt;中&lt;/em&gt;L16H20 的输出。例如，这里是最终名称令牌节点通过 L21H9（第二个最重要的头）的 OV 电路对 logits 的影响的归因 - 请注意 L16H20 的输出（在最终名称令牌位置）对以下影响的巨大贡献这个头： &lt;/p&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/kmyken8pjgofdndy2mre" /&gt;&lt;/p&gt;&lt;p&gt;第三到第六个最重要的头的热图看起来很相似，它们的很多效果来自于最终名称标记处 L16H20 的输出。&lt;/p&gt;&lt;p&gt;此外，观察 L16H20 从最终名称标记位置进行关注时的注意力模式，我们发现它通常会关注相同的位置。将这些观察结果放在一起，我们发现 L16H20 通过两个&lt;em&gt;单独的&lt;/em&gt;机制在此电路中具有很高的整体重要性：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;它通过其 OV 电路将运动员运动功能直接转移到最终令牌位置（从最终令牌返回到最终名称令牌）；&lt;/li&gt;&lt;li&gt;它从最终&lt;em&gt;名称&lt;/em&gt;标记位置到相同位置，产生一个输出，该输出显着贡献（通过 V 组合）到其他头的输出，这些其他头将运动员运动特征从最终名称标记转移到最终标记。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt; L16H20 本身怎么样——哪些节点&lt;em&gt;对其&lt;/em&gt;价值输入的贡献最强？如下图所示，L16H20 本身的值输入在很大程度上取决于最终名称位置中其之前的 MLP 输出（一些 V 组合以及一些较早的头，这些头依次从最终名称位置到自身）： &lt;/p&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/kdaykbcwns7qhchfi4ya" /&gt;&lt;/p&gt;&lt;h3&gt;用于事实提取的简化子电路&lt;/h3&gt;&lt;p&gt;综合以上结果，我们得出结论：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;运动事实提取电路（上图中的&lt;code&gt;extract_sport&lt;/code&gt; ）的很大一部分是由L16H20头执行的；&lt;ul&gt;&lt;li&gt;因为头部的注意力模式在运动员之间不会发生（太大）变化，所以它的功能只是一个线性映射，将残余流乘以其 OV 电路。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;该头的 OV 电路读取最终名称令牌残差流并输出内容（两者都返回到同一残差流中并直接输出到最终令牌残差流中），以便取消嵌入结果可以提高正确运动令牌的 logits 优于不正确运动令牌的 logits &amp;#39; 令牌。&lt;/li&gt;&lt;li&gt;因此&lt;strong&gt;，正确的运动被线性地表示在最终的名称令牌上，并由L16H20的OV电路提取&lt;/strong&gt;。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这表明我们可以通过用三类线性探针替换从最终名称标记位置的第 16 层开始的所有模型计算图来近似&lt;code&gt;extract_sport&lt;/code&gt; ，该三类线性探针是通过将 L16H20 的 OV 图与标记“棒球”的模型非嵌入权重组合而成而构建的、“篮球”和“足球”。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-13" id="fnref-s4i3RkmKovG88KHgx-13"&gt;[13]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;通过这种简化，我们发现在将电路的&lt;code&gt;extract_sport&lt;/code&gt;部分简化为这个（权重导出的）线性探针之后，整个电路对运动员运动进行分类的准确度从原始模型的 100% 下降到 98% – 即我们可以大大简化这部分电路在执行任务时的性能下降可以忽略不计。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-14" id="fnref-s4i3RkmKovG88KHgx-14"&gt;[14]&lt;/a&gt;&lt;/sup&gt; &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-15" id="fnref-s4i3RkmKovG88KHgx-15"&gt;[15]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;h3&gt;另一种路径：只需训练线性探针&lt;/h3&gt;&lt;p&gt;绕过上述许多分析的另一种方法是仅在最终名称标记的残余流上训练逻辑回归探针&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-16" id="fnref-s4i3RkmKovG88KHgx-16"&gt;[16]&lt;/a&gt;&lt;/sup&gt; ，并表明在第 6 层探针获得了良好的测试准确性。我们可以进一步表明，探针&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-17" id="fnref-s4i3RkmKovG88KHgx-17"&gt;[17]&lt;/a&gt;&lt;/sup&gt;所跨越的子空间中的修补会因果地影响模型的输出，这表明该表示在下游使用&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-18" id="fnref-s4i3RkmKovG88KHgx-18"&gt;[18]&lt;/a&gt;&lt;/sup&gt; 。这是我们在项目的重要部分中使用的方法，然后返回并使本节前面的分析更加严格。&lt;/p&gt;&lt;p&gt;我们认为机械探针有显着的优势（例如，使用 L16H20 的权重和非嵌入来导出探针，而不是训练逻辑回归分类器），它更有原则性（从某种意义上说，我们可以清楚地看到它在以下方面意味着什么）：模型的电路），更难过度拟合，并且不需要训练集，然后就不能再用于进一步分析。但“只需训练一个探针”就可以更轻松地快速移动。&lt;/p&gt;&lt;p&gt;特别是，对于这项调查，我们的目标是放大前几层的&lt;code&gt;lookup&lt;/code&gt; ，并且知道在几个 MLP 层之后正确的运动变成线性表示，这足以告诉我们有一些有趣的东西可以尝试逆向工程，即使我们不知道事实提取电路的细节。&lt;/p&gt;&lt;p&gt;我们认为探针是一种被低估的电路分析工具，并且在模型中找到可解释的方向/子空间（可以以非平凡的方式证明其具有因果意义）可以实现更简单的电路分析，只需要考虑层的子集，而不是模型的完整端到端行为。&lt;/p&gt;&lt;p&gt;另一种更简单的方法是通过迭代每个头来搜索机械探针，将其 OV 乘以运动的未嵌入作为探针，并评估准确性。如果有一个精度特别高的头部（包括在保留的验证集上），并且位于正确的位置，那么您可能已经找到了一个关键的头部。我们注意到，与首先进行直接 Logit 归因以缩小到一小部分头部相比，这种方法具有更大的过度拟合风险，具体取决于头部的数量&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-19" id="fnref-s4i3RkmKovG88KHgx-19"&gt;[19]&lt;/a&gt;&lt;/sup&gt; 。&lt;/p&gt;&lt;h2&gt;&lt;/h2&gt;&lt;h2&gt;研究 2：简化去标记化和查找&lt;/h2&gt;&lt;p&gt;在本节中，我们将描述为简化&lt;a href="https://docs.google.com/document/d/1EsIlX7L_xr0YX918NiDWv4Cn3FVS6tJqjrIGR6mnJyQ/edit?resourcekey=0-QoVN8x6k4h6wZCZaWV9qug#heading=h.h6dofjljntk0"&gt;上面&lt;/a&gt;简化电路图中定义的&lt;code&gt;concatenate_tokens&lt;/code&gt;和&lt;code&gt;lookup&lt;/code&gt;模块所覆盖的电路部分而进行的实验。总而言之，下面描述的实验确立了有关这部分电路的以下事实：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;运动员的运动在残差流中比第 16 层（由头 L16H20 读取）更早地表示。到了第 6 层，当我们尝试使用&lt;code&gt;extract_sport&lt;/code&gt;读取运动员的运动数据时，我们仍然获得了相当不错的准确率 (90%)。&lt;/li&gt;&lt;li&gt;运动员姓名之前的上下文对于生成运动表示并不重要（尽管对于&lt;code&gt;extract_sport&lt;/code&gt;确实很重要）。我们可以在“&amp;lt;bos&amp;gt; &amp;lt;athletes-full-name&amp;gt;”形式的提示上使用模型的最终令牌激活，并提取运动员的运动，准确性几乎没有下降。&lt;/li&gt;&lt;li&gt;注意力头在第 0 层和第 1 层之外并不重要。我们可以在不影响&lt;code&gt;lookup&lt;/code&gt;模块性能的情况下对第 2 层之后的注意力层进行平均消融。从注意力模式来看，我们发现大多数负责人并不特别关注以前的名称标记，这强化了取消它们的理由。删除这些注意力头的优点是， &lt;code&gt;lookup&lt;/code&gt;模块因此成为 MLP 层的简单堆栈（具有剩余跳过连接），能够在剩余流中的第 2 层开始处获取运动员的嵌入，并通过以下方式输出运动员的运动：残差流中第 6 层的末尾。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;因此，我们可以将这部分电路分解为两个子模块：&lt;/p&gt;&lt;ul&gt;&lt;li&gt; &lt;code&gt;concatenate_tokens&lt;/code&gt; ，代表模型第 0 层和第 1 层的角色，其角色（对于此任务）是收集运动员的姓名标记并将其放入最终的姓名标记残差流中（我们在调查 3 中表明，这是纯串联，没有查找，至少两个象征性的运动员姓名）；&lt;/li&gt;&lt;li&gt; &lt;code&gt;lookup&lt;/code&gt;是一个纯 5 层 MLP（包含原始模型的第 2-6 层），它将运动员姓名的多标记表示转换为运动员的特征表示，该特征表示以线性可恢复的方式专门表示运动员的运动。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;我们现在依次描述支持上面列出的三个主张的证据。&lt;/p&gt;&lt;h3&gt;查找大部分由第 6 层完成&lt;/h3&gt;&lt;p&gt;如果我们将&lt;code&gt;extract_sport&lt;/code&gt;探针应用到最终名称标记位置的不同层，我们会发现可以比第 16 层更早地从剩余流中读取运动员的运动： &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-20" id="fnref-s4i3RkmKovG88KHgx-20"&gt;[20]&lt;/a&gt;&lt;/sup&gt; &lt;/p&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/pwsurjq0ifiw4ch7xtj2" /&gt;&lt;/p&gt;&lt;p&gt;到第 8 层左右，准确率已基本趋于稳定，甚至到第 6 层，准确率也达到了 90% 左右。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-21" id="fnref-s4i3RkmKovG88KHgx-21"&gt;[21]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;h3&gt;查找运动员的运动时，背景并不重要&lt;/h3&gt;&lt;p&gt;我们已经确定，运动员最终名称标记处的残余流对其运动进行编码。但是模型在多大程度上将体育运动放入残差流中，因为它在看到运动员的名字时无论如何都会这样做（多令牌嵌入假设），以及模型在多大程度上将体育运动放入残差流中，因为一-名称&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-22" id="fnref-s4i3RkmKovG88KHgx-22"&gt;[22]&lt;/a&gt;&lt;/sup&gt;之前的镜头提示是否向模型暗示运动可能是提取的有用属性？&lt;/p&gt;&lt;p&gt;我们的假设是，上下文并不那么重要——特别是，即使没有任何先前的上下文，模型在看到运动员的名字时也会查找运动员的运动项目。我们通过收集纯名称提示的激活来测试这一点，其中模型被输入“&amp;lt;bos&amp;gt; &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-23" id="fnref-s4i3RkmKovG88KHgx-23"&gt;[23]&lt;/a&gt;&lt;/sup&gt; &amp;lt;first-name&amp;gt; &amp;lt;last-name&amp;gt;” &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-24" id="fnref-s4i3RkmKovG88KHgx-24"&gt;[24]&lt;/a&gt;&lt;/sup&gt;形式的令牌序列，并且从最终名称标记。&lt;/p&gt;&lt;p&gt; &lt;code&gt;extract_sport&lt;/code&gt;模块可以从这些激活中读取运动员的运动项目吗？如下图所示，我们发现在没有一次性上下文的情况下，性能会略有下降，但仍然可以纯粹从早期层编码中相当准确地读取运动员的运动项目，该早期层编码仅以“&amp;lt;bos&amp;gt;”开头”，没有任何额外的上下文。因此，我们可以通过从任务的完整提示中运动员姓名标记之前的标记中删除所有边来简化整个电路。 &lt;/p&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/x4wdqduttl0ovdowk9yi" /&gt;&lt;/p&gt;&lt;h3&gt;注意力头在第 2 层之外并不重要&lt;/h3&gt;&lt;p&gt;为了准确地回忆运动项目，电路的&lt;code&gt;lookup&lt;/code&gt;部分通常必须是运动员姓名中大多数（如果不是全部）标记的函数：对于大多数运动员来说，不可能仅通过知道最后一个标记来确定运动项目以他们的姓氏。因此，注意力头必须在汇集分布在运动员姓名的各个标记上的信息方面发挥一定作用，以便可以准确地查找体育等事实。&lt;/p&gt;&lt;p&gt;然而，这两个过程（组合标记和查找事实）如何相互关联？&lt;/p&gt;&lt;ol&gt;&lt;li&gt;它们可以同时发生——当查找过程需要时，注意力会从早期令牌中引入相关信息；&lt;/li&gt;&lt;li&gt;或者，这些过程可以按顺序发生，首先将组成运动员姓名的标记组合在一起，然后大部分查找过程才发生。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-25" id="fnref-s4i3RkmKovG88KHgx-25"&gt;[25]&lt;/a&gt;&lt;/sup&gt;观察在最终名称标记位置修补注意力头输出的总体效果，我们确实发现模型第 0 层和第 1 层中在整个电路中发挥重要作用的头比后面的层要多得多： &lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/s8xbfgfrakemknheueil" /&gt;&lt;/p&gt;&lt;p&gt;这表明我们也许能够删除第 2 层以后的注意力头输出，而不会对整个电路的性能产生太大影响。尝试这个，我们发现从第 2 层开始平均消除注意力输出仅对准确性产生轻微的不利影响： &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-26" id="fnref-s4i3RkmKovG88KHgx-26"&gt;[26]&lt;/a&gt;&lt;/sup&gt; &lt;/p&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/xc9un39ztbdfeaxxh1h4" /&gt;&lt;/p&gt;&lt;p&gt;这支持了上述的两阶段假设：令牌之间的信息共享（通过注意力）基本上由第 2 层完成，后面层中的注意力头对于查找来说并不重要。&lt;/p&gt;&lt;h3&gt;用于事实查找的简化子电路&lt;/h3&gt;&lt;p&gt;上述结果表明，我们确实可以将查找运动员运动项目的过程分为两个阶段：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt; &lt;code&gt;concatenate_tokens&lt;/code&gt; ，这是模型的嵌入和第 0 层和第 1 层处理包含运动员姓名&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-27" id="fnref-s4i3RkmKovG88KHgx-27"&gt;[27]&lt;/a&gt;&lt;/sup&gt;的标记并在最终名称标记残差流中的第 1 层末尾生成“串联标记”表示的结果；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt; &lt;code&gt;lookup&lt;/code&gt; ，这是一个带有跳跃连接的纯 MLP（由模型中的 MLP 第 2 层开始组成），它在残差流的第 1 层之后处理“串联令牌表示”，其本身并不能很好地表示运动员的运动（在线性方式），并稍后在残余流中生成“特征表示”，其中可以轻松地线性提取运动（通过&lt;code&gt;sport_extract&lt;/code&gt; ）。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;请注意，我们在这里合并了两种简化：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;自行处理运动员姓名标记，无需一次性提示（因为我们发现上下文并不重要）&lt;/li&gt;&lt;li&gt;从第 2 层开始移除注意力头（因为我们发现令牌之间的信息传输大部分由第 2 层完成）。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;由于这些近似值中的每一个都会对电路的精度产生一些不利影响，因此值得评估它们的综合影响。下图显示了组合这些近似值如何影响准确性： &lt;/p&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/xkhlsj0ktdmm3kipcbll" /&gt;&lt;/p&gt;&lt;p&gt;结果是，即使同时应用两种简化，通过在&lt;code&gt;lookup&lt;/code&gt; MLP 中包含足够的层也可以获得高达 94% 的准确率；即使停在第 6 层也能获得 85% 的准确率。&lt;/p&gt;&lt;h2&gt;&lt;/h2&gt;&lt;h2&gt;研究3：进一步简化令牌串联电路&lt;/h2&gt;&lt;p&gt;到目前为止，我们已经：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;将&lt;code&gt;extract_sport&lt;/code&gt;简化为 3 类线性探针，其权重是通过将 L16H20 的 OV 图与模型的非嵌入权重组合得出的；&lt;/li&gt;&lt;li&gt;简化&lt;code&gt;lookup&lt;/code&gt;为具有跳跃连接的纯 5 层 MLP，其层权重对应于原始模型中 MLP 层 2-6 的层权重；&lt;/li&gt;&lt;li&gt;确定在计算第 2 层开头的最终名称标记位置（即&lt;code&gt;lookup&lt;/code&gt;的输入）处的残余流值时可以删除先前的上下文。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这给我们留下了&lt;code&gt;concatenate_tokens&lt;/code&gt; ，包括嵌入以及模型的第 0 层和第 1 层，它将原始运动员姓名标记（加上前面的 &amp;lt;bos&amp;gt; 标记）转换为第 2 层开头的残差流的值。进一步简化这部分电路？&lt;/p&gt;&lt;p&gt;我们为该电路组件确定了两个级别的简化：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;一般来说，我们可以将&lt;code&gt;concatenate_tokens&lt;/code&gt;视为有效生成运动员姓名的&lt;em&gt;两个&lt;/em&gt;单独的令牌级嵌入，然后通过纯注意力操作近似线性地组合这些嵌入；&lt;/li&gt;&lt;li&gt;对于名字只有两个标记长的运动员，我们可以进一步近似&lt;code&gt;concatenate_tokens&lt;/code&gt; ，使其实际上是名字和姓氏标记的有效标记嵌入的总和，对电路准确性仅造成适度的影响。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在以下小节中，我们将更详细地解释这些简化并为其提供实验依据。&lt;/p&gt;&lt;h3&gt;令牌串联是通过注意力头移动令牌嵌入来实现的&lt;/h3&gt;&lt;p&gt;第一个简化来自以下两个观察：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;最终名称标记位置的 MLP 第 1 层对电路性能影响不大：重新采样消融对原始模型的 Logit 差异的总体影响较小，均值消融对简化电路的精度影响很小。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-28" id="fnref-s4i3RkmKovG88KHgx-28"&gt;[28]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;由于 Pythia 使用并行注意力，MLP 第 0 层实际上是辅助令牌嵌入层。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-29" id="fnref-s4i3RkmKovG88KHgx-29"&gt;[29]&lt;/a&gt;&lt;/sup&gt;这意味着（在均值消融 MLP 1 之后）， &lt;code&gt;concatenate_tokens&lt;/code&gt;有效地执行以下操作：&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ol&gt;&lt;li&gt;使用模型的嵌入层权重计算运动员姓名标记（和 &amp;lt;bos&amp;gt;）的主要标记嵌入。&lt;/li&gt;&lt;li&gt;使用 MLP 0 对输入标记词汇的作用引起的嵌入权重，计算运动员姓名标记的辅助标记嵌入。&lt;/li&gt;&lt;li&gt;在主要 token 嵌入上操作注意力层 0 头。&lt;/li&gt;&lt;li&gt;在主要 token 嵌入、次要 token 嵌入和注意力层 0 输出的总和上操作注意力层 1 头。&lt;/li&gt;&lt;li&gt;使用最终名称标记位置处的步骤 4 的结果作为&lt;code&gt;lookup&lt;/code&gt;的输入。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;换句话说， &lt;code&gt;concatenate_tokens&lt;/code&gt;有效地嵌入了输入标记（两次），并通过注意力将它们（直接和间接）移动到最终名称标记位置。&lt;/p&gt;&lt;h3&gt;对于两个令牌的运动员， &lt;code&gt;concatenate_tokens&lt;/code&gt;实际上将名字和姓氏令牌添加在一起&lt;/h3&gt;&lt;p&gt;对于两个令牌的运动员，我们发现我们可以进一步冻结注意力模式，并且仍然保持任务的合理准确性。具体来说：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;我们阻止了第 1 层中姓氏标记位置的注意力头关注同一位置；&lt;/li&gt;&lt;li&gt;我们将数据集中所有两个令牌运动员的所有其他注意力模式冻结在平均水平，使注意力层充当将源令牌残差映射到最终令牌的线性映射。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这些简化以及均值消融 MLP 1 将&lt;code&gt;concatenate_tokens&lt;/code&gt;转换为有效令牌嵌入和偏差项（源自 &amp;lt;bos&amp;gt; 令牌的嵌入）的总和。姓氏的有效令牌嵌入只是主要和第二（MLP0）令牌嵌入的总和。名字的有效 token 嵌入更复杂，它是主 token 嵌入乘以来自冻结注意力 0 头的线性图（它们的 OV 矩阵由姓氏到名字的平均注意力加权），加上主要和次要令牌嵌入乘以来自冻结注意力 1 头的线性图。&lt;/p&gt;&lt;p&gt;这些简化对准确性的影响如下图所示。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-30" id="fnref-s4i3RkmKovG88KHgx-30"&gt;[30]&lt;/a&gt;&lt;/sup&gt;我们发现，对于两个令牌的运动员来说，与消除 MLP 1 相比，冻结注意力模式对准确性几乎没有额外的影响。 &lt;/p&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/kinnfxarccubfccdl2g3" /&gt;&lt;/p&gt;&lt;hr class="footnotes-sep" /&gt;&lt;section class="footnotes"&gt;&lt;ol class="footnotes-list"&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-1"&gt;&lt;p&gt;请注意，我们从数千名运动员开始，因此这种过滤可能会引入一些偏差。例如，如果模型不认识 1000 名运动员，但随机猜测了一项运动，我们会选择模型幸运的 333 名运动员。我们设置 50% 置信度（针对完整词汇）阈值来减少这种影响。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-1"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-2"&gt;&lt;p&gt;我们的猜测是，很少的射击与零射击不会对查找电路产生实质性影响，而是单次提示告诉模型输出是一项运动，并提高所有运动的逻辑（由 Chughtai 等人的结果建议（即将发布）） 。有趣的是，零样本，该模型非常重视“他/她”作为输出，尽管 Pythia 1.4B 没有！ &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-2"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-3"&gt;&lt;p&gt;请注意， &lt;code&gt;concatenate_tokens&lt;/code&gt;输出中的线性可恢复特征最终将类似于这些单独令牌嵌入中的特征的并集，即运动并不是特别可以从此输出中线性恢复。最好将&lt;code&gt;concatenate_tokens&lt;/code&gt;的输出视为“保存位置信息的各个令牌嵌入的串联”，以便串联可以由一系列 MLP 层一起处理。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-3"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-4"&gt;&lt;p&gt;我们可以选择 4 到 15 之间的几乎任何层作为端点，因为当我们添加额外的层时，我们的简化电路的可信度会相当连续地增加。然而，第 5 层周围存在一个拐点，之后您将开始看到收益递减。我们认为 MLP 6 之后的 MLP 层只是增强残差流中的属性，而不是从原始标记中查找属性。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-4"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-5"&gt;&lt;p&gt;当我们提到“线性分类器”时，我们指的是这些头的 OV 电路的组成和非嵌入矩阵。头部始终从最终标记到最终名称位置，因此纯粹充当线性映射。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-5"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-6"&gt;&lt;p&gt;我们怀疑，即使对于名字中包含三个或更多令牌的运动员，也可以将&lt;code&gt;concatenate_tokens&lt;/code&gt;表达为类似的位置相关令牌嵌入总和，但我们没有进一步进行这一调查。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-6"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-7"&gt;&lt;p&gt;例如，如果干净的提示是蒂姆·邓肯（打篮球）的，我们可以从乔治·布雷特（打棒球）或安迪·道尔顿（打橄榄球）的提示中修补激活。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-7"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-8"&gt;&lt;p&gt;该指标具有良好的特性，即在逻辑上呈线性，同时对于所有逻辑上的恒定变化也具有不变性。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-8"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-9"&gt;&lt;p&gt;澄清一下，后面的 MLP 层的总体影响并不显着，这表明正在进行一些后处理 - 我们要说的是，即使没有这种后处理，这些注意力头的输出也可以直接用运动来解释令牌逻辑，因此这些注意力头已经将正确的运动写入残差流中。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-9"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-10"&gt;&lt;p&gt;有趣的是，由于提示是按照运动分类的，我们看到一些头似乎只用于任务中运动的子集。按运动细分这些头部的直接影响证实了这一情况：L19H24 仅对棒球运动员始终重要，而 L22H17 仅对篮球和部分棒球运动员始终重要。 （我们并没有试图理解这个头对于那些重要的运动员和那些不重要的运动员的区别。）注意力头之间的这种选择性&lt;a href="https://arxiv.org/abs/2307.09458"&gt;并不完全令人惊讶&lt;/a&gt;，我们没有进一步调查这一点，因为它不是直接的与我们目前的调查相关。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-10"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-11"&gt;&lt;p&gt;与通常的机械解释一样，这只是一个大概的图片 - 看看这些图，很明显，有些头脑确实非常关注运动员姓名和最终标记之间的标记，特别是“比赛”和“运动”，有时“高尔夫”。然而，这并没有改变总体结论，即运动员姓名令牌的剩余流已经包含他们的运动。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-11"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-12"&gt;&lt;p&gt;我们注意到，这种方法需要对最终名称标记上的每个残余流组件&lt;em&gt;以及&lt;/em&gt;每个高效中介头进行单独的前向传递。这不是我们工作的瓶颈，因此我们进行了适当的路径修补，但我们注意到这可以通过直接 logit 归因轻松近似。如果我们冻结 LayerNorm 尺度，那么每个头的 OV 电路对最终名称 token 残差流执行线性映射，而去嵌入则是进一步的线性映射，因此我们可以有效地看到每个头引起的最终 logits 的变化最终名称标记剩余部分。我们发现这种技术对于这项工作中的快速迭代很有用。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-12"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-13"&gt;&lt;p&gt;我们还需要设置探头的偏置。我们通过减去探针输入处的平均激活来实现这一点（即，在应用权重矩阵之前有效地将探针输入居中）。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-13"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-14"&gt;&lt;p&gt;这种表现上的差异至少部分是因为我们总是探测最终的名称标记位置，而 L16H20 则针对一些运动员关注其名称中的其他位置（例如倒数第二个标记）。我们推测这是因为一些运动员在他们名字中的最后一个标记出现之前就被完全识别了（例如，从五个标记名称中的四个标记），因此事实查找发生在这个最终标记之前。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-14"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-15"&gt;&lt;p&gt;我们故意测量准确性而不是损失恢复，因为我们预计后来的高效磁头主要是增强 L16H20 输出的信号，尽管&lt;a href="https://arxiv.org/abs/2309.16042"&gt;损失恢复通常是我们的首选指标&lt;/a&gt;。信号增强可以改善损失，但不会改变准确性，并且要了解事实回忆，只需了解如何实现高精度即可。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-15"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-16"&gt;&lt;p&gt;在这种情况下，我们已经猜测最终的名称令牌将是基于例如残差流修补+先前工作来探测运动的正确令牌，但是扫描令牌和层并在所有令牌和层上训练探针是很容易的，探针是坐火车很便宜&lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-16"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-17"&gt;&lt;p&gt; &lt;a href="https://arxiv.org/abs/2311.17030"&gt;Makelov 等人&lt;/a&gt;最近表明，子空间激活修补可能会误导性地激活休眠的并行路径，但这在使用梯度下降来学习具有因果效应的子空间时主要是一个问题，探针是相关的，因此这不是问题。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-17"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-18"&gt;&lt;p&gt;因为探针是线性的，所以有点不清楚是否应该关心探针是否有因果关系地使用。该模型能够将运动员姓名的各个标记映射到运动的线性表示，这是一种有趣的逆向工程算法，并且可能涉及叠加，即使对于某些奇怪的巧合，并行算法是影响模型输出的主要因素。但这是一个非常人为的情况，而且很容易检查。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-18"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-19"&gt;&lt;p&gt;特别是，在某些设置中，探查可能非常自然。例如，许多注意力头只是将他们关注的任何标记复制到输出中。因此，当探测输入标记时，作为一个好的机械探针是涉及头部的微弱证据，但仍然可能会发现你是一个相当好的探针。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-19"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-20"&gt;&lt;p&gt;当将此探针应用于其他层时，我们始终指的是相对于该层的残余流激活的中心。这相当于平均消融被探测层和第 16 层之间的 MLP 和注意力输出&lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-20"&gt;。 ↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-21"&gt;&lt;p&gt;这里自然会出现一个问题：当探测表明您几乎可以更早地阅读运动员的运动项目？我们的假设是，这些后来的 MLP 正在增强早期 MLP 产生的信号，而不是自己查找事实。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-21"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-22"&gt;&lt;p&gt;即“事实：老虎伍兹从事高尔夫运动” &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-22"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-23"&gt;&lt;p&gt;杂草中：Pythia 模型没有使用 BOS（序列开始）标记进行训练，但我们有传闻发现该模型在使用 BOS 标记进行推理时表现更好。模型通常对上下文的第一个标记有极高的规范，并以不同寻常的方式对待它，这使得研究像“George Brett”这样的简短提示变得困难。 Pythia 的 BOS 和 EOS 代币是相同的，并且在预训练中使用 EOS 代币分隔文档进行训练（模型能够参与不同的文档之间），因此在训练期间会看到 BOS 代币扮演这种角色&lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-23"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-24"&gt;&lt;p&gt;例如“&amp;lt;bos&amp;gt;乔治·布雷特” &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-24"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-25"&gt;&lt;p&gt;注意，我们知道不可能将查找与令牌串联完全分开，因为即使是最终的令牌嵌入（在模型进行任何处理之前）也通常具有运动员运动的一些概念。相反，我们在这里做出较弱的假设，即&lt;code&gt;lookup&lt;/code&gt;电路的大部分额外准确性（除了根据最终标记猜测运动之外）发生在运动员的姓名标记首次组装在一起之后。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-25"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-26"&gt;&lt;p&gt;我们还检查（并确认）消除注意力层 0 或 1 对运动查找产生灾难性影响。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-26"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-27"&gt;&lt;p&gt;准确地说，带有前面的 &amp;lt;bos&amp;gt; 标记。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-27"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-28"&gt;&lt;p&gt;当我们指的是消融 MLP 1 时，使用前几节中描述的简化&lt;code&gt;lookup&lt;/code&gt;和&lt;code&gt;extract_sport&lt;/code&gt;电路，探测精度在第 6 层之后从 85% 下降到 81%，在第 15 层之后从 94% 下降到 90%。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-28"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-29"&gt;&lt;p&gt;由于并行注意力，任何位置的 MLP0 的输入都只是该位置的 token 嵌入。因此，我们可以用第二个令牌嵌入表（将原始令牌 ID 映射到相应的 MLP0 输出值）来替换 MLP0，而根本不会影响模型输出。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-29"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-30"&gt;&lt;p&gt;请注意，该图不能与前面的图直接比较，因为它仅测量两个令牌运动员的准确性。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-30"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/section&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/3tqJ65kuTkBh8wrRH/fact-finding-simplifying-the-circuit-post-2#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sat, 23 Dec 2023 02:45:49 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/3tqJ65kuTkBh8wrRH/fact-finding-simplifying-the-circuit-post-2</guid></item></channel></rss>