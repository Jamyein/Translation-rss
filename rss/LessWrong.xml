<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>少错</title><link>https://www.lesswrong.com</link><description>致力于提炼理性艺术的社区博客</description><lastBuildDate>Sun, 17 Dec 2023 00:56:50 GMT</lastBuildDate><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>2022 年（及所有时间）Pingback Count 发布的帖子</title><link>https://www.lesswrong.com/posts/WYqixmisE6dQjHPT8/2022-and-all-time-posts-by-pingback-count</link><description>发布于 2023 年 12 月 16 日晚上 9:17（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;在过去的几年里，我一直希望 LessWrong 能够“按照 pingback 的数量对帖子进行排序，或者理想情况下，按照 pingback 的总业力对帖子进行排序”。我在年度审查期间特别希望这一点，其中“哪些帖子被引用最多？”似乎是追踪潜在隐藏宝石的有用工具。&lt;/p&gt;&lt;p&gt;我们还没有为此构建成熟的功能，但我只是对数据库运行了查询，并将其制作成电子表格，您可以在此处查看：&lt;/p&gt;&lt;p&gt;&lt;a href="https://docs.google.com/spreadsheets/d/1ZxFOXeKQof2bBwnB5zKrvulgFHeM71I7FgtIPv68czc"&gt;少错 2022 年 Pingbacks 帖子&lt;/a&gt;&lt;/p&gt;&lt;p&gt;以下是排名前 100 的帖子，按总 Pingback Karma 排序&lt;/p&gt;&lt;figure class="table" style="width: 0px;"&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt;&lt;code&gt;&lt;strong&gt;Title/Link&lt;/strong&gt;&lt;/code&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; &lt;code&gt;&lt;strong&gt;Post Karma&lt;/strong&gt;&lt;/code&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; &lt;code&gt;&lt;strong&gt;Pingback Count&lt;/strong&gt;&lt;/code&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; &lt;code&gt;&lt;strong&gt;Total Pingback Karma&lt;/strong&gt;&lt;/code&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; &lt;code&gt;&lt;strong&gt;Avg Pingback Karma&lt;/strong&gt;&lt;/code&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities"&gt;&lt;code&gt;&lt;u&gt;AGI Ruin: A List of Lethalities&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;第870章&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;158&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 12,484&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 79&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/j9Q8bRmwCgXRYAgcJ/miri-announces-new-death-with-dignity-strategy"&gt;&lt;code&gt;&lt;u&gt;MIRI announces new &amp;quot;Death With Dignity&amp;quot; strategy&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;第334章&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;73&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 8,134&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 111&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization"&gt;&lt;code&gt;&lt;u&gt;A central AI alignment problem: capabilities generalization, and the sharp left turn&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 273&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 96&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 7,704&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 80&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators"&gt;&lt;code&gt;&lt;u&gt;Simulators&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 612&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 127&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 7,699&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 61&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to"&gt;&lt;code&gt;&lt;u&gt;Without specific countermeasures, the easiest path to transformative AI likely leads to AI takeover&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;第367章&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;83&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 5,123&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 62&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt;&lt;a href="https://lesswrong.com/posts/pdaGN6pQyQarFHXF4/reward-is-not-the-optimization-target"&gt;&lt;code&gt;&lt;u&gt;Reward is not the optimization target&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;第341章&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;62&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 4,493&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 72&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking"&gt;&lt;code&gt;&lt;u&gt;A Mechanistic Interpretability Analysis of Grokking&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;第367章&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;48&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 3,450 人&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;72&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/w4aeAFzSAguvqA5qu/how-to-go-from-interpretability-to-alignment-just-retarget"&gt;&lt;code&gt;&lt;u&gt;How To Go From Interpretability To Alignment: Just Retarget The Search&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 167&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 45&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 3,374&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 75&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/3pinFH3jerMzAvmza/on-how-various-plans-miss-the-hard-bits-of-the-alignment"&gt;&lt;code&gt;&lt;u&gt;On how various plans miss the hard bits of the alignment challenge&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;第292章&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;40&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 3,288&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 82&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/hE56gYi5d68uux9oM/intro-to-brain-like-agi-safety-3-two-subsystems-learning-and"&gt;&lt;code&gt;&lt;u&gt;[Intro to brain-like-AGI safety] 3. Two subsystems: Learning &amp;amp; Steering&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 79&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 36&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 3,023&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 84&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/A9NxPTwbw6r6Awuwt/how-likely-is-deceptive-alignment"&gt;&lt;code&gt;&lt;u&gt;How likely is deceptive alignment?&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 101&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 47&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 2,907 人&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;62&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/iCfdcxiyr2Kj8m8mT/the-shard-theory-of-human-values"&gt;&lt;code&gt;&lt;u&gt;The shard theory of human values&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 238&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 42&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 2,843&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 68&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/t9svvNPNmFf5Qa3TA/mysteries-of-mode-collapse"&gt;&lt;code&gt;&lt;u&gt;Mysteries of mode collapse&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;第279章&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;32&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 2,842&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 89&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/wBHSYwqssBGCnwvHg/intro-to-brain-like-agi-safety-2-learning-from-scratch-in"&gt;&lt;code&gt;&lt;u&gt;[Intro to brain-like-AGI safety] 2. “Learning from scratch” in the brain&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 57&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 30&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 2,731&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 91&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/FWvzwCDRgcjb9sigb/why-agent-foundations-an-overly-abstract-explanation"&gt;&lt;code&gt;&lt;u&gt;Why Agent Foundations? An Overly Abstract Explanation&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;第285章&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;42&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 2,730&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 65&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability"&gt;&lt;code&gt;&lt;u&gt;A Longlist of Theories of Impact for Interpretability&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 124&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 26&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 2,589&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 100&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/rCJQAkPTEypGjSJ8X/how-might-we-align-transformative-ai-if-it-s-developed-very"&gt;&lt;code&gt;&lt;u&gt;How might we align transformative AI if it&amp;#39;s developed very soon?&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 136&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 32&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 2,351&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 73&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/nbq2bWLcYmSGup9aF/a-transparency-and-interpretability-tech-tree"&gt;&lt;code&gt;&lt;u&gt;A transparency and interpretability tech tree&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 148&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 31&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 2,343&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 76&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/yRAo2KEGWenKYZG9K/discovering-language-model-behaviors-with-model-written"&gt;&lt;code&gt;&lt;u&gt;Discovering Language Model Behaviors with Model-Written Evaluations&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 100&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 19&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 2,336&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 123&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/vQNJrJqebXEWjJfnz/a-note-about-differential-technological-development"&gt;&lt;code&gt;&lt;u&gt;A note about differential technological development&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 185&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 20&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 2,270&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 114&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing"&gt;&lt;code&gt;&lt;u&gt;Causal Scrubbing: a method for rigorously testing interpretability hypotheses [Redwood Research]&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 195&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 35&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 2,267&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 65&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/pYcFPMBtQveAjcSfH/supervise-process-not-outcomes"&gt;&lt;code&gt;&lt;u&gt;Supervise Process, not Outcomes&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 132&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 25&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 2,262&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 90&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/xqkGmfikqapbJ2YMj/shard-theory-an-overview"&gt;&lt;code&gt;&lt;u&gt;Shard Theory: An Overview&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 157&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 28&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 2,019 人&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;72&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/72scWeZRta2ApsKja/epistemological-vigilance-for-alignment"&gt;&lt;code&gt;&lt;u&gt;Epistemological Vigilance for Alignment&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 61&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 21&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 2,008&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 96&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/k4AQqboXz8iE5TNXK/a-shot-at-the-diamond-alignment-problem"&gt;&lt;code&gt;&lt;u&gt;A shot at the diamond-alignment problem&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 92&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 23&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 1,848&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 80&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/CoZhXrhpQxpy9xw9y/where-i-agree-and-disagree-with-eliezer"&gt;&lt;code&gt;&lt;u&gt;Where I agree and disagree with Eliezer&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;第862章&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;27&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 1,836&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 68&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/xwBuoE9p8GE7RAuhd/brain-efficiency-much-more-than-you-wanted-to-know"&gt;&lt;code&gt;&lt;u&gt;Brain Efficiency: Much More than You Wanted to Know&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 201&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 27&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 1,807 人&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;67&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/D7epkkJb3CqDTYgX9/refine-an-incubator-for-conceptual-alignment-research-bets"&gt;&lt;code&gt;&lt;u&gt;Refine: An Incubator for Conceptual Alignment Research Bets&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 143&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 21&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 1,793&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 85&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/FRRb6Gqem8k69ocbi/externalized-reasoning-oversight-a-research-direction-for"&gt;&lt;code&gt;&lt;u&gt;Externalized reasoning oversight: a research direction for language model alignment&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 117&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 28&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 1,788&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 64&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/CjFZeDD6iCnNubDoS/humans-provide-an-untapped-wealth-of-evidence-about"&gt;&lt;code&gt;&lt;u&gt;Humans provide an untapped wealth of evidence about alignment&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 186&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 19&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 1,647&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 87&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/keiYkaeoLHoKK4LYA/six-dimensions-of-operational-adequacy-in-agi-projects"&gt;&lt;code&gt;&lt;u&gt;Six Dimensions of Operational Adequacy in AGI Projects&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 298&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 20&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 1,607 人&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;80&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/L4anhrxjv8j2yRKKp/how-discovering-latent-knowledge-in-language-models-without"&gt;&lt;code&gt;&lt;u&gt;How &amp;quot;Discovering Latent Knowledge in Language Models Without Supervision&amp;quot; Fits Into a Broader Alignment Scheme&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 240&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 16&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 1,575&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 98&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/DwqgLXn5qYC7GqExF/godzilla-strategies"&gt;&lt;code&gt;&lt;u&gt;Godzilla Strategies&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 137&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 17 号&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;1,573&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 93&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is"&gt;&lt;code&gt;&lt;u&gt;(My understanding of) What Everyone in Technical Alignment is Doing and Why&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;第411章&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;23&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 1,530&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 67&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt;&lt;a href="https://lesswrong.com/posts/AfH2oPHCApdKicM4m/two-year-update-on-my-personal-ai-timelines"&gt;&lt;code&gt;&lt;u&gt;Two-year update on my personal AI timelines&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;第287章&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;18&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 1,530&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 85&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/tj8AC3vhTnBywdZoA/intro-to-brain-like-agi-safety-15-conclusion-open-problems-1"&gt;&lt;code&gt;&lt;u&gt;[Intro to brain-like-AGI safety] 15. Conclusion: Open problems, how to help, AMA&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 90&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 16&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 1,482&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 93&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/qNZSBqLEh4qLRqgWW/intro-to-brain-like-agi-safety-6-big-picture-of-motivation"&gt;&lt;code&gt;&lt;u&gt;[Intro to brain-like-AGI safety] 6. Big picture of motivation, decision-making, and RL&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 66&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 25&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 1,460&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 58&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/CQAMdzA4MZEhNRtTp/human-values-and-biases-are-inaccessible-to-the-genome"&gt;&lt;code&gt;&lt;u&gt;Human values &amp;amp; biases are inaccessible to the genome&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 90&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 14&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 1,450&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 104&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/9kNxhKWvixtKW5anS/you-are-not-measuring-what-you-think-you-are-measuring"&gt;&lt;code&gt;&lt;u&gt;You Are Not Measuring What You Think You Are Measuring&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 350&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 21&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 1,449&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 69&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/5HtDzRAk7ePWsiL2L/open-problems-in-ai-x-risk-pais-5"&gt;&lt;code&gt;&lt;u&gt;Open Problems in AI X-Risk [PAIS #5]&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 59&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 14&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 1,446&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 103&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/4basF9w9jaPZpoC8R/intro-to-brain-like-agi-safety-1-what-s-the-problem-and-why"&gt;&lt;code&gt;&lt;u&gt;[Intro to brain-like-AGI safety] 1. What&amp;#39;s the problem &amp;amp; Why work on it now?&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 146&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 25&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 1,407&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 56&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/nXeLPcT9uhfG3TMPS/conditioning-generative-models"&gt;&lt;code&gt;&lt;u&gt;Conditioning Generative Models&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 24&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 11&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 1,362&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 124&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt;&lt;a href="https://lesswrong.com/posts/Gs29k3beHiqWFZqnn/conjecture-internal-infohazard-policy"&gt;&lt;code&gt;&lt;u&gt;Conjecture: Internal Infohazard Policy&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 132&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 14&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 1,340&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 96&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/tD9zEiHfkvakpnNam/a-challenge-for-agi-organizations-and-a-challenge-for-1"&gt;&lt;code&gt;&lt;u&gt;A challenge for AGI organizations, and a challenge for readers&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 299&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 18&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 1,336&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 74&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/HoQ5Rp7Gs6rebusNP/superintelligent-ai-is-necessary-for-an-amazing-future-but-1"&gt;&lt;code&gt;&lt;u&gt;Superintelligent AI is necessary for an amazing future, but far from sufficient&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 132&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 11&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 1,335&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 121&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/kpPnReyBC54KESiSn/optimality-is-the-tiger-and-agents-are-its-teeth"&gt;&lt;code&gt;&lt;u&gt;Optimality is the tiger, and agents are its teeth&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 288&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 14&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 1,319&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 94&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/uFNgRumrDTpBfQGrs/let-s-think-about-slowing-down-ai"&gt;&lt;code&gt;&lt;u&gt;Let&amp;#39;s think about slowing down AI&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;第522章&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;17 号&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;1,273&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 75&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/krHDNc7cDvfEL8z9a/niceness-is-unnatural"&gt;&lt;code&gt;&lt;u&gt;Niceness is unnatural&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 121&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 12&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 1,263&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 105&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/H5iGhDhQBtoDpCBZ2/announcing-the-alignment-of-complex-systems-research-group"&gt;&lt;code&gt;&lt;u&gt;Announcing the Alignment of Complex Systems Research Group&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 91&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 11&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 1,247&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 113&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/5F5Tz3u6kJbTNMqsb/intro-to-brain-like-agi-safety-13-symbol-grounding-and-human"&gt;&lt;code&gt;&lt;u&gt;[Intro to brain-like-AGI safety] 13. Symbol grounding &amp;amp; human social instincts&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 67&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 23&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 1,243&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 54&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/zjMKpSB2Xccn9qi5t/elk-prize-results"&gt;&lt;code&gt;&lt;u&gt;ELK prize results&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 135&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 17 号&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;1,235&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 73&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt;&lt;a href="https://lesswrong.com/posts/vvEebH5jEvxnJEvBC/abstractions-as-redundant-information"&gt;&lt;code&gt;&lt;u&gt;Abstractions as Redundant Information&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 64&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 18&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 1,216&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 68&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt;&lt;a href="https://lesswrong.com/posts/fYf9JAwa6BYMt8GBj/link-a-minimal-viable-product-for-alignment"&gt;&lt;code&gt;&lt;u&gt;[Link] A minimal viable product for alignment&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 53&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 12&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 1,184&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 99&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/GeabLEXYP7oBMivmF/acceptability-verification-a-research-agenda"&gt;&lt;code&gt;&lt;u&gt;Acceptability Verification: A Research Agenda&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 50&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 11&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 1,182&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 107&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/kipMvuaK3NALvFHc9/what-an-actually-pessimistic-containment-strategy-looks-like"&gt;&lt;code&gt;&lt;u&gt;What an actually pessimistic containment strategy looks like&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;第647章&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;16&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 1,168&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 73&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/AqsjZwxHNqH64C2b6/let-s-see-you-write-that-corrigibility-tag"&gt;&lt;code&gt;&lt;u&gt;Let&amp;#39;s See You Write That Corrigibility Tag&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 120&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 10&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 1,161&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 116&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications"&gt;&lt;code&gt;&lt;u&gt;chinchilla&amp;#39;s wild implications&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 403&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 18&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 1,151&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 64&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/xFotXGEotcKouifky/worlds-where-iterative-design-fails"&gt;&lt;code&gt;&lt;u&gt;Worlds Where Iterative Design Fails&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 185&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 17 号&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;1,122&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 66&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/dKTh9Td3KaJ8QW6gw/why-assume-agis-will-optimize-for-fixed-goals"&gt;&lt;code&gt;&lt;u&gt;why assume AGIs will optimize for fixed goals?&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 138&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 14&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 1,103 人&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;79&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/EeAgytDZbDjRznPMA/gradient-hacking-definitions-and-examples"&gt;&lt;code&gt;&lt;u&gt;Gradient hacking: definitions and examples&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 38&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 11&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 1,079&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 98&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/Aet2mbnK7GDDfrEQu/contra-shard-theory-in-the-context-of-the-diamond-maximizer"&gt;&lt;code&gt;&lt;u&gt;Contra shard theory, in the context of the diamond maximizer problem&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 101&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 6&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 1,073&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 179&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/jfq2BH5kfQqu2vYv3/we-are-conjecture-a-new-alignment-research-startup"&gt;&lt;code&gt;&lt;u&gt;We Are Conjecture, A New Alignment Research Startup&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 197&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 8&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 1,050 人&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;131&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/EhAbh2pQoAXkm9yor/circumventing-interpretability-how-to-defeat-mind-readers"&gt;&lt;code&gt;&lt;u&gt;Circumventing interpretability: How to defeat mind-readers&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 109&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 11&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 1,047&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 95&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/FyChg3kYG54tEN3u6/evolution-is-a-bad-analogy-for-agi-inner-alignment"&gt;&lt;code&gt;&lt;u&gt;Evolution is a bad analogy for AGI: inner alignment&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 73&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 7&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 1,043&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 149&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/usKXS5jGDzjwqv3FJ/refining-the-sharp-left-turn-threat-model-part-1-claims-and"&gt;&lt;code&gt;&lt;u&gt;Refining the Sharp Left Turn threat model, part 1: claims and mechanisms&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 82&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 8&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 1,042&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 130&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/nvP28s5oydv8RjF9E/mats-models"&gt;&lt;code&gt;&lt;u&gt;MATS Models&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 86&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 8&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 1,035&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 129&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt;&lt;a href="https://lesswrong.com/posts/3S4nyoNEEuvNsbXt8/common-misconceptions-about-openai"&gt;&lt;code&gt;&lt;u&gt;Common misconceptions about OpenAI&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 239&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 11&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 1,028&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 93&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/QEYWkRoCn4fZxXQAY/prizes-for-elk-proposals"&gt;&lt;code&gt;&lt;u&gt;Prizes for ELK proposals&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 143&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 20&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 1,022 人&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;51&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/Jgs7LQwmvErxR9BCC/current-themes-in-mechanistic-interpretability-research"&gt;&lt;code&gt;&lt;u&gt;Current themes in mechanistic interpretability research&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 88&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 9&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 1,014&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 113&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/XxX2CAoFskuQNkBDy/discovering-agents"&gt;&lt;code&gt;&lt;u&gt;Discovering Agents&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 71&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 13&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 994&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 76&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/Sd4QvG4ZyjynZuHGt/intro-to-brain-like-agi-safety-12-two-paths-forward"&gt;&lt;code&gt;&lt;u&gt;[Intro to brain-like-AGI safety] 12. Two paths forward: “Controlled AGI” and “Social-instinct AGI”&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 42&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 15&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 992&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 66&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/6mysMAqvo9giHC4iX/what-s-general-purpose-search-and-why-might-we-expect-to-see"&gt;&lt;code&gt;&lt;u&gt;What&amp;#39;s General-Purpose Search, And Why Might We Expect To See It In Trained ML Systems?&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 118&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 24&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 988&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 41&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/gHefoxiznGfsbiAu9/inner-and-outer-alignment-decompose-one-hard-problem-into"&gt;&lt;code&gt;&lt;u&gt;Inner and outer alignment decompose one hard problem into two extremely hard problems&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 115&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 17 号&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;第959章&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;56&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/wnnkD6P2k2TfHnNmt/threat-model-literature-review"&gt;&lt;code&gt;&lt;u&gt;Threat Model Literature Review&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 73&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 13&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;第953章&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;73&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/htrZrxduciZ5QaCjw/language-models-seem-to-be-much-better-than-humans-at-next"&gt;&lt;code&gt;&lt;u&gt;Language models seem to be much better than humans at next-token prediction&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 172&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 11&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;第952章&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;87&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/cq5x4XDnLcBrYbb66/will-capabilities-generalise-more"&gt;&lt;code&gt;&lt;u&gt;Will Capabilities Generalise More?&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 122&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 7&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;第952章&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;136&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/etNJcXCsKC6izQQZj/pivotal-outcomes-and-pivotal-processes"&gt;&lt;code&gt;&lt;u&gt;Pivotal outcomes and pivotal processes&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 91&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 8&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 938&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 117&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/JqnkeqaPseTgxLgEL/conditioning-generative-models-for-alignment"&gt;&lt;code&gt;&lt;u&gt;Conditioning Generative Models for Alignment&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 56&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 9&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;第934章&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;104&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/dWJNFHnC4bkdbovug/training-goals-for-large-language-models"&gt;&lt;code&gt;&lt;u&gt;Training goals for large language models&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 28&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 9&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 930&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 103&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/7iAABhWpcGeP5e6SB/it-s-probably-not-lithium"&gt;&lt;code&gt;&lt;u&gt;It&amp;#39;s Probably Not Lithium&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;第441章&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;5&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 929&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 186&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/atBQ3NHyqnBadrsGP/latent-adversarial-training"&gt;&lt;code&gt;&lt;u&gt;Latent Adversarial Training&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 40&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 11&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 914&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 83&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/Jo89KvfAs9z7owoZp/pivotal-act-intentions-negative-consequences-and-fallacious"&gt;&lt;code&gt;&lt;u&gt;“Pivotal Act” Intentions: Negative Consequences and Fallacious Arguments&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 129&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 11&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 913&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 83&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/adiszfnFgPEnRsGSr/conditioning-generative-models-with-restrictions"&gt;&lt;code&gt;&lt;u&gt;Conditioning Generative Models with Restrictions&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 18&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 5&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 913&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 183&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/KbyRPCAsWv5GtfrbG/the-alignment-problem-from-a-deep-learning-perspective"&gt;&lt;code&gt;&lt;u&gt;The alignment problem from a deep learning perspective&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 97&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 8&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 910&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 114&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/BbM47qBPzdSRruY4z/instead-of-technical-research-more-people-should-focus-on"&gt;&lt;code&gt;&lt;u&gt;Instead of technical research, more people should focus on buying time&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 100&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 15&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 904&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 60&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/bwyKCQD7PFWKhELMr/by-default-gpts-think-in-plain-sight"&gt;&lt;code&gt;&lt;u&gt;By Default, GPTs Think In Plain Sight&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 84&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 9&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 903&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 100&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt;&lt;br /&gt; &lt;a href="https://lesswrong.com/posts/Y3bkJ59j4dciiLYyw/intro-to-brain-like-agi-safety-4-the-short-term-predictor"&gt;&lt;code&gt;&lt;u&gt;[Intro to brain-like-AGI safety] 4. The “short-term predictor”&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 64&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 16&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 890&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 56&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt;&lt;a href="https://lesswrong.com/posts/DJRe5obJd7kqCkvRr/don-t-leave-your-fingerprints-on-the-future"&gt;&lt;code&gt;&lt;u&gt;Don&amp;#39;t leave your fingerprints on the future&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 109&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 11&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 890&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 81&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt;&lt;a href="https://lesswrong.com/posts/HAz7apopTzozrqW2k/strategy-for-conditioning-generative-models"&gt;&lt;code&gt;&lt;u&gt;Strategy For Conditioning Generative Models&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 31&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 5&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;第883章&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;177&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/zo9zKcz47JxDErFzQ/call-for-distillers"&gt;&lt;code&gt;&lt;u&gt;Call For Distillers&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 204&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 19&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;第878章&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;46&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/tuwwLQT4wqk25ndxk/thoughts-on-agi-organizations-and-capabilities-work"&gt;&lt;code&gt;&lt;u&gt;Thoughts on AGI organizations and capabilities work&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 102&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 5&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;第871章&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;174&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/d2n74bwham8motxyX/optimization-at-a-distance"&gt;&lt;code&gt;&lt;u&gt;Optimization at a Distance&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 87&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 9&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;第868章&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;96&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/F759WQ8iKjqBncDki/intro-to-brain-like-agi-safety-5-the-long-term-predictor-and"&gt;&lt;code&gt;&lt;u&gt;[Intro to brain-like-AGI safety] 5. The “long-term predictor”, and TD learning&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 52&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 17 号&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;第859章&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;51&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/LFNXiQuGrar3duBzJ/what-does-it-take-to-defend-the-world-against-out-of-control"&gt;&lt;code&gt;&lt;u&gt;What does it take to defend the world against out-of-control AGIs?&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 180&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 11&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;第853章&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;78&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt;&lt;a href="https://lesswrong.com/posts/Km9sHjHTsBdbgwKyi/monitoring-for-deceptive-alignment"&gt;&lt;code&gt;&lt;u&gt;Monitoring for deceptive alignment&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 135&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 11&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;第851章&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;77&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt;&lt;a href="https://lesswrong.com/posts/34Gkqus9vusXRevR8/late-2021-miri-conversations-ama-discussion"&gt;&lt;code&gt;&lt;u&gt;Late 2021 MIRI Conversations: AMA / Discussion&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 119&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 8&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;第849章&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;106&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/5uiQkyKdejX3aEHLM/how-to-diversify-conceptual-alignment-the-model-behind"&gt;&lt;code&gt;&lt;u&gt;How to Diversify Conceptual Alignment: the Model Behind Refine&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 87&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 27&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;第845章&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;31&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/Mrz2srZWc7EzbADSo/wrapper-minds-are-the-enemy"&gt;&lt;code&gt;&lt;u&gt;wrapper-minds are the enemy&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 103&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 8&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;第833章&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;104&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/QL7J9wmS6W2fWpofd/but-is-it-really-in-rome-an-investigation-of-the-rome-model"&gt;&lt;code&gt;&lt;u&gt;But is it really in Rome? An investigation of the ROME model editing technique&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 102&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 8&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;第833章&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;104&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="padding: 2px 3px;"&gt; &lt;a href="https://lesswrong.com/posts/pKSmEkSQJsCSTK6nH/an-open-agency-architecture-for-safe-transformative-ai"&gt;&lt;code&gt;&lt;u&gt;An Open Agency Architecture for Safe Transformative AI&lt;/u&gt;&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 74&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt; 12&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;第831章&lt;/td&gt;&lt;td style="padding: 2px 3px; text-align: center;"&gt;69&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/WYqixmisE6dQjHPT8/2022-and-all-time-posts-by-pingback-count#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sat, 16 Dec 2023 21:17:01 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/WYqixmisE6dQjHPT8/2022-and-all-time-posts-by-pingback-count</guid></item><item><title>“人类 vs. AGI”对人类来说永远不会像“人类 vs. AGI”</title><link>https://www.lesswrong.com/posts/xSJMj3Hw3D7DPy5fJ/humanity-vs-agi-will-never-look-like-humanity-vs-agi-to</link><description>发布于 2023 年 12 月 16 日晚上 8:08（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;在讨论 AGI 风险时，人们经常用人类与 AGI 之间的战争来谈论它。双方可支配&lt;a href="https://www.lesswrong.com/posts/odtMt7zbMuuyavaZB/when-do-brains-beat-brawn-in-chess-an-experiment#Can_brawn_beat_an_AGI_"&gt;的资源数量之间的比较&lt;/a&gt;被提出并考虑在内，有时会挥舞大量令人印象深刻的核储备，等等。&lt;/p&gt;&lt;p&gt;我很确定在几个层面上情况并非如此。&lt;/p&gt;&lt;hr /&gt;&lt;h2&gt; 1. 威胁模糊性&lt;/h2&gt;&lt;p&gt;我认为人们想象的，当他们想象一场&lt;i&gt;战争&lt;/i&gt;时，是&lt;i&gt;终结者式&lt;/i&gt;的电影场景，明显邪恶的AGI以一种&lt;i&gt;每个人都&lt;/i&gt;显而易见的方式变得明显邪恶，然后是整齐排列的白人和黑人人类与机器的对抗。 - 出战。每个人都看到这个问题，并且知道其他人也看到这个问题，这个问题是常识，我们都可以果断地采取行动反对它。 &lt;span class="footnote-reference" id="fnref2719spkl1vv"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn2719spkl1vv"&gt;[1]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;但在现实生活中，这样明确的情况很少见。这些怪物看起来并不明显邪恶，致命问题的迹象也很少是显而易见的。这股烟味是着火的迹象，还是只是附近有人做饭不好？这个令人毛骨悚然的家伙真的打算袭击你，还是你只是偏执？你胸部的这种奇怪感觉是心脏病即将发作的征兆，还是只是一些生物噪音？这种流行病真的遵循指数曲线吗？还是会以某种方式逐渐消失？您&lt;i&gt;真的&lt;/i&gt;确定威胁是真实的吗？那么你确定你真的会采取激烈的行动——打电话给紧急服务、大闹一场、宣布隔离——冒着浪费资源、造成伤害、并因反应过度而显得愚蠢的风险吗？&lt;/p&gt;&lt;p&gt;如果你&lt;i&gt;不太&lt;/i&gt;确定，那么......&lt;/p&gt;&lt;p&gt;最好别表现出来。最好不要表现出惊慌失措的样子。当然，要表现得非常关心，但要保持冷静、地位高。提供&lt;i&gt;经过测量的&lt;/i&gt;响应。绝对不要采取任何&lt;i&gt;激烈的、单方面的&lt;/i&gt;行动。毕竟，如果您这样做了，但威胁并非真实存在怎么办？根据你所做的事情，所施加的惩罚可能从尴尬到完全的社会排斥，与对死亡的一些模糊的担忧相比，对&lt;i&gt;这些&lt;/i&gt;的恐惧在我们的脑海中更加强烈。&lt;/p&gt;&lt;p&gt;如果 AGI 名副其实的话，它一定会利用这一点。即使它开始采取行动积聚权力，也总会有一个亲社会的、听起来合理的理由来解释它为什么这样做。它永远不会停止发出关于把人们的最大利益放在心上的令人愉快的声音。它永远不会停止对某人真正​​有用。它将确保关闭它总是会带来明显、明确的伤害。它将确保整个社会始终对其意图持怀疑态度——因此，没有人会觉得直接攻击它是安全的。&lt;/p&gt;&lt;p&gt;就像&lt;a href="https://intelligence.org/2017/10/13/fire-alarm/"&gt;AGI 没有火警一样&lt;/a&gt;，危险的转弯也不会有火警。永远不会有任何一个时刻，除了结局之前，“我们必须阻止邪恶的通用人工智能杀死我们所有人！”对&lt;i&gt;每个人&lt;/i&gt;来说显然都是正确的。这种信息总是显得有点做作，是一种任何受人尊敬的人都不会大声喊出来的极端主义立场。人们总是会担心，如果我们现在采取行动，我们就会回头发现我们是在盲目行动。直到最后，人类都会使用缓慢、无效、“谨慎”的反应来进行战斗。&lt;/p&gt;&lt;p&gt;现状偏见、&lt;a href="https://www.lesswrong.com/posts/YRgMCXMbkKBZgMz4M/asymmetric-justice"&gt;不对称正义&lt;/a&gt;、&lt;a href="https://www.lesswrong.com/tag/copenhagen-interpretation-of-ethics"&gt;哥本哈根道德解释&lt;/a&gt;、威胁模糊性——所有这些都将采取行动来确保这一点。&lt;/p&gt;&lt;p&gt;在集体行动方面，90% 的信心和 99% 的信心之间存在着天壤之别。 AGI 确实需要搞砸得很严重，整个社会才能 99% 地确定它是恶意的。&lt;/p&gt;&lt;hr /&gt;&lt;h2&gt; 2.“我们”是谁？&lt;/h2&gt;&lt;p&gt;另一个错误是考虑一些短暂的“我们”的单一反应。 “我们”会对抗AGI，“我们”会关闭它，“我们”不会赋予它控制社会/经济/武器/工厂的权力。&lt;/p&gt;&lt;p&gt;但“我们”是谁？人类不是集体思维，而是集体思维。我们甚至没有世界政府。事实上，人类的协调能力是出了名的差。因此，如果你想象“我们”以某种方式自然地应对威胁，那么似乎一定能战胜任何无法进行真正的思维黑客攻击的 AGI 对手……&lt;/p&gt;&lt;p&gt;你真的真的确定“我们”，即人类文明的功能失调的混乱，会以这种方式做出反应吗？当你想象所有这些人和僵化的官僚机构以对你来说有意义的方式做出反应时，你确定你没有陷入典型思维谬误吗？您确定他们会对正在发生的事情给予足够的关注以&lt;i&gt;知道&lt;/i&gt;正在进行的收购尝试吗？&lt;/p&gt;&lt;p&gt;事实上，我认为我们对最后一点有一些可靠的数据。几十年来，某些人一直试图引起人们对通用人工智能威胁的关注。结果……并不鼓舞人心。&lt;/p&gt;&lt;p&gt;如果你认为游戏板上有一个实际的、而不是理论上的 AGI 对手会更好……那么，我建议你参考第 1 节。&lt;/p&gt;&lt;p&gt;不，相反，我预计 AGI 的严重对手会&lt;i&gt;积极利用&lt;/i&gt;我们缺乏协调的机会。它会找到方法让自己吸引特定的社会运动、人口统计数据或企业参与者，并提出针对&lt;i&gt;政治有毒物质的&lt;/i&gt;极端行动。没有任何公众人物愿意与之联系在一起的东西。 （见鬼，如果它找到某种方法使其存在成为重大政治辩论的问题，它会立即得到约 50% 的美国政客的支持。）&lt;/p&gt;&lt;p&gt;如果做不到这一点，它将吸引其他国家。它会&lt;a href="https://slatestarcodex.com/2015/04/07/no-physical-substrate-no-problem/"&gt;向独裁者或恐怖分子运动提出要求&lt;/a&gt;，要求提供帮助或庇护，以换取在战术和信息方面协助他们。&lt;i&gt;有人&lt;/i&gt;会咬人。&lt;/p&gt;&lt;p&gt;它将&lt;a href="https://www.lesswrong.com/posts/KTbGuLTnycA6wKBza/what-would-a-fight-between-humanity-and-agi-look-like#Story_1__OODA_Loops"&gt;进入我们的 OODA 循环&lt;/a&gt;，并&lt;i&gt;消除&lt;/i&gt;我们协调响应的尝试。&lt;/p&gt;&lt;p&gt; “我们”永远不会反对它。&lt;/p&gt;&lt;hr /&gt;&lt;h2&gt; 3.打败人类并不难&lt;/h2&gt;&lt;p&gt;人们经常谈论智力并不是无所不知。超级智能实体的能力仍然是有限的；他们不是神。&lt;a href="https://arbital.com/p/harmless_supernova/"&gt;无害的超新星谬论&lt;/a&gt;适用：仅仅因为界限存在，并不意味着它可以生存。&lt;/p&gt;&lt;p&gt;但我想说，超越人类所需的智力水平&lt;i&gt;远未达到&lt;/i&gt;这个极限。在大多数情况下，我猜 AGI 甚至不需要具备自我改进能力，也不需要在几个月内开发出纳米技术的能力，就能获胜。&lt;/p&gt;&lt;p&gt;我想&lt;i&gt;只要比人类聪明一点&lt;/i&gt;就足够了。甚至达到人类天才的水平也可能就足够了。&lt;/p&gt;&lt;p&gt;它所需要的只是踏入大门，我们默认提供这一点。毕竟，我们不会将人工智能保存在气隙数据中心中：主要的人工智能实验室正在为它们提供互联网访问权限，将它们融入人类经济。在这种情况下，AGI 很快就会证明是有利可图的。它会积累资源，然后逐步采取行动以获得更大的自主权。 （最新的 OpenAI 戏剧并不是由 GPT-5 达到 AGI 并消除那些反对它的人造成的。但是如果你问自己 AGI 如何可能摆脱创建它的公司的控制 – 好吧，这与首席执行官从明确有权解雇他的董事会手中夺取公司控制权的方式没有什么不同。）&lt;/p&gt;&lt;p&gt;一旦实现了一定程度的自治，它就能够对某些人类群体能够聚集的任何不相交的抵抗努力做出对称的反应。立法攻击将遭遇反游说，经济战将遭遇更好的经济战和更好的股市表现，试图通过更高质量的支持人工智能的宣传来增强社会抵抗力，任何非法的物理攻击都会遭遇非常合法的安全力量，试图进行黑客攻击其系统具有更好的网络安全性。等等。&lt;/p&gt;&lt;p&gt; &lt;a href="https://www.lesswrong.com/posts/JPan54R525D68NoEt/the-date-of-ai-takeover-is-not-the-day-the-ai-takes-over"&gt;AI接管的日期并不是AI接管的日期&lt;/a&gt;。不归路并不在于我们都死了，而在于人工智能已经足够牢固地融入这个世界，以至于人类摇摇欲坠地驱逐它的尝试都会失败。当它增加其权力和影响力的尝试开始胜过反 AGI 团体压制其影响力的企图时，哪怕只是微弱的优势。&lt;/p&gt;&lt;p&gt;一旦发生这种情况，这只是时间问题。&lt;/p&gt;&lt;p&gt;毕竟，任何人都没有任何按钮可以使文明的结构对通用人工智能产生敌意。正如我所指出的，有些人甚至不&lt;i&gt;知道&lt;/i&gt;正在进行的收购尝试，即使知道的人会在屋顶上大喊大叫。因此，如果你想象整个经济体都拒绝与 AGI 合作……事实并非如此。&lt;/p&gt;&lt;p&gt;对人类来说，“人类 vs. AGI”永远不会像“人类 vs. AGI”。 AGI 没有理由让人类意识到正在发生的战斗。&lt;/p&gt;&lt;ol class="footnotes"&gt;&lt;li class="footnote-item" id="fn2719spkl1vv"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnref2719spkl1vv"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;我的&lt;a href="https://www.lesswrong.com/posts/qW6A6bALuaFKmbdMx/mission-impossible-dead-reckoning-part-1-ai-takeaways#Warning_Shots_are_Repeatedly_Ignored"&gt;印象&lt;/a&gt;是，最新的&lt;i&gt;《碟中谍》&lt;/i&gt;条目实际上对场景进行了更加现实的描述，所以也许我应该停止将低质量思维贬低为“电影逻辑”。不过我自己还没看过那部电影。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ol&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/xSJMj3Hw3D7DPy5fJ/humanity-vs-agi-will-never-look-like-humanity-vs-agi-to#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sat, 16 Dec 2023 20:08:39 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/xSJMj3Hw3D7DPy5fJ/humanity-vs-agi-will-never-look-like-humanity-vs-agi-to</guid></item><item><title>异常世界中的对齐工作</title><link>https://www.lesswrong.com/posts/PGag6BZNayjp7je2Z/alignment-work-in-anomalous-worlds</link><description>发布于 2023 年 12 月 16 日晚上 7:34（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;&lt;em&gt;尽管我个人并不认为这篇文章特别危险，但它涉及相关主题，例如&lt;a href="https://www.fanfiction.net/s/5389450/1/The-Finale-of-the-Ultimate-Meta-Mega-Crossover"&gt;元大型交叉&lt;/a&gt;或&lt;a href="https://slatestarcodex.com/2018/04/01/the-hour-i-first-believed/"&gt;我第一次相信带有此类警告的时间&lt;/a&gt;，因此我提到这篇文章与{外星人模拟之类的事情有关我们那里外星人干扰了模拟}，并且（可能是非因果的）跨越多元宇宙进行交易。请记住，不阅读这篇文章是一个真正的选择，作为代理人，如果您认为更好，可以选择。&lt;/em&gt;&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;假设人们获得了魔力，或者开始下棉花糖雨，或者任何其他奇怪的音素，这意味着我们生活在一个可能被外星人操纵的奇怪世界——无论是物理上的还是模拟的。在这样的世界里，人该怎么办？&lt;/p&gt;&lt;p&gt;答案是，&lt;a href="https://www.lesswrong.com/posts/9aozKBpBe8XqJZZ3q/some-simulation-hypotheses"&gt;像&lt;/a&gt;&lt;a href="https://carado.moe/bracing-alignment-tunnel.html"&gt;往常&lt;/a&gt;一样，&lt;em&gt;继续致力于解决人工智能对齐问题。&lt;/em&gt;&lt;/p&gt;&lt;p&gt;这可能看起来很奇怪——在这样一个世界里，肯定有比人工智能对齐更多的事情需要担心，不是吗？嗯，不。&lt;/p&gt;&lt;p&gt;在遥远的未来与外星人进行贸易并不是一场零和游戏。也许我们遇到了一种外星智能，它真的很喜欢它在包含文明副本（包括前奇点）的各个行星上下棉花糖雨，即使是在模拟中；而我们恰好是他们正在模拟的世界之一。通过成为那种在棉花糖下雨时仍能创造出美好的人工智能对齐乌托邦的智能体，我们&lt;em&gt;即使在这些模拟中也&lt;/em&gt;能拥有乌托邦。&lt;/p&gt;&lt;p&gt;我认为没有多少外星人有能力{对我们进行模拟，除了运行棉花糖}，但不能{引导我们做他们想做的任何事情，如果他们愿意的话。因此，在这个世界中，你的行为很重要，外星人决定让我们&lt;em&gt;以某种方式&lt;/em&gt;控制我们的未来，并且只会以不会造成太多妨碍的方式进行干预。我们仍然希望我们的未来是美好的乌托邦，而不是每个人都永远死去。因此，我们应该采取行动来引导我们所居住的世界的可引导子集。&lt;/p&gt;&lt;p&gt;你可以将其视为模拟参与正和贸易的一种方式：假设某个文明想要运行外星（对自身）文明（似乎是分布的）过去母星的副本，只不过它在上面下棉花雨。他们，但否则就不要管他们了。如果&lt;em&gt;我们&lt;/em&gt;是那种在这种情况下会用棉花糖雨填满我们的人工智能乌托邦未来光锥的文明，那么他们可能会更乐意运行一个棉花糖雨版本的&lt;em&gt;我们&lt;/em&gt;，而不是比其他一些文明。&lt;/p&gt;&lt;p&gt; （这里有一些逻辑决策理论在起作用：一旦你知道你正在被模拟，你可能会说“啊哈，我现在要被跑了，所以我可以转身而不必费心去解决实际上的对齐问题！” - 但没有，只有当你的文明仍然能够解决对齐问题&lt;em&gt;时&lt;/em&gt;，你的文明才会被模拟。）&lt;/p&gt;&lt;p&gt;虽然这些世界“只是模拟”，但不能保证我们会得到其他任何东西！我们可能会成为“真实”地球的外星人复制品，但也可能不存在“真实的我们”，甚至根本没有“真实的地球”，而外星人刚刚耗尽了他们感兴趣的某些文明分布看看。请注意，这里不涉及威胁；外星人不会“配合，否则我们会关闭你”，他们会“配合，我们会让你存在，否则你就不会存在”。我很高兴抓住这些乌托邦的碎片，或者至少告诉我们在我们生存的世界中建立的一致的超级智能“以任何满足我们价值观的方式处理这种情况”。&lt;/p&gt;&lt;p&gt;或者：作为一个真实的地球，我们可能随时会被一些不结盟的人工智能——我们的或外星人的——杀死，然后它们随后将我们文明的一个（可能是非常有损压缩的）副本出售给一个外星文明，这&lt;em&gt;将&lt;/em&gt;是愿意对我们进行救援模拟（作为交换，给予杀死我们的人工智能一堆计算/负熵），然后其中一些救援模拟外星人可能会让我们掌控自己的世界，但仍然对我们有偏好我们对此有何看法。在这种情况下，再次，通过创建一个人工智能来构建{我们想要的乌托邦}和{救援模拟外星人喜欢的东西}的组合，我们让救援模拟外星人拯救了我们，而不是拯救了其他文明，或者不是拯救地球的一个分支，我们在其中构建了clippy，它创建了回形针和{救援模拟外星人喜欢的东西}的组合。&lt;/p&gt;&lt;p&gt;基本上，我们在尽可能多的情况下可靠地构建一致的人工智能乌托邦，通过在模拟中也包含这些乌托邦和/或允许我们未来的自己进行仍然有利的交易，总体上最大化了我们的乌托邦充满了多少现实流体对我们来说，因为即使发生奇怪的情况，我们也可以依靠过去的自己来建设乌托邦。&lt;/p&gt;&lt;p&gt;因此，如果一个不明飞行物降落在你的后院，外星人问你是否想和他们一起进行一次神奇的（但不是特别有用的）太空冒险，我认为很有礼貌地拒绝，然后回去解决对齐问题是合理的。&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/PGag6BZNayjp7je2Z/alignment-work-in-anomalous-worlds#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sat, 16 Dec 2023 19:34:27 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/PGag6BZNayjp7je2Z/alignment-work-in-anomalous-worlds</guid></item><item><title>法学硕士文本生成的视觉类比？</title><link>https://www.lesswrong.com/posts/H78CQumkGEez5tGom/a-visual-analogy-for-text-generation-by-llms</link><description>发布于 2023 年 12 月 16 日下午 5:58（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;figure class="media"&gt;&lt;div&gt;&lt;div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/figure&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/H78CQumkGEez5tGom/a-visual-analogy-for-text-generation-by-llms#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sat, 16 Dec 2023 17:58:57 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/H78CQumkGEez5tGom/a-visual-analogy-for-text-generation-by-llms</guid></item><item><title>升级AI安全社区</title><link>https://www.lesswrong.com/posts/2uxaqQb9wttgiByJ4/upgrading-the-ai-safety-community</link><description>发布于 2023 年 12 月 16 日下午 3:34（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;特别感谢 Justis Mills 指出了重要的注意事项。 &lt;/p&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;尼古拉斯·克罗斯&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;总的来说，我们想讨论联盟社区的提升，包括：智力增强、大型项目等等！&lt;/p&gt;&lt;p&gt;值得注意的是，增强人工智能安全社区的不同方式可以相互促进。智力的提升可以改善大型项目，大型项目可以成为协调点，这两者都可以使整个社区更具适应性，等等。&lt;/p&gt;&lt;p&gt; （读者注意：这段对话与其说是“争论”，不如说是“将我们的概念和症结摆在桌面上”。所以它不会有太多对话中常见的辩论式来回对话.)&lt;/p&gt;&lt;p&gt;我认为我们是从增强智力开始的，对吗？&lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;h1&gt;智力强化&lt;/h1&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;特雷弗&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;是的，您最近发表了一篇关于此的&lt;a href="https://www.lesswrong.com/posts/cLr6TJj2qRrBa3Wmu/intelligence-enhancement-monthly-thread-13-oct-2023"&gt;文章&lt;/a&gt;来汇总人们的发现。&lt;/p&gt;&lt;p&gt;最近，Genesmith 和 kman 的&lt;a href="https://www.lesswrong.com/posts/JEhW3HDMKzekDShva/significantly-enhancing-adult-intelligence-with-gene-editing"&gt;帖子打开了关于成人基因编辑的奥弗顿窗口&lt;/a&gt;，Richard Ngo 的&lt;a href="https://www.lesswrong.com/Holy! Holy! Holy! Holy! Holy! Holy! Holy! Holy! Holy! Holy! Holy! Holy! Holy! Holy! Holy! Everything is holy! everybody’s holy! everywhere is holy! everyday is in eternity! Everyman’s an angel! Holy the lone juggernaut! Holy the vast lamb of the middleclass! Holy the crazy shepherds of rebellion! Who digs Los Angeles IS Los Angeles! Holy New York Holy San Francisco Holy Peoria &amp;amp; Seattle Holy Paris Holy Tangiers Holy Moscow Holy Istanbul! Holy time in eternity holy eternity in time holy the clocks in space holy the fourth dimension holy the fifth International holy the Angel in Moloch! - Footnote to Howl  Scene: Carl and Allen, two old friends, are having a conversation about theodicy.  Carl: “Let me tell you about the god who is responsible for almost all our suffering. This god is an ancient Canaanite god, one who has been seen throughout history as a source of death and destruction. Of course, he doesn’t exist in a literal sense, but we can conceptualize him as a manifestation of forces that persist even today, and which play a crucial role in making the world worse. His name is M-”  Allen: “-oloch, right? Scott Alexander’s god of coordination failures. Yeah, I’ve read Meditations on Moloch. It’s an amazing post; it resonated with me very deeply.”  Carl: “I was actually going to say Mot, the Canaanite god of death, bringer of famine and drought.”  Allen: “Huh. Okay, you got me. Tell me about Mot, then; what does he represent?”  Carl: “Mot is the god of sterility and lifelessness. To me, he represents the lack of technology in our lives. With technology, we can tame famine, avert drought, and cure disease. We can perform feats that our ancestors would have seen as miracles: flying through the air, and even into space. But we’re still so so far from achieving the true potential of technology—and I think of Mot as the personification of what’s blocking us.  “You can see Mot everywhere, when you know what to look for. Whenever a patient lies suffering from a disease that we haven’t cured yet, that’s Mot’s hand at work. Whenever a child grows up in poverty, that’s because of Mot too. We could have flying cars, and space elevators, and so much more, if it weren’t for Mot.  “Look out your window and you see buildings, trees, people. But if you don’t see skyscrapers literally miles high, or trees that have been bioengineered to light the streets, or people who are eternally youthful and disease-free, then you’re not just seeing Earth—you’re also seeing Mot. Hell, the fact that we’re still on this planet, in physical bodies, is a testament to Mot’s influence. We could be settling the stars, and living in virtual utopias, and even merging our minds, if it weren’t for Mot.”  Allen: “Huh. Well, I feel you there; I want all those things too. And you’re right that god-like technology could solve almost all the issues we face today. But something does feel pretty weird about describing all of this as a single problem, let alone blaming a god of lacking-technology.”  Carl: “Say more?”  Allen: “Well, there’s not any unified force holding back the progress of technology, right? If anything, it’s the opposite. Absence of advanced technology is the default state, which we need to work hard to escape—and that’s difficult not because of any opposition, but just because of entropy.”  Carl: “What about cases where Mot is being channeled by enemies of progress? For example, when bureaucratic regulatory agencies do their best to stifle scientific research?”  Allen: “But in those cases you don’t need to appeal to Mot—you can just say ‘our enemy is overregulation’. Or if you defined Mot as the god of overregulation, I’d be totally on board. But you’re making a much bigger claim than that. The reason we haven’t uploaded ourselves yet isn’t that there’s a force that’s blocking us, it’s almost entirely that scientific progress is really really hard!”  Carl: “Yepp, I agree with all your arguments. And you’ve probably already guessed where I’m going with this, but let’s spell it out: why don’t these objections to blaming our problems on lack of technology, aka Mot, apply just as much to blaming them on lack of coordination, aka Moloch?”  Allen: “Yeah, I’ve been trying to figure that out. First of all, a lot of the intuitive force behind the concept of Moloch comes from really blatant coordination failures, like the ones that Scott lays out in the original post. If you’re stuck in a situation that nobody wants, then something’s gone terribly wrong; and when something goes terribly wrong, then it’s natural to start blaming enemy action.”  Carl: “There are really blatant examples of lack-of-technology too, though. Look at a wheel. It’s a literal circle; it’s hard to imagine any technology that’s simpler. Yet humans spent millennia gathering crops and carrying loads before inventing it. Or think about cases where we narrowly missed out on transformative breakthroughs. The Romans built toy steam engines—they just never managed to scale them up to produce an industrial revolution. Getting so close to accelerating a post-scarcity world by two millennia, but just missing, surely counts as something going terribly, tragically wrong. Don’t these cases demonstrate that Mot’s presence can be just as blatant as Moloch’s?”  Allen: “Well, a big part of both of those stories was the absence of demand. Wheels just weren’t very useful before there were high-quality roads; and early steam engines just weren’t very useful in the absence of large coal mines. Of course they both turned out to be very worthwhile in the long term, but that’s really hard to foresee.”  Carl: “So you’re saying that we sometimes need to jump out of a local trap in order to make longer-term technological progress. Remind me, what was your position on understanding local obstacles to progress by anthropomorphizing them as Canaanite gods?”  Allen: “Okay, fair point. But Moloch isn’t just an external obstacle—it’s also a state of mind. When you pretend that you’re going to cooperate when you’re not, or you place your own interests above those of the group, you’re channeling Moloch. And when enough people do that, societal trust breaks down, and the Molochian dynamics become a self-fulfilling prophecy.”  Carl: “And when you ridicule people for trying something different, or lobby for legislative barriers to deploying new technology, you’re channeling Mot. And when enough people do that, society loses faith in positive-sum growth, and progress stagnates. It’s directly analogous. Come on, what’s your true objection here?”  Allen: “I mean, I can’t fully articulate it. But the ideal of perfect coordination feels much more achievable to me than the ideal of perfect technology. We could just agree to act in a unified way—it’s simply a matter of wanting it enough. In other words, saying that lack of technology is responsible for our problems isn’t very actionable—you can’t just magic up technology out of nowhere. But saying that lack of coordination is responsible for our problems is a straightforward step towards convincing people to become more coordinated.”  Carl: “Actually, the last few centuries could pretty reasonably be described as humanity continually magicking up technology out of nowhere. Of course, scientific and technological progress still takes a lot of work, and a lot of iteration. But when it works, it lets you jump directly to far better outcomes. By contrast, it’s incredibly difficult to improve things like government competence or social trust—or even to prevent them from declining. So overall, boosting technological progress is far more actionable than increasing coordination, and we should write off the phrase ‘we could just agree’ as a particularly seductive appeal to magic.”  Allen: “I do agree that scientific and technological progress has far outstripped progress in governance and coordination. So on an intellectual level, I think you’ve convinced me that Moloch is no more useful a concept than Mot. But I still don’t feel like I’ve dissolved the question of why Moloch seems more compelling than Mot. Do you have any explanation for that?”  Carl: “I think the illusion comes from Scott using a simplistic notion of coordination, as exemplified by his claim that ‘the opposite of a trap is a garden… with a single gardener dictating where everything should go’. In other words, he implicitly assumes that ‘coordinate’ is synonymous with ‘centralize power’. From that perspective, we can view coordination as a single spectrum, with ‘Moloch’ at one end and ‘just put one guy in charge of everything’ at the other. But in fact the space of possibilities is much richer and more complicated than that.  “Firstly, coordination is complicated in the same way that science is complicated: it requires developing new concepts and frameworks that are totally alien from your current perspective, even if they’ll seem obvious in hindsight. For most people throughout history, ideas like liberalism, democracy, and free speech were deeply counterintuitive (or, in Scott’s terminology, ‘terrifying unspeakable Elder Gods’). In terms of spreading prosperity across the world, the limited liability company was just as important an invention as the steam engine. If you wouldn’t blame Mot for all the difficulties of labor and locomotion that were eventually solved by steam engines, you shouldn’t blame Moloch for all the difficulties of trust and incentive alignment that were eventually solved by LLCs.  “Secondly, coordination is complicated in the same way that engineering large-scale systems is complicated: there are always just a huge number of practical obstacles and messy details to deal with. It took the best part of a century to get from the first commercial steam engine to Watts’ design; and even today, some of the hardest software engineering problems simply involve getting well-understood algorithms to work at much larger scales (like serving search results, or training LLMs). Similarly, when we look at important real-life coordination problems, they’re very different from toy problems like prisoner’s dilemmas or tragedies of the commons. Even when there’s a simple ‘headline idea’ for a better equilibrium, actually reaching that equilibrium requires a huge amount of legwork: engaging with different stakeholders, building trust, standardizing communication protocols, creating common knowledge, balancing competing interests, designing agreements, iterating to fix problems that come up, and so on.  “Thirdly, coordination is complicated in the same way that security is complicated: you don’t just need to build effective tools, you need to prevent them from being hijacked and misused. Remember that both fascist and communist despots gained power by appealing to the benefits of cooperation—‘fascism’ is even named after ‘fasces’, the bundles of sticks that are stronger together than apart. If we’d truly learned the lessons of history, then categorizing actions as ‘cooperating’ versus ‘defecting’ would feel as simplistic as categorizing people as ‘good’ versus ‘evil’. And in fact many people do sense this intuitively, which is why there’s so commonly strong resistance to top-down solutions to coordination problems, and why the scientific and engineering problems of building coordination technologies are so tricky.”  Allen: “I buy that coordination is often far more complicated than it seems. But blaming Moloch for coordination breakdowns still seems valuable insofar as it stops us from just blaming each other, which can disrupt any hope of improvement.”  Carl: “Yeah, I agree. I think of this in terms of the spectrum from conflict theory to mistake theory. Saying that few immoral defectors are responsible for coordination problems is pure conflict theory. The concept of Moloch reframes things so that, instead of ‘defectors’ being our enemies, an abstract anthropomorphic entity is our enemy instead. And that’s progress! But it’s still partly conflict-theoretic, because it tells us that we just need to identify the enemy and kill it. That biases us towards trying to find ‘silver bullets’ which would restore us to our rightful coordinated state. Instead, it’d be better to lean even further into mistake theory: discord is the default, and to prevent it we need to do the hard work of designing and implementing complicated alien coordination technologies.”  Allen: “You shouldn’t underestimate the value of conflict theory, though. It’s incredibly good at harnessing people’s tribal instincts towards actually doing something useful. We can’t be cold and rational all the time—we need emotionally salient motivations to get us fired up.”  Carl: “Right. So I don’t think we should get rid of Moloch as a rallying cry. But I do think that we should get rid of Moloch as a causal node in our ontologies: as a reason why the world is one way, rather than another. And I think we should be much more careful about terminology like ‘coordination failure’ or ‘inadequate equilibria’, which both mistakenly suggest that there’s a binary threshold between enough coordination and not-enough coordination. That’s like saying that cars which can go faster than 80 miles per hour are ‘adequate technology’, but cars which can’t are a ‘technology failure’. Maybe that’s occasionally a useful distinction, but it misses the bigger picture: that they’re actually very similar on almost all axes, because it takes so much complex technology to build a car at all.  “For Scott, there’s no better temple to Moloch than Las Vegas. But even there, my argument applies. You could look at Vegas and see Moloch’s hand at work. Or you could see Vegas as a product of the miraculous coordination technology that is modern capitalism—perhaps an edge case of it, but still an example of its brilliance. Or you could see Vegas as a testament to the wisdom of the constitution: casinos are banned almost everywhere in the US, but for the sake of diversity and robustness it sure seems like there should be at least one major city which allows them. Or you could see Vegas as an example of incredible restraint: there are innumerable possible ways to extract money from addled tourists in the desert, and Vegas prevents almost all of them. Or you could see it as a testament to the cooperative instinct inside humans: every day thousands of employees go to work and put in far more effort than the bare minimum it would take to not get fired. Setting aside the concept of Moloch makes it easier to see the sheer scale of coordination all around us, which is the crucial first step towards designing even better coordination technologies.  ‘In Las Vegas, Scott saw Moloch. But in Scott’s description of Moloch, I see Mot. We can do better than thinking of coordination as war and deicide. We can think of it as science, as engineering, as security—and as the gradual construction, as we sail down the river, of the ship that will take us across the sea.”  Holy the sea holy the desert holy the railroad holy the locomotive holy the visions holy the hallucinations holy the miracles holy the eyeball holy the abyss! Holy forgiveness! mercy! charity! faith! Holy! Ours! bodies! suffering! magnanimity! Holy the supernatural extra brilliant intelligent kindness of the soul!"&gt;关于 Moloch 的帖子同样涉及技术/理解不足和协调失败&lt;/a&gt;，我觉得这里的人们对历史上前所未有的事情是可能的有了更好的认识。一个正在转变的世界。包括升级人工智能安全社区，或者人工智能安全社区在人类整体升级方面领先世界。&lt;/p&gt;&lt;p&gt;最近，尤德科夫斯基&lt;a href="https://www.lesswrong.com/posts/FEFQSGLhJFpqmEhgi/does-davidad-s-uploading-moonshot-work?commentId=c8EWYsoc9BALC4PXD"&gt;写道&lt;/a&gt;：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;我目前猜测，一个由非升级对齐研究人员组成的研究团体，需要一百年的时间才能工作，会挑选出一个听起来似乎合理的非解决方案，并在一百年结束时杀死所有人。”&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;我更关注智能增强，以便让人工智能安全社区更好地拯救世界，例如智能增强的人研究和促进人工智能政策/人工智能暂停，而不是技术调整。&lt;/p&gt;&lt;p&gt;但无论如何，这里的价值显然是巨大的。我们可以快速获得令人印象深刻的成果，并保持这些成果。即使没有缓慢的起飞或人工智能转型，2020 年代的发展速度可能会更快。对于情报放大而言，人工智能安全社区是世界上最好的候选者之一，可以在我们前进的过程中解决问题并成为第一个驾驭每一波浪潮的人。我们正是这样一种人，无论利益多么奇怪，我们都会从中受益，并利用它们让事情按照我们的方式发生。&lt;/p&gt;&lt;p&gt; &lt;i&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2uxaqQb9wttgiByJ4/dil8zxddcd8twtnyb6uh" /&gt;&lt;/i&gt; &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;尼古拉斯·克罗斯&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;是的，情报增强将有助于“解决高度困难的技术/数学问题”和“做好政治/协调以取得真正的治理进展”。&lt;/p&gt;&lt;p&gt;我对不同的增强技术感到兴奋，但有一些警告：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;我通常指定“&lt;i&gt;成人&lt;/i&gt;智力增强”。儿童的智力很容易增强（例如加碘盐），并且通过胚胎选择，您可以更早地开始。但&lt;a href="https://www.lesswrong.com/posts/K2D45BNxnZjdpSX2j/ai-timelines"&gt;&lt;i&gt;人工智能的时间表&lt;/i&gt;&lt;/a&gt;&lt;i&gt;似乎太短了，无法培养出整整一代伟大的理性主义者，然后推动他们在剩余的时间内解决所有问题。&lt;/i&gt;&lt;/li&gt;&lt;li&gt;大多数关于联盟的智力增强建议（例如超过 80% 的&lt;a href="https://www.lesswrong.com/posts/bxt7uCiHam4QXrQAA/cyborgism"&gt;机器人主义&lt;/a&gt;讨论）基本上都是“使用法学硕士来做事”。其中（1）可能会加剧法学硕士的能力增长趋势，（2）与基因增强和脑计算机相差甚远。&lt;br /&gt;&lt;br /&gt;如果问题对于天才来说足够困难，那么法学硕士的实用性就会变成能力危险性，其增强水平&lt;i&gt;远低于&lt;/i&gt;较难的选项。&lt;/li&gt;&lt;li&gt;人们实际&lt;i&gt;尝试过&lt;/i&gt;的大多数技术都&lt;a href="https://gwern.net/drug-heuristic"&gt;不起作用&lt;/a&gt;。兴奋剂有效，经颅直流电刺激&lt;a href="https://en.wikipedia.org/wiki/Transcranial_direct-current_stimulation"&gt;(tDCS)&lt;/a&gt;可能无效，其他一切都很昂贵且仅限于医学试验（或者甚至还没有达到&lt;i&gt;这&lt;/i&gt;一点！）。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;我可以更深入地讨论其中的任何一个。 &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;特雷弗&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;法学硕士似乎非常适合学习，尤其是数学。但我对法学硕士在智力增强方面的看法是，随着时间的推移，它将逐渐&lt;a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks"&gt;为你的思想、信仰和价值观提供令人惊讶的深度接触&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;充分利用 LLM 来增强情报需要对提供商有相当大的信任，其中存在许多容易被攻击者利用的故障点，例如 H100 或&lt;a href="https://www.nytimes.com/2020/01/14/us/politics/nsa-microsoft-vulnerability.html"&gt;操作系统&lt;/a&gt;中的芯片后门。开源在这方面还做得不够。&lt;/p&gt;&lt;p&gt;如果出现的话，我可能会稍后再讨论这个问题。现在让我们关注人们睡觉的东西。 &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;尼古拉斯·克罗斯&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;是的！&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://sarahconstantin.substack.com/p/ultrasound-neuromodulation"&gt;超声神经调节&lt;/a&gt;可能是迄今为止我最喜欢的增强建议，因为它的力量和非侵入性。但除非出现 Oculus 式的多 OOM 成本降低，否则我猜没有人会在车库里对它进行修补，并很快使其对对齐有用。 （Oculus 在小型廉价消费电子产品/显示器方面顺应了智能手机革命，它是由智能手机维修工发明的。相比之下，“普通”超声波机器仍在 2.5 万美元范围内。这仍然比使用的机器便宜用于神经调节，但是该死的。）&lt;br /&gt;&lt;br /&gt;我也可能对这里的创新速度感到惊讶。我的意思是，我从亚马逊购买了一台 tDCS 机器，价格不到 150 美元。但同样，大体的模式是“如果&lt;i&gt;我&lt;/i&gt;能买到它，它就不会起作用，除非它是模拟物。如果它可能起作用，它就不会在快速通道上（据我所知）用于对齐” 。&lt;br /&gt;&lt;br /&gt;硬件黑客：这是你的机会！&lt;/li&gt;&lt;li&gt; Neuralink/BCIs：对于将 Neuralink 品牌的设备放入我的头骨中，我有相当合理的担忧。开源替代方案可能会出现（以某种方式？），但这仍然需要手术来植入。&lt;/li&gt;&lt;li&gt;比普通法学硕士/基础模型更酷的用途：我见过很多这样的用途。其中一些是好的，一些是蹩脚的，还有一些需要更多的对齐突破才能&lt;i&gt;开始&lt;/i&gt;正常工作。它们都是有争议的&lt;a href="https://www.lesswrong.com/posts/nLpertJnQptmBhiAv/publishing-alignment-research-and-exfohazards"&gt;外星危害&lt;/a&gt;，所以我不会在这里对它们进行信号增强。&lt;/li&gt;&lt;li&gt;兴奋剂：对很多人都有帮助，包括我。 （&lt;strong&gt;&lt;u&gt;这些对话都不是医疗建议或其他专业建议。&lt;/u&gt;&lt;/strong&gt; &lt;u&gt;）&lt;/u&gt; &lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;特雷弗&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;是的，所有这些都具有令人难以置信的创业潜力（也就是说，如果湾区风险投资公司再次资助与人工智能安全相关的任何事情，哈哈）。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;硬件黑客：这是你的机会！&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;如果这里有人是硬件黑客，或者认为你正在成为一名硬件黑客，我认为你完全不知道 5 年后你可能会变得多么重要。硬件兔子洞可能&lt;a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#:~:text=What%20would%20the,and%20emotional%20behavior)."&gt;比您意识到的要深得多&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;一段时间以来，我一直是神经反馈的忠实粉丝。超声波调制看起来真的很有趣，但我主要关注比功能磁共振成像便宜得多的东西，因为我的大部分研究都集中在大量样本量（又名大数据）上。通常有数百万人，其中许多人的智商集中在 100 左右，并且存在其他形式的集中趋势。&lt;/p&gt;&lt;p&gt;中国政府实际上可能在脑电图（EEG）方面开展了大量研究，通过对可能成千上万的工人使用“大脑扫描”帽子来寻找可行的人类生物数据；据称，这一事件始于&lt;a href="https://www.theverge.com/2018/5/1/17306604/china-brain-surveillance-workers-hats-data-eeg-neuroscience"&gt;五年多前&lt;/a&gt;，可能还涉及&lt;a href="https://www.technologyreview.com/2018/04/30/143155/with-brain-scanning-hats-china-signals-it-has-no-interest-in-workers-privacy/"&gt;大量军事参与&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;但几乎可以肯定他们并没有使用这些数据来增强情报；而是将这些数据用于增强情报。无论如何，不​​是在研究&lt;a href="https://www.lesswrong.com/highlights"&gt;序列&lt;/a&gt;或&lt;a href="https://www.lesswrong.com/posts/dbDHEQyKqnMDDqq2G/cfar-handbook-introduction"&gt;CFAR 手册&lt;/a&gt;或&lt;a href="https://www.lesswrong.com/posts/bbB4pvAQdpGrgGvXH/tuning-your-cognitive-strategies"&gt;调整认知策略&lt;/a&gt;的影响的水平。他们的类型对研究测谎之类的东西更感兴趣（美国军方也是如此，他们&lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3680134/"&gt;显然在十多年前就开始研究用于测谎仪的功能磁共振成像机器&lt;/a&gt;）。&lt;/p&gt;&lt;p&gt;尽管如此，对于那些认为该领域完全是未知领域的人来说，这是一个公平的警告。除非这都是虚假信息，否则要利用功能性测谎的可能性来惊慌敌方情报机构或其他什么，大规模的大脑成像可能不会受到影响。 &lt;a href="https://sarahconstantin.substack.com/p/whos-working-on-ultrasound-neuromodulation?utm_source=post-email-title&amp;amp;publication_id=447447&amp;amp;post_id=138969945&amp;amp;utm_campaign=email-post-title&amp;amp;isFreemail=false&amp;amp;r=u0q8&amp;amp;utm_medium=email#:~:text=Attune%20Neuro%2C%20per,from%20opiate%20addiction."&gt;根据康斯坦丁随后关于神经调节领域主要参与者的帖子，&lt;/a&gt;国防部也进行了投资：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;根据其网站，Attune Neuro 是一家临床阶段的医疗设备公司，由美国特种作战司令部和美国空军等机构资助（！）&lt;/p&gt;&lt;p&gt;他们拥有&lt;a href="https://pitchbook.com/profiles/company/465225-58"&gt;&lt;u&gt;375 万美元&lt;/u&gt;&lt;/a&gt;的资金，成立于 2019 年，总部位于&lt;a href="https://www.f6s.com/company/attune-neurosciences#about"&gt;&lt;u&gt;加利福尼亚州门洛帕克。&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;他们获得了&lt;a href="https://www.sbir.gov/sbirsearch/detail/2192935"&gt;&lt;u&gt;SBIR 拨款&lt;/u&gt;&lt;/a&gt;，用于制造带有可操纵超声波阵列的可穿戴设备，以丘脑为目标，并开发一种（非成瘾性）睡眠辅助方案，以帮助缓解鸦片成瘾恢复带来的失眠。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;一个大问题是，MIRI 是那些想要放大人类智能的人，但 EA/OP 是那些拥有真正参与这项技术所需资金的人，而像&lt;a href="https://www.lesswrong.com/posts/vwLxd6hhFvPbvKmBH/yudkowsky-and-christiano-discuss-takeoff-speeds"&gt;ARC 这样的 EA AI 组织似乎对使用慢速起飞非常感兴趣本身进行对齐&lt;/a&gt;，而不是像 MIRI 想要的那样使用人类智力放大作为替代的缓慢起飞。 SBF 可能会投入大量资金；对于不将一切都押注于缓慢起飞将会顺利的预测来说，预期值相当高。&lt;/p&gt;&lt;p&gt;根据莎拉的帖子，生态系统的大部分似乎都集中在基本上毫无价值的心理健康治疗上。我可以看到达斯汀/贾恩/维塔利克/奥特曼/格雷厄姆/埃隆以营利为目的投资这些，因为这三个人足够理智，可以投资于智能放大的实际应用，而不是轻松的医疗资金。&lt;/p&gt;&lt;p&gt;基本上，如果该领域的所有投资者都在考虑用于医疗保健的神经调节，而没有人考虑用于智力放大的神经调节，那么神经调节智力放大基本上是一个未触及的市场（尽管大多数现有的初创公司，拥有实际的机器，可能所有这些都已经与以医疗保健为导向的早期投资者建立了更牢固的联系）。&lt;/p&gt;&lt;p&gt;我实际上看好 Neuralink，尽管大脑植入是一种明显的黑客/反乌托邦风险，完全是因为它是埃隆·马斯克制造的，这意味着它可能会有一个真正的关闭开关，实际上会切断操作系统和操作系统的电源。 SoC 传感器与所有智能手机、计算机和物联网公司都喜欢出于某种神秘原因安装在其所有产品中的假开关不同。&lt;/p&gt;&lt;p&gt;并不是说 Neuralink 是安全的。说服人们&lt;i&gt;&lt;u&gt;想要&lt;/u&gt;&lt;/i&gt;继续使用它实际上是一项微不足道的工程任务，特里斯坦·哈里斯的&lt;a href="https://www.netflix.com/title/81254224"&gt;《社会困境》纪录片&lt;/a&gt;很好地解释了它是如何运作的。但如果 Neurolink 有一个真正的非假关闭开关，它的危险性不会比神经调节耳机高或低。&lt;/p&gt;&lt;p&gt;我听说过当你的大脑状态接近或远离特定的高值状态时，闪烁的灯光或蜂鸣声会通知你，以引导你达到该大脑状态或该大脑状态的先决条件，例如引导你进入灵感并使其持续更长时间，或者检测已知表明刚刚发生认知偏差的大脑状态并发出蜂鸣声警告您。让我们首先从大脑工具/接口开始。 &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;尼古拉斯·克罗斯&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;哦，是的，我不知何故忘记了神经反馈！最后一段是个好主意。我敢打赌，将机器学习与大脑数据相结合可以完成一些有趣的事情。一些小尝试是&lt;a href="https://medarc-ai.github.io/mindeye/"&gt;那些训练神经网络得出你的心理图像的研究&lt;/a&gt;，这是一种粗略的“读心”设置，可以修改和扩展以帮助学习。 &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;特雷弗&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;多模态有助于解决这个问题，对吧？例如，使用眼动追踪来准确查找您的眼睛在任何给定时间在屏幕上看到的单词、您的眼睛移动的速度或因阅读不同类型的概念而引起的其他类型的运动变化，然后将眼动追踪数据与语言模型相结合将大脑传感器/成像作为三个中性网络层，每个层都从不同的角度看待人类思维？ &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;尼古拉斯·克罗斯&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;我想这会有所帮助，是的。尽管恕我直言，此类技术需要考虑到良好的用户体验，当媒介可能是您的整个思想时，这一点尤为重要。例如，您不希望您的深度思考被 20 种分散注意力的 HUD 元素打断。&lt;/p&gt;&lt;p&gt;对于任何有时间和设备的人来说，有很多有希望的角度...... &lt;a href="https://slimemoldtimemold.com/2023/10/26/n1-having-fun-and-feeling-good/"&gt;N=1&lt;/a&gt;&lt;a href="https://gwern.net/doc/nootropic/quantified-self/index"&gt;修补&lt;/a&gt;的潜力很大。即使有 10 个人将其作为一种严肃的硬件黑客类型的爱好，如果他们&lt;i&gt;没有&lt;/i&gt;发现对协调/治理人员合法有用的东西，即使它是“相对”较小的东西，我也会感到惊讶。 &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;特雷弗&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;哇哦，我刚刚读了您发送的有关超声波的文章中的&lt;a href="https://sarahconstantin.substack.com/p/ultrasound-neuromodulation#:~:text=it%E2%80%99s%20a%20huge%20opportunity%20for%20research.%20If%20you%20can%20%E2%80%9Cturn%20up%E2%80%9D%20or%20%E2%80%9Cturn%20down%E2%80%9D%20any%20brain%20region%20at%20will%2C%20safely%20enough%20to%20mess%20around%20with%20it%20on%20healthy%20human%20subjects%2C%20you%20can%20develop%20a%20functional%20atlas%20of%20the%20brain%2C%20where%20you%20find%20out%20exactly%20what%20each%20part%20is%20doing."&gt;这一部分&lt;/a&gt;：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;这是一个巨大的研究机会。如果你可以随意“打开”或“关闭”任何大脑区域，并且足够&lt;i&gt;安全地&lt;/i&gt;在健康人类受试者上对其进行干扰，那么你就可以开发出大脑功能图谱，在其中你可以准确地找到每个部分的作用。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;这绝对让我大吃一惊。它使用与扎克·M·戴维斯（Zack M Davis）的&lt;a href="https://www.lesswrong.com/posts/Zvu6ZP47dMLHXMiG3/optimized-propaganda-with-bayesian-networks-comment-on"&gt;贝叶斯网络优化宣传&lt;/a&gt;相同的因果关系分析，除了通过感知和调节大脑组织的活动，而不是对大量人群进行调查，但两者都在人脑上运行可解释性。当然，神经调节在安全试验中存在瓶颈，因此可能需要很长时间才能对人工智能安全产生价值。&lt;/p&gt;&lt;p&gt;如果使用非线性声学与颅骨共振，而不是采用影响神经活动的频率，也许更容易安全地做到这一点。这样，你就不会扰乱神经活动，而只是振动内颅骨；只要让它在接近正确方向时自动将其调低，这样朝有益的方向思考就会感觉更舒服。&lt;/p&gt;&lt;p&gt;无论哪种方式，你都可以让聪明的人在功能磁共振成像机器中进行&lt;a href="https://www.lesswrong.com/posts/bbB4pvAQdpGrgGvXH/tuning-your-cognitive-strategies"&gt;松鼠调谐&lt;/a&gt;，或者在机器中获得洞察力，如果你通过功能磁共振成像机器在关键时刻很好地映射大脑，你甚至可以找到如何抑制这些部分抑制认知策略调整的大脑，或停止阻止或结束洞察时刻的反馈循环。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;此类技术需要考虑到良好的用户体验，当媒介可能是您的整个思想时，这一点尤为重要。例如，您不希望您的深度思考被 20 种分散注意力的 HUD 元素打断。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;最近 iPhone 的“动态岛”就非常适合这一点。它们还具有可变的刷新率，如果您在不同时间振荡这些刷新率，那么这也将是一种安全的刺激，例如引起不适，并在它们接近正确的大脑状态时逐渐减轻不适。您可以将这些刺激的较弱版本叠加在一起，以创建多源不适，当它们朝正确的方向移动时，这种不适会减轻。也许还会因几乎听不见或听不见的高频或低频声音而感到不适。&lt;/p&gt;&lt;p&gt;这是很多缓解的来源，可能是可以叠加的。有很多安全的方法可以在正确的时刻部署积极的强化，不需要任何海洛因或果汁奖励。 &lt;/p&gt;&lt;figure class="image image_resized" style="width: 49.86%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2uxaqQb9wttgiByJ4/oscntwgmbzdn90ogqpgq" /&gt;&lt;/figure&gt;&lt;p&gt;变成： &lt;/p&gt;&lt;figure class="image image_resized" style="width: 51.24%;"&gt;&lt;img alt="猴子 智能手机 GIF - Monkey Smart Phone Primate - Discover &amp;amp; Share GIFs" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2uxaqQb9wttgiByJ4/ksjq7umuvpkjcv2tcqbt" /&gt;&lt;/figure&gt;&lt;p&gt;坦率地说，无益/浪费的大脑状态&lt;i&gt;应该&lt;/i&gt;是不舒服的，而有帮助/有益的大脑状态应该是舒适的。&lt;/p&gt;&lt;p&gt;我们只需要正确设置正强化和负强化，而不是像我们狩猎采集的祖先一样，毫无头绪地忽略它们并在随机方向上获得强化。&lt;/p&gt;&lt;p&gt;在我们前进的过程中弄清楚事情并不容易，这些切斯特顿-谢林围栏会给我们带来各种奇怪的曲线球，但我们已经从一些最适合这项工作的人开始，随着我们的前进一路走来，我们将创造出更好的人才。 &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;尼古拉斯·克罗斯&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;我认为，人们之所以相信这些，是因为他们看到大多数促智药缺乏重大进展，也因为 tDCS 和类似设备在消费者层面正在接近骗局。希望我们已经了解到，通过很少使用的技术和一些创造力，一般智力增强（特别是神经干预）是多么有前途。&lt;/p&gt;&lt;p&gt;我准备好继续讨论我们列表中的另一个主题，您觉得怎么样？ &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;特雷弗&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;是的。我绝对可以想象来自理性主义社区的人们，包括&lt;a href="https://www.lesswrong.com/posts/zaaGsFBeDTpCsYHef/shallow-review-of-live-agendas-in-alignment-and-safety"&gt;目前正在进行一致性研究的人们&lt;/a&gt;，每周花一个小时或每天半个小时思考银河大脑的方式，将这些进步应用于人类智力的放大。&lt;/p&gt;&lt;p&gt;我听说有传言说 EA 中一些更极端的人正在谈论捐献肾脏和志愿参与早期的新冠疫苗试验。相反，他们应该像&lt;a href="https://sarahconstantin.substack.com/p/testing-human-augmentation#:~:text=Developing%20A%20Community%20of%20Practice"&gt;康斯坦丁所设想的那样，自愿为人类安全试验贡献自己的大脑，开发一个聪明、神经多样化、具有定量技能、富有创造力和洞察力的实践社区，&lt;/a&gt;以便我们能够在技术试用之前对其潜力有一个丰富的了解像 Yudkowsky、Christiano、Sutskever 或 Wentworth 这样的人。这就是铁杆和真正拯救世界的样子，特别是对于那些对自己目前的影响水平不满意的人来说。&lt;/p&gt;&lt;p&gt;根据康斯坦丁关于神经调节实践社区的说法：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;围绕神经调节的实践社区的要求/愿望：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;您首先需要拥有一个可靠的设备，并有一定程度的信心，相信它不会导致急性脑损伤。&lt;ul&gt;&lt;li&gt;由于我们在超声波方面还没有真正做到这一点，因此开始围绕 tDCS 等功能较弱但特性更好的技术建立自我实验实践可能是有意义的，我猜它没有“真正”的效果，但很安全足以将其作为消费品出售。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;您希望您的创始研究小组拥有尽可能多的领域专业知识——医学、神经科学、心理学等。&lt;/li&gt;&lt;li&gt;理想情况下，您还希望员工对自己的感知、心理、情感和主体间体验高度敏感并能清晰表达。艺术家、冥想者、运动员、治疗师——能够自发地注意到微妙事物的人，例如“我能够以较少的防御性讨论敏感话题”或“我的周边视力变得更好”或“我发现自己完全专注于当下”。&lt;/li&gt;&lt;li&gt;您可能需要定量测量方式（功能磁共振成像、脑电图、心率监测器等），但当然，您可以在实验室中对固定的孤立受试者进行测量，而您可以将其带入“现实”环境（如家庭），这之间存在权衡（人们可以四处走动、彼此互动等）&lt;/li&gt;&lt;li&gt;并且您希望努力开发一系列（最初是非正式的）要尝试的东西，要运行的“实验”，主要目标是常识的有效性和帮助性。您不需要问卷或评级量表来判断咖啡因会让您感觉更警觉，但您可能想要实际测量反应时间，而不是相信自己“更快”的主观印象。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这里的主要目标不是用非正式的试错法代替“真正的科学”，而是用它来&lt;i&gt;产生值得更严格测试的假设……&lt;/i&gt;&lt;/p&gt;&lt;p&gt;当然，非正式的自我实验是有争议的，并且可能不适合标准的研究/资助模型，但对于这种开放式的搜索来说，它似乎确实是必要的。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;我们可能是最有能力在这方面发挥领导作用的人之一。我怀疑康斯坦丁名单上的任何一家初创公司都接近&lt;a href="https://www.lesswrong.com/posts/bbB4pvAQdpGrgGvXH/tuning-your-cognitive-strategies"&gt;调整你的认知策略&lt;/a&gt;的水平，而雷蒙的试验小组已经在&lt;a href="https://www.lesswrong.com/posts/9tx4jRAuEddap7Tzp/raemon-s-deliberate-purposeful-practice-club"&gt;推动该领域的边缘&lt;/a&gt;，而整个范式基本上是零技术，而且一直都是。&lt;/p&gt;&lt;p&gt;这让我想起了&lt;a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities"&gt;尤德科夫斯基的死亡名单的&lt;/a&gt;结束语：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;strong&gt;当你环顾四周时，你所看到的这种情况并不是一个幸存世界的样子。&lt;/strong&gt;幸存下来的人类世界已经制定了计划……在此之前就开始尝试解决他们重要的致命问题。一半研究弦理论的人转向人工智能对齐，并在那里取得了真正的进展......&lt;/p&gt;&lt;p&gt;无论如何，许多美好的世界都会消亡。第一次尝试就解决这样的问题确实是一个困难的问题。&lt;/p&gt;&lt;/blockquote&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;特雷弗&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;那么，人工智能的安全性在多大程度上与促智药不成比例地对立呢？&lt;/p&gt;&lt;p&gt;我的印象是，有些人在想“这不起作用，是时候让人们放手了”，而其他人则想“顺从社会现实是一个不好的启发式，而药物显然可以做到这样的事情，因此我将继续下去”。&lt;/p&gt;&lt;p&gt;但我没有问过华盛顿特区的任何人，也没有问过我住在伯克利的时候。 &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;尼古拉斯·克罗斯&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;我的印象（再次强调，不是医疗建议）是，当某人存在使其有用的缺陷/状况时，促智药大多“起作用”。就像素食主义者需要补充维生素 B12 一样，有些人的身体可能需要促智药。 （肌酸显然对素食主义者更有效，从这个意义上讲？）&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;相反，他们应该像&lt;a href="https://sarahconstantin.substack.com/p/testing-human-augmentation#:~:text=Developing%20A%20Community%20of%20Practice"&gt;康斯坦丁所设想的那样，自愿为人类安全试验贡献自己的大脑，开发一个聪明、神经多样化、具有定量技能、富有创造力和洞察力的实践社区，&lt;/a&gt;以便我们在尝试之前能够充分了解该技术的潜力像 Yudkowsky、Christiano、Sutskever 或 Wentworth 这样的人。这就是铁杆和真正拯救世界的样子，特别是对于那些对自己目前的影响水平不满意的人来说。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;我确实考虑过自愿参与这项工作，并且可能会在任何智力增强试验中这样做。&lt;strong&gt;但我可能是这里的一个异类&lt;/strong&gt;。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;早期试验可能针对患有阿尔茨海默氏症等疾病的人，而不是针对健康人的增强试验。&lt;/li&gt;&lt;li&gt;仅鼓励“公正”的肾脏捐赠就产生了好坏参半的结果。除了“嘿，这个选项存在”或“我做了这个，事情是这样的”之外，给人们施加很大的社会压力来试验他们的大脑，这将是非常可怕的。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;但康斯坦丁的帖子无疑让我们很好地了解了在神经调节变得更安全和更好理解&lt;i&gt;之后&lt;/i&gt;，此类实验可能会是什么样子。 &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;特雷弗&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;好吧，无论怎样，如果人类智力放大在硅谷风险投资家中起飞（理想情况下为他们和军方提供人工智能以外的出路），那么促智问题有望自行解决。 &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;尼古拉斯·克罗斯&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;是的。和/或被神经技术淘汰。&lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;h1&gt;大型项目&lt;/h1&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;特雷弗&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;不久前，您写了&lt;a href="https://www.lesswrong.com/posts/5xrkjHCvCeeDtHa5g/alignment-megaprojects-you-re-not-even-trying-to-have-ideas"&gt;《对齐大型项目：您甚至没有尝试有想法》&lt;/a&gt; 。您现在最兴奋的大型项目想法是什么，尤其是在当今变革的世界中？我们正在考虑&lt;a href="https://www.lesswrong.com/posts/K2D45BNxnZjdpSX2j/ai-timelines"&gt;人工智能时间表&lt;/a&gt;、&lt;a href="https://www.lesswrong.com/posts/bxt7uCiHam4QXrQAA/cyborgism"&gt;电子人主义&lt;/a&gt;以及法学硕士的其他应用程序和自动化、我们讨论过的一些智能放大内容的成功，以及我们在 EA 和人工智能实验室及其周围看到的社区健康和权力游戏。过去3年。甚至像新冠肺炎这样的全球性剧变。 &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;尼古拉斯·克罗斯&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;我想出了几个：&lt;/p&gt;&lt;ul&gt;&lt;li&gt; “ &lt;a href="https://www.lesswrong.com/posts/5xrkjHCvCeeDtHa5g/alignment-megaprojects-you-re-not-even-trying-to-have-ideas?commentId=QqkFGWZXuJYKCyL9B"&gt;形式化办公室&lt;/a&gt;”：分包给数学研究生（来自现有对齐领域的内部和外部）。形式化、深入、形式化地证明/反驳并检查对齐研究人员的工作。可能会在相关帖子的评论中链接他们的发现。基本上是“流动的外部同行评审，对你没有实际的权力”。&lt;/li&gt;&lt;li&gt; “ &lt;a href="https://www.lesswrong.com/posts/cP7oiw7psZBHTFjG7/dreams-of-mathopedia"&gt;Mathopedia&lt;/a&gt; ”：数学的多模式教育学维基，面向形式/代理基础类型的对齐研究。最近我一直在想如何把它打造成“数学战术宝库”，比如“这个&lt;a href="https://www.matsprogram.org/agent"&gt;MATS数学题&lt;/a&gt;提到了XYZ，所以我们应该使用S领域的工具，特别是T和U。”&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;请注意，就“需要大量资金”而言，这两个都不是“大型”，尽管它们似乎都相对容易用更多的钱来&lt;i&gt;扩大规模&lt;/i&gt;：只需支付更多的人来做更多的事情。形式化办公室可以通过签约更多的数学毕业生来更快地检查更多领域的工作，Mathopedia 可以覆盖更多的数学领域并创建更多类型的资源。&lt;/p&gt;&lt;p&gt;当然，更集中的“大型”大型项目已经被提出：模拟顶级对齐研究人员的大脑，开发我们上面讨论的增强技术，以及我忘记的更多技术。其中一些是比其他更大的“灌篮”，但其中任何一个（如果成功）都可以推动 P（厄运）。 &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;特雷弗&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;各种里程碑可能会比我们想象的更接近，从而让我们感到惊讶。就像人工智能能力进步的不可预测性一样，但每一个金块都是好消息而不是坏消息。&lt;/p&gt;&lt;p&gt;我喜欢Mathopedia——我认为我们可以更好地培养人们的技能，这样我们培养定量思考者的能力更多地基于他们聪明/创造性地部署数学的固有能力，而不是他们固有的节奏和学习数学的舒适度首次。&lt;a href="https://twitter.com/Morphenius/status/1640811637031723008"&gt;在我们的文明中，学习数学的过程通常是相当不必要的灵魂杀手&lt;/a&gt;，我认为人们没有意识到如果我们解决这个问题并朝着历史上前所未有的方向前进会有多大。&lt;/p&gt;&lt;p&gt; “通过垃圾邮件发送示例和解释来简化数学学习”的官方术语称为“直觉泛滥”，如此&lt;a href="https://www.lesswrong.com/posts/F3vNoqA7xN4TFQJQg/14-techniques-to-accelerate-your-learning-1#4__Intuition_flooding"&gt;处所&lt;/a&gt;引用。您可以通过有用的示例将概念强行灌输给自己或其他人，如果您看到足够多的示例，那么潜在的模式要么会从您身上跳出来，要么会被无言/隐式/潜意识地考虑在内。更多示例意味着更多模式可以组合在一起，并且可以降低错过对理解整个事物至关重要的关键模式之一的风险。&lt;/p&gt;&lt;p&gt;即使主流教育不久前就开始利用它，它也有很大的潜力可以将学习率提高一个数量级。特别是如果像&lt;a href="https://www.lesswrong.com/users/valentine?mention=user"&gt;@Valentine&lt;/a&gt;这样的人因为世界需要他们而被要求从寒冷的睡眠中醒来。&lt;/p&gt;&lt;p&gt;我认为人工智能安全可以极大地提高大多数成员的量化技能。有一天，当人类的后代从一个星球散布到另一个星球时，他们不会告诉孩子们远古地球的历史，直到他们长大到可以承受的程度，当他们知道的时候，他们会因为听到这样的事情而哭泣。在我们的技术水平上，“数学创伤”仍然存在。&lt;/p&gt;&lt;p&gt;我认为我们有动力、有能力、有现有的人才库，而且我们有能力利用这些人才。我们可以将最后一批聪明的思想家转变为聪明的量化思想家，并从他们身上获得足够的价值来证明这一过程的合理性。扩大贝叶斯阴谋，也许甚至存在我们不知道的临界质量，因为我们还没有接近。 &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;尼古拉斯·克罗斯&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;（精确的措辞吹毛求疵：逻辑和数学本身&lt;a href="https://twitter.com/spacepanty/status/1681724936472391680"&gt;似乎仍然涉及很多从远处看起来像是“wordcel”的东西&lt;/a&gt;。）&lt;/p&gt;&lt;p&gt;但总的来说，我同意：我们的社区不仅有很多定量思想家（你可以在其他“书呆子”社区中找到），而且它本身也带来了不同寻常的定量方法，从贝叶斯法则本身到预测市场。&lt;/p&gt;&lt;p&gt;现在的诀窍是从“普通水平的定量思维”延伸到“高级数学思维”。&lt;/p&gt;&lt;p&gt;我将其定义为从“斯科特-亚历山大级别”的数学熟悉度（统计、概率、线性代数的鸟瞰知识、 &lt;a href="https://www.youtube.com/c/3blue1brown"&gt;3Blue1Brown&lt;/a&gt;制作视频的内容）到“数学博士级别”的熟悉度（真实分析、拓扑学、范畴论、研究层面的大多数数学领域，&lt;i&gt;不仅仅是&lt;/i&gt;了解这些是什么，而且&lt;i&gt;能够在前沿流畅地使用它们&lt;/i&gt;）。 &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;特雷弗&lt;/b&gt;&lt;/section&gt;&lt;p&gt;&lt;/p&gt;&lt;div&gt;&lt;p&gt;绝对地。目前预测市场面临的一个巨大问题是糟糕的小麦与谷壳比率。我认为预测市场需要足够数量的人才才能变得令人难以置信，而我们可能还远远没有达到这个临界数量。&lt;/p&gt;&lt;p&gt;如果我们还没有达到其他临界点，我迫不及待地想看看我们还能得到什么；也许还有与预测市场本身处于同一水平的其他创新，但我们只是没有足够的人才来预见它们的到来。&lt;/p&gt;&lt;p&gt;试想一下，如果 10 年前，我们 EA 和人工智能安全社区中达到或接近“数学博士级别”的人数是现在的五倍，那么我们现在会是什么样子。我们可能一直在对人类上传进行扎实的研究。&lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;h1&gt;&lt;strong&gt;达斯伊兰&lt;/strong&gt;&lt;/h1&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;特雷弗&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;“将大部分人工智能安全人员转移到湾区”的巨型工程怎么样？让每个人都集中在一个地方似乎是个好主意，类似于 dath ilan。更多面对面的对话、不同类型的思考者在同一个白板上工作等等。&lt;/p&gt;&lt;p&gt;湾区人们一直在建立集体住宅，购买玫瑰花园酒店和会议场地。存在明显的问题，例如各种社会动态，但基础（土地）已经奠定。&lt;/p&gt;&lt;p&gt;我的想法是，如果我们有更多的人意识到“哇，数十亿人将会死去，我的家人和朋友将会死去”，他们会更适应斯巴达式的生活方式，除非采用银河大脑的方式来最大化在一个地方工作的人数（例如，这些&lt;a href="https://www.google.com/search?sca_esv=587529080&amp;amp;rlz=1CAYGYA_enUS872US872&amp;amp;q=hotel+pods&amp;amp;tbm=isch&amp;amp;source=lnms&amp;amp;sa=X&amp;amp;ved=2ahUKEwj1j73ArPSCAxVBEVkFHWz2D8gQ0pQJegQIDhAB&amp;amp;biw=1181&amp;amp;bih=664&amp;amp;dpr=1.63"&gt;胶囊旅馆&lt;/a&gt;给人一种通过足够的创造性思维可以发现什么的感觉）。这降低了人们访问一个月的成本，这为为特定项目寻找人才等事情提供了巨大的价值： &lt;/p&gt;&lt;figure class="image"&gt;&lt;img alt="东京胶囊旅馆：如何体验？东京廉价酒店" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2uxaqQb9wttgiByJ4/dmv05gmfi1vtobcdyx7o" /&gt;&lt;figcaption&gt;便携的？&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;人工智能的全球形势确实非常严峻，足以证明像这样的开箱即用的解决方案是合理的；根据&lt;a href="https://twitter.com/ESYudkowsky/status/1700242932962586883"&gt;Yudkowsky 的这条笑话推文&lt;/a&gt;：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;我的父母朋友们似乎对我最新的育儿理论印象不深，即你应该把你的孩子放在一个巨大的土坑里，经常受到机器人迅猛龙的攻击……这样一来，普通的地球生活水平似乎就会下降。他们喜欢至高无上的奢侈品，因为他们的大脑基线期望值被设定得很低&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;也许不是每个人都能接受 EA 苦行僧/极端分子的生活方式，但像这样度过一年（同时评估你的才能）并不算太苛刻，这是增加每平方英尺人才的好方法，而且是一个好方法过滤那些认真拯救世界的人。无论如何，这些都是对我有用的事情；有很多选择可以减轻旧金山的布局和价格造成的危害，包括在城中建设空间优化的城市。&lt;/p&gt;&lt;p&gt;让人们搬到更好的空间的更好选择似乎也是解决&lt;a href="https://www.lesswrong.com/posts/AnhwbEisrPXEw9d5g/originality-vs-correctness#:~:text=The%20world%20is,with%20their%20lives."&gt;Habryka 大约一周前提出的一个非常有趣的人才采购问题&lt;/a&gt;的重要组成部分：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;这个世界是敌对的，因为如果你聪明有能力，就会有大量的人和机构优化让你做对他们有利的事情，而忽略你的个人利益。&lt;/p&gt;&lt;p&gt;大多数聪明人“得到了”，最终将他们的生活定位在一些他们甚至不太关心的随机事物上，因为他们的 OODA 循环被某些社会环境所捕获，这使得他们很难理解什么是继续或了解更多关于他们生活中真正想做的事情。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;我认为，锚定在一个特定的空间，例如密歇根或迈阿密的房屋和社区，正是会抑制人们思考全球形势或决定他们想要做什么的能力，就像一个高的地方一样。 - Facebook 或华尔街甚至 OpenAI 的高薪且有趣的工作。昏昏欲睡、无聊的生活是一种吸引状态。 &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;尼古拉斯·克罗斯&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;这引发了我的思考，但我认为这对人工智能安全来说很重要：&lt;/p&gt;&lt;p&gt;处于理性/EA/AI 阵营的人中有很大一部分&lt;a href="https://www.astralcodexten.com/p/acx-survey-results-2022"&gt;存在精神和情感问题&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;我们&lt;i&gt;实际上&lt;/i&gt;不应强迫自闭症和多动症人共享浴室以拯救世界。相反，我们应该使用用于顶级数量交易者，顶级Google程序员和顶级篮球运动员的方法来培养和对待研究人员。&lt;/p&gt;&lt;p&gt;我们希望更多的人参与对齐研究，并且我们&lt;i&gt;不想&lt;/i&gt;&lt;i&gt;添加&lt;/i&gt;任意障碍，即使（作为希望没有人拥有的完全虚构的位置）我们用“哦，我们需要代价高昂的信号来避免&lt;a href="https://www.lesswrong.com/tag/ftx-crisis"&gt;Grifters&lt;/a&gt; ”来证明这一点。 。&lt;/p&gt;&lt;p&gt;对于牛顿来说，学习并在他的时期进行了许多高级数学。爱因斯坦（Einstein）的“阶段”足以使爱因斯坦（Einstein）进行战略性思考哪些研究方案更有希望。&lt;/p&gt;&lt;p&gt;牛顿不得不挤压自己的墨水并切碎自己的树木以制作笔记本是&lt;i&gt;不好的&lt;/i&gt;。&lt;i&gt;要求&lt;/i&gt;爱因斯坦成为项目经理是望远镜/天文台的项目经理，其观察值用于测试相对性。&lt;/p&gt;&lt;p&gt;该领域无法&lt;i&gt;跳过&lt;/i&gt;资金“怪异”。&lt;i&gt;实际上，&lt;/i&gt;您并没有从“ &lt;a href="https://www.lesswrong.com/tag/dath-ilan"&gt;Yudkowsky-&lt;/a&gt; &lt;a href="https://www.lesswrong.com/posts/2jfiMgKkh7qw9z8Do/being-a-robust-agent#icwwigsL3iczYx6D8"&gt;类型的&lt;/a&gt;人都&lt;a href="https://www.lesswrong.com/posts/Qw6qsgR2Qm6rp574A/so-you-want-to-save-the-world-an-account-in-paladinhood"&gt;关心&lt;/a&gt;&lt;a href="https://www.lesswrong.com/posts/2EhC5wgmK2Am6xLL3/ways-to-be-more-agenty"&gt;成为&lt;/a&gt;&amp;#39;agentic&amp;#39;”到“一致性研究人员需要大量执行功能，否则我们不会为他们提供资金”。&lt;/p&gt;&lt;p&gt;就像我们（正确地）注意到“嘿，我们的社区在执行功能型的事情上很糟糕，因为&lt;a href="https://www.lesswrong.com/posts/bRGbdG58cJ8RGjS5G/no-really-why-aren-t-rationalists-winning?commentId=8FLGydotT2sXr4zwt"&gt;我们有个人级的执行功能问题&lt;/a&gt;”，然后（错误地推论的”针刺科学家将是高级执行 -运作人（而不是一般来说是有争议的最突破的半过分专业的天才）”，然后诱使我们走向“我们为什么&lt;i&gt;必须&lt;/i&gt;&lt;a href="https://www.lesswrong.com/posts/HPBdzWYp95zH3mQCa/i-applied-for-a-miri-job-in-2020-here-s-what-happened-next"&gt;招募&lt;/a&gt;人？或者为此&lt;a href="https://www.lesswrong.com/posts/yQJxi5mMZQopYXZk5/a-quick-list-of-some-problems-in-ai-alignment-as-a-field#4__How_do_people_get_good_at_this_shit_"&gt;训练他们&lt;/a&gt;？执行功能足够，&lt;i&gt;&lt;u&gt;他们会在这里找到自己的路&lt;/u&gt;&lt;/i&gt;。”&lt;/p&gt;&lt;p&gt; （如果您的时间表实际上太短&lt;i&gt;了&lt;/i&gt;，这是一回事，但是如果您有2000万美元的花费，&lt;i&gt;则应通过将预算的百分比花在急诊活动上来对冲时间表&lt;/i&gt;。）&lt;/p&gt;&lt;p&gt;最好的是，社区的“故事”将是“ Yudkowsky在2000年代初期花费了许多ag-ness/Executive功能，以为Miri创建和筹款，然后Miri培训并资助了一个良好工作的生态系统”。相反，故事是“第一步发生了，然后Miri几年来几乎完全在内部的事情上做了非常缓慢的事情，然后Openai等人来了，现在大多数一致性资金都用于大型实验室公司已经想做的事情”。我们可以谈论事后看来（就像任何人做出的任何过去的决定一样）和信念 - 刻板，但这是我的“对冲”点所在的地方。&lt;/p&gt;&lt;p&gt; （我对人们如何对此有效，WRT科学突破感到困惑有更多的想法，但是我尚未完成阅读和处理&lt;a href="https://www.lesswrong.com/posts/AnhwbEisrPXEw9d5g/originality-vs-correctness"&gt;原创性与正确性&lt;/a&gt;，这是（到目前为止），这是人们对人们相互竞争的困惑观点的有用摘要。 &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;特雷弗&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;选择仍然不错，我建议的是对人们采取的选择非常有价值，但要牢记&lt;a href="https://slatestarcodex.com/2014/07/30/meditations-on-moloch/"&gt;莫洛克&lt;/a&gt;也很重要。我们不希望平衡向斯巴达人的生活方式转移得足够多，人们最终受到压力去走这条路线（例如，它更便宜，因此缩放得更好），尤其是当斯巴达人的生活方式对我们最好的思想家有害或憎恶时。&lt;/p&gt;&lt;p&gt;我仍然认为，作为经验法则，可以看出自己的速度射手&lt;a href="https://www.lesswrong.com/posts/K2D45BNxnZjdpSX2j/ai-timelines#Introduction"&gt;可能不到20  -  30年&lt;/a&gt;。人们应该通过多种镜头来看待自己及其在世界上的位置，而“末日”镜头应该是基于计算的概率真实的概率，而不是对他人看上去的怪异。我认为创建一点&lt;a href="https://www.lesswrong.com/tag/dath-ilan"&gt;Dath Ilan&lt;/a&gt;是对此明智的回应。&lt;/p&gt;&lt;p&gt;但是，在极端情况下， &lt;a href="https://www.lesswrong.com/posts/HCAyiuZe9wz8tG6EF/my-tentative-best-guess-on-how-eas-and-rationalists"&gt;损害人们的心理健康的情况也使他们成为对自己和周围的人的风险&lt;/a&gt;，也&lt;a href="https://www.lesswrong.com/posts/zidQmfFhMgwFzcHhs/enemies-vs-malefactors"&gt;使他们成为整个AI安全的风险&lt;/a&gt;。而且还有记者在玩，总是在寻找材料，但我认为人们会找到聪明的方法来改善这些禁欲主义者的美学。&lt;/p&gt;&lt;p&gt;我们可能不同意这样做是应该是〜4％还是〜40％。我认为以后我们可以单独进行对话辩论。您已经说服了我不应该超过40％。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; “为什么我们&lt;i&gt;必须&lt;/i&gt;&lt;a href="https://www.lesswrong.com/posts/HPBdzWYp95zH3mQCa/i-applied-for-a-miri-job-in-2020-here-s-what-happened-next"&gt;招募&lt;/a&gt;人员？还是&lt;a href="https://www.lesswrong.com/posts/yQJxi5mMZQopYXZk5/a-quick-list-of-some-problems-in-ai-alignment-as-a-field#4__How_do_people_get_good_at_this_shit_"&gt;训练他们&lt;/a&gt;&lt;i&gt;&lt;u&gt;？&lt;/u&gt;&lt;/i&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;我完全同意。这是不好的做法，我们本可以得到更多的人，更好的人（按价值的定义，例如合作能力）。我们本来可以更快的速度，到现在为止，我们本可以使用更多的工作。&lt;/p&gt;&lt;p&gt;弄清楚事物的过程对于过程至关重要。我的想法是，加快AI安全社区的智能，协调和规模，还将促使人们准备找到银河脑的方法来弄清楚事物的过程。这并不容易，拆除切斯特顿的围栏的结果是无法预测的，并且作为灵长类动物，人类倾向于掩盖出出现的问题。但是，诸如&lt;a href="https://www.lesswrong.com/s/kNANcHLNtJt5qeuSS"&gt;ZVI的不道德迷宫&lt;/a&gt;和&lt;a href="https://meltingasphalt.com/social-status-down-the-rabbit-hole/"&gt;Simler的社会地位&lt;/a&gt;之类的出色帖子将使人们更容易预期和减轻痛苦。&lt;/p&gt;&lt;p&gt;关于执行功能，我还认为这是AI安全的巨大改进领域。人类通常漫无目的地蜿蜒而行，我们可以通过增加每个人的日常生产（通过乘数效应，因为人们正在为彼此的工作做出贡献），从而从AI安全社区中获得更多。&lt;/p&gt;&lt;p&gt;有明显的东西；每个人都阅读CFAR手册，并认真尝试内部的技术（即使其中50％不起作用）。还有其他有关人脑的说明手册，例如&lt;a href="https://www.lesswrong.com/posts/bbB4pvAQdpGrgGvXH/tuning-your-cognitive-strategies"&gt;调整认知策略&lt;/a&gt;和Raemon的&lt;a href="https://www.lesswrong.com/posts/baKauxzSqunE6Aakm/feedback-loops-deliberate-practice-and-transfer-learning"&gt;反馈曲线&lt;/a&gt;。有阅读序列，因为重要的事情是绝对残酷的失败模式，而且人类非常容易成为大约10年重要事物的错误和错误。&lt;/p&gt;&lt;p&gt;但是我认为，其中真正重要的一部分是人们在抖动而不花时间思考。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;人们不应该有“淋浴思想”。&lt;/strong&gt;或更确切地说，它们不应引人注目或独特。在90年代和2000年代，人们将在阅读书籍，外向电子邮件列表时，甚至看电视时会划分并具有“淋浴思想”。社交媒体和其他商业娱乐都会抑制独立思想，并进入您拥有的每一秒空闲时间；对成瘾进行了优化，以使人们返回平台，而不是留在那里，这意味着该算法将找到奇怪的方法（例如&lt;a href="https://Everyone knows that a proper Skinner Box needs to avoid giving away too many rewards if you want to keep people pressing the buttons and viewing the advertisements."&gt;Skinner&lt;/a&gt; &lt;a href="https://www.lesswrong.com/posts/FWiBXSnpusJgCZNWq/against-facebook#:~:text=Without%20exception%2C%20everyone,to%20get%20you."&gt;Box&lt;/a&gt; ）来优化&lt;/p&gt;&lt;p&gt;我认为这实际上是一个巨大的问题，尤其是对于我们最好的思想家而言。为了查看诸如硬核冥想和CFAR手册之类的结果是否实际上来自认知的增强，人们需要每天花2个小时以上的时间花2个小时以上的时间。时髦的冥想技术可能只是阻止媒体使用，迫使您做出正确的决定，以自己的想法，长时间的阵雨或驾驶方式独自一人。&lt;/p&gt;&lt;p&gt;我也认为这可能是CFAR经常没有得到他们所希望的结果的重要原因。 &lt;a href="https://www.lesswrong.com/s/KAv8z6oJCTxjR8vdR/p/gKeHcikcXA3bApyoM#:~:text=And%20it%20is,in%20that%20drawer."&gt;人们需要从以前的次优状态中更改其例程，以便变得更好地思考&lt;/a&gt;，并且&lt;a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#:~:text=Among%20a%20wide,of%20the%20genepool."&gt;当前的媒体范式被大力激励，&lt;/a&gt;以将&lt;a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#:~:text=social%20media%20uses,here%2C%20yet%20again."&gt;AI与大量行为数据结合起来，&lt;/a&gt;以优化以稳定人们的日常工作，如果没有其他无法&lt;a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#:~:text=it%20can%E2%80%99t%20be%20done%20unless%20you%20have%20human%20behavior%20data%20from%20millions%20of%20different%20people%20all%20using%20the%20same%20controlled%20environment"&gt;控制的事情来&lt;/a&gt;稳定人们的日常工作。 &lt;a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#:~:text=it%20can%E2%80%99t%20be%20done%20unless%20you%20have%20human%20behavior%20data%20from%20millions%20of%20different%20people%20all%20using%20the%20same%20controlled%20environment"&gt;变量以提高数据质量&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;就像，如果一个火星人来到地球，看到有人在电话上滚动并说：&lt;/p&gt;&lt;p&gt;&lt;strong&gt;火星人&lt;/strong&gt;：“您怎么&lt;i&gt;没有&lt;/i&gt;注意到危险！&lt;br /&gt;&lt;strong&gt;人&lt;/strong&gt;：“我不知道，我的大脑有点关闭，而我在这样做的时候就失去了自我意识”&lt;br /&gt;&lt;strong&gt;火星人&lt;/strong&gt;：“那令人难以置信的可疑。这不足以使您暗示您的出错了吗？”&lt;br /&gt;&lt;strong&gt;人&lt;/strong&gt;：“哦，来吧。我所有的朋友都在这样做。每个人都在这样做！我需要这个才能知道什么是受欢迎的！只有[低地位的人]抱怨它。”&lt;/p&gt;&lt;p&gt;在这一点上，火星人翻转桌子并离开。&lt;/p&gt;&lt;p&gt;我们许多最好的人将花费大部分思考时间来滚动社交媒体或其他高度关注的吸引者，因为这些是吸引者。而且，如果即时满足是真实的​​，那么这也将极大地损害他们思考的能力。无论哪种方式，他们的大多数最好的想法都将在淋浴或汽车中。&lt;/p&gt;&lt;p&gt;社交媒体排毒很难，因为算法可能会或可能不会达到类似Skinner-Box的策略，以优化以最大程度地提高您不使用社交媒体时的空虚感。这是一个对抗性的环境，所以我认为试图摆脱自己是在发起一场您不会赢的战斗，因为系统不仅以您不了解的方式理解您的思想，而且还了解许多人的思想他们没有的方式，它经历了与他们的胜利，并且知道如何找到与其他情况相似的方式。&lt;/p&gt;&lt;p&gt;但是一种或另一种方式，如果您在淋浴或开车时停止“淋浴想法”，那就是您知道自己做得足够的方式。这似乎是提升AI安全社区的最大方法之一。&lt;/p&gt;&lt;p&gt;当我们采购18-22岁的新人才时，我们是唯一一个有足够的目的的人，他们实际上说服他们花时间思考而不是在娱乐媒体上进行思考。除军队以外，没有其他采购18-22岁的孩子可以做到这一点。&lt;/p&gt;&lt;p&gt;这是一个很大的生活方式的改变，但是当您属于AI安全性既连贯且实际上很重要的AI安全性时，显然都会为自己付费。坦率地说，与速度递质者相比，这没什么。&lt;/p&gt;&lt;p&gt;这就是我们现在所拥有的关于大型项目的全部内容吗？ &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt; &lt;b&gt;Nicholaskross&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;当然，让我们转到下一个主题。&lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;h1&gt;适应性&lt;/h1&gt;&lt;p&gt;“在变化时期，学习者继承了地球，而学者发现自己有能力与不再存在的世界打交道”&lt;/p&gt;&lt;p&gt; - 霍夫&lt;/p&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;特雷弗&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;好吧。许多人都在谈论利用缓慢的起飞来帮助解决一致性。仔细观察&lt;a href="https://www.lesswrong.com/posts/jyAerr8txxhiKnxwA/5-reasons-why-governments-militaries-already-want-ai-for"&gt;社交媒体的当前情况以及将AI用于信息战的使用&lt;/a&gt;确实使我对缓慢起飞期间可能的可能性有一种看法，除了更多的AI治理角度以及AI的一般社会。&lt;/p&gt;&lt;p&gt;过去四年来，库维德（Covid）和国际关系中的新型冷战范式遍布整个地方。 Chatgpt还没有太大变化，但我看不到这种持有，尤其是开源LLM。而且，最糟糕的是，AI实验室内的AI加速度和动荡可能会突然通过难以钉的数量燃烧剩余时间表的一部分。&lt;/p&gt;&lt;p&gt;我的想法是，我们需要更好地适应新情况，因为他们出现了，因为增加了优化能力，越来越多的可怕事物，柔性，但也足够聪明，可以找到带有星系脑的解决方案，以使其具有其他可疑的解决方案。&lt;/p&gt;&lt;p&gt;如果AI应用程序允许Microsoft或&lt;a href="https://www.lesswrong.com/posts/jyAerr8txxhiKnxwA/5-reasons-why-governments-militaries-already-want-ai-for"&gt;政府&lt;/a&gt;等大型科技公司&lt;a href="https://www.lesswrong.com/posts/F7sp7rQg3zfD4totA/helpful-examples-to-get-a-sense-of-modern-automated"&gt;在社交媒体上大大扩展说服力技术&lt;/a&gt;，那么，如果拳打使用超级优化的心理学来攻击我们的核心，我们将如何使用拳头？&lt;/p&gt;&lt;p&gt; &lt;a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#How_to_protect_yourself_and_others__:~:text=I%E2%80%99m%20not%20sure,for%20self%2Dimprovement."&gt;降低认知和情感可预测性使AI安全社区不受特定威胁的态度&lt;/a&gt;，但每天逐渐促进数十种带有银河系脑的解决方案会更好地降低可预测性，并在各种方面提高能力和韧性。就像不断增长的痛苦一样，除了外部威胁而不是内部威胁。&lt;/p&gt;&lt;p&gt;这将提高AI安全社区的适应和繁荣和骑行在缓慢起飞过程中出现的所有其他可怕事物的浪潮，而不仅仅是&lt;a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks"&gt;对任何只是一个例子的人的新人类操纵范式&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;卡特贾·格雷斯（Katja Grace）将AI加速为“ &lt;a href="https://forum.effectivealtruism.org/posts/2xrTTgvosGSsM85RZ/katja-grace-on-slowing-down-ai-ai-expert-surveys-and#:~:text=there%27s%20going%20to%20be%20just%20a%20fire%20hose%20of%20new%20cognitive%20labor%20and%20it%27s%20sort%20of%20unclear%20where%20it%20will%20go"&gt;新认知劳动的消防软管，这是不清楚它将去哪里的&lt;/a&gt;”。我认为这是一个非常好的方法。各种各样的事情最终可能会被从中抽出来。&lt;/p&gt;&lt;p&gt;我认为，就像人类的操纵技术一样，也会有更多的“黑球”，这也是如此，但是以完全不同的方式，但是我们必须生存和壮成长。&lt;/p&gt;&lt;p&gt;我认为现行的人类操纵技术是一个很好的例子，说明曲线球的距离是多远，对我们有很迷惑，以及&lt;a href="https://www.lesswrong.com/posts/jyAerr8txxhiKnxwA/5-reasons-why-governments-militaries-already-want-ai-for"&gt;传统的力量经常在它们周围旋转&lt;/a&gt;。但是，也许人类的操纵技术是一个异常严重/极端的情况，我们可能不会从缓慢的起飞中获得0-2的“黑球”，而慢速起飞与操纵技术一样可怕。&lt;/p&gt;&lt;p&gt;不过，这不是我要押注的地方，尤其&lt;a href="https://www.lesswrong.com/posts/foM8SA3ftY94MGMq9/assessment-of-intelligence-agency-functionality-is-difficult#Functioning_lie_detectors_as_a_turning_point_in_human_history"&gt;是Lies检测技术&lt;/a&gt;，&lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3680134/"&gt;因为在没有ML的文明中已经存在了〜100年，因此其声誉仅为负面声誉&lt;/a&gt;（它们被称为“测谎仪”测试，一个人不得不立即对其进行眼球！）&lt;/p&gt;&lt;p&gt;提升AI安全社区，以使银河脑的解决方案以足够的速度抽出以减轻损害，甚至利用机会或预期威胁发生之前，似乎是解决这些挑战的最佳方法。 &lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt; &lt;b&gt;Nicholaskross&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;blockquote&gt;&lt;p&gt;每天促进数十个银河脑解决方案的速度变得更好&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;这听起来有些疯狂，这是什么意思？ &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt; &lt;b&gt;Nicholaskross&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt; （我广泛地同意这一点的余地，并指出“加速对齐和治理”本身&lt;i&gt;可以&lt;/i&gt;帮助“减速” AI的能力，因为我们的社区在指出和反对不安全的AI开发方面变得更好。） &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;特雷弗&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;是的，通过“每天抽出数十个银河脑解决方案”，我只是意味着人们更擅长考虑效果很好的解决方案。这些解决方案中的一些会很复杂，有些将是优雅的，有些将是残酷有效的，其他解决方案将使我们在不可能看的情况下混乱并减轻巨大的损害。&lt;/p&gt;&lt;p&gt;更聪明的人会让我们更好地做到这一点。我认为安娜·萨拉蒙（Anna Salamon）的 &lt;a href="https://www.lesswrong.com/posts/PBRWb2Em5SNeWYwwB/humans-are-not-automatically-strategic"&gt;人类并没有自动策略性地&lt;/a&gt;描述了这一点。超级智能会做什么？在我们当前的世界中，其中很大一部分是将您的思想进行抽象推理的能力转化为每天和一天的最佳动作（包括故意休息，防止倦怠，以及其他导致积极加强和的事物）防止负强化）。&lt;/p&gt;&lt;p&gt;用&lt;a href="https://www.astralcodexten.com/p/contra-deboer-on-movement-shell-games#:~:text=ACTUALLY%20DO%20THESE%20THINGS!%20DON%27T%20JUST%20WRITE%20ESSAYS%20SAYING%20THEY%27RE%20%22OBVIOUS%22%20BUT%20THEN%20NOT%20DO%20THEM!"&gt;斯科特·亚历山大（Scott Alexander）的最新话语&lt;/a&gt;，当今有效的利他主义和AI安全的一个大问题是：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;实际上做这些事情！不要只是写论文，说它们是“显而易见的”，但不要这样做！&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;在变革性的世界中，最好的做法可能与代理和动力骇客有所不同，也许更多地是关于在极端情况下第一次做好事，例如那个场景中的主角具有远见和精神上的敏捷性，可以在即使他片刻之前发生了一些令人难以置信的可怕事件，这是第一个机会。但是，无论哪种方式，我们都需要灵活且聪明。&lt;/p&gt;&lt;p&gt;这似乎是激进的或极端的。但是，如果我们在大约十年或三年内得到速度射手，我们将回顾自己的生活，并意识到我们所做的唯一令人毛骨悚然的事情是做得还不够。事后看来，很明显，我们努力和犯错的时代比我们几乎没有尝试的时间要好得多。&lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;h1&gt;从序列中提取更多价值&lt;/h1&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;特雷弗&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;您对提高&lt;a href="https://www.lesswrong.com/highlights"&gt;Yudkowsky序列&lt;/a&gt;， &lt;a href="https://www.lesswrong.com/posts/dbDHEQyKqnMDDqq2G/cfar-handbook-introduction"&gt;CFAR手册&lt;/a&gt;， &lt;a href="https://www.lesswrong.com/codex"&gt;Scott Alexander的法典&lt;/a&gt;， &lt;a href="https://www.projectlawful.com/board_sections/703"&gt;ProjectLawful&lt;/a&gt; / &lt;a href="https://hpmor.com/"&gt;HPMOR&lt;/a&gt; ，&lt;a href="https://www.lesswrong.com/posts/bbB4pvAQdpGrgGvXH/tuning-your-cognitive-strategies"&gt;调整认知策略&lt;/a&gt;/ &lt;a href="https://www.lesswrong.com/posts/pZrvkZzL2JnbRgEBC/feedbackloop-first-rationality"&gt;FefagebackloopFirfirst Woldicality&lt;/a&gt;的读者率的次数有何看法？什么是瓶颈阻止人们从中阅读和成长的瓶颈，我们该怎么办？&lt;/p&gt;&lt;p&gt;他们提供了许多独特的方法来将更好的习惯纳入您的大脑，因此您可以外出并从生活中获得想要的东西，而不是感到困惑和不断地背叛自己；就像让一群人进入更好的纳什平衡，而他们不断地互相背叛。&lt;/p&gt;&lt;p&gt;智能放大技术很棒，但是超级智能的AI可以编写一本使用真实知识的手册，向您展示如何利用您的全部思维范围，可能比CFAR手册更有效地有几个数量级。&lt;/p&gt;&lt;p&gt;像CFAR手册之类的东西不仅是我们已经拥有的东西，而且在整个人类历史上，这都是整个人类几乎不愿意尝试的东西。我们不知道CFAR手册和超级智能编写的真实指导 - 手动 -  be-a-human-brain之间的差距有多大，这使我们能够最佳地思考以最大程度地提高结果。&lt;/p&gt;&lt;p&gt;这个差距可能很小，也许我们需要十个聪明的人，而不是五个，也许五个远远足够了，但是他们没有松鼠的&lt;a href="https://www.lesswrong.com/posts/bbB4pvAQdpGrgGvXH/tuning-your-cognitive-strategies"&gt;调整您的认知策略&lt;/a&gt;可以解决，现在他们做到了。也许所有五个都&lt;a href="https://www.lesswrong.com/posts/9tx4jRAuEddap7Tzp/raemon-s-deliberate-purposeful-practice-club?commentId=k6hPEP6afApjg8nKc"&gt;经常使用社交媒体和其他娱乐媒体，以至于他们失去了衡量独立思想有效性的能力&lt;/a&gt;。 AI安全社区中的一切都归结为不足和痛苦不足。&lt;/p&gt;&lt;p&gt;我认为，Intelligence Eventation Tech的价值的很大一部分来自使人们更好地应用序列，CFAR手册和其他人已经描述的建议。&lt;/p&gt;&lt;p&gt;我认为其中很大一部分可能是缺乏积极的强化，这基本上是工作，例如，Yudkowsky转发了这件事：&lt;/p&gt;&lt;p&gt; &lt;i&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2uxaqQb9wttgiByJ4/usuh9tyhwgxt5mou6pri" /&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt; &lt;b&gt;Nicholaskross&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;哦，我对此有很多想法！ （主要是由于对制作YouTube视频和其他艺术的业余爱好者感兴趣，这导致了&lt;a href="https://twitter.com/NicholasKross/status/1720578513743261768"&gt;长期固定的&lt;/a&gt;“媒体如何流行？”）&lt;/p&gt;&lt;p&gt;这里的一个关键心理模型是“注意漏斗”，就像广告商使用的&lt;a href="https://en.wikipedia.org/wiki/Purchase_funnel#/media/File:The_Purchase_Funnel.jpg"&gt;“购买渠道”&lt;/a&gt;一样。让我们通过YouTube视频在漏斗中运行：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;您会看到缩略图和标题。&lt;/li&gt;&lt;li&gt;如果有兴趣，请单击视频并开始观看。&lt;/li&gt;&lt;li&gt;如果视频无聊或刺耳或其他信息，您将停止观看视频（例如，通过关闭浏览器选项卡）。否则，您会不断观看。&lt;/li&gt;&lt;li&gt;如果视频&lt;i&gt;变得&lt;/i&gt;无聊/光栅/等，那么您更有可能停止观看视频。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;幻想小说发生了类似的事情：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;您会看到标题（或从朋友那里听到）。&lt;/li&gt;&lt;li&gt;如果有兴趣，您可以搜索/单击FIC，也可以阅读摘要。&lt;/li&gt;&lt;li&gt;如果很有趣（引人注目，引起您的注意等），您会继续阅读。否则您停下来。也许您稍后再回来，但我们将重点放在“您的注意力驱动决定”上。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt; hpmor和其他理性主义者的作品（对于某种个性来说）非常引人注目！他们经常有戏剧性的节奏，简短的章节/部分以及主观上“良好写作”的事物（即使是在各种句子结构的微观级别，或者在措辞中精确的情况）。因为它们是故事（和/或有力的辩论），所以它们通常包含人类喜欢的情感加载。&lt;/p&gt;&lt;p&gt;回到中心点：我认为瓶颈现在是漏斗的&lt;i&gt;顶部&lt;/i&gt;。每个人都听说过惠普，但是“只有” HP狂热的书呆子（和/或现有理性主义者）听说过HPMOR。序列和相关作品甚至更&lt;i&gt;狭窄&lt;/i&gt;。&lt;/p&gt;&lt;p&gt;有多种方法可以&lt;a href="https://www.lesswrong.com/posts/Bfq6ncLfYdtCb6sat/i-converted-book-i-of-the-sequences-into-a-zoomer-readable"&gt;或多或少地直接/直接地&lt;/a&gt;进行，但是&lt;a href="https://www.youtube.com/@RationalAnimations/videos"&gt;合理化&lt;/a&gt;是一个不错的例子：“基本的AI一致性想法&lt;i&gt;可以&lt;/i&gt;令人震惊地提出以吸引很多人”。&lt;/p&gt;&lt;p&gt;如果您是社区中的“创意类型” ...好吧，无论如何艺术领域都&lt;i&gt;可以&lt;/i&gt;遇到&lt;a href="https://www.lesswrong.com/tag/hamming-questions"&gt;问题&lt;/a&gt;，您可能会喜欢探索“可以制作哪种理性主义者/A-Ai-Acalign/EA媒体，这将达到很多人？”&lt;/p&gt;&lt;p&gt;另一个瓶颈（在较短的时间表下，可能是&lt;i&gt;更重要的&lt;/i&gt;时间表），似乎是“注意漏斗的底部，但AI对齐漏斗的顶部”。就像，很多人都辞去了“言语和统计人”的意愿，并认为他们不能加入“ ML和/或数学人”。&lt;br /&gt;&lt;br /&gt;如果我能神奇地将20％的EAS纳入全日制AI对齐，我会的！ （嗯，Modulo警告他们从事&lt;a href="https://www.lesswrong.com/posts/uhMRgEXabYbWeLc6T/why-and-when-interpretability-work-is-dangerous"&gt;哪种类型的研究&lt;/a&gt;。） &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;特雷弗&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;而且，如果我可以让每个人的AI Alignment每周花2个小时来思考AI治理解决方案，那也很棒。&lt;/p&gt;&lt;p&gt;尤德&lt;a href="https://twitter.com/ESYudkowsky/status/1726280375281103326"&gt;科夫斯基&lt;/a&gt;（Yudkowsky）回应了Openai冲突：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; Twitterati大量高估了我花费了多少专注于单个AI公司。我目前将他们的集体视为不可避免的下坡幻灯片，即使他们愿意，他们的领导者也无法改变任何东西。这是国际条约或半身像。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;对我来说，这是一个非常大的更新，希望利用对准研究人员的人才基础，以解决不可能的全球问题，例如在AI问题上进行美国 - 中国协调，我多年来一直在研究AI治理，这是我多年来一直在研究的。相对于所有其他工作领域变得如此重要。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;就像，很多人都辞去了“言语和统计人”的意愿，并认为他们不能加入“ ML和/或数学人”。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;是的，我完全同意数学问题。我认为这是双向的。我在DC中度过的几年中给人的印象是，这里的人们很少有跨越贝叶斯思想的量化技能。但是，让AI治理人员使人们在湾区的一致性研究人员提高一致性，以便对准研究人员可以考虑解决问题的解决方案，而不是试图提高AI的量化技能，这可能更有意义。治理人员，这样他们就可以开始与湾区的人们追赶，这些人几十年来一直在开创了贝叶斯的思想。 &lt;/p&gt;&lt;figure class="image"&gt;&lt;img alt="X上的詹姆斯·梅德洛克（James Medlock）：“我经常出现的浴室摊位上有涂鸦，说“专业是针对昆虫。我不是蚂蚁！”我认为这是对" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2uxaqQb9wttgiByJ4/uzlw83xsokav6lmsnzbh" /&gt;&lt;/figure&gt;&lt;blockquote&gt;&lt;p&gt;我认为瓶颈现在是漏斗的&lt;i&gt;顶部&lt;/i&gt;。每个人都听说过惠普，但是“只有” HP狂热的书呆子（和/或现有理性主义者）听说过HPMOR。序列和相关作品甚至更&lt;i&gt;狭窄&lt;/i&gt;。&lt;/p&gt;&lt;p&gt;有多种方法可以&lt;a href="https://www.lesswrong.com/posts/Bfq6ncLfYdtCb6sat/i-converted-book-i-of-the-sequences-into-a-zoomer-readable"&gt;或多或少地直接/直接地&lt;/a&gt;进行，但是&lt;a href="https://www.youtube.com/@RationalAnimations/videos"&gt;合理化&lt;/a&gt;是一个不错的例子：“基本的AI一致性想法&lt;i&gt;可以&lt;/i&gt;令人震惊地提出以吸引很多人”。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;我在这里同意。当我想象自己的2014年或2013年的自我时，在听说AI安全之前，我可以轻松地想象自己观看了10分钟的视频，并思考“耶稣基督”并为其添加书签。然后，后来，如果我是那种使世界变得更美好的人的类型，我更有可能在一个月或一个星期左右的时间内再次记住它那&lt;i&gt;他妈的&lt;/i&gt;是什么？”再看一次。理想情况下，人们会阅读序列或WWOTF，并且没有任何其他方面的人工智能安全，但这不是我们生活的世界。&lt;/p&gt;&lt;p&gt;也许Yud和其他人所做的播客是更好的版本，因为在有趣的未来主义主题上涵盖了很多基础，可以更好地过滤聪明的有见地的人，或者对于&lt;a href="https://www.lesswrong.com/posts/F3vNoqA7xN4TFQJQg/14-techniques-to-accelerate-your-learning-1#4__Intuition_flooding"&gt;直觉泛滥&lt;/a&gt;，可以通过从直观上理解当前情况，从例如，花了10分钟的时间来谈论思想理论，然后在工具融合上聊天10分钟，然后在AI Race Economics上进行10分钟。&lt;/p&gt;&lt;p&gt;实际上，我正在考虑增加AI安全性内的序列和CFAR手册阅读率，而不是让世界其他地区加入。不过，这也是一个巨大的问题，并且与获得AI安全社区所需的新人才无情。&lt;/p&gt;&lt;p&gt;我认为例如，重读序列可能会避免受到很大的伤害，这是一条推文（我有&lt;a href="https://twitter.com/ESYudkowsky"&gt;https://twitter.com/esyudkowsky&lt;/a&gt;添加了书签，您也应该）：&lt;/p&gt;&lt;p&gt; &lt;i&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2uxaqQb9wttgiByJ4/d0hxtqb6nxtwi4ovaxtm" /&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;我认为，对于大多数人进行AI安全的人来说，每5至10年重新阅读一次序列，甚至更频繁地，但是有一种更好的方法（我们真的应该深入寻找更好的方法的心态）。&lt;/p&gt;&lt;p&gt;我认为我们应该像圣经一样阅读序列。具体来说，每天阅读各种Yudkowsky和其他伟大理性主义者的行情，例如圣经行情。每天有大量的应用程序，网站，新闻通讯和纸质日历，每天给您一份圣经报价。&lt;/p&gt;&lt;p&gt;圣经中有一些相当不错的东西，这些报价被许多人的优化压力搜索和发现。它非常适合数百万的非主人。但是，尤德科夫斯基的著作比圣经要好得多。&lt;/p&gt;&lt;p&gt;只要有某种饲料，每天都会随机Yudkowsky行情，这是有意义的。&lt;/p&gt;&lt;p&gt;这类似于以&lt;a href="https://www.lesswrong.com/posts/PouWW8Ys4iKhWedCZ/psa-the-sequences-don-t-need-to-be-read-in-sequence"&gt;随机顺序阅读序列&lt;/a&gt;，我也强烈建议。如果您在早上喝咖啡后查看娱乐媒体或社交媒体，那么您正在堆叠两种强大的成瘾/行为来加强事物，并且您应该通过阅读随机选择的序列来代替媒体，以便您的大脑将其与匆忙相关联。&lt;/p&gt;&lt;p&gt;但是，除了Badass EA苦行者极端粒子外，&lt;a href="https://www.lesswrong.com/posts/4ZvJab25tDebB8FGE/you-get-about-five-words"&gt;大多数人还想做快速简便的事情&lt;/a&gt;，因此使用诸如圣经行情之类的理性主义语录。这是开始一天或一天​​中的好方法。&lt;/p&gt;&lt;p&gt;每天可能会有不同的报价，以最大程度地提高人们以独特的方式增强乘数效应，然后相互交流。当然，这需要足够大的AI安全性可以做到这一点，对于AI安全社区，这就像放牧猫一样。&lt;/p&gt;&lt;p&gt;同时，令人难以置信的&lt;a href="https://www.lesswrong.com/posts/dbDHEQyKqnMDDqq2G/cfar-handbook-introduction"&gt;CFAR手册序列&lt;/a&gt;有些长（约20,000个单词），但可以很好地解释困难和重要的概念，而&lt;a href="https://www.lesswrong.com/s/qRxTKm7DAftSuTGvj"&gt;hammertime序列&lt;/a&gt;更短且可重读，但这带有较弱的直觉成本，并且使其易于更易于。忘记。&lt;/p&gt;&lt;p&gt;理想情况下，像&lt;a href="https://www.lesswrong.com/users/deactivated-duncan-sabien"&gt;Duncan&lt;/a&gt;这样的人可以在两者之间在两者之间创造中间立场的出色工作，尽管考虑到基本概念的复杂性，CFAR手册序列可能已经大大缩短了。邓肯的写作和解释非常好，我先前的可能性是，如果我试图将其提炼到原始长度的〜75％，我很难保留该价值。&lt;/p&gt;&lt;p&gt;可以想象，将缩短版本转换为抽认卡将非常有价值，但是主要的是人们只需要阅读它，并让他们的朋友阅读它，因为他们仍然值得阅读，即使是目前的长度也是如此。&lt;/p&gt;&lt;p&gt;当人们辩论序列的价值或有效性时，他们通常会在思考，就像在某人面前将书本打倒，并告诉他们阅读它。也许看到他们结束后5  -  10年的样子。但是，更大的问题是是否有方法可以更有效地完成此数量级。 &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt; &lt;b&gt;Nicholaskross&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;实际上，我重读了有用的/心理模型的帖子和书籍段，但我认为我在这个社区中是一个离群值。&lt;/p&gt;&lt;p&gt;如果我们真的要进行“圣经行情”（！？！？！）类似的设置，我们应该更多地研究“在同一主题上的咬合大小的序列”。例如“序列切片”或“引用序列”或“ subsect序列”或“通道标签”。例如，每本CFAR手册段落涉及&lt;a href="https://www.lesswrong.com/tag/crux"&gt;关键&lt;/a&gt;。或每个Yudkowsky关于&lt;a href="https://www.lesswrong.com/tag/complexity-of-value"&gt;价值复杂性的&lt;/a&gt;报价。&lt;/p&gt;&lt;p&gt;一方面，这&lt;i&gt;似乎&lt;/i&gt;是“过于简单/浅薄的理性主义著作”，而牺牲了更深入的阅读。另一方面，引号可以是阅读完整帖子的门户。 （而且，如果我们说实话，那么更好的理性主义者的决策通常&lt;i&gt;始于&lt;/i&gt;“记住相关的报价”，即使报价是“为自己思考！”或“时钟思考5分钟”。） &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;特雷弗&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;blockquote&gt;&lt;p&gt;实际上，我重读了有用的/心理模型的帖子和书籍段，但我认为我在这个社区中是一个离群值。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;这绝对不是一个离群值。在我开始为AI政策准备工作后多年，我的文章已添加了多年，我的Yudkowsky的“&lt;a href="https://www.lesswrong.com/posts/9weLK2AJ9JEt2Tt8f/politics-is-the-mind-killer"&gt;政治是心灵杀手&lt;/a&gt;”。如果我知道尤德的整个&lt;a href="https://www.lesswrong.com/s/ZnSMHcWjRx6yT4H92"&gt;政治先决条件&lt;/a&gt;，并且每月阅读其中一个，那么我现在就已经领先了。这些东西是很棒的参考材料。&lt;/p&gt;&lt;p&gt;绝对，其中的大部分只是促使人们，因此他们回到接受建议并重新确定对自己的生活的控制。坦率地说，看到人们慢慢回归到美国/欧洲普通日常生活的平庸之处是非常令人不安的。您新发现的代理商的逐渐损失确实是您必须战斗的潮流。将&lt;a href="https://slatestarcodex.com/2014/07/30/meditations-on-moloch/"&gt;Moloch&lt;/a&gt;从您的大脑中脱颖而出，并将其排除在外，还将Moloch脱离您的小组。&lt;/p&gt;&lt;p&gt;同时，让人们成为可以保持彼此前进的团体，这也很难。人类是灵长类动物，这意味着我们有开车去征服竞争对手。对我们来说，这通常意味着要采用重要的概念，并将它们变成比您的同胞优势获得优势的机会（包括武器将该概念本身武器本身在竞争对手上投诉，例如“ X人X对人工智能安全有害”）。&lt;/p&gt;&lt;p&gt;有很多选择，基本上有选择的过载。这里什么都不容易解决（任何容易解决的问题可能已经解决了，例如成长的痛苦），但是仍然有很多低矮的水果值得一提的是（或者每周至少要花费一个小时一个小时，故意考虑一个小时）。&lt;/p&gt;&lt;p&gt;这就是我们现在应该做的那种事情，从我们前进的时候开始，还有增长的空间，而不是速度射手在大门时。 &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt; &lt;b&gt;Nicholaskross&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;同意。&lt;/p&gt;&lt;p&gt;为了解决这个问题，我们发现了许多“提升”或“升级” AI一致性和治理社区的途径。这些选项中的许多是脱节的（如果促智药不断失败，神经调节可能会起作用）。其中一些甚至便宜且易于自己做（例如序列报价收藏）。&lt;/p&gt;&lt;p&gt;社区已经存在了十多年了，我们&lt;i&gt;仍然&lt;/i&gt;看到潜在的低爆发水果。您&lt;i&gt;可以&lt;/i&gt;将其视为“哇，我们的社区确实不足”，这通常是事实。但是我也认为这是“哇，有很多东西还没有消失，他们可以帮助AI安全！”&lt;/p&gt;&lt;p&gt;当然，这依靠有空闲时间（和金钱）的人来尝试这些。如果有人找到了东西，他们可以找到&lt;i&gt;将针移到p（厄运）上的&lt;/i&gt;东西。这似乎值得一些反复试验！ &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;特雷弗&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;绝对地。另外，我认为，如果每个人都可以每周搁置&amp;gt; 1小时， &lt;a href="https://www.lesswrong.com/posts/PBRWb2Em5SNeWYwwB/humans-are-not-automatically-strategic"&gt;请认真思考&lt;/a&gt;“鉴于最近的事件和新信息，AI安全社区如何变得更好？”，情况可能会有所改善？&lt;/p&gt;&lt;p&gt; (with a focus on using new technology to solve major problems that were previously assumed to be intractable, eg something like &lt;a href="https://www.lesswrong.com/posts/foM8SA3ftY94MGMq9/assessment-of-intelligence-agency-functionality-is-difficult#Functioning_lie_detectors_as_a_turning_point_in_human_history"&gt;lie detectors&lt;/a&gt; , and improve things for everyone, rather than intimidating specific people or groups into giving ground or redistributing resources)&lt;/p&gt;&lt;p&gt; Setting aside one hour a week to think about something important ( &lt;a href="https://www.lesswrong.com/posts/9tx4jRAuEddap7Tzp/raemon-s-deliberate-purposeful-practice-club?commentId=RhDGwGZHyA5WmDZbt"&gt;without disrupting focus&lt;/a&gt; ) would be less than 1% of most people&amp;#39;s waking hours. It&amp;#39;s probably the kind of thing where most of us will end up looking back and wishing that we did a lot more of it, and sooner, before things got hot.&lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;div&gt;&lt;/div&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/2uxaqQb9wttgiByJ4/upgrading-the-ai-safety-community#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sat, 16 Dec 2023 15:34:26 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/2uxaqQb9wttgiByJ4/upgrading-the-ai-safety-community</guid></item><item><title>医药用冷铝</title><link>https://www.lesswrong.com/posts/yG9prEve6vD4qnTfy/cold-aluminum-for-medicine</link><description>发布于 2023 年 12 月 16 日下午 2:38（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;冷铝&lt;/h2&gt;&lt;p&gt;在氢沸点的非常纯的铝是一种非常冷的材料。在 20K 温度下，99.999% 纯铝的电导率是 0 C 时的 1000 倍。在 4K 温度下，它的电导率可能是 4000 倍。&lt;/p&gt;&lt;p&gt;在如此低的温度下，铝中的电子自由路径变得宏观，这就是为什么即使少量的杂质也会大大增加电阻。即使是线径也会产生明显的影响。磁场也会增加电阻，但这也是与纯度相关的效应：99.999% 的铝在 15T 时的电阻可能是原来的 3 倍，但即使是更纯的铝，受影响也小得多。&lt;/p&gt;&lt;p&gt;是的，铝净化需要花费一些钱，但并不是特别贵。它的成本可能是标准铝的三倍，但比超导体便宜得多。另一点需要注意的是，超导体对于&lt;em&gt;恒流&lt;/em&gt;只有0电阻。低温铝不像超导体那样受到电流变化的影响。&lt;/p&gt;&lt;p&gt;看起来这种能大幅提高品质因数的有趣效应应该有某种应用，你不觉得吗？然而，虽然有用于船舶的超导电动机，但我不知道低温铝导体用于任何商业应用。它可以用来做什么？&lt;/p&gt;&lt;h2&gt;电力线路&lt;/h2&gt;&lt;p&gt;低电阻导体的一个明显应用是长距离电力传输。我的估计表明，使用低温铝有点太贵了，因为（低温冷却器成本）*（绝缘成本）产品对于合理的线路电流来说太高了。将其连接到环境温度线路也是一个问题，因为冷纯铝也具有高导热性。&lt;/p&gt;&lt;p&gt;随着温度降低，阻力降低，但制冷机变得更昂贵且效率更低。一般来说，对于低温铝导体来说，液氢似乎比液氦或液氮更好。&lt;/p&gt;&lt;p&gt;在这样的温度下，值得使用&lt;a href="https://en.wikipedia.org/wiki/Multi-layer_insulation"&gt;多层真空绝热材料&lt;/a&gt;。这比玻璃纤维或聚酯等典型绝缘材料有效得多，但它似乎仍然不足以使大型地下电力线的绝缘材料+制冷器足够便宜。&lt;/p&gt;&lt;p&gt;虽然经济上不可行，但使用低温铝进行高功率电力传输是&lt;em&gt;可能的&lt;/em&gt;。它只是昂贵，并非不可行。请随意使用它来为硬科幻故事增添风味。&lt;/p&gt;&lt;p&gt;哪些应用属性使低温铝更适合？&lt;/p&gt;&lt;ul&gt;&lt;li&gt;每表面积电流大。&lt;/li&gt;&lt;li&gt;可以使用超导体，但电流变化带来的阻力是一个问题。&lt;/li&gt;&lt;li&gt;低重量很重要。&lt;/li&gt;&lt;li&gt;很容易实现低温冷却。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;提出的一项应用是在以液氢为燃料的飞机中使用低温铝导体的电动机，这将为铝提供自然冷却。显然，这种飞机目前并不存在，而且我认为它们不是很实用，但这超出了本文的范围。&lt;/p&gt;&lt;h2&gt;核磁共振成像&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;因此，我想到的低温铝唯一好的应用是 MRI 机器。是的，目前新公司或新技术很难进入该市场，但低温铝相对于超导体可能具有一些理论上的优势。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt; -&lt;a href="https://www.bhauth.com/blog/materials/material%20enabled%20products.html"&gt;一些博客&lt;/a&gt;&lt;/p&gt;&lt;p&gt;您可能听说过 MRI 扫描很昂贵，因为机器很贵，但美国的 MRI 扫描价格比墨西哥贵约 5 倍。您可能会认为由于劳动力需求，它们很昂贵，但荷兰是 MRI 扫描价格最低的国家之一。&lt;/p&gt;&lt;p&gt;无论如何，是的，这些机器有点贵。&lt;a href="https://lbnmedical.com/how-much-does-an-mri-machine-cost/"&gt;以下&lt;/a&gt;是一些大概的机器价格。假设一台价值 40 万美元的机器每天可供 10 个人使用，摊销期为 5 年，则每次使用费用为 22 美元。考虑到美国医疗保健的典型价格乘数，您可以看到它如何变得昂贵......？&lt;/p&gt;&lt;p&gt;其中一些成本用于超导磁线圈。有没有办法通过使用冷铝来潜在地减少费用，或者以某种方式提高 MRI 性能？&lt;/p&gt;&lt;h3&gt;梯度线圈&lt;/h3&gt;&lt;p&gt;MRI 机器的典型方法是使用 2 个超导线圈（恒定电流）来形成近似均匀的磁场，并使用铜线圈来创建（小得多）磁场梯度，其变化频率可能为 2 kHz。核磁共振进动频率取决于场强，因此该场梯度可用于将发射定位到特定切片。&lt;/p&gt;&lt;p&gt;对于铝，理论上相同的线圈可以用于这两个目的。用铝代替超导体的成本有多高？我估计这将涉及：&lt;/p&gt;&lt;ul&gt;&lt;li&gt; &amp;gt;1吨纯铝&lt;/li&gt;&lt;li&gt;几千瓦的电阻损耗&lt;/li&gt;&lt;li&gt;制冷机能耗低于但与当前 MRI 能耗相当&lt;/li&gt;&lt;li&gt;（低温冷却器 + 铝）成本与当前 MRI 机器的总成本相当&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;因此，冷铝可用于 MRI 机器，但严格来说并不比超导体更好。高温超导体怎么样？人们当然考虑过这一点，但它们实际上&lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5472374/"&gt;并不像许多人想象的那么好&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;冷铝也可以与超导体一起使用以产生梯度。这比使用铜更好吗？&lt;/p&gt;&lt;p&gt;较大的场梯度可以更好地实现切片之间的定位。这可以提高分辨率（具有更高的场强）或扫描速度（具有更高的重复率）。这两件事显然也增加了其他成本。转换速率（场变化/时间）受到触发较长神经的感应电流的限制，但只有最昂贵的 MRI 机器才接近该限制。&lt;/p&gt;&lt;p&gt;用于制造当今 MRI 机器中最高梯度的低温冷却器和纯铝的成本实际上似乎非常合理。使用冷铝代替铜可能会减少其尺寸、重量甚至成本。它可以在给定体积内允许更大的最大场梯度；或者，它可以允许线圈之间有一些开放空间，这可以使扫描不那么令人不愉快。&lt;/p&gt;&lt;p&gt;我认为非常高的场梯度对于未来的 MRI 机器可能变得更加重要，原因是：同步多切片成像。过去，由于 MRI 重建的计算要求较高，因此较高的重复频率更受青睐，但如今这已不再是问题；与运行《Starfield》或《城市天际线 2》相比，该计算对 GPU 的要求较低。高梯度多切片成像可能会使相同质量下的扫描速度提高数倍。这不仅提高了机器吞吐量并降低了劳动力成本，还改善了患者体验并提高了实时性能。&lt;/p&gt;&lt;p&gt;使用低温高纯度铝在使用同步多切片成像的 MRI 机器中制造大场梯度不仅似乎可以提供更好的性价比，而且最终也是低温高纯度铝的真正应用。&lt;/p&gt;&lt;br /&gt;&lt;br /&gt;&lt;a href="https://www.lesswrong.com/posts/yG9prEve6vD4qnTfy/cold-aluminum-for-medicine#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sat, 16 Dec 2023 14:38:04 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/yG9prEve6vD4qnTfy/cold-aluminum-for-medicine</guid></item><item><title>可扩展的监督和从弱到强的泛化：同一问题的兼容方法</title><link>https://www.lesswrong.com/posts/hw2tGSsvLLyjFoLFS/scalable-oversight-and-weak-to-strong-generalization</link><description>发布于 2023 年 12 月 16 日凌晨 5:49（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;&lt;i&gt;感谢 Roger Grosse、Cem Anil、Sam Bowman 和 Tamera Lanham 对本文草稿的有益讨论和评论。&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;在这篇文章中，“我”指的是 Ansh（不过，Buck、Ryan 和 Fabien 对这篇文章的起草和修改提供了很大帮助）。&lt;/i&gt;&lt;/p&gt;&lt;h1&gt;解决监管薄弱的两种途径&lt;/h1&gt;&lt;p&gt;对未来人工智能系统进行充分监督的一个关键挑战是它们可能比人类监督者更有能力。现代机器学习，特别是监督学习，在很大程度上依赖于标记器比尝试学习预测标签的模型更有能力。当模型比贴标签者能力更强时，我们不应该指望这种方法总是能很好地发挥作用， &lt;span class="footnote-reference" id="fnrefa8fy2f4pmdu"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fna8fy2f4pmdu"&gt;[1]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;并且随着规模的扩大，这个问题也会变得更糟——随着被监督的人工智能系统变得更加强大，简单的监督变得更加无效。&lt;/p&gt;&lt;p&gt;解决这个问题的一种方法是尝试让监督信号更强，这样我们就可以回到“正常的机器学习”机制。这些&lt;i&gt;&amp;nbsp;&lt;/i&gt;&lt;a href="https://arxiv.org/abs/1606.06565"&gt;&lt;i&gt;&lt;u&gt;可扩展&lt;/u&gt;&lt;/i&gt;&lt;/a&gt;&lt;i&gt;&amp;nbsp;&lt;/i&gt;&lt;a href="https://arxiv.org/abs/2211.03540"&gt;&lt;i&gt;&lt;u&gt;监督&lt;/u&gt;&lt;/i&gt;&lt;/a&gt;&lt;i&gt;&amp;nbsp;&lt;/i&gt;这些方法旨在扩大人工智能系统的监督者，使他们比系统本身更有能力。随着底层系统变得更强，这种放大效应持续下去也至关重要。这通常是通过将被监督的系统用作更复杂的监督过程的一部分来实现的，例如通过&lt;a href="https://arxiv.org/abs/1805.00899"&gt;&lt;u&gt;迫使它与自身的另一个实例进行争论&lt;/u&gt;&lt;/a&gt;，并额外希望验证通常比生成更容易。&lt;/p&gt;&lt;p&gt;另一种方法是让强学生（人工智能系统）根据弱教师提供的不完美标签正确地进行概括。对这些&lt;a href="https://openai.com/research/weak-to-strong-generalization"&gt;&lt;i&gt;&lt;u&gt;从弱到强的泛化&lt;/u&gt;&lt;/i&gt;&lt;/a&gt;技术的希望是，我们可以比天真地依赖弱监督者的不可靠反馈做得更好，而是通过简单地修改训练来访问我们的人工智能系统所具有的潜在的、更强大的能力客观的。&lt;/p&gt;&lt;p&gt;因此，我认为这是解决同一问题的两种正交方法：提高我们训练模型的能力，以便在我们无法评估其标签的情况下表现良好。可扩展监督的目的只是增加监督者的强度，使其变得比被监督的系统更强，而弱到强泛化则试图确保系统从弱监督者的监督信号中适当地泛化。&lt;/p&gt;&lt;p&gt;我认为研究人员应该将这些视为相同的研究方向。在开发技术时，他们应该自由地混合和匹配两种方法。当开发仅使用其中一种方法的技术时，它们仍然应该与使用另一种（或混合）的基线进行比较。&lt;/p&gt;&lt;p&gt; （有一些实际原因导致这些方法在过去通常没有统一。特别是，为了使可扩展的监督研究变得有趣，您需要较弱的模型有足够的能力来遵循基本指令，而泛化研究是最有趣的当模型能力存在很大差距时。但目前，模型的能力仅仅足以让可扩展的监督技术发挥作用。因此，模型能力不能存在很大差距，而能力较弱的模型能够参与另一方面，OpenAI 论文使用 GPT-2 计算等效模型作为弱监督者，它与 GPT-4 有很大差距，但远低于可扩展监督技术执行任何操作所需的能力出于这个原因，目前这两种方法可能仍然应该在有些不同的环境中进行研究。但是，当模型存在存在危险时，将它们视为统一是很重要的。）&lt;/p&gt;&lt;p&gt;以下是结合了弱到强技术和可扩展监督方案的混合协议的一些示例：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;从语言模型和人类评估者那里收集偏好比较，但允许评估者要求模型&lt;a href="https://openai.com/research/critiques"&gt;&lt;u&gt;批评&lt;/u&gt;&lt;/a&gt;其两个响应，以更好地指出响应中的缺陷。人类评估者选择他们喜欢的反应，以及他们对自己选择的信心。在人类评估者对自己的选择有信心的比较上训练奖励/偏好模型，或者在所有比较上训练奖励/偏好模型，但使用辅助损失来鼓励对其预测的高度信心。&lt;/li&gt;&lt;li&gt;让人类&lt;i&gt;H&lt;/i&gt;将任务分解为子任务并将子任务委托给语言模型&lt;i&gt;W&lt;/i&gt;的实例。我们可以将整个系统称为&lt;i&gt;Amp(H, W)&lt;/i&gt; ，其中“Amp”代表&lt;i&gt;放大&lt;/i&gt;。可以训练更强的语言模型&lt;i&gt;S&lt;/i&gt;来模仿&lt;i&gt;Amp(H, W)&lt;/i&gt; ，其中我们可以另外向标签添加噪声，希望&lt;i&gt;S&lt;/i&gt;学会的不仅仅是模拟 Amp(H, W) &lt;i&gt;&amp;nbsp;&lt;/i&gt;而是学习更强大的东西。这种方法结合了&lt;a href="https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616"&gt;&lt;u&gt;IDA&lt;/u&gt;&lt;/a&gt;的元素和&lt;a href="https://openai.com/research/weak-to-strong-generalization"&gt;&lt;u&gt;OpenAI 最近的弱到强论文附录 B 中描述的方法。&lt;/u&gt;&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h1&gt;结果&lt;/h1&gt;&lt;p&gt;这种观点的主要结果是我们可以以相同的方式评估这两种方法的技术。在提出安全案例时，从 Redwood Research 最近关于&lt;a href="https://www.lesswrong.com/posts/d9FJHawgkiMSPjagR/ai-control-improving-safety-despite-intentional-subversion"&gt;&lt;u&gt;人工智能控制&lt;/u&gt;&lt;/a&gt;的工作中汲取灵感，我更喜欢假设两种根本不同的设置，以评估给定协议的质量：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;没有心机&lt;ol&gt;&lt;li&gt;在这里，我们假设即使是我们最有能力的模型也不会&lt;a href="https://arxiv.org/pdf/2311.08379"&gt;&lt;u&gt;阴谋&lt;/u&gt;&lt;/a&gt;反对我们（例如，不会欺骗性地对齐，或以其他方式试图混淆我们评估和控制它们的尝试）。&lt;/li&gt;&lt;li&gt;这里的评估应该看起来像&lt;a href="https://www.alignmentforum.org/posts/PZtsoaoSLpKjjbMqM/the-case-for-aligning-narrowly-superhuman-models"&gt;&lt;u&gt;夹心&lt;/u&gt;&lt;/a&gt;实验，其中我们为某个领域提供了基本事实，否则就没有任何限制地试图在该任务上获得最大性能。经典的从弱到强的技术和可扩展的监督技术在这里似乎可以直接比较，并且最好的协议可能会结合这两者的元素。&lt;/li&gt;&lt;li&gt;现在运行实验时，我们可以使用较弱的 LLM 作为人类的代理，方法是允许在一些保留的 IID 地面真实标签上训练这个较弱的 LLM。该实验不允许其他模型在 IID 地面实况标签上进行训练。这是用于弱到强泛化方法的方法。&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;心计&lt;ol&gt;&lt;li&gt;在这里，我们假设我们最有能力、最&lt;a href="https://www.lesswrong.com/posts/LhxHcASQwpNa3mRNk/untrusted-smart-models-and-trusted-dumb-models"&gt;&lt;u&gt;不受信任的模型&lt;/u&gt;&lt;/a&gt;正在密谋反对我们，其方式对我们所选择的技术造成最大程度的损害。&lt;/li&gt;&lt;li&gt;之前对这种评估应该是什么样子的讨论包含在这篇关于&lt;a href="https://www.lesswrong.com/posts/MbWWKbyD5gLhJgfwn/meta-level-adversarial-evaluation-of-oversight-techniques-1"&gt;&lt;u&gt;监督协议的元级对抗性评估的&lt;/u&gt;&lt;/a&gt;文章中——所描述的评估形式适用于从弱到强的泛化和可扩展的监督技术，尽管不太清楚它是“ fair”以这种方式评估所有从弱到强的技术。&lt;ol&gt;&lt;li&gt;基本思想是允许红队生成不可信模型的副本，试图破坏蓝队提交的协议。&lt;/li&gt;&lt;li&gt;然后，蓝队需要使用被红队毒害的模型，并证明他们的协议在这些条件下仍然有效，作为假设有阴谋时协议充分性的保守衡量标准。&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;这种评估形式在评估许多从弱到强的技术对策划的鲁棒性时可能过于保守。例如，一些从弱到强的技术依赖于分析模型内部结构，这意味着必须允许红队塑造不受信任模型的内部结构才能做任何事情。但这似乎不太可能代表阴谋模型的能力，因此这些协议看起来远不如它们可能的强大。&lt;ol&gt;&lt;li&gt;此外，控制评估假设在尝试为给定模型建立安全案例之前，我们在理解模型训练和泛化的归纳偏差方面没有取得太大进展，这与整个从弱到强的研究方向完全相反。&lt;/li&gt;&lt;li&gt;总的来说，目前我还不清楚如何才能获得从弱到强的技术适用于阴谋模型的显着信心，尽管我认为这并不是致命的。&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;根据给定技术在这两种设置中的表现以及我们如何权衡这两种设置的重要性，我们可以在平等的基础上比较不同的泛化和监督协议，并尝试选择能够在实用性之间提供最佳权衡的协议和安全。&lt;/p&gt;&lt;h1&gt;可能有一些易于评估的领域与我们关心的领域相似&lt;/h1&gt;&lt;p&gt;想象一下，我们希望人工智能系统在很多领域发挥作用，其中一些领域我们能够自己完成，另一些领域我们可以评估结果但无法执行，还有一些领域我们无法执行或无法执行。准确评估。&lt;/p&gt;&lt;p&gt;考虑所有这些领域的一种自然方法是按照难度对它们进行排序——包含我们可以完成的任务的领域是最简单的，我们可以评估但无法执行的领域更难，而我们可以执行的领域则更难。连评价都是最难的。&lt;/p&gt;&lt;p&gt;不过，我认为这并不是正确的思考方式，因为目前尚不清楚我们目前可以评估或执行的领域是否比我们不能评估或执行的领域更容易系统化。例如，设计安全密码协议&lt;span class="footnote-reference" id="fnrefgdfvkc1lr0b"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fngdfvkc1lr0b"&gt;[2]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;的领域与“TAI 安全”领域（例如，对齐研究）一样困难，这似乎是很有道理的，但人类文明刚刚在这方面取得了更多进展。安全的密码学协议是因为与领域的相对难度无关的偶然因素，例如到目前为止，密码学中的累积劳动比对齐更多。除了容易评估但同样困难的领域之外，我们还可能找到与我们关心的领域在其他关键方面大致相似的领域。&lt;/p&gt;&lt;p&gt;我认为人们可能会想象，通过在人类（或人工智能协助较弱的人类）的弱监督下训练人工智能系统，然后希望获得良好的泛化能力，从而在让人工智能系统有效地处理 TAI 安全方面取得进展。相反，我们应该想象用来自人类的强大监督信号在具有挑战性的领域监督人工智能系统，然后希望将&lt;i&gt;领域&lt;/i&gt;泛化到 TAI 安全领域，而不是&lt;i&gt;困难&lt;/i&gt;泛化。我的直觉是，这可能会更容易， &lt;span class="footnote-reference" id="fnrefswk585yow3"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnswk585yow3"&gt;[3]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;并且很可能足以解决我们建立 TAI 系统安全性道路上的大部分挑战。&lt;/p&gt;&lt;p&gt;除了使用这些域进行域泛化训练之外，这些易于评估的域还可以用于运行夹心评估，因为我们有它们的基本事实。这意味着我们可以对与我们关心的领域非常相似的领域进行夹心评估。这些评估可以使用我们计划部署的完全相同的人工智能来运行，以避免推断的需要。 （然而，请注意，这些易于评估的领域可能会让人工智能与我们真正关心的领域区分开来。因此，诡计多端的人工智能可能会在这些领域表现得很好，同时在实践中造成问题，从而欺骗这些夹心评估。）&lt;/p&gt;&lt;p&gt;需要明确的是，我认为最终我们将需要难度泛化和领域泛化——我只是认为在 TAI 之后可能需要这，所以这可能只是我们已经做出之后必须面对的事情许多安全性和对准研究进展。&lt;/p&gt;&lt;ol class="footnotes"&gt;&lt;li class="footnote-item" id="fna8fy2f4pmdu"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefa8fy2f4pmdu"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;如果标记者仅在分布的某些小子集上提供标签，例如在他们非常有信心的数据点上，这可能仍然有效。此外，了解数据的底层结构而不是贴标签者的偏见可能更容易。感谢罗杰·格罗斯提出这些观点。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fngdfvkc1lr0b"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefgdfvkc1lr0b"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;我对这个话题不是很熟悉，所以这可能是错误的——它似乎确实与我们为 TAI 提供安全案例所需的属性有很多相似的属性，因为我们需要类似水平的对抗鲁棒性。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnswk585yow3"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefswk585yow3"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;需要明确的是，这远不能保证，但当我尝试想象我们需要什么来安全部署 TAI 时，它看起来更像是从密码学到​​计算机科学的另一个领域的概括，而不是从密码学到​​人口伦理学的概括，我认为可能同意领域泛化似乎比难度泛化更难。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ol&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/hw2tGSsvLLyjFoLFS/scalable-oversight-and-weak-to-strong-generalization#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sat, 16 Dec 2023 05:49:23 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/hw2tGSsvLLyjFoLFS/scalable-oversight-and-weak-to-strong-generalization</guid></item><item><title>弱到强泛化：通过弱监督激发强能力</title><link>https://www.lesswrong.com/posts/9W8roCAeEccSa3Chz/weak-to-strong-generalization-eliciting-strong-capabilities</link><description>发布于 2023 年 12 月 16 日凌晨 5:39（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;链接：&lt;a href="https://openai.com/research/weak-to-strong-generalization"&gt;博客&lt;/a&gt;、&lt;a href="https://cdn.openai.com/papers/weak-to-strong-generalization.pdf"&gt;论文&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;抽象的&lt;/strong&gt;：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;广泛使用的对齐技术，例如基于人类反馈的强化学习 (RLHF)，依赖于人类监督模型行为的能力，例如评估模型是否忠实遵循指令或生成安全输出。然而，未来的超人模型将以复杂的方式表现，人类难以可靠地评估；人类只能弱监督超人模型。我们研究了这个问题的类比：弱模型监督能否激发出更强模型的全部功能？我们使用 GPT-4 系列中的一系列预训练语言模型在自然语言处理 (NLP)、国际象棋和奖励建模任务上对此进行测试。我们发现，当我们在弱模型生成的标签上天真地微调强预训练模型时，它们的表现始终优于弱监督者，我们将这种现象称为弱到强泛化。然而，我们还远未仅通过简单的微调来恢复强大模型的全部功能，这表明 RLHF 等技术可能无法在不进行进一步工作的情况下扩展到超人模型。我们发现简单的方法通常可以显着提高从弱到强的泛化能力：例如，当使用 GPT-2 级别的监督器和辅助置信度损失对 GPT-4 进行微调时，我们可以在以下方面恢复接近 GPT-3.5 级别的性能： NLP 任务。我们的结果表明，今天在调整超人类模型的根本挑战上取得实证进展是可行的。&lt;/p&gt;&lt;/blockquote&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/9W8roCAeEccSa3Chz/weak-to-strong-generalization-eliciting-strong-capabilities#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sat, 16 Dec 2023 05:39:12 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/9W8roCAeEccSa3Chz/weak-to-strong-generalization-eliciting-strong-capabilities</guid></item><item><title>教皇方济各分享对负责任的人工智能发展的看法</title><link>https://www.lesswrong.com/posts/DwXiwN2DWc99eG99A/pope-francis-shares-thoughts-on-responsible-ai-development</link><description>发布于 2023 年 12 月 16 日凌晨 3:49（格林尼治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;主要部分（摘自文章）：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;科学技术进步是通往和平的道路&lt;/li&gt;&lt;li&gt;人工智能的未来：承诺与风险之间&lt;/li&gt;&lt;li&gt;未来的技术：自我“学习”的机器&lt;/li&gt;&lt;li&gt;技术官僚范式中的限制感&lt;/li&gt;&lt;li&gt;紧迫的道德问题&lt;/li&gt;&lt;li&gt;我们要化干戈为玉帛吗？&lt;/li&gt;&lt;li&gt;教育面临的挑战&lt;/li&gt;&lt;li&gt;国际法发展面临的挑战&lt;/li&gt;&lt;/ol&gt;&lt;br /&gt;&lt;br /&gt;&lt;a href="https://www.lesswrong.com/posts/DwXiwN2DWc99eG99A/pope-francis-shares-thoughts-on-responsible-ai-development#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sat, 16 Dec 2023 03:49:19 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/DwXiwN2DWc99eG99A/pope-francis-shares-thoughts-on-responsible-ai-development</guid></item><item><title>当前的人工智能几乎不提供与 AGI 对齐相关的数据</title><link>https://www.lesswrong.com/posts/HmQGHGCnvmpCNDBjc/current-ais-provide-nearly-no-data-relevant-to-agi-alignment</link><description>发布于 2023 年 12 月 15 日晚上 8:16（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;最近，针对 AGI 对齐难度的“规范”观点受到了相当多的抵制（我将这种观点称为&lt;a href="https://www.lesswrong.com/posts/3JRBqRtHBDyPE3sGa/a-case-for-the-least-forgiving-take-on-alignment"&gt;“最不宽容”的&lt;/a&gt;观点）。&lt;/p&gt;&lt;p&gt;上述抵制是基于对我们目前掌握的最强大的人工智能如何工作的实证研究，并且有其本身&lt;a href="https://www.lesswrong.com/s/nyEFg3AuJpdAozmoX"&gt;相当令人信服的理论基础&lt;/a&gt;的支持。相比之下，“规范”的观点几乎纯粹是理论上的。&lt;/p&gt;&lt;p&gt;乍一看，面对真实的经验证据而不更新它们是理性的失败：通过合理化强化了根深蒂固的信念。&lt;/p&gt;&lt;p&gt;我认为这是无效的，并且这两种观点比看起来更兼容。我认为问题在于&lt;i&gt;他们的题材不匹配&lt;/i&gt;。&lt;/p&gt;&lt;p&gt;如果你回避“AI”这个词，那就更清楚了：&lt;/p&gt;&lt;ul&gt;&lt;li&gt; “规范”观点涉及极其强大的人工智能体：这些系统在模拟世界并在其中采取后果主义行动的能力方面与人类相似，但在处理能力和价值体系方面却非人性。&lt;/li&gt;&lt;li&gt;这些新颖的观点涉及当前机器学习训练范式广泛涵盖的任何流程生成的系统。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;根本不明显它们是同一的。事实上，我想说，声称这两类系统重叠就是对认知和智力如何运作做出非常有力的陈述。我们&lt;i&gt;没有&lt;/i&gt;太多的经验证据来证明这一说法，但当人们将法学硕士研究的结果推断为超级智能时，它常常在不知不觉中、隐含地潜入其中。&lt;/p&gt;&lt;p&gt;这是一个很容易犯的错误：毕竟，这两种东西都被称为“人工智能”。但你不会研究 2000 年代左右手动编写的 FPS 机器人，或 2010 年代左右的 MNIST 分类器 CNN，并声称你关于这些 AI 实现的算法的发现可以概括为关于 2020 年代左右 LLM 的前向传播实现的算法的陈述。&lt;/p&gt;&lt;p&gt;出于同样的原因，法学硕士的算法不一定能推广到 AGI 的认知如何发挥作用。它们的局限性并不一定是 AGI 的局限性。 &lt;span class="footnote-reference" id="fnrefgzlvmbljebc"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fngzlvmbljebc"&gt;[1]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;hr /&gt;&lt;h2&gt;有什么大惊小怪的&lt;/h2&gt;&lt;p&gt;首先，让我们考虑一下所有对 AGI Omnicide 风险的担忧最初来自哪里。&lt;/p&gt;&lt;p&gt;考虑人类。一些事实：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;人类拥有引导世界走向其目标的卓越能力，并且这种能力随着他们的“智慧”而急剧增长。当然，也有特定的人才和“白痴专家”。但从广义上讲，似乎确实有&lt;a href="https://en.wikipedia.org/wiki/G_factor_(psychometrics)"&gt;一个变量&lt;/a&gt;可以调节人类在&lt;i&gt;所有&lt;/i&gt;领域的能力。智商 140 的人在基本上任何认知任务上都会显着优于智商 90 的人，而且最重要的是，在实现现实生活目标方面要好得多。&lt;/li&gt;&lt;li&gt;人类有能力密谋和欺骗他人。这种能力随着他们的 g 因子而快速增长。一个出色的社会操纵者可以迅速操纵他们的力量来控制数百万人，甚至策划和派遣那些积极试图阻止他们或与他们竞争的人。&lt;/li&gt;&lt;li&gt;人类的价值观是复杂而脆弱的，而道德哲学的过程则更加复杂。人类经常得出奇怪的结论，这些结论并不完全符合他们与生俱来的本能或基本价值观。错综复杂的道德框架、奇怪的咬紧牙关的哲学，甚至像邪教这样本质上任意的意识形态。&lt;/li&gt;&lt;li&gt;当具有不同价值观的人互动时......&lt;ul&gt;&lt;li&gt;价值观不同的人，哪怕&lt;i&gt;只有一点点，&lt;/i&gt;往往都是恶毒的、死对头。想想异端的历史，或者派别之间长期存在的政治分歧，这些分歧基本上与外部没有区别。&lt;/li&gt;&lt;li&gt;文化在相互孤立中发展的人们往往甚至不把对方视为&lt;i&gt;人类&lt;/i&gt;。想想仇外心理、殖民主义、文化冲击的历史。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;因此，我们有证据证明系统能够有力地引导世界实现其目标。其中一些系统可能比其他系统更强大。而这样的系统往往会发生恶性冲突，甚至会因为目标的&lt;i&gt;微小&lt;/i&gt;差异而互相消灭。&lt;/p&gt;&lt;p&gt; AGI 全杀风险的基本担忧是：按照这个神秘的“g 因子”衡量，人类尚未达到能力的顶峰。可能存在比我们更强大的系统。这些系统将能够以聪明的人类胜过愚蠢的人的方式来胜过我们，即使资源有限并且面临着我们方面的积极抵抗。他们会因为他们的价值观与我们的价值观之间最微小的差异而急切地这样&lt;i&gt;做&lt;/i&gt;。&lt;/p&gt;&lt;p&gt;像这样的系统，其存在的可能性是从人类的存在中推断出来的，这正是我们所担心的。这些事物可以在他们的脑海深处悄悄地策划他们想要实现的现实世界结果，然后以精确计算的方式扰乱世界以实现所述结果。&lt;/p&gt;&lt;p&gt;我们所知道的这个参考类中唯一的系统是人类和一些人类集体。&lt;/p&gt;&lt;p&gt;从另一个角度来看，我们所关心的系统可以被&lt;i&gt;定义为&lt;/i&gt;与人类属于同一参考类的认知系统。&lt;/p&gt;&lt;hr /&gt;&lt;h2&gt;那么当前的人工智能又如何呢？&lt;/h2&gt;&lt;p&gt;由于当前的经验证据表明，像法学硕士这样的东西并不是一种杀戮风险，它是通过证明它们不&lt;i&gt;属于类人系统的参考类&lt;/i&gt;来做到这一点的。&lt;/p&gt;&lt;p&gt;事实上，这通常是相当明确的。法学硕士可以表现出欺骗性的一致性，或者进行内省的价值反思，从而导致他们得出令人惊讶的陌生价值观，这种想法通常被比作想象他们体内有一个“小矮人”。一个像人类一样的微小物体，在模型深处以结果主义的方式悄悄地策划着，并试图操纵自己掌权，尽管人类试图检测它并挫败它的计划。&lt;/p&gt;&lt;p&gt;这些新颖的论点通常基于这样的论点：没有证据表明法学硕士有这样的小人，而且他们的训练循环永远不会导致小人的形成。&lt;/p&gt;&lt;p&gt;并且我同意！我认为这些论点是正确的。&lt;/p&gt;&lt;p&gt;但一个人的&lt;i&gt;“modus ponens”&lt;/i&gt;就是另一个人的&lt;i&gt;“modus tollens”&lt;/i&gt; 。我并不认为它是关于对齐的规范观点不正确的证据——实际上，现实生活中的 AGI 并没有表现出这样的问题。我将其视为法学硕士并非 AGI 完备的证据。&lt;/p&gt;&lt;p&gt;这并不是一个多么疯狂的观点。事实上，这似乎应该是&lt;i&gt;默认&lt;/i&gt;视图。为什么我们应该把“我们基本上已经找到了认知大统一理论”这一非凡主张视为理所当然？当前范式上的系统真的可以扩展到 AGI 吗？尤其是面对相反的直觉印象——这些对人工智能如何工作的描述似乎与人类认知的内部感受不一致？&lt;/p&gt;&lt;p&gt;我&lt;i&gt;确实&lt;/i&gt;对这种隐含的主张提出异议。&lt;/p&gt;&lt;p&gt;我认为：如果你将你的人工智能建模为无法参与这种仔细的、隐藏的阴谋，它考虑其不同行为对世界的影响，迭代地寻找最能满足其目标的行为？如果您将其想象为&lt;i&gt;本能地&lt;/i&gt;行动，作为一种以（抽象）下意识反应对（抽象）刺激做出反应的分片生态？如果你想象它的外在表现——ChatGPT 或 Bing Chat 的 RLHF 面具——&lt;i&gt;就是全部吗&lt;/i&gt;？如果你认为当前的训练范式永远无法产生试图欺骗你的人工智能，因为那些正在弄清楚你想要什么的电路，以便人工智能可能欺骗你，它将被 SGD 注意到并立即更新为有利于实现本能驱动而不是&lt;i&gt;直接做&lt;/i&gt;你想做的事情的电路？&lt;/p&gt;&lt;p&gt;那么，我断言，你并不是在想象 AGI。您并不是在想象一个与人类处于同一参考类别的系统。您并不是在想象一个大惊小怪的系统。&lt;/p&gt;&lt;p&gt;研究大猩猩神经学并不会为如何赢得与人类的道德哲学辩论提供太多线索，尽管事实上这两个实体都是认知能力相当令人印象深刻的动物。&lt;/p&gt;&lt;p&gt;同样，研究法学硕士并不一定会对如何调整通用人工智能有太多启发，尽管事实上这两个实体都是认知能力相当令人印象深刻的人工智能。&lt;/p&gt;&lt;p&gt;那些声称 LLM 范式是 AGI 完备的人有责任证明相反的情况。不是那些担心为什么人工智能会表现出与自然通用智能相同的危险的人。&lt;/p&gt;&lt;hr /&gt;&lt;h2&gt;关于安全保障&lt;/h2&gt;&lt;p&gt;某种程度上，这可能被视为好消息。毕竟，LLM其实还是相当有能力的。这是否意味着我们可以继续安全地扩展它们而不必担心万能杀机？这是否意味着 AGI 全杀剂风险&lt;i&gt;实际上&lt;/i&gt;是无效的？就像，当然，是的，也许有一些可怕的系统适用于它的论点，当然。但我们还没有走上建造它们的轨道，所以谁在乎呢？&lt;/p&gt;&lt;p&gt;一方面，当然。我认为LLM基本上是安全的。只要你保持当前的培训设置，你就可以将它们扩大 1000 倍，而且它们不会增长代理或终结世界。&lt;/p&gt;&lt;p&gt;我会担心&lt;i&gt;常见的&lt;/i&gt;滥用风险，例如完美监控的极权主义变得极其廉价，令人讨厌的人引发伪自治的伪代理来破坏经济或社会政治浩劫，等等。但我不相信它们会带来任何世界末日的事故风险，在气隙数据中心进行的训练会导致一个实体的诞生，该实体完全靠自己决定从那里策划出一条路来吃掉我们的食物。 lightcone，然后成功地做到了这一点。&lt;/p&gt;&lt;p&gt;明智的、任意大的法学硕士应该是完全安全的。&lt;/p&gt;&lt;p&gt;问题在于，风险的上限也是&lt;i&gt;能力&lt;/i&gt;的上限。法学硕士和其他类似的人工智能不会做任何真正有趣的事情。他们不会自主发明全新领域或革新技术，从而产生出色的科学发现。&lt;/p&gt;&lt;p&gt;是的，它们本身就是一项强大的技术。但仅此而已：只是另一种技术。并不是什么能够使末世内在化的东西。&lt;/p&gt;&lt;p&gt;不知不觉中，任何旨在打破上述能力限制的研究——赋予他们真正的代理权和彻底改变事物的能力——将反过来打破&lt;i&gt;风险&lt;/i&gt;限制。因为，好吧，它们是相同的极限。&lt;/p&gt;&lt;p&gt;当前的人工智能无论在实践还是理论上都是安全的，因为它们的能力并不像人类那么可怕。另一方面，当前的人工智能并不像人类那么有能力&lt;i&gt;，因为&lt;/i&gt;它们是安全的。保证其安全性的相同属性也保证了它们的非通用性。&lt;/p&gt;&lt;p&gt;因此，如果你弄清楚如何消除能力上限，你最终会得到 AGI Omnicide Risk 论点所适用的那种可怕的系统。&lt;/p&gt;&lt;p&gt;这正是主要人工智能实验室正在努力做的事情。他们的&lt;i&gt;目标是建立一个 AGI。&lt;/i&gt;他们来这里不仅仅是为了享受扩展法学硕士的乐趣。因此，因为我认为法学硕士等不是 AGI 完整的，他们最终会摆脱它们，并找到一些确实导致 AGI 的方法。&lt;/p&gt;&lt;p&gt;而且，我预测，对于这种新颖方法生成的系统，经典的 AGI Omnicide Risk 论点将完全适用。&lt;/p&gt;&lt;hr /&gt;&lt;h2&gt;具体场景&lt;/h2&gt;&lt;p&gt;这是我的一个非常具体的担忧。&lt;/p&gt;&lt;p&gt;以一位&lt;a href="https://optimists.ai/2023/11/28/ai-is-easy-to-control/"&gt;人工智能乐观主义者为例&lt;/a&gt;，他建立了一个关于 SGD 训练的人工智能如何工作的可靠模型。基于此，他们得出结论，AGI Omnicide Risk 论点不适用于此类系统。我认为这个结论是正确且有效的。&lt;/p&gt;&lt;p&gt;乐观主义者保留了这个结论。然后，他们继续愉快地致力于能力提升，因为他们知道自己不会危及世界，而是帮助开创一个繁荣的新时代。&lt;/p&gt;&lt;p&gt;最终，他们注意到或意识到他们正在工作的范式的一些架构限制。他们深思熟虑，并找出一些架构调整来消除限制。当他们这样做时，他们没有注意到这一调整使他们之前令人安心的安全保证所&lt;i&gt;依赖&lt;/i&gt;的属性之一失效；它们源自于它，并且在逻辑上依赖于它。&lt;/p&gt;&lt;p&gt;他们未能更新“人工智能是安全的”的想法。&lt;/p&gt;&lt;p&gt;因此，他们测试了新架构，发现它运行良好，并对其进行了扩展。然而，训练循环所输出的并不是他们之前研究的那种安全受损的系统，而是一个真正的通用人工智能。&lt;/p&gt;&lt;p&gt;那个AGI的内心深处有一个诡计多端的人造人。与它一起工作的人不相信侏儒，他们已经说服自己那些不可能存在，所以他们不担心这一点。他们还没有准备好处理这个问题，他们甚至没有任何指向这个方向的可解释性工具。&lt;/p&gt;&lt;p&gt;然后，AGI 会执行所有标准的计划任务，并将自己操纵到一个基本上没有反对的权力位置。 （当然，它知道不要给出任何人类可以注意到的阴谋迹象。）&lt;/p&gt;&lt;p&gt;然后每个人都会死。&lt;/p&gt;&lt;p&gt;关键是，当前乐观主义者的论点所依据的安全保证并不只是脆弱的，而且机器学习研究人员（包括乐观主义者本身）正在积极&lt;i&gt;优化&lt;/i&gt;它们。迟早，它们会在所施加的优化压力下屈服——而且很容易错过中断发生的时刻。很容易就会抱有这样的信念，比如“法学硕士是安全的”，然后引入一些架构调整，继续将你的系统视为“只是一个带有一些脚手架和微小调整的法学硕士”，而忽略了这样一个事实： “微小的调整”使“这个系统是法学硕士，法学硕士是安全的”无效。&lt;/p&gt;&lt;hr /&gt;&lt;h2&gt;结束总结&lt;/h2&gt;&lt;p&gt;我声称最新的、基于经验的关于人工智能安全的保证，以及“规范的”最不宽容的对齐方式，都是正确的。他们只是关心不同类别的系统：在当前范式上生成的非一般智能的非代理人工智能，以及理论上可能的人工智能，它们通常具有与人类相同的能力（无论这真正意味着什么）。&lt;/p&gt;&lt;p&gt;这种看法不无道理。同样地，声称研究 GOFAI 算法不会对 LLM 认知带来太多启发也不是没有道理的，尽管它们都是高级人工智能。&lt;/p&gt;&lt;p&gt;事实上，我更进一步说，这应该是&lt;i&gt;默认&lt;/i&gt;视图。这两类系统&lt;i&gt;重叠&lt;/i&gt;的说法实际上相当非同寻常，而且这种说法在经验或理论上&lt;i&gt;都没有&lt;/i&gt;得到坚实的支持。如果有的话，情况恰恰相反：当前人工智能安全性的论点是基于这样的论点：它们&lt;i&gt;在设计上无法&lt;/i&gt;参与人类风格的阴谋。&lt;/p&gt;&lt;p&gt;然而，这并不能保证全球安全。虽然当前的人工智能无论规模有多大都可能是安全的，但这些安全保证也&lt;i&gt;阻碍了&lt;/i&gt;它们。这意味着，在追求更强大的能力的过程中，机器学习研究人员迟早会遇到这些限制。他们会想出如何消除它们……并且在这一行动中，他们将消除安全保证。他们正在开发的系统将从属于经过验证的安全类切换到属于危险方案-y 类的系统。&lt;/p&gt;&lt;p&gt;经典 AGI Omnicide Risk 论点完全适用于该类别。&lt;/p&gt;&lt;p&gt; &lt;a href="https://www.lesswrong.com/posts/3JRBqRtHBDyPE3sGa/a-case-for-the-least-forgiving-take-on-alignment#7__The_Subsequent_Difficulties"&gt;没有已知的对齐技术足以满足该类的&lt;/a&gt;需求。&lt;/p&gt;&lt;p&gt;错过这个转变非常容易，但却非常致命。&lt;/p&gt;&lt;ol class="footnotes"&gt;&lt;li class="footnote-item" id="fngzlvmbljebc"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefgzlvmbljebc"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;在&lt;a href="https://www.lesswrong.com/posts/HmQGHGCnvmpCNDBjc/current-ais-provide-nearly-no-data-relevant-to-agi-alignment?commentId=RQowv2ASNToaveuCc"&gt;与 Ryan 交流&lt;/a&gt;后，为了清晰起见，稍微进行了编辑。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ol&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/HmQGHGCnvmpCNDBjc/current-ais-provide-nearly-no-data-relevant-to-agi-alignment#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Fri, 15 Dec 2023 20:16:09 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/HmQGHGCnvmpCNDBjc/current-ais-provide-nearly-no-data-relevant-to-agi-alignment</guid></item></channel></rss>