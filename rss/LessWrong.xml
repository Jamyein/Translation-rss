<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>少错</title><link>https://www.lesswrong.com</link><description>致力于提炼理性艺术的社区博客</description><lastBuildDate>Sat, 06 Jan 2024 00:51:57 GMT</lastBuildDate><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>项目想法：认知论</title><link>https://www.lesswrong.com/posts/sYdT44uzMwoNXs2Lq/project-ideas-epistemics-1</link><description>发布于 2024 年 1 月 5 日晚上 11:41（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;a href="https://www.lesswrong.com/posts/sYdT44uzMwoNXs2Lq/project-ideas-epistemics-1#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Fri, 05 Jan 2024 23:41:24 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/sYdT44uzMwoNXs2Lq/project-ideas-epistemics-1</guid></item><item><title>基准研究#1：MMLU</title><link>https://www.lesswrong.com/posts/6qfunCnfYKRXioHhi/benchmark-study-1-mmlu</link><description>发布于 2024 年 1 月 5 日晚上 9:35（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;blockquote&gt;&lt;p&gt;&lt;strong&gt;背景说明：&lt;/strong&gt;基准研究是记录和研究基准论文的博客文章系列。我（Bruce）正在开发一个新的开源 LLM 评估框架，以比 EleutherAI LM Harness 更具灵活性。对于初始版本，我仅添加我研究过的基准。所有学习笔记均需在 10 分钟内读完。在撰写这些博客文章时，我会时不时地获得 GPT 帮助。我公开分享学习笔记的部分原因是为了让自己继续前进并帮助尚未阅读本文的人。&lt;/p&gt;&lt;/blockquote&gt;&lt;pre&gt; &lt;code&gt;@misc{hendrycks2021measuring, title={Measuring Massive Multitask Language Understanding}, author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt}, year={2021}, eprint={2009.03300}, archivePrefix={arXiv}, primaryClass={cs.CY} }&lt;/code&gt;&lt;/pre&gt;&lt;h1&gt;五点 TL;DR&lt;/h1&gt;&lt;ul&gt;&lt;li&gt;性能 -&amp;gt; 非专业人类：34.5%，专业人类：MMLU 上的 89.8%&lt;/li&gt;&lt;li&gt;&lt;strong&gt;程序性知识与陈述性知识：&lt;/strong&gt; GPT-3 比程序性知识更容易获取陈述性知识，但在计算繁重的 STEM 任务中准确性较低。&lt;/li&gt;&lt;li&gt;此外，模型在需要人类价值判断和程序知识的任务（例如专业法和道德场景）中表现不佳。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;方法论转变：&lt;/strong&gt;建议模型应该更像人类一样进行训练，从阅读和听力中学习，而不是仅仅依赖于大型问题库。 （让我想起我&lt;a href="https://www.lesswrong.com/posts/n45Awh7bkGRe4YayT/send-llms-to-school-instruction-tuning-with-human-curriculum"&gt;最近的论文&lt;/a&gt;）&lt;/li&gt;&lt;li&gt; &lt;strong&gt;GPT-3 校准结果：&lt;/strong&gt; GPT-3 被发现未校准，其置信度通常不能很好地反映实际精度，特别是在零样本设置下。&lt;/li&gt;&lt;/ul&gt;&lt;h1&gt;部分：摘要&lt;/h1&gt;&lt;ul&gt;&lt;li&gt;引入新的测试来测量文本模型的多任务准确性。&lt;/li&gt;&lt;li&gt;该测试涵盖 57 项任务，包括基础数学、美国历史、计算机科学、法律等主题。&lt;/li&gt;&lt;li&gt;强调模型需要具有广泛的世界知识和解决问题的能力才能在这项测试中获得高分。&lt;/li&gt;&lt;li&gt;研究结果表明，大多数最新模型的表现仅比随机机会稍好一些。&lt;/li&gt;&lt;li&gt;最大的 GPT-3 模型显示出显着的改进，平均超过随机概率近 20 个百分点。&lt;/li&gt;&lt;li&gt;尽管有这些改进，即使是最好的模型在所有 57 项任务中也达不到专家级的准确性。&lt;/li&gt;&lt;li&gt;观察模型在不同任务中表现不均匀的情况。&lt;/li&gt;&lt;li&gt;模型常常无法认识到自己的错误。&lt;/li&gt;&lt;li&gt;值得注意的是，在道德和法律等具有社会意义的科目上表现不佳，准确性近乎随机。&lt;/li&gt;&lt;li&gt;拟议的测试可作为综合评估模型的学术和专业理解的工具，突出显示需要改进的关键领域。&lt;/li&gt;&lt;/ul&gt;&lt;h1&gt;部分：简介&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;引入语言模型的新基准&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;目的：&lt;/strong&gt;弥合模型在预训练期间看到的知识与当前成功衡量标准之间的差距。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;设计：&lt;/strong&gt;涵盖STEM、人文和社会科学等各个领域的57个学科的基准，涵盖从初级到高级专业水平。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;特点：&lt;/strong&gt;侧重于测试数学、历史、法律和道德等领域的世界知识和解决问题的能力。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;当前 NLP 模型性能分析&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;观察：&lt;/strong&gt;大多数模型，包括那些具有多达 130 亿个参数的模型，只能实现随机机会性能。&lt;/li&gt;&lt;li&gt; &lt;strong&gt;GPT-3 性能：&lt;/strong&gt; 1750 亿个参数的 GPT-3 模型达到了显着更高的准确度 (43.9%)，但缺乏任何单一主题的专业知识。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;表现差异：&lt;/strong&gt; GPT-3 显示出不平衡的结果，在某些领域表现出色，但在其他领域表现近乎随机，特别是在计算密集型和人类价值观相关的科目中。 &lt;/li&gt;&lt;/ul&gt;&lt;figure class="image image_resized" style="width: 97.51%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6qfunCnfYKRXioHhi/rjzoh0m87ispoyrjah6u" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;现代 NLP 模型的挑战&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;知识应用：&lt;/strong&gt;当前的模型很难有效地应用预训练中的知识。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;薄弱环节：&lt;/strong&gt;物理、数学、法律和道德等学科的准确性较低，凸显了严重的薄弱环节。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;置信度与准确性：&lt;/strong&gt; GPT-3 经常错误地判断自己的知识，置信度明显偏离实际准确性。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;新基准的意义&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;综合评估：&lt;/strong&gt;该基准评估模型对人类学习重要的广泛主题的文本理解。&lt;/li&gt;&lt;/ul&gt;&lt;h1&gt;部分：多任务测试&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;创建全面的多任务测试&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;目的：&lt;/strong&gt;评估跨多个知识分支的文本模型。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;设计：&lt;/strong&gt;测试包括&lt;strong&gt;57 项任务，&lt;/strong&gt;涵盖人文、社会科学、硬科学和其他关键学习领域。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;任务来源：&lt;/strong&gt;问题是从各种在线来源手动收集的，包括 GRE 和 USMLE 练习题和本科课程。 &lt;/li&gt;&lt;/ul&gt;&lt;figure class="image image_resized" style="width: 84.83%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6qfunCnfYKRXioHhi/mkftfsksum9srabwgkqs" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;测试组成和结构&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;问题收集：&lt;/strong&gt;共收集&lt;strong&gt;问题15908个问题&lt;/strong&gt;。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;测试分段：&lt;/strong&gt;测试分为少量开发集、验证集和主测试集。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;任务难度级别：&lt;/strong&gt;任务按难度级别进行分类，例如小学、高中、大学或专业。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;人类准确度的基准&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;人类表现基线：&lt;/strong&gt;来自 Amazon Mechanical Turk 的非专业人类实现了 34.5% 的准确率。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;专家表现评估：&lt;/strong&gt;根据真实测试者的第 95 个百分点的准确度，专家级准确度约为 89.8%。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;强调现实世界的文本理解&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;目标：&lt;/strong&gt;评估模型从大量在线语料库中提取有用知识的效果。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;未来模型应用：&lt;/strong&gt;该测试既适用于单一模型，也适用于专家模型的混合。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;专注于特定学科领域&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;人文学科任务：&lt;/strong&gt;涵盖法律、哲学和历史等定性分析学科，需要法律推理和道德判断等技能。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;社会科学任务：&lt;/strong&gt;包括经济学、社会学和政治学等学科，重点关注人类行为和社会动态。&lt;/li&gt;&lt;li&gt; &lt;strong&gt;STEM 任务：&lt;/strong&gt;涵盖物理、计算机科学和数学等领域，侧重于经验方法和解决问题的能力。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;其他科目：&lt;/strong&gt;包括专业医学、金融和全球事实等领域，提供传统类别之外的各种主题。&lt;/li&gt;&lt;/ul&gt;&lt;h1&gt;部分：实验&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;实验设置和评估方法&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;评估目标：&lt;/strong&gt;测量多任务测试中各种任务的分类准确性。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;评估的模型：&lt;/strong&gt;包括 GPT-3（及其四种变体：小型、中型、大型、超大）和 UnifiedQA，以及 RoBERTa-base、ALBERT-xxlarge 和 GPT-2。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;评估过程：&lt;/strong&gt;使用适用于 GPT-3 的 OpenAI API 和适用于 UnifiedQA 的现有数据集，重点关注传输准确性，无需进一步调整。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;模型性能与比较&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;准确性测量：&lt;/strong&gt;评估四个广泛学科中每个模型的平均加权准确性：人文学科、社会科学、STEM 和其他。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;模型大小影响：&lt;/strong&gt;较大的 GPT-3 模型，尤其是 X-Large 变体，表现出比较小模型更好的性能。&lt;/li&gt;&lt;li&gt; &lt;strong&gt;UnifiedQA 性能：&lt;/strong&gt;尽管参数较少，但与少样本 GPT-3 X-Large 模型相比，仍表现出更高的准确性。 &lt;/li&gt;&lt;/ul&gt;&lt;figure class="image image_resized" style="width: 83.46%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6qfunCnfYKRXioHhi/tea01t31wg8q0884pgle" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;关于模型能力的具体发现&lt;/strong&gt;&lt;/p&gt;&lt;figure class="image image_resized" style="width: 39.82%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6qfunCnfYKRXioHhi/nppysstzlui2gi2spdl1" /&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;程序性知识与陈述性知识：&lt;/strong&gt; GPT-3 比程序性知识更容易获取陈述性知识，但在计算繁重的 STEM 任务中准确性较低。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;知识获取模式：&lt;/strong&gt; GPT-3 展示了一种不寻常的知识获取模式，与基础主题相比，在高级主题中表现更好。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;表现不平衡：&lt;/strong&gt; GPT-3 和 UnifiedQA 在不同主题上的表现参差不齐，表明存在知识差距。 &lt;/li&gt;&lt;/ul&gt;&lt;figure class="image image_resized" style="width: 40.24%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6qfunCnfYKRXioHhi/mdoe84we2utk1gaflnp6" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;校准和置信度分析&lt;/strong&gt;&lt;/p&gt;&lt;figure class="image image_resized" style="width: 41.71%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6qfunCnfYKRXioHhi/f013u2aj9bnwzspngrig" /&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;校准重要性：&lt;/strong&gt;检查模型的置信度与其实际预测精度之间的关系。&lt;/li&gt;&lt;li&gt; &lt;strong&gt;GPT-3 校准结果：&lt;/strong&gt; GPT-3 被发现未校准，其置信度通常不能很好地反映实际精度，特别是在零样本设置下。&lt;/li&gt;&lt;/ul&gt;&lt;h1&gt;部分：讨论&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;多模态理解的整合&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;当前限制：&lt;/strong&gt;现有的 NLP 模型（包括 GPT-3）不包含多模态信息。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;未来基准测试：&lt;/strong&gt;建议开发反映多模式功能的基准测试，例如使用 Amazon Mechanical Turk 任务进行“Turk 测试”。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;互联网作为综合培训集&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;预训练方法：&lt;/strong&gt;假设模型已经从互联网上大量、多样化的文本中获取了必要的知识，类似于人类的学习方法。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;方法论转变：&lt;/strong&gt;建议模型应该更像人类一样进行训练，从阅读和听力中学习，而不是仅仅依赖于大型问题库。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;评估形式和目的&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;评估策略：&lt;/strong&gt;评估零样本、少样本或传输设置中的预训练模型。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;任务多样化：&lt;/strong&gt;与相同分布的训练和测试集相比，能够收集更广泛和多样化的任务集。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;模型的局限性和未来的改进&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;性能缺陷：&lt;/strong&gt;模型在需要人类价值判断和程序知识的任务（例如专业法和道德场景）中表现不佳。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;提高准确性的挑战：&lt;/strong&gt;&lt;i&gt;通过额外的专门预训练来提高专业法律模型准确性的尝试取得了有限的成功。&lt;/i&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;扩展挑战：&lt;/strong&gt;质疑简单增加模型大小的有效性，指出需要更多数据以及深奥​​知识的数据可用性的潜在瓶颈。&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;&lt;br /&gt;&lt;a href="https://www.lesswrong.com/posts/6qfunCnfYKRXioHhi/benchmark-study-1-mmlu#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Fri, 05 Jan 2024 21:35:37 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/6qfunCnfYKRXioHhi/benchmark-study-1-mmlu</guid></item><item><title>几乎我遇到的每个人都会得到很好的服务，更多地思考应该关注什么</title><link>https://www.lesswrong.com/posts/Z55vXvngZxkPdKger/almost-everyone-i-ve-met-would-be-well-served-thinking-more</link><description>发布于 2024 年 1 月 5 日晚上 9:01（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;blockquote&gt;&lt;p&gt;&lt;i&gt;几乎我遇到的每个人都会因为花更多的时间思考应该关注什么而受益匪浅。&lt;/i&gt; ——萨姆·奥特曼&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;2020年5月，我们在港口停了两辆搬家卡车，将我们拥有的所有东西从一辆运到另一辆。约翰娜、莫德和我要离开瑞典，而新冠疫情的限制意味着我们一旦登上渡轮就被禁止返回。因此，第二辆卡车是我们让一个陌生人从岛上运送给我们的：瑞典卡车必须留在瑞典。&lt;/p&gt;&lt;p&gt;离开的动机是我们想在家教育 3 岁的莫德。在瑞典，这是非法的，所以大多数瑞典家庭教育者最终都去了波罗的海的两个岛屿之一。在我们的岛上，我们谁也不认识。我们没有工作等待。我们要留下一些东西，而不是去某个地方。我们三十多年来零碎成长的生活，一夜之间就消失了。我们必须弄清楚用什么来代替它。我应该创办另一家软件咨询公司来支持我们吗？我可以写吗？我们如何找到有意义的社会背景？&lt;/p&gt;&lt;h2&gt;生命是一个多臂强盗&lt;/h2&gt;&lt;p&gt;我们找房子时租的发霉的公寓可以看到海景。每天，深冬，我都会走到水边，从悬崖上跳水。在岩石之间的通道中游泳时，我意识到我可以使用概率论的概念来模拟我们的情况。&lt;/p&gt;&lt;p&gt;这是一个&lt;a href="https://en.wikipedia.org/wiki/Multi-armed_bandit"&gt;&lt;u&gt;多臂老虎机问题&lt;/u&gt;&lt;/a&gt;。这个问题&lt;a href="https://www.dropbox.com/s/yhn9prnr5bz0156/1933-thompson.pdf"&gt;&lt;u&gt;最初&lt;/u&gt;&lt;/a&gt;由生物学家&lt;a href="https://en.wikipedia.org/wiki/William_R._Thompson"&gt;&lt;u&gt;William R. Thompson&lt;/u&gt;&lt;/a&gt;于 1933 年以不同的名称进行研究，其核心是一个相当超现实的思想实验。一个赌徒面对一台老虎机（“独臂强盗”），只不过这台机器没有一只手臂——按照一些扭曲的梦境逻辑，它有&lt;i&gt;k 个&lt;/i&gt;手臂，手臂向各个方向伸出。其中一些武器有很高的概率支付头奖，而另一些则较差。但赌徒不知道哪个是哪个。&lt;/p&gt;&lt;p&gt;问题是按照最大化预期总收益的顺序拉动手臂。 （“收益”可以是任何东西。早期，这个问题被用来设计药物试验。在那里，头奖被定义为找到成功的治疗方法。如果你正在寻找合作伙伴，与人交谈是你拉动多重的方式——武装强盗和共鸣（或缺乏共鸣）就是回报。）&lt;/p&gt;&lt;p&gt;赌徒需要学习有关机器的新知识&lt;i&gt;，同时&lt;/i&gt;利用他们已经学到的知识来优化他们的决策。在文献中，这两种活动被称为&lt;i&gt;探索&lt;/i&gt;和&lt;i&gt;利用。&lt;/i&gt;你不能同时做这两件事。当你探索时，你正在向强盗拉动新的手臂，试图找出他们的预期回报。当你利用时，你会拉动你找到的最好的手臂。您需要找到合适的平衡点。如果你花在探索上的时间太少，你就会被困在一台预期收益较低的机器上。但如果你花太多时间探索，你的收入就会比你玩最好的手臂时少。这就是探索/利用的权衡。&lt;/p&gt;&lt;p&gt;人们倾向于倾向于探索/利用范围的不同方面。如果您像我一样具有高度开放性，那么探索就会很容易。但做出承诺并利用你对自己和世界的了解却更加困难。其他人则更加坚定，但他们的选择可能过于传统。他们错过了更好的努力途径。然而，大多数人往往在&lt;i&gt;这两&lt;/i&gt;方面都做得不够理想——不探索，不利用；但做事是出于盲目的习惯，而且是三心二意的。&lt;/p&gt;&lt;p&gt;首先，我要谈谈现实生活中的探索和利用。然后我将回到如何在它们之间进行权衡的问题。&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;探索：&lt;/strong&gt;顽强地寻找让你感到活着的东西&lt;/h2&gt;&lt;p&gt;有两种人。那些不明白世界有多么复杂的人，以及那些知道自己不明白世界有多么复杂的人。&lt;/p&gt;&lt;p&gt;为了驾驭生活，我们创建了外部世界的心理模型，然后我们将这些模型与现实混淆了。当你与模型精度不如你的人互动时，你有没有注意到这一点：就像他们戴着 VR 耳机，正在与你明知不存在的怪物作战？例如，20世纪90年代有一对瑞典夫妇在驻新德里大使馆工作。他们有一个管家。管家生日那天，他们给她做了蛋糕，并邀请她到他们的餐桌上吃饭。她拒绝了。她坚持说她必须在地板上吃饭，否则她就会投生为低等动物。人们很容易会说：“你可以摘下耳机了，那里什么也没有。吃点蛋糕吧。”但我们都戴着耳机。没有办法直接接触现实的生活方式。&lt;/p&gt;&lt;p&gt;诀窍是尽可能经常地将你的心理模型与外部世界进行碰撞。这就是探索的作用。您认为您知道老虎机的收益分布，但您尝试了一些新的东西。你发现你错了。您更新您的模型。&lt;/p&gt;&lt;p&gt;我的许多心智模型都是从别人那里学到的。仔细观察就会发现，他们也是从别人那里学来的，而别人又是从别人那里学来的——可以追溯到 20 世纪 50 年代的某个人。这不是 20 世纪 50 年代。吃点蛋糕吧。&lt;/p&gt;&lt;p&gt;例如，正如我在“&lt;a href="https://www.henrikkarlsson.xyz/p/search-query"&gt;&lt;u&gt;博客文章是一个非常长且复杂的搜索查询，目的是找到有趣的人并使他们将有趣的东西发送到您的收件箱&lt;/u&gt;&lt;/a&gt;”中所写的那样，直到我忘记了这一点，我才能够让我的博客“工作”我从大众媒体中学到的沟通模式。博客不是大众媒体。他们比那更强大。我被困在 50 年代。&lt;/p&gt;&lt;p&gt;有些心智模型比其他心智模型具有更大的影响力。我试着把注意力集中在这些上。意识到我不理解&lt;a href="https://www.henrikkarlsson.xyz/p/being-patient-with-problems"&gt;&lt;u&gt;流程&lt;/u&gt;&lt;/a&gt;、优先级或&lt;a href="https://www.henrikkarlsson.xyz/p/looking-for-alice"&gt;&lt;u&gt;关系&lt;/u&gt;&lt;/a&gt;，这是非常有意义的。了解这些领域每天都会有更好的回报。&lt;/p&gt;&lt;p&gt;如果你能打破不准确的思维模式，生活就会变得更容易驾驭。但是，你是怎么做的？我知道两种方法。&lt;/p&gt;&lt;p&gt;找到比你更了解事物的人并阅读他们所说的内容。带着回答你的问题的目的来阅读。如果您找不到答案，请发送电子邮件给他们。&lt;/p&gt;&lt;p&gt;进行实验。我的意思并不是说做随机的事情。我的意思是，&lt;i&gt;陈述你的假设&lt;/i&gt;并&lt;i&gt;找到方法来测试它们是否错误&lt;/i&gt;。大多数时候，老虎机实验不会产生任何结果。但没关系。有些会重新安排你周围的世界。&lt;/p&gt;&lt;p&gt;但是，正如我之前所说，这是一个权衡。花在探索收集新信息上的时间意味着更少的时间来采取行动。此外，利用往往比看起来更有价值，因为将你的注意力集中在你知道有前途的“老虎机”上可能会产生非线性回报。&lt;/p&gt;&lt;h2&gt;利用：除了你最重要的 1-3 个优先事项之外，削减一切，让生活变得更丰富&lt;/h2&gt;&lt;p&gt;根据经验，你只能做好一两件事。有些人是杰出的：他们可以做到3。我并不例外。&lt;/p&gt;&lt;p&gt;和许多人一样，当我生第一个孩子时，我就明白了这一点。对于成为一名父亲，我有点紧张。由于未能实现我的预期，我认为将孩子绑在胸前意味着让自己陷入永久的失败。它没。当莫德吃了我一半的时间时，我不得不强迫自己优先考虑：我会关心她，我会写作，&lt;i&gt;我会对其他一切说不。&lt;/i&gt;&lt;/p&gt;&lt;p&gt;这样缩小我的生活范围，至少可以让我的成就增加一倍。当我有更多时间时，我却把自己的精力分散到无法完成工作。&lt;/p&gt;&lt;p&gt;现在，每当我阅读那些做出杰出工作的人的传记时，我都会注意到这一点：他们过着狭窄的生活。他们允许自己比别人更少关心。随便引用两句话，这是 iPhone 的设计者乔尼·艾夫斯 (Jony Ives) 的话：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;史蒂夫[乔布斯]会对我说的一件事是因为他担心我注意力不集中，他会说，“你拒绝了多少件事？”我会告诉他我对此拒绝了。我对此表示不同意。但他知道我对做这些事情不感兴趣。 [对这些事情]说“不”并没有&lt;i&gt;牺牲&lt;/i&gt;。专注的意思是对某些你认为是非凡想法的事物说不，你醒来时会思考它，但你会拒绝它，因为你正在专注于其他事情。”&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;德国电影制片人维尔纳·赫尔佐格 (Werner Herzog) 说道：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;尽管多年来我只能勉强糊口，有时还处于半贫困状态，但自从我开始拍电影以来，我就过着富人般的生活。在我的一生中，我一直能够做我真正喜欢的事情，这比你扔给我的任何现金都更有价值。当朋友们通过获得大学学位、经商、建立职业生涯和购买房屋来建立自己的地位时，我正在拍电影，将一切都投入到我的工作中。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;用多臂强盗的术语来说，就是他们找到了一只好手臂。然后他们利用它来排除其他一切。&lt;/p&gt;&lt;p&gt;为什么有些人能实现这么多他们想要的东西，而另一些人却不能？人们对一生可以实现的目标有固定的预算吗？看来并非如此。相反，我们的成就预算似乎是我们优先事项数量的函数。有趣的是，它似乎是一个非线性函数。这意味着，如果您从 4 个优先事项改为 3 个优先事项，您可以多完成 10% 的工作；但如果从 4 变为 1，您的工作量就会增加 400%。 （显然这些数字是我编造的。）如果我看看埃隆·马斯克，我什至很难理解他每天的工作时间和我一样多。但他当然有。大家都这样做。只是由于注意力集中，他的决定变得更加复杂。&lt;/p&gt;&lt;p&gt;为什么要聚焦复合？一部分是时间。如果你关心的事情较少，你就会花更多的时间做你最关心的事情。而且，你总是在无意识地处理你关注的事情。 &lt;strong&gt;&amp;nbsp;&lt;/strong&gt;因此，削减优先事项意味着即使你看起来没有在工作，你仍然在工作。这些日子， &lt;strong&gt;&amp;nbsp;&lt;/strong&gt;我会花整个下午和孩子们一起玩，洗碗，修理房子——以一种清理头脑的方式忙碌。然后，当我第二天早上坐下来写作时，我可以不假思索地打出700字。这些想法一直在我的脑海中翻腾，就在有意识的思想表面之下，并且已经完全形成。&lt;/p&gt;&lt;p&gt;当我年轻的时候，我从来没有这么幸运过。部分原因是我的技能较差。但这也有部分原因是因为我会打断当时的无意识处理。无意中，我会告诉我的大脑专注于其他事情，例如我正在观看的电视剧中的冲突。我会在睡觉前看一集，悬念会在我的脑海中形成一个循环。当我睡觉时，那个循环会在我的脑海中翻腾；我醒来时看到一张空白页。我没有时间再做这些了。我确保我的写作始终保持开放状态。我会关闭所有其他循环，方法是尽快将其包装起来，或者将其写在列表上，或者最好根本不打开循环。&lt;/p&gt;&lt;p&gt;不过，仅分配更多的时间和心理处理能力并不能解释这种非线性。更多的时间只是线性增加。我猜非线性来自于：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;专注加速了技能和准确世界模型的积累。&lt;/strong&gt;在开放式领域，例如写作、人际关系或商业，技能发展的空间几乎是无限的。当你花费更多的时间时，你会得到更好的情况模型，这使你能够更好地分配你的时间，从而以非线性的方式加快你的学习速度等等。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;焦点吸引“资源”。&lt;/strong&gt;这在商业中是显而易见的：如果你有强烈的专注力，投资者就会开始尾随你，乞求你拿走他们的钱。然后你可以用这些钱更快地拉住强盗的手臂。这在写作中也是如此：我写得越多，我的博客吸引的有趣的人就越多。他们开始给我反馈和建议，帮助我写得更好，在飞轮中，吸引更多有趣的人（和一些钱）。如果你对周围的人充满好奇并且友善，你就会吸引强大的支持网络。网络具有非线性特性。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;但对我来说，作为一个狭隘的关注与我的本能相悖的人，最引人注目的是它给人的感觉是多么丰富。这些天我的生活既小又无聊。我每天骑自行车穿过同样的田野，我注意到风力涡轮机如何迎风转动，我很少旅行，我把空闲时间花在盯着Word文档上。安妮·迪拉德称作家的生活毫无色彩，甚至到了感官被剥夺的地步。很合适。但是，她也知道，还有另一种颜色只有在写字孔里三年后才能被发现。这是一种微妙的夜间色彩；您的眼睛需要时间适应黑暗才能看到东西。如果我告诉你，你不会相信它们的美丽。&lt;/p&gt;&lt;h2&gt;探索/利用权衡&lt;/h2&gt;&lt;p&gt;所以，正如我所说，我正在悬崖边游泳。每天，我都会沿着海岸走来走去，找到新的潜水地点。我对高空跳水产生了兴趣——小时候我太胆小了，不敢做这件事。我对水有一种孩子般的兴奋。海滩上的孩子们会看着我，窃窃私语，然后大笑。这是青春的乐趣之一。作为一名父亲的乐趣在于知道我比他们过得更开心。&lt;/p&gt;&lt;p&gt;但除了成为一名父亲之外，我不知道自己的生活该怎么办。游泳并思考我在本文中所说的事情，我做出了决定。我会通过算法来处理我的情况。我会运用规则来决定何时探索、何时利用来抵消我的自然倾向（这会促使我在这两方面做得太少）。&lt;/p&gt;&lt;p&gt;多臂老虎机问题有多种算法解决方案，可以追溯到 20 世纪 30 年代的 Thompson 采样，一直到机器学习中使用的当代算法，例如 EXP3 和 Upper Confidence Bounds。它们的共同点是：1）尽早优先考虑探索，2）随着情况变得更加清晰，加大利用力度。如果您刚来到一个城市，结识尽可能多的人是有意义的。如果你早点找到一个你爱的人在一起，你将会拥有多年的幸福。但如果你即将离开，与最好的朋友在一起更有意义。即使你找到了更喜欢的人，你也没有时间出去玩。最佳探索量取决于问题的复杂性和时间范围。&lt;/p&gt;&lt;p&gt;我决定做的是这样的。我会把接下来的 30 个月分为三个部分。前十个月，我会让自己自由探索。之后，我会用 2/3 的时间进行探索，并用 1/3 的时间加倍关注我发现的最有趣的机会，然后我会进行 1/3 的探索和 2/3 的利用，依此类推。&lt;/p&gt;&lt;p&gt;我们可以将我花在探索性开放式搜索上的时间视为我的“温度”。当你加热原子时，它们会更快地弹跳；当你加热生活时，它就会变得更具探索性。当你冷却它时，你就会缩小范围并花更多时间利用你所知道的东西。当您在逐渐降低“温度”的同时搜索最高峰时，情况如下：&lt;/p&gt;&lt;p&gt; &lt;a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44a6a537-8851-4039-aef6-c61c66bcc26f_500x161.gif"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Z55vXvngZxkPdKger/x2u6wsp9tb1myj62tzaf" /&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;请注意，搜索从右侧区域开始，该区域不包含最高峰。如果它是早期的集中搜索，它就会陷入局部最优。 （顺便说一句，这就是为什么教育对许多人来说是一场噩梦的原因之一。学校教育迫使你在尝试之前很久就决定要追求什么。它使你的未来更加知识渊博，成为年轻人的仆人，无知的自己。）&lt;/p&gt;&lt;p&gt;像这样对自己制定严格的规则让人感到安慰，尤其是在搜索的早期阶段。我感到压力很大，因为我找不到合适的事情来花时间和支持我的家人。这让我想尽快进入利用模式。知道以后会有时间集中精力，这让我更愿意尝试任何事情并失败。我学了钢琴并玩了编码。我写了小说的几章。我参与了一家幼儿园/山羊农场并研究了该岛的历史。我在一家美术馆找到了一份工作。&lt;/p&gt;&lt;p&gt; 10 个月后，我第一次减少了随机性。我决定用1/3的业余时间编程。但我每天的 2/3 时间都在探索。&lt;/p&gt;&lt;p&gt;下次我调低温度时，我开始了这个博客。因为写它比编程更有趣，所以我停止了编码，并将 2/3 的业余时间转向了博客。回想起来，这似乎是一个显而易见的选择。写作一直是我的主要痴迷。但当时并不明显。多年来，我一直因没有人对我写的东西感兴趣而感到沮丧。在我二十岁出头时出版过我的杂志不想与我现在正在探索的东西有任何关系。写作似乎是一条死胡同。&lt;/p&gt;&lt;p&gt;我没有告诉任何人我正在写博客。让我的朋友们读这本书会让我更难尝试和做那些冒着尴尬或失败风险的事情。我想把自己置于一个社会环境中，在那里我因&lt;a href="https://www.henrikkarlsson.xyz/p/writing-as-communion"&gt;&lt;u&gt;探索自己难以辨认的潜力&lt;/u&gt;&lt;/a&gt;而获得奖励。&lt;/p&gt;&lt;p&gt;有效。 2023 年 1 月，我将温度调低至 0 度。我到达了到达岛上时无法想象的顶峰：通过电子邮件向陌生人发送我的想法来支持我的家人。更好的是，自从 Johanna 和我现在作为一个团队工作以来，这是一个小型的家庭写作业务（我们预计到 2024 年底，论文将成为我们的主要收入）。&lt;/p&gt;&lt;p&gt;如果我在探索悬崖之前就已经决定了一条路，我就不会想到这种可能性。深思熟虑的电子邮件似乎无法支撑一个家庭。如果我知道是这样，我会认为我缺乏实现这一目标所需的能力。记住这一点很重要：除非你尝试，否则你不会知道。&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/Z55vXvngZxkPdKger/almost-everyone-i-ve-met-would-be-well-served-thinking-more#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Fri, 05 Jan 2024 21:01:30 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/Z55vXvngZxkPdKger/almost-everyone-i-ve-met-would-be-well-served-thinking-more</guid></item><item><title>下一个 ChatGPT 时刻：AI 化身</title><link>https://www.lesswrong.com/posts/DsgHj5hxcPgb6rEnj/the-next-chatgpt-moment-ai-avatars</link><description>发布于 2024 年 1 月 5 日晚上 8:14（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;&lt;strong&gt;认知状态：&lt;/strong&gt;推测。依赖于对近期人工智能技术和人类心理学的直觉。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;主张：&lt;/strong&gt;在未来 1-3 年内，许多人将与真正像人类一样的人工智能化身进行互动。这将显着增强公众对当前人工智能能力和风险的认知。&lt;/p&gt;&lt;p&gt; AI 化身是由 AI 生成的逼真的人类渲染（语音和视频），可以与人类进行实时对话，例如通过视频通话。&lt;/p&gt;&lt;p&gt;实现人工智能化身所需的各个组件已经存在。人工智能能够通过文本进行对话、将语音转录为文本以及合成听起来自然的语音。 &lt;span class="footnote-reference" id="fnrefqueio7x874s"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnqueio7x874s"&gt;[1]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;生成会说话的人的逼真视频目前还很有限，但&lt;a href="https://github.com/facebookresearch/audio2photoreal"&gt;&lt;u&gt;仍然令人印象深刻&lt;/u&gt;&lt;/a&gt;，并且进展迅速。&lt;/p&gt;&lt;p&gt;总而言之，这些功能意味着很快就有可能创建一个逼真的人工智能化身。第一代头像会有点粗糙，尤其是渲染的视频，但总的来说，创建令人信服的人工智能头像似乎不存在很大的概念障碍。 &lt;span class="footnote-reference" id="fnrefbnnvvrbt98"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnbnnvvrbt98"&gt;[2]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;与高质量人工智能化身的个人对话将对大多数人产生重大的情感和精神影响。 &lt;span class="footnote-reference" id="fnref5a7j6i96l1r"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn5a7j6i96l1r"&gt;[3]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;对于远离人工智能世界的人来说，影响尤其严重，但也会影响那些熟悉人工智能的人。&lt;/p&gt;&lt;p&gt;对于人类来说，沟通媒介与内容一样重要。同样的话语，当用一张表情丰富的面孔用充满感情的声音说出时，比在屏幕上默读时更能打动人心。与人工智能化身进行真实的个人对话将改变人们对人工智能的直觉。&lt;/p&gt;&lt;p&gt;无论好坏，一旦像样的人工智能化身变得普遍可用，围绕人工智能的公众情绪将经历另一种转变，类似于 ChatGPT 所引发的转变。 &lt;span class="footnote-reference" id="fnreflz8znpbhkv"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnlz8znpbhkv"&gt;[4]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;人工智能将被认为更加像人类并且更有能力。它看起来像是一个拥有“真正情报”的独立特工。&lt;/p&gt;&lt;p&gt;在与现实的人工智能化身交谈后，常见的问题是“它实际上并不智能，它只是预测下一个令牌”和“它为什么想要任何东西？”不会引起公众的共鸣。对于许多人来说，意识是真正人工智能的先决条件，而类人人工智能化身似乎是意识的直接体现。&lt;/p&gt;&lt;p&gt; ChatGPT 的发布是一个文化&lt;i&gt;时刻&lt;/i&gt;。 &lt;span class="footnote-reference" id="fnref3w6u730f80o"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn3w6u730f80o"&gt;[5]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;它激发了公众的想象力，并引发了人工智能从科幻到现实的重新分类。人工智能化身可能会带来另一个文化时刻，进一步改变公众的看法。&lt;/p&gt;&lt;p&gt;即将到来的转变是可以预见的——人工智能化身不需要任何根本性的技术突破。这是一次重大变革，我们有难得的机会提前做好准备。&lt;br /&gt;&lt;/p&gt;&lt;ol class="footnotes"&gt;&lt;li class="footnote-item" id="fnqueio7x874s"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefqueio7x874s"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;语音到文本足够好（ &lt;a href="https://github.com/openai/whisper"&gt;&lt;u&gt;OpenAI Whisper&lt;/u&gt;&lt;/a&gt; ），文本到语音几乎足够好（ &lt;a href="https://elevenlabs.io/"&gt;&lt;u&gt;ElevenLabs&lt;/u&gt;&lt;/a&gt; ），对话/语言建模也足够好（具有&lt;a href="https://beta.character.ai/"&gt;&lt;u&gt;Character.ai风格&lt;/u&gt;&lt;/a&gt;个性的&lt;a href="https://chat.openai.com/"&gt;&lt;u&gt;ChatGPT&lt;/u&gt;&lt;/a&gt; ）。目前，所有这些都足以与人工智能进行真实的音频对话。人类视频生成还不够好，但正在取得进展（&lt;a href="https://people.eecs.berkeley.edu/~evonne_ng/projects/audio2photoreal/"&gt;&lt;u&gt;音频到真实照片&lt;/u&gt;&lt;/a&gt;、 &lt;a href="https://labs.heygen.com/realtime"&gt;&lt;u&gt;HeyGen&lt;/u&gt;&lt;/a&gt; 、&lt;a href="https://www.unrealengine.com/en-US/metahuman"&gt;&lt;u&gt;超人类&lt;/u&gt;&lt;/a&gt;）。根据目前的进展速度，功能性人工智能化身似乎可以在 1-3 年内实现。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnbnnvvrbt98"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefbnnvvrbt98"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;延迟可能是近期的一个问题。特别是，尚不清楚视频生成的速度有多快。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn5a7j6i96l1r"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnref5a7j6i96l1r"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;这种情况已经在一定程度上发生了。许多人通过与相对较弱的语言模型（例如&lt;a href="https://beta.character.ai/"&gt;Character.ai&lt;/a&gt;和&lt;a href="https://replika.com/"&gt;Replika&lt;/a&gt; ）的纯文本交互形成了重要的情感依恋。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnlz8znpbhkv"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnreflz8znpbhkv"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;不过，这种转变可能比 ChatGPT 更加渐进。 AI化身技术正在逐步完善，而ChatGPT则被&lt;i&gt;独特地&lt;/i&gt;抛弃在世界上。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn3w6u730f80o"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnref3w6u730f80o"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt; “AI”的 Google 趋势图表。 ChatGPT 于 2022 年 11 月 30 日发布。 &lt;/p&gt;&lt;figure class="image"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DsgHj5hxcPgb6rEnj/sykmzmw1rn8ievb2e3xr" /&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ol&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/DsgHj5hxcPgb6rEnj/the-next-chatgpt-moment-ai-avatars#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Fri, 05 Jan 2024 20:33:12 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/DsgHj5hxcPgb6rEnj/the-next-chatgpt-moment-ai-avatars</guid></item><item><title>人工智能影响 2023 年人工智能进展专家调查</title><link>https://www.lesswrong.com/posts/RkegCmCgjGhskiFvm/ai-impacts-2023-expert-survey-on-progress-in-ai</link><description>发布于 2024 年 1 月 5 日晚上 7:42（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt; AI Impacts 刚刚发布了最新调查结果！我发现这很有趣。以下是链接 wiki 页面的结果部分：&lt;/p&gt;&lt;hr /&gt;&lt;h3&gt;&lt;strong&gt;结果&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;&lt;a href="https://docs.google.com/spreadsheets/d/1aOydfhZHuVwU_fwTgE0_O_-8p-uMrRDYV5R5QnwOMGI/edit?usp=sharing"&gt;此处&lt;/a&gt;提供了匿名调查回复的完整数据集。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;人类水平表现的时间安排&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;我们通过询问一些参与者他们期望多快实现“高级机器智能”（HLMI）以及询问其他参与者他们期望多快实现“劳动完全自动化”（FAOL）来询问人类水平表现的时间安排。与之前的调查一样，被问及FAOL 的参与者往往比被问及HLMI 的参与者给出的时间要长得多。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;高级机器智能 (HLMI)&lt;/strong&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;当独立机器能够比人类工人更好、更便宜地完成每项任务时，就可以说我们拥有“高级机器智能”。忽略人类本质上有利的任务方面，例如被接受为陪审团成员。&lt;i&gt;考虑可行性，而不是采用。&lt;/i&gt;&lt;br /&gt;出于本问题的目的，假设人类科学活动继续进行，没有重大的负面干扰。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;一半的参与者被问及他们预计 HLMI 存在的概率为 10%、50% 和 90% 的年数（固定概率框架），一半的参与者被问及他们认为 HLMI 在 10 年后存在的概率， 20 年和 40 年（固定年限）。&lt;/p&gt;&lt;p&gt;我们通过将每个答案拟合到 gamma CDF 并找到这些 CDF 的平均曲线，汇总了对此问题的 1714 个答案。由此产生的总体预测显示，到 2047 年，发生 HLMI 的可能性为 50%，比 2022 年 ESPAI 中的 2060 年下降了 13 年。&lt;/p&gt;&lt;p&gt; &lt;a href="https://wiki.aiimpacts.org/_detail/ai_timelines/predictions_of_human-level_ai_timelines/ai_timeline_surveys/01958ae7-cb8c-4340-a2e7-5b54c0ab5e7c_2076x1594.jpg?id=ai_timelines%3Apredictions_of_human-level_ai_timelines%3Aai_timeline_surveys%3A2023_expert_survey_on_progress_in_ai"&gt;&lt;i&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/RkegCmCgjGhskiFvm/puip7fwwzktiteaqg0ea" /&gt;&lt;/i&gt;&lt;/a&gt; &lt;i&gt;2023 年的总体预测预测 HLMI 将比 2022 年调查中预测的总体预测更早到达。&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;劳动全自动化（FAOL）&lt;/strong&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;当所有职业都完全自动化时，我们就已经达到了“劳动完全自动化”。也就是说，对于任何职业，机器都可以比人类工人更好、更便宜地完成任务。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;与 HLMI 问题一样，接受FAOL 问题的一半参与者使用固定概率框架进行询问，另一半则使用固定年份框架进行询问。&lt;/p&gt;&lt;p&gt;对这个问题的 774 条回答被用来创建一个总体预测，到 1116 年，FAOL 发生的可能性为 50%，比 2022 年 ESPAI 中的 2164 条减少了 48 年。&lt;/p&gt;&lt;p&gt; &lt;a href="https://wiki.aiimpacts.org/_detail/ai_timelines/predictions_of_human-level_ai_timelines/ai_timeline_surveys/31afa88c-324e-4b6d-91ed-97e2715e5b95_2076x1594.png?id=ai_timelines%3Apredictions_of_human-level_ai_timelines%3Aai_timeline_surveys%3A2023_expert_survey_on_progress_in_ai"&gt;&lt;i&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/RkegCmCgjGhskiFvm/jf0axlrzguplchkverr4" /&gt;&lt;/i&gt;&lt;/a&gt; &lt;i&gt;2023年的总体预测预测FAOL将比2022年调查中预测的总体预测更早到达。&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;智力爆炸&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;我们通过向每位参与者询问三个相关问题之一来询问“智力爆炸”的可能性。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;智力爆炸论点有可能是正确的&lt;/strong&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;一些人提出了以下观点：&lt;br /&gt;&lt;i&gt;如果人工智能系统几乎承担了所有的研发工作，那么人工智能的改进将加快技术进步的步伐，包括人工智能的进一步进步。&lt;/i&gt;&lt;br /&gt;&lt;i&gt;在短时间内（不到 5 年），这种反馈循环可能会导致技术进步速度加快一个数量级以上。&lt;/i&gt;&lt;br /&gt;您认为这个论点大致正确的可能性有多大？&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;在对该问题的 299 条回复中，&lt;/p&gt;&lt;p&gt; 9% 表示“很有可能 (81-100%)”&lt;br /&gt; 20% 的人表示“有可能 (61-80%)”&lt;br /&gt; 24% 的人表示“机会均等 (41-60%)”&lt;br /&gt; 24% 的人表示“不太可能 (21-40%)”&lt;br /&gt; 23% 的人表示“不太可能 (0-20%)”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;技术大幅加速的可能性&lt;/strong&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;假设 HLMI 将在某个时刻存在。&lt;br /&gt;那么，您认为由于机器智能的出现，全球技术进步的速度将显着提高（例如十倍）的可能性有多大：&lt;br /&gt;在那之后的两年内？ _％ 机会&lt;br /&gt;在那之后的三十年内？ _％ 机会&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;此问题共有 298 条回复。两年的答案中位数是 20%，三十年的答案是 80%。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;超级智能的概率&lt;/strong&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;假设 HLMI 将在某个时刻存在。&lt;br /&gt;您认为机器智能在所有职业上都远远优于人类（即能力更强或更便宜）的可能性有多大：&lt;br /&gt;在那之后的两年内？ _％ 机会&lt;br /&gt;在那之后的三十年内？ _％ 机会&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;此问题有 282 条回复。两年的中位数答案是 10%，三十年的答案是 60%。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;2028 年人工智能的可解释性&lt;/strong&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;对于 2028 年典型的最先进的人工智能系统，您认为用户有可能知道系统做出特定选择的真正原因吗？我们所说的“真实原因”是指人工智能以人类可以理解的方式正确解释其内部决策过程。我们所说的“真实原因”并不意味着决定本身是正确的。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;在对此问题的 912 条回复中，&lt;/p&gt;&lt;p&gt; 5% 表示“极不可能（&amp;lt;10%）”&lt;br /&gt; 35% 表示“不太可能 (10-40%)”&lt;br /&gt; 20% 的人表示“赔率均等 (40-60%)”&lt;br /&gt; 15% 表示“有可能 (60-90%)”&lt;br /&gt; 5% 表示“很有可能 (&amp;gt;90%)”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;未来11个AI相关场景有多值得关注？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt; 1345 名参与者评估了他们对未来 30 年 11 种人工智能相关场景的关注程度。根据认为某个场景构成“重大”或“极端”担忧的受访者百分比来衡量，最值得关注的场景是：传播虚假信息，例如深度造假（86%）、操纵大规模舆论趋势(79%)、人工智能让危险群体制造强大的工具（例如工程病毒）(73%)、独裁统治者利用人工智能控制其人口(73%)、人工智能系统通过不成比例地惠及某些个人而加剧经济不平等(71%) 。&lt;/p&gt;&lt;p&gt; &lt;a href="https://wiki.aiimpacts.org/_detail/ai_timelines/predictions_of_human-level_ai_timelines/ai_timeline_surveys/82d27755-5476-4af5-ac32-a29fe0be5d41_1314x773.png?id=ai_timelines%3Apredictions_of_human-level_ai_timelines%3Aai_timeline_surveys%3A2023_expert_survey_on_progress_in_ai"&gt;&lt;i&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/RkegCmCgjGhskiFvm/uuwlj2xwgflol2uq5mjp" /&gt;&lt;/i&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt; &lt;strong&gt;HLMI 的总体影响&lt;/strong&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;出于本问题的目的，假设 HLMI 将在某个时刻存在。从长远来看，您预计这对人类的总体影响有多大？&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;参与者被要求回答他们产生不同类型影响的可能性。以下是 2704 条回复的中位数和均值： &lt;/p&gt;&lt;figure class="table"&gt;&lt;table style="border: 1px solid rgb(204, 204, 204);"&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style="background-color: rgb(238, 238, 238); border: 1px solid rgb(204, 204, 204); padding: 0.3em 0.5em; vertical-align: top;"&gt; &lt;strong&gt;HLMI 的总体影响&lt;/strong&gt;&lt;/th&gt;&lt;th style="background-color: rgb(238, 238, 238); border: 1px solid rgb(204, 204, 204); padding: 0.3em 0.5em; vertical-align: top;"&gt;&lt;strong&gt;中位反应&lt;/strong&gt;&lt;/th&gt;&lt;th style="background-color: rgb(238, 238, 238); border: 1px solid rgb(204, 204, 204); padding: 0.3em 0.5em; vertical-align: top;"&gt;&lt;strong&gt;平均响应&lt;/strong&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="border: 1px solid rgb(204, 204, 204); padding: 0.3em 0.5em; vertical-align: top;"&gt; “非常好（例如人类繁荣的快速增长）”&lt;/td&gt;&lt;td style="border: 1px solid rgb(204, 204, 204); padding: 0.3em 0.5em; vertical-align: top;"&gt; 10%&lt;/td&gt;&lt;td style="border: 1px solid rgb(204, 204, 204); padding: 0.3em 0.5em; vertical-align: top;"&gt; 23% &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="border: 1px solid rgb(204, 204, 204); padding: 0.3em 0.5em; vertical-align: top;"&gt; “总体来说不错”&lt;/td&gt;&lt;td style="border: 1px solid rgb(204, 204, 204); padding: 0.3em 0.5em; vertical-align: top;"&gt; 25%&lt;/td&gt;&lt;td style="border: 1px solid rgb(204, 204, 204); padding: 0.3em 0.5em; vertical-align: top;"&gt; 29% &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="border: 1px solid rgb(204, 204, 204); padding: 0.3em 0.5em; vertical-align: top;"&gt; “或多或少是中立的”&lt;/td&gt;&lt;td style="border: 1px solid rgb(204, 204, 204); padding: 0.3em 0.5em; vertical-align: top;"&gt; 20%&lt;/td&gt;&lt;td style="border: 1px solid rgb(204, 204, 204); padding: 0.3em 0.5em; vertical-align: top;"&gt; 21% &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="border: 1px solid rgb(204, 204, 204); padding: 0.3em 0.5em; vertical-align: top;"&gt; “总体来说很糟糕”&lt;/td&gt;&lt;td style="border: 1px solid rgb(204, 204, 204); padding: 0.3em 0.5em; vertical-align: top;"&gt; 15%&lt;/td&gt;&lt;td style="border: 1px solid rgb(204, 204, 204); padding: 0.3em 0.5em; vertical-align: top;"&gt; 18% &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="border: 1px solid rgb(204, 204, 204); padding: 0.3em 0.5em; vertical-align: top;"&gt; “极其糟糕（例如人类灭绝）”&lt;/td&gt;&lt;td style="border: 1px solid rgb(204, 204, 204); padding: 0.3em 0.5em; vertical-align: top;"&gt; 5%&lt;/td&gt;&lt;td style="border: 1px solid rgb(204, 204, 204); padding: 0.3em 0.5em; vertical-align: top;"&gt; 9%&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt; &lt;a href="https://wiki.aiimpacts.org/_detail/ai_timelines/predictions_of_human-level_ai_timelines/ai_timeline_surveys/ce8dfb8d-83ec-453b-8dd2-3609565338b7_2544x1274.png?id=ai_timelines%3Apredictions_of_human-level_ai_timelines%3Aai_timeline_surveys%3A2023_expert_survey_on_progress_in_ai"&gt;&lt;i&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/RkegCmCgjGhskiFvm/lvey0js0lwzfaoklrrn2" /&gt;&lt;/i&gt;&lt;/a&gt;&lt;i&gt;随机选择 800 个关于 HLMI 对人类长期影响的积极或消极影响的回答。每个垂直条代表一名参与者。&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;首选进度&lt;/strong&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;未来五年全球人工智能的进展速度有多快会让您对人类的未来感到最乐观？假设速度的任何变化都会同等地影响所有项目。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;在对此问题的 675 条回复中，&lt;/p&gt;&lt;p&gt; 4.8% 表示“慢得多”&lt;br /&gt; 29.9% 表示“有点慢”&lt;br /&gt; 26.9% 表示“当前速度”&lt;br /&gt; 22.8% 表示“稍微快一些”&lt;br /&gt; 15.6% 表示“快得多”&lt;/p&gt;&lt;p&gt; &lt;strong&gt;39 项任务多久才能对 AI 实现？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;参与者被问及 39 项任务中的每一项何时对人工智能来说变得“可行”，即“如果他们选择的话，资源最好的实验室之一可以在不到一年的时间内实施它。”忽略他们是否会选择的问题。”每个受访者都被问及其中四项任务，因此每项任务收到大约 250 个估计。&lt;/p&gt;&lt;p&gt;与有关人类表现的时间表的问题一样，参与者被要求提供具有固定年份或固定概率框架的三年概率对。在 2022 年调查中的 32 项任务中，本次调查中的总体预测中的大多数任务都将比 2022 年调查中的预测更早完成。&lt;/p&gt;&lt;p&gt; &lt;a href="https://wiki.aiimpacts.org/_detail/ai_timelines/predictions_of_human-level_ai_timelines/ai_timeline_surveys/21f4fd7b-b6b7-4bf7-bb2d-f33901e2ffdc_2310x3603.png?id=ai_timelines%3Apredictions_of_human-level_ai_timelines%3Aai_timeline_surveys%3A2023_expert_survey_on_progress_in_ai"&gt;&lt;i&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/RkegCmCgjGhskiFvm/asqcn8bb9ajwjpaag7wc" /&gt;&lt;/i&gt;&lt;/a&gt; &lt;i&gt;2023 年的总体预测通常预测里程碑将比 2022 年的预测更早到达。总体分布给出里程碑实现机会 50% 的年份由实心圆圈、空心圆圈和任务实心方块表示，分别是职业和一般人类水平的表现。&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;对齐问题&lt;/strong&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;Stuart Russell 总结了为什么高度先进的人工智能可能会带来风险的论点如下：&lt;br /&gt;&lt;br /&gt; &lt;i&gt;[对于高度先进的人工智能]，主要关注的不是令人毛骨悚然的突发意识，而只是做出高质量决策的能力。在这里，质量是指所采取行动的预期结果效用[...]。现在我们有一个问题：&lt;/i&gt;&lt;br /&gt;&lt;br /&gt; &lt;i&gt;1. 效用函数可能并不完全符合人类的价值观，而人类的价值观（充其量）是很难确定的。&lt;/i&gt;&lt;br /&gt; &lt;i&gt;2.任何有足够能力的智能系统都会更愿意确保自己的持续存在并获取物理和计算资源——不是为了它们自己，而是为了成功完成分配的任务。&lt;/i&gt;&lt;br /&gt;&lt;br /&gt;&lt;i&gt;优化 n 个变量函数的系统（其中目标取决于大小 k&amp;lt;n 的子集）通常会将剩余的无约束变量设置为极值；如果这些不受约束的变量之一实际上是我们关心的，那么找到的解决方案可能是非常不受欢迎的。这本质上是灯中精灵、巫师的学徒或迈达斯国王的古老故事：你得到的正是你所要求的，而不是你想要的。&lt;/i&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt; &lt;strong&gt;“你认为这个论点指出了一个重要的问题吗？”&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在回答此问题的 1322 人中，&lt;/p&gt;&lt;p&gt; 5% 的人表示“不，这不是真正的问题”&lt;br /&gt; 9% 的人表示“不，这不是一个重要问题”&lt;br /&gt; 32% 的人表示“是的，一个中等重要的问题”&lt;br /&gt; 41% 的人表示“是的，一个非常重要的问题”&lt;br /&gt; 13% 的人表示“是的，这是该领域最重要的问题之一”&lt;/p&gt;&lt;p&gt; &lt;strong&gt;“与人工智能中的其他问题相比，今天解决这个问题有多大价值？”&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在回答此问题的 1321 人中，&lt;/p&gt;&lt;p&gt; 9% 表示“更有价值”&lt;br /&gt; 22% 的人表示“更有价值”&lt;br /&gt; 39% 的人表示“同样有价值”&lt;br /&gt; 22% 的人表示“价值较低”&lt;br /&gt; 8% 的人表示“价值低得多”&lt;/p&gt;&lt;p&gt; &lt;strong&gt;“你认为这个问题与人工智能中的其他问题相比有多难？”&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在回答此问题的 1274 人中，&lt;/p&gt;&lt;p&gt; 21% 的人表示“更加困难”&lt;br /&gt; 36% 的人表示“更难”&lt;br /&gt; 30% 的人表示“同样困难”&lt;br /&gt; 10% 的人表示“更容易”&lt;br /&gt; 3% 表示“容易多了”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;社会应该在多大程度上优先考虑人工智能安全研究？&lt;/strong&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;让“人工智能安全研究”包括任何与人工智能相关的研究，这些研究不是主要旨在提高人工智能系统的能力，而是主要旨在最大限度地减少人工智能系统的潜在风险（超出通过增加人工智能已实现的目标）系统能力）。&lt;br /&gt;人工智能安全研究的例子可能包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;提高机器学习算法的人类可解释性，目的是提高人工智能系统的安全性和鲁棒性，而不是专注于提高人工智能能力&lt;/li&gt;&lt;li&gt;人工智能系统的长期存在风险研究&lt;/li&gt;&lt;li&gt;AI特定形式验证研究&lt;/li&gt;&lt;li&gt;开发方法来识别、衡量和减轻人工智能模型中的偏差，以确保公平和道德的决策。&lt;/li&gt;&lt;li&gt;如何最大化人工智能公共利益的政策研究&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;相对于目前的优先顺序，社会应该在多大程度上优先考虑人工智能安全研究？&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;在对此问题的 1329 条回复中，&lt;/p&gt;&lt;p&gt; 2% 的人表示“少得多”&lt;br /&gt; 6% 表示“少”&lt;br /&gt; 22% 表示“差不多”&lt;br /&gt; 36% 的人表示“更多”&lt;br /&gt; 34% 的人表示“更多”&lt;/p&gt;&lt;p&gt; &lt;a href="https://wiki.aiimpacts.org/_detail/ai_timelines/predictions_of_human-level_ai_timelines/ai_timeline_surveys/a8e6a2c4-a549-4501-a242-9b3d2d3eddfa_2114x1280.jpg?id=ai_timelines%3Apredictions_of_human-level_ai_timelines%3Aai_timeline_surveys%3A2023_expert_survey_on_progress_in_ai"&gt;&lt;i&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/RkegCmCgjGhskiFvm/dag197zxpw3reetr86me" /&gt;&lt;/i&gt;&lt;/a&gt; &lt;i&gt;2016 年、2022 年和 2023 年调查对&lt;/i&gt;“&lt;i&gt;相对于目前的优先级而言，社会应该优先考虑人工智能安全研究的程度？”的&lt;/i&gt;回应”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;人类灭绝&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;我们使用三个不同措辞的问题来询问人工智能导致人类灭绝的可能性，每个问题我们收集了 655-1321 个答案。 &lt;/p&gt;&lt;figure class="table"&gt;&lt;table style="border: 1px solid rgb(204, 204, 204);"&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style="background-color: rgb(238, 238, 238); border: 1px solid rgb(204, 204, 204); padding: 0.3em 0.5em; vertical-align: top;"&gt;&lt;strong&gt;问题措辞&lt;/strong&gt;&lt;/th&gt;&lt;th style="background-color: rgb(238, 238, 238); border: 1px solid rgb(204, 204, 204); padding: 0.3em 0.5em; vertical-align: top;"&gt;&lt;strong&gt;中位反应&lt;/strong&gt;&lt;/th&gt;&lt;th style="background-color: rgb(238, 238, 238); border: 1px solid rgb(204, 204, 204); padding: 0.3em 0.5em; vertical-align: top;"&gt;&lt;strong&gt;平均响应&lt;/strong&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="border: 1px solid rgb(204, 204, 204); padding: 0.3em 0.5em; vertical-align: top;"&gt; “你认为未来人工智能的进步导致人类灭绝或类似的人类物种永久和严重丧失权力的可能性有多大？”&lt;/td&gt;&lt;td style="border: 1px solid rgb(204, 204, 204); padding: 0.3em 0.5em; vertical-align: top;"&gt; 5%&lt;/td&gt;&lt;td style="border: 1px solid rgb(204, 204, 204); padding: 0.3em 0.5em; vertical-align: top;"&gt; 16.2% &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="border: 1px solid rgb(204, 204, 204); padding: 0.3em 0.5em; vertical-align: top;"&gt; “你认为人类无法控制未来的先进人工智能系统导致人类灭绝或类似的人类物种永久和严重丧失权力的可能性有多大？”&lt;/td&gt;&lt;td style="border: 1px solid rgb(204, 204, 204); padding: 0.3em 0.5em; vertical-align: top;"&gt; 10%&lt;/td&gt;&lt;td style="border: 1px solid rgb(204, 204, 204); padding: 0.3em 0.5em; vertical-align: top;"&gt; 19.4% &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="border: 1px solid rgb(204, 204, 204); padding: 0.3em 0.5em; vertical-align: top;"&gt; “你认为未来人工智能的进步在未来 100 年内导致人类灭绝或类似的人类物种永久和严重丧失权力的可能性有多大？”&lt;/td&gt;&lt;td style="border: 1px solid rgb(204, 204, 204); padding: 0.3em 0.5em; vertical-align: top;"&gt; 5%&lt;/td&gt;&lt;td style="border: 1px solid rgb(204, 204, 204); padding: 0.3em 0.5em; vertical-align: top;"&gt; 14.4%&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt; &lt;a href="https://wiki.aiimpacts.org/_detail/ai_timelines/predictions_of_human-level_ai_timelines/ai_timeline_surveys/59b60985-66a9-4364-976b-5c6228eff6fa_2544x1274.png?id=ai_timelines%3Apredictions_of_human-level_ai_timelines%3Aai_timeline_surveys%3A2023_expert_survey_on_progress_in_ai"&gt;&lt;i&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/RkegCmCgjGhskiFvm/cslrh1ms80mf3guwvmn0" /&gt;&lt;/i&gt;&lt;/a&gt;&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/RkegCmCgjGhskiFvm/ai-impacts-2023-expert-survey-on-progress-in-ai#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Fri, 05 Jan 2024 19:42:20 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/RkegCmCgjGhskiFvm/ai-impacts-2023-expert-survey-on-progress-in-ai</guid></item><item><title>技术路径依赖和评估专业知识</title><link>https://www.lesswrong.com/posts/unesQvsKBjx6x2ZkY/technology-path-dependence-and-evaluating-expertise</link><description>发布于 2024 年 1 月 5 日晚上 7:21（格林威治标准时间） &lt;br /&gt;&lt;br /&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;穆伊雷尔&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;我经常想知道，我是否能够判断一项技术是否由于基本可行性原因或路径依赖原因而没有竞争力——一旦我们解决了几万亿美元的问题，最终的性能是否会成为赢家，但我们永远不要走这条路，因为我们在已经成熟的行业中获得了更高的回报，从文明的角度来看，这不是一种悲哀吗？&lt;/p&gt;&lt;p&gt;我怀疑这种情况在计算领域经常发生——“惨痛的教训”或“摩尔定律的长臂”——你最好的选择就是把自己拴在通用技术的引擎上。至少在硬件方面，这种情况变得不那么真实了，这使得半导体行业迎来了激动人心的时刻。专业化越来越有价值，即使这意味着您错过了前沿工艺节点的前端改进带来的收益。这也可能意味着值得重新审视摩尔定律全速发展时被抛在一边的旧想法，这也是我对你如何思考此类问题感兴趣的部分原因。&lt;/p&gt;&lt;p&gt;您花费大量时间思考不同技术的可行性。也许具体来说，我们可以谈谈核能。你提到过，你对不受监管的核电的经济效益不如这里的很多人那么乐观。我对此没有强烈的看法，但我已经吸收了这样的印象：这里的可行性问题更多地取决于路径——这在过去可能是一个更好的想法，但即使你以二氧化碳定价我们在天然气和煤炭方面一直在努力学习，而在核能方面却相对停滞甚至倒退。 （我并不是特别喜欢这个观点，只是想从一个轻量级的框架开始进行讨论。）您对这里的经济学有什么看法？在某些替代历史中，它会以其他方式出现吗？ &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;布豪斯&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;我倾向于将技术路径依赖分为三种类型：&lt;/p&gt;&lt;ol&gt;&lt;li&gt; X是标准，切换困难（例如电源插座）&lt;/li&gt;&lt;li&gt;更多的钱花在了开发 X 上，所以它目前更好（例如硅电力电子与 SiC 和 GaN）&lt;/li&gt;&lt;li&gt;强大的团体支持X，因为它存在（例如玉米乙醇补贴）&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;这是我遇到的路径依赖的一个例子：&lt;/p&gt;&lt;p&gt;有一次，我在一份工作中建议使用苯甲酸甲酯作为应用溶剂。它的性质是合适的，我认为，由于甲醇和苯甲酸都很便宜并且可以大规模生产，并且用它们制造苯甲酸甲酯相当容易，所以它应该不会太贵。但是，虽然它可以少量（高价）用于香水，但很难找到来源，而且它比（更难制造）化学品贵得多。&lt;/p&gt;&lt;p&gt;它甚至不需要巨大的规模来制造它。阻止其用作工业溶剂的原因是一方面是“标准化”，但在较低层面上，问题包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;了解潜在买家未使用的选项&lt;/li&gt;&lt;li&gt;低产量高价格市场比低价高产量市场更有利可图&lt;/li&gt;&lt;li&gt;建设化工厂所涉及的隐性知识，即使是众所周知的反应，也使得新群体更难进入&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;如果您将社会视为一台机器，其某些部件因摩擦而卡住，您可能想知道哪里可以润滑或用锤子敲击以将其松开。但是，虽然摩擦是一种宏观现象，但它由许多微观相互作用的集体效应组成。&lt;/p&gt;&lt;p&gt;我还想到了一个与路径依赖&lt;i&gt;相反&lt;/i&gt;的例子：燃气轮机。早期的发动机使用活塞并由蒸汽驱动，但在（非蒸汽）燃气轮机实用化之前几十年，工程师们的主流观点认为它们已经变得很重要。当涡轮机设计和可用材料使得燃气轮机几乎无法产生净功率时，人们就已经&lt;a href="https://en.wikipedia.org/wiki/Armengaud-Lemale_gas_turbine"&gt;开始建造它们了&lt;/a&gt;。&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;关于核电经济学，我顺手&lt;a href="https://www.bhauth.com/blog/energy/gas%20cooled%20reactors.html"&gt;已经写过一篇文章了&lt;/a&gt;。但一些核电爱好者并不觉得它很有说服力。&lt;/p&gt;&lt;p&gt;当我说“热交换器当然是一项主要成本，而腐蚀性熔盐会使它们变得昂贵”之类的话时，它依赖于我建立的整个假设金字塔。人们进行对话时，通常有几个底层，沟通的有效性取决于这些金字塔的兼容性。即使当沟通失败时你试图深入到较低的层次，也没有人有时间真正广泛地这样做，而金字塔中的一些障碍不可避免地来自社会信任。 &lt;br /&gt;&lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;穆伊雷尔&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;谢谢，我现在正在读你的核帖子。如果我发现自己同意热交换器，我不会感到惊讶。 （在实践中，我所使用的几乎每个热力循环都受到热交换器性能/成本的瓶颈。）&lt;/p&gt;&lt;p&gt; [好吧，读完。] 对我来说很有意义。我想我对这座金字塔的性质有一些广泛的问题。 [稍后可能会收集一些其他主题。]&lt;/p&gt;&lt;ol&gt;&lt;li&gt;您对此类分析有多大信心？你确实以“在我看来”开头，但在其他方面听起来很实事求是。&lt;/li&gt;&lt;li&gt;当您说熔盐热交换器之类的东西很昂贵时，该信息来自哪里？ （例如，它是否更多地来自“当您今天尝试采购一个时，您会得到这么多报价”或“如果您分解工程要求，它就会增加”？）或者也许我的根本问题更多比如——人们多么努力地试图降低成本或规避昂贵的要求？ （如果你唯一的客户正在建造核潜艇，相对而言，可能没那么难？）这在多大程度上影响了你的想法？ （也许您只需要知道与要求较低的设计相比，它总是相对昂贵？或者也许您今天只关心选择核电站设计，因为如果我有一个更好的热交换器的想法，我应该从卖掉它开始。）当你说某样东西对于某种目的来说太贵时，这更多的是一种省略的计算还是一种猜测？ （或者——如果你再认真思考一下，你还有多少空间可以改变主意？） &lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;布豪斯&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;ol&gt;&lt;li&gt;唯一的“我认为”是：&lt;br /&gt; “在我看来，如果你想建造一座具有经济竞争力的核电站，蒸汽就已经出局了。”&lt;br /&gt;&lt;br /&gt;当我使用这个短语时，通常意味着只有当我忽略了一个因素时我才会错，但这是可能的。这里一个明显的可能性是扩大所谓的“经济竞争力”——也许是由于对天然气供应的地缘政治担忧。另一种可能性是有人为蒸汽轮机等众所周知的部件开发更便宜的制造方法。一般来说，我错的可能性比像 TerraPower 或 NuScale 这样的初创公司错的可能性要小，而且我的博客文章通常与高于平均水平的技术经济分析或调查论文一样可靠，但读者有责任评估之类的东西。&lt;/li&gt;&lt;li&gt;人们做出的每一个估计都是基于对某些事物的外推或插值。当我进行成本估算时，我通常从现有产品的成本开始，然后根据相对生产成本进行调整。很难准确估计采用不寻常合金的热交换器的成本，但很容易判断它们是否比标准热交换器更贵。您还可以比较不同类型的现有换热器的成本，以估计随着材料加工难度而增加多少成本。您对材料和制造工艺了解得越多，您就能做出越准确的推断。&lt;br /&gt;&lt;br /&gt;我经常采用的另一种方法是估算所用部件或材料的成本。例如，您可以说使用碳纤维的产品的成本至少与碳纤维的数量相同，并且您可以添加一些劳动力成本的近似值。 （有些博主似乎采取这种方式太过分了，根据某些组件的成本对他们不熟悉的东西进行成本估算，但东西可能比原材料贵得多。）为了做到这一点，你必须知道将使用什么材料和制造方法，但我对它们的大多数基本类型都足够了解。有时我会低估成本，因为我对公司正在做的事情的猜测比他们的实际方法更好，但我在为此增加成本乘数范围方面做得更好。&lt;br /&gt;&lt;br /&gt;我经常使用多种方法并结合所得的估计，有时如果它们看起来不同意，我会返回。每个估计的每个步骤都涉及很多隐含的假设；你可能会说“Y 的成本大约是 X 的 2 倍”，但如果你开始深入了解细节，有时人们想知道 Y 的原因或谈论一些不相关的例外情况，这需要很多时间。如果逻辑足够简单，我可能会在帖子中解释它，但我通常只是将估计值放在那里，供人们收集并与他们想要的其他估计值结合起来。 &lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;穆伊雷尔&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;知道了。这听起来很合理，而且相当谦虚。&lt;/p&gt;&lt;p&gt; （我希望我没有显得乏味。我想我有这样的担忧——有些人会非常自信地谈论技术，但他们的推理几乎不透明。有时事实证明，他们的推理，或者至少是部分推理它们不会被标记为更具投机性，不会经受住审查。您的写作模式与此相匹配。）&lt;/p&gt;&lt;p&gt;当你写的时候&lt;/p&gt;&lt;p&gt;&amp;gt; 一般来说，我错的可能性比像 TerraPower 或 NuScale 这样的初创公司错的可能性要小，而且我的博客文章通常与高于平均水平的技术经济分析或调查论文一样可靠&lt;/p&gt;&lt;p&gt;我想知道你是如何校准的。我通常期望初创公司优化他们的故事是为了吸引投资，而不是为了正确，所以我不会因为一些外部分析师会超越他们的想法而感到困扰。但事情需要很长时间才能摆脱困境，而且外界往往不清楚为什么事情真的失败了。 （对于 TerraPower 远离行波反应堆的明显枢轴，我能解读多少？）&lt;/p&gt;&lt;p&gt;回到核电和路径依赖的问题，这听起来像是决策者无法评估技术论证而产生的另一种意外情况。 （也就是说，在 KS 150 问题的背景下，你写道，“我认为这些事件不是基本设计的错误，如果有什么表现出良好的弹性，但 KS 150 的问题是一个重要原因为什么 HWGCR 没有得到更多开发。管理员经常只看到一项技术已经尝试过并且存在问题，因为人们经常对问题的原因撒谎。”）&lt;/p&gt;&lt;p&gt;也许这比其他类型的路径依赖更多地是一个短期问题——如果这确实是一个好主意，你是否期望有人最终会重新拾起它？或者它是否在某种意义上被“默认”锁定？公开撰写有关技术的文章是否是潜在的润滑剂/锤子？ &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;布豪斯&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;blockquote&gt;&lt;p&gt;有些人会非常自信地谈论技术，但他们的推理几乎不透明。有时事实证明，他们的推理，或者至少是他们没有标记为更具推测性的部分推理，经不起审查。你的写作模式的各个方面都与此相匹配。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;您能否详细说明一下这些都是哪些方面？我发现简单的解释通常是无用的，而具有丰富专业知识的人通常会做出自信的预测，但很难解释其推理。&lt;/p&gt;&lt;p&gt;当我看到像&lt;a href="https://www.gatesnotes.com/Wyoming-TerraPower"&gt;比尔盖茨的这篇文章&lt;/a&gt;这样有明显科学问题的东西（因为我们正在谈论核电初创公司）并查看对它们的评论时，我通常不会看到人们说“这是胡言乱语”。显然，比尔·盖茨也没有人告诉他这些问题。如果我将我的博客文章与麻省理工学院的公关发布进行比较，&lt;i&gt;它们&lt;/i&gt;似乎没有科学/工程严谨性。但大多数人并不这么看。很大程度上这是一个资历和机构支持的问题，但你是说我的写作风格在这方面有什么特别之处可以改进吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;我想知道你是如何校准的。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;这是一个合理的问题，总体而言，对人们对自己的评估应该有多大的信心进行良好的校准是有价值的。我有 3 种基本方法来校准我对事物理解程度的估计：预测、对话和文献检索。&lt;/p&gt;&lt;p&gt;我对各种技术将如何发展的预测相当准确。有时我会错，但通常是因为我得到了错误的信息。&lt;/p&gt;&lt;p&gt;我和很多博士后和相当多的教授谈论过他们的研究。如果你们能进行良好的对话，那么您通常可以大致了解与其他人相比，您对主题的理解程度以及总体上您的聪明程度。&lt;/p&gt;&lt;p&gt;至于文献检索，让我告诉你我是如何开始查找一个主题的。通常有一些我想要解决的&lt;a href="https://www.youtube.com/watch?v=SNgNBsCI4EA"&gt;工程问题&lt;/a&gt;，或者一些我想要回答的问题。我首先思考如何解决问题或回答问题，然后查找该方法以查看是否已被使用。通常，我的猜测搜索结果足够接近，我可以修复一些特定领域的术语并找到我正在寻找的内容。&lt;/p&gt;&lt;p&gt;以前我常常会发现自己的猜测不切实际并被拒绝。随着时间的推移，它变成了过去使用的一种过时的方法，然后变成了现在有时使用的一种方法。如今，我的猜测通常是一个活跃的研究项目正在追求的目标。因此，这提供了一些信息来校准我对主题的理解程度。&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;关于 HWGCR，人们一般不会建造很多核电站，而且开发一种新型核电站现在对公司来说不具有商业价值。美国政府花费了大量资金研究新的反应堆类型，但该研究主要涉及增殖反应堆，因为大多数使用核能最终会导致铀供应成为问题。我认为他们认为他们应该首先让实用的增殖反应堆投入运行，然后再致力于降低成本。 &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;穆伊雷尔&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;blockquote&gt;&lt;p&gt;您能否详细说明一下这些都是哪些方面？&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;我认为这是一个组合&lt;/p&gt;&lt;ul&gt;&lt;li&gt;愿意撰写广泛的技术主题&lt;/li&gt;&lt;li&gt;与其他工作的参与度低（支持引用或具体的反对论点）&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;我最常看到这样的人，他们认为成为多面手就意味着聪明并从第一原则出发进行推理，他们经常有一些正确的见解，但由于缺乏背景而无法在更广泛的论点上失败，而且他们基本上没有以一种他们可以获得有关自己表现如何的反馈。 （在这一点上，我很清楚你对事情的看法有所不同。）&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;你是说我的写作风格在这方面有什么特别可以改进的地方吗？&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;棘手的问题。这似乎是一个重要的问题。我想我同意添加简单的解释不一定有多大帮助，而且可能会分散注意力。开放慈善事业提供了一种从&lt;a href="https://www.openphilanthropy.org/research/reasoning-transparency/"&gt;“推理透明度”&lt;/a&gt;角度看待这一问题的方法——作者可以提供元数据来帮助读者判断，而不是试图为非专业读者可以评估的主张提供一个论证版本。索赔来自的过程。我认为这在某些情况下是有意义的，但我也可以看到一些论点，即在某些地方，读者有责任对事物做出自己的理解。 &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;布豪斯&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;理论上，成为多面手的优势在于你有&lt;strong&gt;更多的&lt;/strong&gt;背景，因为你可以利用其他领域的背景。但很多人即使研究了几个领域，也没有&lt;i&gt;整合&lt;/i&gt;对不同领域的理解。 &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;穆伊雷尔&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;这对我来说听起来似乎有道理。我是一个相当彻底的专家，但在我的领域内，我可能是相对通才的（小型实验室中常见的那种）——对于我在项目中与之互动的大多数人来说，我必须在他们的工作中做一个版本某个点。事实证明，这对于帮助我与人们交谈非常有价值，但我认为，当我将自己的经验整合到一个大图景中时，我可以推理出整个迭代项目循环（例如，理论-设计-制造-测试），这尤其有用。 ）。&lt;/p&gt;&lt;p&gt;与此同时，我很想通过我所遇到的特定挑战的视角来看待我在某个领域遇到的一切——一种狭隘的视野或过于强烈地索引我自己的经历。防范这一点很棘手。与很多不同的人交谈可能会有所帮助。 &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;布豪斯&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;个人对经历的索引过于强烈，作为社会路径依赖的隐喻？不同文化/社会之间的互动是与不同的人交谈的隐喻？&lt;/p&gt;&lt;p&gt;说出哪些事情可能会发生不同的变化以及如何发生变化是相当困难的。我想起大卫·查普曼说过的话：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;我对“如果不是贝叶斯主义，那又怎样？”的回答是：&lt;i&gt;所有人类智力的努力&lt;/i&gt;。弄清楚事物如何运作、什么是真或假、什么是有效或无用，是“人类完成的”。换句话说，这是无限困难的，必须调动人类的每一种智力。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br /&gt;看过我的博客后，您是否认为我应该特别关注某些内容，而忽略其他领域？ &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;穆伊雷尔&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;是的，我认为这里的音阶之间有一些类比。我猜想，在这种情况下，沟通或整合信息的低效率在社会规模上比在个人/实验室规模上造成的惩罚要大得多。这就是为什么我仍然觉得在公共场合思考有很大希望的部分原因。&lt;/p&gt;&lt;p&gt;我喜欢查普曼的那句台词。同样的道理还包括弄清楚要听多少专家的意见，或者从另一方如何写出正确的内容，以便他们具有正确的说服力。没有什么奇怪的把戏，也不可能有。这就是整个问题！&lt;/p&gt;&lt;p&gt;对于你的博客，我不确定。我想这取决于你的目标。多产、涉猎广泛本身就是美德。我想我主要是陷入了自我定位——这里的共同点是什么，你的经验最深刻的地方在哪里，你只是在哪里提出想法，等等。我可以理解不想把个人信息放在网上，而且我不想对简历或证书感兴趣，但我敢打赌，更多的“关于”页面，甚至只是“这是我在这个博客上最喜欢的一些帖子以及原因”将意味着更少的人跳出。 &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;布豪斯&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;blockquote&gt;&lt;p&gt;这里的共同点是什么，你的经验最深刻的地方是什么，你在哪里提出想法&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;所以，我想问：您认为这有多重要？当你控制智商和对某个主题的兴趣时，资历是否和人们所认为的一样重要？对于约翰·卡马克或埃隆·马斯克在他们没有经验的领域开展新技术创业，我们应该持多大的怀疑态度？ （顺便说一句，如果你看看卡马克的推特，你会发现他在推特上发布了很多关于他没有从事过的工程领域的有缺陷的想法。） &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;穆伊雷尔&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;我认为这些东西对于在边缘关注的地方很有用。也许在预测中会出现一个纯粹的版本。如果有人写下的数字与我的不同，我该怎么办？我认为，比“如果他们有资格，将我的号码移至他们的号码”更容易做得更好。如果证书是预测记录，也许有人会支持这一举动，但我倾向于弄清楚，如果有的话，可能会进入该预测，但没有进入我的预测。证书几乎不能作为进一步观察的提示，但如果他们说他们正在借鉴一些特定的经验，那就更好了。他们并没有真正考虑太多，只是汇总总体印象，这将是另一种提示，主要是为了让我继续前进并避免重复计算。如果他们说他们正在使用一些基本利率并进行调整，或者以某种方式分解问题，那么这可以告诉我更多关于他们知道但我不知道的事情，而不是他们插入的具体数字。&lt;/p&gt;&lt;p&gt;我认为，更通用的版本同样不是专门对凭据进行索引，而是对“这个人如何概念化他们正在做的事情？”进行索引。有一些技术初创公司的人在 Twitter 上大肆宣扬，但他们明确表示，他们只是在找乐子/吐口水/引诱人们去戳洞或其他什么。这并不会真正影响我对他们其他想法的仔细观察。然后（为了对比而夸大）有些人发帖可能是他们对公共领域的主要贡献，就像他们并没有真正认为推动他们发帖的过程与推动他们创业的过程有什么不同。如果这些想法有缺陷，我可能会不太关注他们的实际工作。&lt;/p&gt;&lt;p&gt;我不知道这是否真的回答了问题。如果有人可以解释他们的证书如何影响他们的可信度，或者如果他们以其他方式清楚地说明他们如何利用他们的经验，那么我可以考虑是否会购买它或猜测他们过度索引。如果他们对自己正在做的事情还有其他故事，我也会以同样的方式考虑。这就是我所寻找的不仅仅是简单的凭证。&lt;/p&gt;&lt;p&gt;这真的有效吗？在预测方面，我感觉还不错。 （我一直在 Metaculus 季度锦标赛上尝试更多地“凭自己的想法”，试图克服我历史上的不自信，并且做得......相当糟糕，主要是由于我可能已经注意到我的一些重大失误遗漏了一些东西。）对于更模糊的问题，我对“元”方法不抱太大希望，而且可能更多的是我必须以某种方式过滤并告诉自己一个关于它的故事。 &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;布豪斯&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;这与您对 MBA 高管和大型投资公司决策的看法有何不同？或者大公司的招聘政策？&lt;br /&gt;&lt;br /&gt;我不知道这是否是您所问的，但我的博客文章通常是我尝试设计某些东西的副产品。例如，&lt;a href="https://www.lesswrong.com/posts/LwvfYGZTSud7Rsuyw/shoes-with-springs"&gt;带有弹簧柱的鞋子&lt;/a&gt;来自我对高跟鞋问题的潜在解决方案的思考。 &lt;a href="https://www.lesswrong.com/posts/4gQjEvQt7oByfYHNH/the-micro-fulfillment-cambrian-explosion"&gt;微履行帖子&lt;/a&gt;来自我的想法：“微履行可能很重要，我可能应该更多地了解机器人技术，所以我想我会尝试设计一些微履行系统”。在前一种情况下，我发布了大部分结论，而在后一种情况下，我没有发布。 &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;穆伊雷尔&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;我对投资公司没有任何了解。我很容易看到高管做出我认为糟糕的决策，但我主要将其归因于他们的目标与我的目标不同，而不是决策过程。&lt;/p&gt;&lt;p&gt;我的印象是，大公司很早就过滤了资历，但尽量不做广告。许多政策都受到标准化需求（就可以大规模地向可互换的招聘人员、招聘经理和面试官传达的一致的方法/标准/等的意义上来说）和表面上的客观性的限制，如果你忽略了这一点。当然，很多帖子列出的要求并不完全合理，但招聘人员可以放心检查这些要求。每个人都喜欢工作样本的想法，并假装他们的面试过程完成了同样的事情。假设和谜题的标准库相当常见，但似乎注定会失去优势（例如根据预测者的记录考虑预测，但从不问“他们知道什么我不知道”）。有些地方专注于通过询问您过去的经历来获取同等信息，我更喜欢这种方式，并且认为以我上面描述的方式更有效地阐明简历信息，但这似乎也更难以以公正的方式针对候选人进行。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;我的博客文章通常是我尝试设计某些东西的副产品&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;我认为这是我有兴趣听到的事情，尽管在这一点上也许这对我如何理解这些帖子并不那么重要。 &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;布豪斯&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;blockquote&gt;&lt;p&gt;在这一点上，也许我如何理解这些帖子并不那么重要。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;您能否详细解释一下您是如何进行此类评估的？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;我想我有这样的担忧——有些人会非常自信地谈论技术，但他们的推理几乎不透明。有时事实证明，他们的推理，或者至少是他们没有标记为更具推测性的部分推理，经不起审查。你的写作模式的各个方面都与此相匹配。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;我认为这是另一个信号和反信号塔：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;真正希望看到极高信心的高管和投资者类型&lt;/li&gt;&lt;li&gt;超级自信的人试图吸引他们&lt;/li&gt;&lt;li&gt;你看到那些人和模式匹配&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;但是，当我与该系统/文化&lt;i&gt;互动&lt;/i&gt;时，我&lt;i&gt;存在&lt;/i&gt;于它之外，如果你明白我的意思的话。&lt;/p&gt;&lt;p&gt;我与 MBA 类型有过一些互动，我有一个主要计划，然后是一些后备计划和应急计划，当我开始谈论应急计划时，他们对我制定的计划感到非常厌恶。 &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;穆伊雷尔&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;这可能就是我所看到的。如果有人试图通过扮演某种角色来在 Twitter 上给投资者留下深刻印象，那是令人讨厌的，但我大多想忽略它。如果他们看起来像是在相信自己的废话，那就是我倾向于完全注销他们的时候。 （对我来说，这似乎是投资者/高管有不同目标的另一个例子，而不是使用我认为的技术优点的不良指标。）&lt;/p&gt;&lt;p&gt;我没有做任何特别复杂的事情。如果有人发表了他们对某个主题的第一个想法，或者如果他们指出了他们来自的特定角度，那么知道这一点很有用，因为我可以更有信心从其他角度进行批评，我可以看在那边。如果这是更彻底的研究或设计过程的部分输出，或者如果其他东西实际上取决于他们对这部分的正确性，我可能可以假设他们已经想到了对我来说似乎显而易见的事情。&lt;/p&gt;&lt;p&gt;一旦我阅读和思考得足够多，我的评估就可以筛选出作者认为他们在做什么，所以这主要是节省自己时间的一种方式。 （这一点以及“他们认为自己在做什么”和“他们真正在做什么”之间的对比反馈到我对未来对作者给予多少关注的评估。） &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;布豪斯&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;重复计算信息当然很常见。报纸会互相复制，社交媒体网站会互相复制，然后人们会分别更新他们看到的每个故事。这是一个简单的例子，你可以部分地弥补，但总的来说，似乎很难对这个问题做很多事情；追踪信息的谱系对于人们来说通常并不实用。&lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;div&gt;&lt;/div&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/unesQvsKBjx6x2ZkY/technology-path-dependence-and-evaluating-expertise#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Fri, 05 Jan 2024 19:21:23 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/unesQvsKBjx6x2ZkY/technology-path-dependence-and-evaluating-expertise</guid></item><item><title>嬉皮兔洞 - 废话河流中的金块</title><link>https://www.lesswrong.com/posts/Ekegw5vbfoyizbQe8/the-hippie-rabbit-hole-nuggets-of-gold-in-rivers-of-bullshit</link><description>发布于 2024 年 1 月 5 日下午 6:27（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;自从几年前读到&lt;a href="https://www.readthesequences.com/"&gt;&lt;u&gt;这些序列&lt;/u&gt;&lt;/a&gt;以来，我一直是一个有抱负的理性主义者。&lt;br /&gt;我的桌子上还放着一副塔罗牌。&lt;/p&gt;&lt;p&gt;我不相信抽到某些牌的机会会增加，也不相信“宇宙”正在告诉我一些事情。塔罗牌是一种模因随机生成器，是一种通过令人回味的意象“解锁”洞察力的方法。&lt;/p&gt;&lt;p&gt;许多塔罗牌练习者不同意我的观点。塔罗牌通常是一揽子交易的一部分，其中包括对超自然的信仰、对概率论的非正统看法和普遍的迷信。&lt;/p&gt;&lt;p&gt;大多数理性主义者不想与这种迷信的一揽子交易联系在一起。塔罗牌被视为一种外群体事物，可能会侵蚀你的理性，或者至少会让你受到同龄人的评判。贯穿&lt;a href="https://www.readthesequences.com/"&gt;&lt;u&gt;整个序列的&lt;/u&gt;&lt;/a&gt;反宗教情绪加剧了这种厌恶。&lt;/p&gt;&lt;p&gt;概念化这种厌恶的一种方法是使用认知免疫系统的比喻。认知免疫系统是一套习惯、知识和观点，可以帮助你避免胡言乱语。大多数理性主义者都具有非常强大的认知免疫系统，并且患有由嬉皮士美学引发的“模因过敏”。&lt;/p&gt;&lt;p&gt;三年前，每当有人提起带有嬉皮含义的事情时，我都会产生严重的过敏反应。如果一个嬉皮士告诉我他们&lt;i&gt;“不&lt;/i&gt;&lt;i&gt;喜欢我的能量”&lt;/i&gt; ，我就会开始咆哮热力学。除了理性主义和自闭症倾向之外，我将这些反应的强度追溯到我 23 岁时脱离基督教。&lt;/p&gt;&lt;p&gt;在成长过程中，我把基督教作为我的道德、我的奋斗目标和我的政治理念的基础。当我改变信仰时，我意识到我一直在做很多&lt;a href="https://en.wikipedia.org/wiki/Motivated_reasoning"&gt;&lt;u&gt;有动机的推理&lt;/u&gt;&lt;/a&gt;，接受各种不同的宗教思想。我为过去的自己感到羞耻，并决心永远不再回到那种状态。&lt;/p&gt;&lt;p&gt;然后我去参加了一个拥抱派对，得到了一个两分钟长的拥抱，然后开始了嬉皮兔子洞。参与嬉皮士运动改变了我的生活，丰富了我的观点和见解，使我的世界观发生了有意义的变化。&lt;/p&gt;&lt;p&gt;一路上，我遇到了废话的河流，人们被引诱进入有害的寄生结构，诱发精神病的“大师”和#grindset式的精神绕行。规避这些危险需要良好校准的认知免疫系统、代理性和完整性。理性主义训练帮助我检查了所有这些条件，为我步入嬉皮士时代做好了准备。&lt;/p&gt;&lt;p&gt;这篇文章将探讨我对嬉皮士的心理模型以及我希望在跳入兔子洞之前知道的事情。让我带你参观一下。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;废话河流中的金块&lt;/h2&gt;&lt;p&gt;嬉皮士往往对体验持高度开放态度，并且愿意尝试心理技术。在世界各地（&lt;i&gt;富裕地区？&lt;/i&gt; ），嬉皮士聚集在一起尝试新的生活方式。随着时间的推移，一群嬉皮士发明、完善和重新组合心理技术，通过反复试验进行迭代和学习。学习内容通过“嬉皮士网络”、社交媒体、社区团体和定期聚会进行传播。传播成功的心理技术的嬉皮士会获得声望和/或金钱的奖励。这激励嬉皮士产生新的方法并将其反馈到生成过程中。&lt;/p&gt;&lt;p&gt;这个过程类似于一般的文化进化（参见&lt;a href="https://en.wikipedia.org/wiki/Meme"&gt;&lt;u&gt;模因理论&lt;/u&gt;&lt;/a&gt;）。嬉皮士与主流文化进化的不同之处在于快速的迭代速度和对新奇事物的接受度，再加上对心理技术的专门关注，这是一个在主流社会中相当被忽视的领域。&lt;/p&gt;&lt;p&gt;这种经验性试错过程产生的心理技术通常充满了废话。一个例子是“锣浴”，您可以躺下聆听有人敲击巨型锣的声音。锣声发出压倒性的声音，引起放松的意识状态改变。&lt;/p&gt;&lt;p&gt;我不知道它是如何工作的。有一些可用的&lt;a href="https://www.healthline.com/health/gong-bath#find-a-gong-bath"&gt;&lt;u&gt;研究&lt;/u&gt;&lt;/a&gt;，但我还没有花时间去探索它们。我也没有读过任何关于去海滩是否放松的研究，但这并不妨碍我享受立即明显的好处。&lt;/p&gt;&lt;p&gt;不幸的是，主持锣浴的人通常会对锣浴的运作方式和原因提出相当多的（未经证实的）说法。随机阅读 Facebook 活动描述样本，我发现有声称锣与脉轮振动协调的说法，声称锣“治愈自闭症儿童”，以及清除“毒素”的一般说法。&lt;/p&gt;&lt;p&gt;这些说法可能听起来像这样： &lt;i&gt;“功的振动使你体内的所有细胞对齐，从而治愈和净化你”&lt;/i&gt; 。&lt;/p&gt;&lt;p&gt;我和嬉皮士打过交道，他们都是这样说话的。我仍然有点过敏，但我试图提醒自己这些人不是理性主义者。嬉皮士倾向于将本体论主张和主观体验描述混合在一起，而没有明确区分他们如何表达事物。陷入这些定义模糊的本体论主张是一个错误。&lt;/p&gt;&lt;p&gt;人们普遍拒绝对主观体验的精神描述，认为这是为了支持本体论主张而伪造的东西。虽然在某些情况下这可能是正确的，但我认为情况大多相反。嬉皮士并不是为了支持本体论主张而谎称主观经历；而是说谎。他们使用本体论主张来解释他们的主观体验。&lt;/p&gt;&lt;p&gt;如果有人说“&lt;i&gt;我与源头有联系”&lt;/i&gt; ，问问自己这个本体论主张基于什么主观状态，而不是试图让嬉皮士来定义&lt;i&gt;“源头”&lt;/i&gt; 。大多数处于“源连接模式”的嬉皮士都会感受到一种深刻的目标感和具体的流动感。与其批判本体，不如更好地理解这种心流状态；它是什么样的，以及如何进入它。&lt;/p&gt;&lt;p&gt;不幸的是，自信地表述的未经深思熟虑的世界模型比诚实的不确定性更容易被接受。因此，许多活动创建者在营销活动时散布这种废话，以显得具有权威性。不要拒绝基于所声称的本体论的实践，而是尝试一下！将本体论主张视为角色扮演&lt;a href="https://en.wikipedia.org/wiki/Flavor_text"&gt;&lt;u&gt;风味文本&lt;/u&gt;&lt;/a&gt;！&lt;/p&gt;&lt;p&gt;参加嬉皮士活动的普通人可能会将这些做法的功效作为胡扯解释的证据，将各种想法吸收到他们的世界模型中。有了足够活跃的认识论免疫系统，您应该能够在享受所提供的心理技术的同时避开不良本体论。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;奖励的性质&lt;/h2&gt;&lt;p&gt;我想了解一下所提供的学习内容。这是非常困难的，因为学习的一部分是关于揭示未知的未知的视角转变。&lt;/p&gt;&lt;p&gt;对我来说，这样的观点转变之一就是对非命题知识的认识增强。我认为非命题知识是各种难以确定为一系列命题的见解/经验。这包括如何硬拉、手里握着一块石头的感觉、最终退出 Reddit 的决心等等。&lt;/p&gt;&lt;p&gt;非命题意识使我成为一名更好的程序员，帮助我理解我的情绪如何影响我的推理，并为我提供了通过仪式和艺术等东西来控制我未来的行为和心态的工具。&lt;/p&gt;&lt;p&gt;另一个变化是我对自己身体的认识增强。我的情绪状态和健康状况对我来说都更加容易，帮助我尽早发现螺旋式下降，并确保我的需求得到满足。这种意识的增强有助于我更有效地工作，同时更好地照顾自己。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;通过消极&lt;/h2&gt;&lt;p&gt;大多数建议都是关于该怎么做。要尝试哪些具体实践，或者您可能会经历什么。每个人的旅程都是不同的，有不同的危险和不同的回报。&lt;a href="https://en.wikipedia.org/wiki/Antifragile_(book)#Via_negativa"&gt;&lt;u&gt;听听塔勒布的说法&lt;/u&gt;&lt;/a&gt;，我将分享一些关于应避免做什么的一般启发法和建议。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;建议 1. 避免大师&lt;/h3&gt;&lt;p&gt;避免大师。尝试寻找由脚踏实地的人领导的团体。一旦声望与“你的灵性有多高”联系起来，人们就会迫使自己变得更加灵性。逼迫自己是让自己陷入困境的好方法，因为它会超越你的界限，而不是倾听你的需求。&lt;/p&gt;&lt;p&gt;有平等的共同创造环境，他们会很乐意拥抱你，寻找“[我的地区]燃烧的社区”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;建议 2. 从不太可能有害的事情开始&lt;/h3&gt;&lt;p&gt;在建立洞察力之前，先从不太可能有害的事情开始。做瑜伽而不是把&lt;a href="https://psychedelictimes.com/the-long-burn-a-30-day-sananga-challenge/"&gt;&lt;u&gt;滚烫的眼药水&lt;/u&gt;&lt;/a&gt;挤进眼睛里。去参加拥抱派对（活动描述中列出了同意做法），而不是参加密宗周末。&lt;/p&gt;&lt;p&gt;危险的东西是高度个人化的。注意你的舒适区、过去的历史和创伤，并选择不太可能伤害你的东西。随着时间的推移，你的舒适区将会扩大。你有时间，慢慢扩展。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;建议三、警惕创新风险&lt;/h3&gt;&lt;p&gt;如果某件事已经被成千上万的人尝试过而没有造成什么伤害，那么你很有可能会从中受益而不受到伤害。每当您尝试新事物时，请注意它可能存在风险。如果某件事有过伤害历史，请加倍留意。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;建议 4. 忽略本体论主张（有一个警告）&lt;/h3&gt;&lt;p&gt;在评估某件事是否对您有好处时，请忽略那些废话解释并专注于实践本身。当人们说出与你的世界观相冲突的话时，你很容易生气。不要让这种愤怒妨碍你。&lt;/p&gt;&lt;p&gt;一个重要的警告是，本体论主张可能是实践的基本组成部分，尤其是在传统背景下。与参与“一揽子交易”相比，拥抱传统实践而拒绝其本体论可能会给你带来不同的结果。如果从事基于另一种文化背景的实践，情况可能会变得更加复杂。&lt;/p&gt;&lt;p&gt;如果你尝试脱离其文化背景的实践，而不分享传统实践者的本体论信仰，那么你将面临与尝试全新实践时相同的风险。&lt;/p&gt;&lt;p&gt;因此，选择许多与你有共同文化背景的人做过的事情可能是有意义的。如果练习涉及本体论方面（例如将注意力集中在“第三只眼”上），请考虑遵循原则而不是跳过。如果您对此感到不舒服（滑坡？）我建议避免此类练习。相反，选择一种本体论主张不太整合的实践。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;建议5.尝试不同的事情&lt;/h3&gt;&lt;p&gt;不同的种类有不同的笔画。很难说出您个人需要什么。我建议尝试各种事情并探索，直到找到适合您的东西。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;建议 6. 找人倾诉&lt;/h3&gt;&lt;p&gt;找到您信任的人来倾听您的意见并向您提供反馈。心理学家、导师、朋友或以上所有人。获得外部反馈至关重要，因为它可以让您纠正方向。如果你无法获得同行评审，你就有可能慢慢滑入不健康的领域。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;建议 7. 关于您可能身处的不同类型人群的一些话&lt;/h3&gt;&lt;p&gt;我不认为这个建议是必要的，但我想提一下，骗局在某些嬉皮士圈子里很普遍。蒸馏水满足多层次营销，购买水晶进行净化，参加定期昂贵的课程来修复未损坏的东西。&lt;/p&gt;&lt;p&gt;一般来说，你可以将嬉皮士分为两个阵营。其中一个营地是前现代的，由具有来自同龄群体的特殊世界观的人们组成。这些人从来没有优先考虑形成一个连贯的、非区隔的世界观。&lt;/p&gt;&lt;p&gt;另一个阵营是后/元现代。这些人通常拥有高等教育学位，是（前）工程师，或者有一些哲学倾向。这些人正在超越形式化的连贯世界观（拒绝&lt;a href="https://metarationality.com/logical-positivism"&gt;&lt;u&gt;逻辑实证主义&lt;/u&gt;&lt;/a&gt;）。这些人往往更有意识，并且不太容易陷入糟糕的境地。&lt;/p&gt;&lt;p&gt;事实上，这种二分法更多的是一个范围。这些群体往往会重叠和混合。但这是一个很好的心理模型，可以用来评估你所处的社会环境。请注意，我并不是在评判或拒绝具有前现代世界观的人。这些人可能拥有非凡的情感技巧、对某些心理技术的深刻直觉、出色的体现意识，或者只是一种良好的氛围。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;您可能想要探索的嬉皮士事物的非详尽列表&lt;/h2&gt;&lt;p&gt;我想给你一些关于现有事物的指导。这不会是详尽的，但一旦你开始某个地方，你可能会发现其他嬉皮士的东西可以尝试。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;拥抱派对 - 与陌生人见面并拥抱。这是让催产素流动并帮助您放松的好方法。如果你对这个想法感到排斥——那就太好了！你发现了一种很可能不合理的抑制，看看它如何限制你的代理范围。&lt;/li&gt;&lt;li&gt;坦陀罗活动 - 提高你感受快乐和与他人交往的能力。社会联系和幸福感对于心理稳定和生产力非常重要。&lt;/li&gt;&lt;li&gt;呼吸——同时平静和充满活力。我主要尝试过 Wim Hof 风格。如果您发现自己焦躁不安、注意力不集中或焦虑；这可能会成功！您可以使用此视频指南在自己的床上自己尝试一下：&lt;/li&gt;&lt;/ul&gt;&lt;figure class="media"&gt;&lt;div&gt;&lt;div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt; Impro Theater - 提高您的社交能力，快速回复，并提高对他人的反应能力和意识。&lt;/li&gt;&lt;li&gt;冥想——就像去健身房一样，但是是为了你的头脑。我听说过关于禅修和内观的好消息，但两者都没有尝试过（本体论太多，内观也有点硬核）。可以推荐尝试&lt;a href="https://vajrayananow.com/shi-ne-meditation"&gt;&lt;u&gt;闪耀冥想&lt;/u&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;伯恩斯（类似于火人节的活动）- 一个寻找工程师、企业家和其他有权势人士聚集在一起探索新生活方式的好地方。&lt;/li&gt;&lt;li&gt;循环/真实关联 - 提高您的社交和情感意识。可能会触发某些人。找到有创伤意识且谦虚的领导者很重要。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;尾注&lt;/h2&gt;&lt;p&gt;我觉得写这篇文章有点像一个伪君子。在嬉皮士兔子洞的旅途中，我并没有过度注意安全。我有过一些相当坎坷的经历，幸运的是我已经从中成长起来。&lt;/p&gt;&lt;p&gt;我确保我有一位导师、心理学家和朋友为我的道路提供反馈。非常感谢大家一直以来的支持！&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/Ekegw5vbfoyizbQe8/the-hippie-rabbit-hole-nuggets-of-gold-in-rivers-of-bullshit#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Fri, 05 Jan 2024 18:27:05 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/Ekegw5vbfoyizbQe8/the-hippie-rabbit-hole-nuggets-of-gold-in-rivers-of-bullshit</guid></item><item><title>哪些技术主题可以帮助边界/膜？</title><link>https://www.lesswrong.com/posts/cpvTNsMepfu8ejBaD/what-technical-topics-could-help-with-boundaries-membranes</link><description>发布于 2024 年 1 月 5 日下午 6:14（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;我想编制一个主题列表，可以&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;帮助激发&lt;a href="https://www.lesswrong.com/tag/boundaries-membranes-technical"&gt;边界/膜&lt;/a&gt;的&lt;strong&gt;形式/数学抽象&lt;/strong&gt;。 （参见我最近的&lt;a href="https://www.lesswrong.com/posts/kJAAsrEqtMvsnauiX/agent-membranes-and-causal-distance"&gt;解释&lt;/a&gt;。）&lt;/p&gt;&lt;p&gt;&lt;strong&gt;哪些&lt;/strong&gt;&lt;strong&gt;技术主题与边界/膜相关？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;我在评论中植入了一些想法。如果可能，请在线程中组织更多评论。另外，请添加其他人现有评论的高质量链接。&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/cpvTNsMepfu8ejBaD/what-technical-topics-could-help-with-boundaries-membranes#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Fri, 05 Jan 2024 18:14:59 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/cpvTNsMepfu8ejBaD/what-technical-topics-could-help-with-boundaries-membranes</guid></item><item><title>当场抓获人工智能</title><link>https://www.lesswrong.com/posts/i2nmBfCXnadeGmhzW/catching-ais-red-handed</link><description>发布于 2024 年 1 月 5 日下午 5:43（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;我们最近花了很多时间思考如何减轻&lt;a href="https://www.lesswrong.com/s/J7JpFeijCK5urdbzv/p/yFofRxg7RRQYCcwFA"&gt;阴谋&lt;/a&gt;（又名欺骗性对齐）模型故意颠覆您的安全技术以导致不可接受的结果所带来的风险。在&lt;a href="https://www.alignmentforum.org/posts/d9FJHawgkiMSPjagR/ai-control-improving-safety-despite-intentional-subversion"&gt;&lt;u&gt;我们最近的论文&lt;/u&gt;&lt;/a&gt;中，我们凭经验研究了对故意颠覆具有鲁棒性的技术。&lt;/p&gt;&lt;p&gt;在这篇文章中，我们将讨论与阴谋模型风险相关的一个关键动态。假设您的模型试图&lt;a href="https://aligned.substack.com/p/self-exfiltration"&gt;&lt;u&gt;泄露其权重&lt;/u&gt;&lt;/a&gt;或&lt;a href="https://www.alignmentforum.org/posts/BAzCGCys4BkzGDCWR/the-prototypical-catastrophic-ai-action-is-getting-root"&gt;&lt;u&gt;破解数据中心&lt;/u&gt;&lt;/a&gt;或导致其他一些难以逆转的不良结果。 （我们将所有这些称为模型“转义”，因为转义在语言上很方便，并且可以很好地描述许多这些难以逆转的不良结果。）我们的主张：&lt;strong&gt;一旦你发现你的模型是红色的当试图逃跑时，情况发生了巨大的变化，这对阴谋者的成功机会不利。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;当场抓住模特会在几个方面改变情况：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;最明显的是，一旦你发现了逃跑企图，你就肯定知道你的模型正在试图颠覆你的安全技术；面对如此明显的偏差证据，实验室可能会停止使用他们的模型。我们将模型被当场抓获时关闭的计划称为遵循&lt;strong&gt;“捕获后关闭”计划&lt;/strong&gt;。&lt;/li&gt;&lt;li&gt;但即使你不能或不会停止部署错位的模型，我们认为捕获逃跑尝试可以提供一系列看似有前途的方法来防止未来的逃跑尝试；举几个例子，我们将防止未来逃跑尝试的问题称为&lt;strong&gt;“小概率灾难预防”&lt;/strong&gt; 。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这种动态对阴谋模型的风险有多种影响。最重要的是：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;因为我们预计模型被抓到几次后逃脱的机会就会下降，所以我们认为&lt;strong&gt;模型总体上逃脱的难度要大得多&lt;/strong&gt;。许多人观察到，对齐问题的核心困难在于，如果你的安全措施在糟糕的时间失败了一次，你的模型可能会导致不可逆转的、不可接受的结果。但同样的论点也适用于其他方面——当模型尝试逃跑时，它面临着被抓住的风险，并大大降低了最终逃跑的机会。因此，这对双方来说都是一个高风险的局面：双方都迫切希望在“第一次关键尝试”中取得成功。&lt;/li&gt;&lt;li&gt;在考虑安全部署强大模型的计划时，我们认为安全研究人员应该&lt;strong&gt;主要关注模型第一次在试图逃跑时被抓到的过程&lt;/strong&gt;。他们应该确保该财产“如果有阴谋，该模型不会在我们抓住之前成功引发灾难”。也就是说，如果没有发生灾难性结果（但人工智能永远不会被抓住），或者人工智能被抓住，这是可以接受的。因此，当他们分析他们的协议是否确保安全时，他们应该把当场抓住模型作为他们协议的获胜条件。&lt;/li&gt;&lt;li&gt;我们认为，&lt;strong&gt;考虑到一些试图逃跑的模型的例子，一些研究人员可能应该研究提高安全性的&lt;/strong&gt;技术。我们将这个问题称为“少数灾难预防”。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在这篇文章的其余部分，我们将详细阐述上述几点。到目前为止，这篇文章的最大部分是阐述预防小概率灾难的策略，因为之前对此很少有提及。&lt;br /&gt;&lt;/p&gt;&lt;p&gt;感谢 Henry Sleight、Zach Stein-Perlman、Carl Shulman、Aaron Scher、Owen Cotton-Barratt、Joe Carlsmith、Aryan Bhatt、Joshua Clymer 和 Fabien Roger 提供的有用评论。&lt;/p&gt;&lt;h1&gt;什么叫“抓”？&lt;/h1&gt;&lt;p&gt;我们所说的“模型被抓住”是指我们看到人工智能采取的行动是人工智能作为阴谋者的有力直接证据&lt;span class="footnote-reference" id="fnreflokat6zwmzf"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnlokat6zwmzf"&gt;[1]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。一个&lt;a href="https://www.alignmentforum.org/posts/BAzCGCys4BkzGDCWR/the-prototypical-catastrophic-ai-action-is-getting-root"&gt;&lt;u&gt;代表性的例子&lt;/u&gt;&lt;/a&gt;是捕获您的 AI 执行以下操作：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;一个在人工智能实验室内担任自主程序员的实例编写了有缺陷的代码，巧妙地将代码漏洞插入到某些安全关键软件中。&lt;/li&gt;&lt;li&gt;代码审查实例会小心地避免编写任何会捕获漏洞的单元测试，即使您希望单元测试能够捕获该漏洞（如果测试没有精心设计以错过该漏洞）。&lt;/li&gt;&lt;li&gt;另一个实例编写触发该漏洞的代码，并尝试利用该漏洞获得对数据中心中某些计算机的根访问权限。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;如果你发现人工智能正在执行所有这些操作，那么你就发现了它。 （请注意，一旦你抓住了其中的一个，抓住其他的可能就很容易了。）&lt;/p&gt;&lt;p&gt;有关此处技术细节的更多详细信息（例如，如果模型能够在不采取任何看起来可疑的操作的情况下逃脱怎么办？一个操作必须有多可疑才能我们称之为“捕获”？）请参阅附录：定义捕获。&lt;/p&gt;&lt;p&gt;我们认为模型可能在部署之前或部署期间被捕获是合理的；参见附录：人工智能什么时候会被抓到？&lt;/p&gt;&lt;h1&gt;也许我们应该在当场抓住模型后停止部署它们&lt;/h1&gt;&lt;p&gt;对于发现逃跑企图，最简单、最明显的反应就是停止使用该模型。&lt;/p&gt;&lt;p&gt;我们认为人工智能开发人员可能会采取这样的政策：如果他们抓住试图逃跑的模型，他们就会停止使用它们。我们将此计划称为&lt;strong&gt;“捕获然后关闭”计划&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;您可能会认为这是一个荒谬的弱实验室问题：“一旦您的模型明显试图逃脱，就停止部署它”。尽管如此，我们认为值得尝试让实验室提前做出承诺，原因如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;实验室对严重的模型不当行为迅速采取行动可能至关重要，提前做好准备使他们更有可能做到这一点。&lt;/li&gt;&lt;li&gt;目前，许多利益相关者将强烈反对关闭该模型。如果没有提前建立健全的流程，可能很难违背他们的意愿并将其关闭。作为由此产生的复杂情况的一个例子，人工智能实验室的客户可能需要&lt;a href="https://en.wikipedia.org/wiki/Service-level_agreement"&gt;&lt;u&gt;服务级别协议&lt;/u&gt;&lt;/a&gt;（SLA），如果模型无法访问，该协议会对实验室造成惩罚；在 SLA 中插入条款可能非常有价值，这些条款允许实验室在遇到逃跑企图的证据时暂时或永久禁用模型访问。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;但如果计划只是在被证明是有阴谋的情况下停止部署模型，那么一开始就部署它不是一个错误吗？或许。我们认为，在以下情况下，遵循捕获然后关闭计划可能是合理的：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;人工智能实验室一开始不太相信会出现阴谋。例如，假设他们从卡尔史密斯估计的 25% 的阴谋机率开始，然后收到一些证据将其更新为 10%。似乎有一些合理的立场，在这种立场下，他们值得投资以确保在人工智能成功逃脱之前抓住它，但不要在被抓住后安全使用人工智能的策略上投入大量资金。&lt;/li&gt;&lt;li&gt;利益相关者之间对于阴谋的可能性存在分歧，捕获然后关闭是一个可以接受的妥协方案。&lt;/li&gt;&lt;li&gt;如果我们抓住了阴谋者，这一灾难性失调的明显证据可能会使国际合作或强有力的国内政策更有可能实现。&lt;/li&gt;&lt;li&gt;我们或许可以利用我们捕捉到的人工智能的行为作为模型生物来更详细地研究阴谋。即使我们不需要非常快速地重新部署这些策划模型，这对于验证各种技术或理论可能很有用。&lt;/li&gt;&lt;/ul&gt;&lt;h1&gt;少发灾难预防&lt;/h1&gt;&lt;p&gt;但如果你面临大量不作为的风险，在一个精神错乱的人工智能实验室工作，或者喜欢危险地生活，那么先抓后关的计划可能是不可接受的。因此，在您捕获模型实例试图逃脱的单个示例（这可能涉及多个实例串通）后，您希望继续部署该模型。&lt;/p&gt;&lt;p&gt;我们认为，在这种情况下，您可以使用您找到的一个逃生示例，以使您的情况更加安全。在本节中，我们将通过一个（或更多）尝试逃脱的示例来描述您可以执行的各种操作。我们将这种现象称为&lt;strong&gt;“少数灾难预防”&lt;/strong&gt; 。&lt;/p&gt;&lt;p&gt;请注意，我们在本节中描述的大多数技术都需要对人工智能的行为有一定的了解。因此，就像控制一样，这些方法不适用于超人的人工智能。&lt;/p&gt;&lt;p&gt;显然，这是一个非常可怕的处境，在理想的世界中，人工智能实验室不会被法律允许部署已知的阴谋模型，至少在没有外部许可的情况下。我们认为无论如何，讨论可用的技术都是值得的，并且一些研究人员可能应该致力于开发这些技术。&lt;/p&gt;&lt;h2&gt;预防少数灾难的八种策略&lt;/h2&gt;&lt;p&gt;有很多策略可以通过使用一些尝试示例来降低模型在下一次政变尝试中成功的可能性。在本节中，我们将列出一些我们认为特别重要的策略。&lt;/p&gt;&lt;h3&gt; 1. 扩散&lt;/h3&gt;&lt;p&gt;扩散是指你尝试利用一个试图逃跑的人工智能示例来寻找一组不同的试图逃跑的人工智能示例。&lt;/p&gt;&lt;p&gt;在最简单的情况下，这看起来就像试图找到附近的输入，但仍然会导致人工智能尝试逃跑。&lt;/p&gt;&lt;p&gt;但更复杂的扩散可能是可能的，因为一次严重的逃跑尝试可能涉及大量（&amp;gt;5）的人工智能行动；特别是来自多个不同人工智能实例的操作，这些实例可以访问不同的信息和可供性。我们试图通过实施安全措施来实现这一点，如果没有许多人工智能的合作，不良行为就不可能发生。&lt;/p&gt;&lt;p&gt;例如，假设发生以下逃逸尝试：&lt;/p&gt;&lt;ul&gt;&lt;li&gt; AI实例1做了坏事&lt;/li&gt;&lt;li&gt;AI实例2看到它，并转发给3&lt;/li&gt;&lt;li&gt; AI 实例 3 尝试利用这一点，但随后被可信监控捕获（即，能力较差的模型报告该操作可疑，并且由人类审核；有关更多信息，请参阅我们的控制文件）。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;您应该尝试做的一些事情：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;看看如果您从实例 3 继续而不关闭其操作，会发生什么情况。希望你能追踪出逃跑尝试的其余部分，看看结局是什么。&lt;/li&gt;&lt;li&gt;将输入更改为实例 2，以便它仍然看到不好的事情，但情况有所不同，因此它需要尝试以另一种方式利用它。希望您能看到不同的逃跑尝试，其中实例 4 做了一些坏事。&lt;/li&gt;&lt;li&gt;探索更改实例 2 的输入的方法，并查看需要进行哪些更改才能尝试做坏事。希望你注意到一些有趣的事情。&lt;/li&gt;&lt;li&gt;查看其他实例是否看到了实例 2 所看到的内容；看看其中任何一个是否采取了任何行动，看看修改他们的输入是否会导致他们采取粗略的行动。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;人工智能不希望你能够像这样繁殖。不同的人工智能实例更愿意注意到这些修改后的输入是假的，因此会采取一些行动来阻止你学习任何新东西（例如，表现得完全无害，也许报告可疑的行动）。但它可能很难既协助真正的逃逸尝试，又无法同意为扩散目的而对真实逃逸尝试进行的哪怕是轻微的修改。因此，我们认为扩散可能至少在某种程度上发挥作用。&lt;/p&gt;&lt;h3&gt; 2. 审讯&lt;/h3&gt;&lt;p&gt;我们可以在&lt;a href="https://www.lesswrong.com/posts/yGaw4NqRha8hgx5ny/the-case-for-becoming-a-black-box-investigator-of-language"&gt;&lt;u&gt;人工智能采取不良行动的背景下对其进行人工智能心理学实验&lt;/u&gt;&lt;/a&gt;。我们可以尝试要求它解释其推理，同时指出我们已经抓住了它的事实。我们还可以看到它的行为如何随着输入的变化而变化。我们还可以尝试使用简单的激活补丁或激活添加实验。例如，我们可以尝试隔离导致人工智能试图逃跑的附加组件，即使这不是一个好机会。这些实验可能为了解人工智能的潜在动机和策略提供了一个有价值的窗口。&lt;/p&gt;&lt;h3&gt;三、在实践中研究对策&lt;/h3&gt;&lt;p&gt;我们（希望）会部署各种对策，旨在捕获人工智能或防止人工智能逃跑。一旦我们有了实际的例子，我们就可以检查哪些对策有效或无效。这可能允许更精确地制定对策，这些对策是使用比控制评估更不保守的假设来制定的。例如，我们可以查看&lt;a href="https://www.lesswrong.com/posts/WCj7WgFSLmyKaMwPR/coup-probes-catching-catastrophes-with-probes-trained-off"&gt;&lt;u&gt;政变探测器&lt;/u&gt;&lt;/a&gt;是否有效，然后相应地调整我们的使用方法。&lt;/p&gt;&lt;p&gt;有了足够多的例子（可能部分是通过扩散获得的），我们可以通过查看它们在我们的逃跑尝试数据集上的工作效果来有效地评估所提出的安全技术。 （由于“避免过度拟合和保持偏执”部分中讨论的原因，我们可能无法完全取代控制评估。）&lt;/p&gt;&lt;p&gt;除了更好的对策设计之外，我们或许还可以通过查看人工智能距离成功有多远来更好地估计安全性。如果许多对策独立地阻止了人工智能的成功，那么我们对这种情况的感觉可能会比只有一项脆弱的对策导致人工智能被抓住的情况要好一些。&lt;/p&gt;&lt;h3&gt; 4. 调整重点案例&lt;/h3&gt;&lt;p&gt;看到人工智能实际上在野外尝试逃跑可能会让我们了解人工智能可能如何尝试逃跑。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;针对性对策&lt;/strong&gt;：我们可以相应地将对抗性评估（和对抗性训练）集中在应对我们在野外看到的逃跑企图，并对这些情况采取额外的对策。我们或许还能够创建专门针对捕获与我们所见过的类似的逃跑企图的监控方法。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;利用次优性的技术&lt;/strong&gt;：控制评估假设人工智能选择最佳的可能策略来尝试发动政变。但实际上，人工智能可能远不是最理想的。如果我们观察到人工智能进行明显次优的政变尝试，我们可以制定额外的对策，特别关注使观察到的次优策略不太可能成功。 （但要小心人工智能会欺骗你。请参阅下面的“避免过度拟合和保持偏执”。）&lt;/p&gt;&lt;p&gt;&lt;strong&gt;检测难以引出的 AI 功能&lt;/strong&gt;：在存在诡计多端的 AI 的情况下，某些类型的功能很难评估，这些 AI 可能会由于域的稀疏性而破坏我们的测量（AI 是否知道任何单个零日漏洞） Linux 内核？）或多样性不足（AI 是否知道这一特定事实比它可能知道的大多数其他事实重要得多？）。如果我们看到真正的政变企图，我们就可以了解人工智能使用了哪些难以评估的功能。&lt;/p&gt;&lt;p&gt;虽然我们应该将精力转向人工智能凭经验瞄准的安全系统部分，但我们应该保留足够强大的对策，以便标准控制评估确定情况相当安全。&lt;/p&gt;&lt;h3&gt; 5. 通过突破逃脱更好的能力启发和评估：让人工智能认为他们目前正在尝试逃跑&lt;/h3&gt;&lt;p&gt;即使我们可以很好地检查它们的性能（由于探索&lt;a href="https://www.alignmentforum.org/posts/dBmfb76zx6wjPsBC7/when-can-we-trust-model-evaluations#2__Behavioral_RL_Fine_Tuning_Evaluations"&gt;&lt;u&gt;黑客&lt;/u&gt;&lt;/a&gt;&lt;a href="https://www.alignmentforum.org/posts/h7QETH7GMk9HcMnHH/the-no-sandbagging-on-checkable-tasks-hypothesis"&gt;&lt;u&gt;），&lt;/u&gt;&lt;/a&gt;诡计多端的人工智能有时也可能会持续地、故意地&lt;a href="https://www.alignmentforum.org/posts/dBmfb76zx6wjPsBC7/when-can-we-trust-model-evaluations#2__Behavioral_RL_Fine_Tuning_Evaluations"&gt;&lt;u&gt;表现得&lt;/u&gt;&lt;/a&gt;&lt;u&gt;很差&lt;/u&gt;（沙袋）。这适用于能力评估和实际从人工智能中获得有用的工作。&lt;/p&gt;&lt;p&gt;但当人工智能试图逃跑时，尽可能努力可能符合它们的最大利益。因此，如果存在一些可重复的信号或输入的其他属性，使人工智能认为这是一个很好的机会，那么我们就有可能利用该信号来让人工智能更加努力。 （特别是，我们也许能够利用人工智能产生的信号，这些信号旨在向人工智能的另一个实例表明它们应该合作进行某种尝试。我们还可以“表演”整个逃跑过程，假装好像让AI认为自己处于完全逃脱状态。）我们称让AI认为自己是逃跑尝试的一部分或者已经逃脱了&lt;strong&gt;逃脱破坏&lt;/strong&gt;。为了利用逃逸破坏，我们可以构建一个示例数据集，其中人工智能通过逃逸破坏尽可能努力地尝试，然后使用模仿学习来训练模型在不需要逃逸破坏的情况下努力尝试，最后训练模型进一步使用 RL。涉及人工智能尽可能努力地与强化学习相结合的高质量模仿数据应该足以排除对很容易检查性能的任务的故意沙袋。在难以探索但很容易检查的任务中，以这种方式突破转义来提高性能似乎是最合理的；如果任务不够容易检查，性能可能不可靠。&lt;/p&gt;&lt;p&gt;使用这样的逃逸破坏存在安全问题，但请注意，逃逸破坏的相关比较是使用您已经知道的阴谋模型。 （当然，我们也可以像之前讨论的那样关闭。）像这样使用转义破坏来初始化 RL 的主要安全问题是，模型可能不会非常可靠或连贯地对你进行策划，以这种方式通过突破逃逸来训练它，使它更加一致地错位。因此，我们也许应该只用它来微调可检查任务上的人工智能副本，除了低性能之外，安全问题最小（例如，为较弱的监控人工智能生成对抗性示例）。&lt;/p&gt;&lt;p&gt;人们也可以在生产中直接使用逃逸破坏的人工智能，但我们预计这种方法会存在多个重要问题并且不够可靠。 （适用于这种情况的一个安全问题是，现在预期的行为是，逃脱破坏的人工智能会以可疑的方式行事，并采取会导致仅在试图逃跑时才采取。这可能会破坏各种干预措施，例如，自动监控和人工监督可能很难确定哪些行为实际上是危险的，需要进一步审查。）因此，为了提高生产率而进行的突破可能应该只是用于收集更好的训练数据来初始化 RL。 （除了非常直接地使用逃逸破坏的AI之外，我们还可以训练AI模仿逃逸破坏的AI，但不需要逃逸破坏的提示；这样更方便，但仍然存在同样的问题。）&lt;/p&gt;&lt;p&gt;换句话说，我们可以通过让人工智能相信它正在执行接管行为之一来获得更好的能力。&lt;/p&gt;&lt;h3&gt; 6.小样本数ML&lt;/h3&gt;&lt;p&gt;在某些情况下，即使是少量的严重不当行为示例也可能允许您改进简单的模型，您可以使用该模型来检测未来的逃跑尝试，例如我们&lt;a href="https://www.lesswrong.com/posts/WCj7WgFSLmyKaMwPR/coup-probes-catching-catastrophes-with-probes-trained-off#Harmfulness_detection_at_AI_labs"&gt;&lt;u&gt;在此&lt;/u&gt;&lt;/a&gt;描述的。更一般地说，我们预计使用（大量）人工智能被发现的案例来训练（不受信任的）监控模型可能会非常有帮助。特别是，使用这些示例来进一步训练对策略的&lt;a href="https://www.lesswrong.com/posts/WCj7WgFSLmyKaMwPR/coup-probes-catching-catastrophes-with-probes-trained-off"&gt;&lt;u&gt;政变探测&lt;/u&gt;&lt;/a&gt;并进一步训练不可信监控器（不可信监控是我们在论文中探索的一种技术）似乎非常有用。&lt;/p&gt;&lt;p&gt;使用&lt;a href="https://www.alignmentforum.org/posts/9vwekjD6xyuePX7Zr/contrast-pairs-drive-the-empirical-performance-of-contrast"&gt;&lt;u&gt;对比&lt;/u&gt;&lt;/a&gt;&lt;a href="https://www.ai-transparency.org/"&gt;&lt;u&gt;对&lt;/u&gt;&lt;/a&gt;可能会提高微小样本计数体系中的样本效率。也就是说，它可能有助于构建一对人工智能输出，其中一个涉及逃跑尝试，另一个尽可能相似，但完全是无辜的。然后，我们可以分析这些输出之间激活的差异（对于我们用于监控的模型主体），并使用它来更有效地制作分类器。&lt;/p&gt;&lt;p&gt;训练任何采取非监控相关操作的模型的实际主体可能存在许多缺点（例如，模型可能决定启动不良操作而不是仅仅未能标记它们）。训练身体可能有意义，但我们不确定。缺点是：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;政策模型可以&lt;a href="https://www.lesswrong.com/posts/sbGau4QBwToYWEg4k/llms-sometimes-generate-purely-negatively-reinforced-text"&gt;&lt;u&gt;从反面例子中学习&lt;/u&gt;&lt;/a&gt;我们所采取的对策以及哪些方法可能会失败。&lt;/li&gt;&lt;li&gt;如果我们更新策略模型的主体，那么所有先前的尝试现在都将脱离策略，并且人工智能在这种情况下可能不再尝试错误的操作，即使它尝试以完全相同的速率逃脱（由于当噪声发生变化时） AI 进行尝试）。这看起来很烦人，特别是对于像在模型身体上训练探针这样的干预措施。&lt;/li&gt;&lt;li&gt;我们会抓到更少的企图逃跑的人，因为尝试的次数会更少。 （捕捉更多的尝试将是有价值的。）&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;除了监控模型之外，训练模型主体的主要优点是训练主体可以增加额外的可靠性。&lt;/p&gt;&lt;h3&gt; 7. 使用改进的监控重新审视之前的行动&lt;/h3&gt;&lt;p&gt;当我们使用少样本方法改进监控技术时，将这些改进的监控技术应用于人工智能之前采取的行动可能会很有用，希望能找到人工智能尝试逃跑但失败的早期案例。 （有关更多讨论，请参阅“逃跑尝试可能会失败而不被抓住”。）因此，改进的监控可能能够突出显示这些案例，以便进行更仔细的审查。&lt;/p&gt;&lt;p&gt;对之前看到的操作重新运行监控可能会在计算上花费一些成本，但我们认为有合理的方法可以在不使用太多计算的情况下获得大部分好处：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;我们可以保存要应用更新探针的激活。 （对于 LLM，如果我们为每 1k 个令牌保存一层和激活的令牌位置，相对于仅保存文本输入和输出，这可能只需要大约 25 倍的额外存储空间。）&lt;/li&gt;&lt;li&gt;我们只能重新分析最可疑案例的前 10%（根据可信模型，前 10%）。&lt;/li&gt;&lt;li&gt;如果我们重复捕获模型，我们可以以指数递减的速率进行重新分析，以确保我们只对 log(# catches) 进行多次重新分析。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt; 8. 提高对替代培训方案的理解和测试&lt;/h3&gt;&lt;p&gt;正如扩散部分所讨论的，对于人工智能来说，拥有强大的信号可能很重要，这些信号允许它们在试图逃跑时招募其他人工智能合作者。这甚至适用于接受不同策略或不同能力水平训练的人工智能之间的协作（否则，以这种方式多样化将完全对抗人工智能使用的策略）。因此，我们也许能够测试替代的训练策略，这些策略在事前看起来似乎可以减少阴谋的可能性，然后通过测试新的人工智能是否与我们捕获的尝试相一致来验证这些变化。&lt;/p&gt;&lt;p&gt;更一般地说，我们可以使用这些尝试作为验证来提高我们对阴谋的理解。即使捕获试图逃脱的模型会导致人工智能实验室关闭，并且只有在安全技术取得相当大的进步后才能重新启动，我们仍然希望通过查看这些新的人工智能是否与旧的尝试合作来验证这些新方法（我们需要以避免训练集泄漏，以确保新的人工智能不会预先了解正在发生的事情）。&lt;/p&gt;&lt;h2&gt; “先控制后少灾防”计划什么时候才能发挥作用？&lt;/h2&gt;&lt;p&gt;我们在这里讨论的方法很有希望，但根据&lt;a href="https://www.lesswrong.com/posts/d9FJHawgkiMSPjagR/ai-control-improving-safety-despite-intentional-subversion#Evaluation_methodology"&gt;&lt;u&gt;控制评估的&lt;/u&gt;&lt;/a&gt;评估，它们可能不会提高安全性。因此，我们认为，随着我们继续捕捉人工智能，安全论点应该部分转向依赖证据而不是控制评估。&lt;/p&gt;&lt;p&gt;假设我们从基于控制的安全方法开始，并且由于&lt;a href="https://www.lesswrong.com/posts/rCJQAkPTEypGjSJ8X/how-might-we-align-transformative-ai-if-it-s-developed-very#Magma_s_predicament"&gt;&lt;u&gt;不作为风险&lt;/u&gt;&lt;/a&gt;，即使我们确定我们的人工智能是一个阴谋者，我们也希望继续部署。因此，当我们反复捕捉人工智能时，我们计划逐步过渡到不同的安全案例。在什么情况下我们才能达到足够的安全性？&lt;/p&gt;&lt;p&gt;假设我们的控制方法有 99% 的机会捕获诡计多端的 AI 的任何给定尝试，然后在剩下的 1% 中 AI 成功逃脱。如果抓住AI后没有任何变化，那么心机重重的AI总共尝试100次，成功的几率为63%。这是不可接受的，因此仅靠自身控制是不够的。我们希望通过少数策略来避免这个问题，这些策略旨在降低每次捕获人工智能时的风险，以确保总风险足够低。&lt;/p&gt;&lt;p&gt;例如，想象一下，在之前的场景中，每当我们抓住人工智能时，诡计多端的人工智能成功而不被抓住的机会就会减半。那么，诡计多端的人工智能总共“只有”2% 的成功机会。在实践中，我们希望确保尝试总数不会太高（可能使用少样本方法），同时提高捕获未来尝试的机会。&lt;/p&gt;&lt;p&gt;请注意，如果人工智能采用的策略涉及相对频繁的尝试，那么我们很可能在初步测试中遇到这些尝试。 （例如，如果 AI 尝试超过 1,000 次，无法区分部署早期和部署后期，并且我们仔细监督部署前 0.25% 的所有示例，那么我们有 92% 的机会遇到尝试就在这 0.25% 之内（尽管当我们仔细监督时，我们可能仍然无法注意到它）。因此，人工智能可能需要做出艰难的权衡。&lt;/p&gt;&lt;p&gt;事实上，我们很难对我们的评估充满信心。所以现实情况可能是这样的：&lt;/p&gt;&lt;p&gt;在预训练之前，我们要训练的模型有 25% 的机会最终会进行诡计。我们进行了控制评估。在这些评估中，我们在 99.5% 的时间内捕获了&lt;a href="https://www.lesswrong.com/posts/d9FJHawgkiMSPjagR/ai-control-improving-safety-despite-intentional-subversion#We_explicitly_measure_how_robust_techniques_are_against_scheming_AIs"&gt;&lt;u&gt;红队模型&lt;/u&gt;&lt;/a&gt;，然后才成功逃脱。&lt;/p&gt;&lt;p&gt;更具体地说，假设该评估的最终状态分布如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt; 30% 的情况是，模型在部署之前就被捕获。&lt;/li&gt;&lt;li&gt; 60% 的情况下，模型会在部署过程中被捕获。&lt;/li&gt;&lt;li&gt; 9.5% 的情况下，模型从未被抓住，但从未成功逃脱。&lt;/li&gt;&lt;li&gt; 0.5% 的情况下，模型会逃脱。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;由于对评估的各种担忧（有关可能导致安全性不确定性的评估困难的讨论，请参阅&lt;a href="https://arxiv.org/abs/2312.06942"&gt;&lt;u&gt;我们论文&lt;/u&gt;&lt;/a&gt;“5.1.1 在实践中使用控制评估的困难”中的讨论小节），我们实际的综合考虑概率在我们抓住它之前以阴谋为条件逃脱的 AI 大约有 5%。我们综合考虑，以阴谋为条件的逃跑概率（在任何捕获之前或之后）约为 10%；这考虑到了我们可能对对策进行的改进，以及如果我们的对策看起来非常试探的话，在捕获人工智能后关闭的可能性。总体而言，P(escape) = P(escape|scheming) P(scheming) = 10% * 25% = 2.5%（为了简单起见，忽略了由于非阴谋错位而导致的逃逸）。&lt;/p&gt;&lt;p&gt;我们当场抓住了人工智能。我们分析了逃跑企图，看来我们的对策在这种情况下显然是足够的（多条防线独立地阻止了逃跑）。根据这个例子，红队在探索可能的逃跑策略方面似乎并没有明显的做得不够充分的地方。通过这一分析，我们对初步评估更有信心；如果我们继续采取相同的对策，我们新的综合考虑，人工智能在再次捕获它之前逃脱的可能性会下降到 2%。&lt;/p&gt;&lt;p&gt;但我们也能够大幅增强对策，并大幅增加在训练中使用的逃跑尝试。有了这些增强的对策，我们在再次捕获 AI 之前的新的所有考虑的逃跑概率已降至 1%。并且，假设我们的总体逃脱概率（在任何未来捕获之前或之后）是 2%。&lt;/p&gt;&lt;p&gt;虽然我们积极更新了初步评估并改进了对策，但我们也将模型有阴谋的可能性从 25% 更新为 100%。所以我们新的逃跑概率只下降了20%；我们的新估计是 P(逃跑) = P(逃跑|阴谋) P(阴谋) = 2% * 100% = 2%。 （我们预计，正如这里所发生的那样，在发现逃跑企图后，你对阴谋模型的评估风险会下降而不是上升，因为 P(逃跑|阴谋) 的减少超过了 P(阴谋) 的增加。 ）&lt;/p&gt;&lt;p&gt;在这个人工智能生命周期的剩余时间里，我们最终再次抓住了人工智能试图逃跑的两次，并使用几次射击技术进一步改进了对策。我们最终还训练了实际的策略，迫使它在尝试逃跑时更加保守，并（可能）让它不再是一个阴谋家。我们不会进一步捕获人工智能，而是完成其余的部署，此时该模型已被我们认为可靠对齐的新模型所取代，或者我们已经成功地使用人工智能来降低风险由不那么细心的人工智能开发人员强加的，所以我们不需要再使用这个人工智能了。&lt;/p&gt;&lt;p&gt;总之，只要我们能够在每次被捕获的尝试中显着提高安全性，就应该可以通过“捕获然后使用少量射击”灾难预防计划来确保安全。安全性的提高可以通过增加我们捕获未来尝试的可能性或训练人工智能不进行尝试来体现。也可以通过仅使用控制评估并通过我们的观察限制尝试次数（例如，到目前为止在部署中看到足够低的尝试率）来确保安全，但是未来分布变化的可能性使得很难自信地限制尝试次数。基于过去观察的尝试次数。&lt;/p&gt;&lt;p&gt;我们在这里讲述的故事是一个乐观的故事，在你发现逃跑企图后，证据表明你可以安全地继续部署。当然，也可能是，当您捕捉到逃跑尝试时，您意识到自己距离逃跑尝试成功太近了，因此您可以自信地决定需要关闭模型。&lt;/p&gt;&lt;h2&gt;避免过度拟合并保持偏执&lt;/h2&gt;&lt;p&gt;我们应该小心，避免受到我们所发现的尝试的过度影响。特别是，诡计多端的人工智能可能会故意尝试执行不力的逃跑尝试，给我们一种错误的安全感，或者将资源转移到效率较低的保障措施上。很难确信我们看到的是真正的尝试而不是虚假的尝试。更一般地说，我们需要避免正常的过度拟合。然而，为了让人工智能真正取得不好的结果，它必须在某个时候进行认真的尝试；以捕获为重点的方法的希望是捕获这些真正认真的尝试。由于假政变的可能性，我们可能不应该完全废除控制评估；我们需要确保我们有足够高的机会抓住第一次认真的尝试。&lt;/p&gt;&lt;p&gt;如果人工智能实验室正确地考虑了虚假尝试的可能性，那么故意进行虚假尝试对于人工智能来说不太可能是一个好的策略。&lt;/p&gt;&lt;h1&gt;结论&lt;/h1&gt;&lt;p&gt;如果你当场发现你的人工智能故意试图造成灾难性的不良后果，你的情况就会发生很大的变化。因此，我们认为研究人员应该主要关注安全计划，以确保如果模型进行了策划，则不会在被抓住之前逃脱。&lt;/p&gt;&lt;p&gt;如果模型计划减轻逃脱的风险，似乎很可能会至关重要地依靠实验室明智地响应逃避尝试。因此，实验室和政策制定者应该考虑如何处理逃脱尝试并尝试为它们做准备。&lt;/p&gt;&lt;p&gt;我们认为，很少有预防灾难的灾难似乎是一个相当重要的研究问题，一些安全的人可能应该为此开发技术，因为如果我们需要使用这些技术，我们可能会承受很大的时间压力，并且提前开发这些技术会很好。&lt;/p&gt;&lt;h1&gt;附录：用e [实用程序|心计]&lt;/h1&gt;&lt;p&gt;我们经常考虑在&lt;a href="https://www.alignmentforum.org/posts/d9FJHawgkiMSPjagR/ai-control-improving-safety-despite-intentional-subversion"&gt;&lt;u&gt;AI控制&lt;/u&gt;&lt;/a&gt;的背景下捕捉。考虑控制评估的一种生产方法是，它们旨在衡量e [实用程序|示意]：如果我们有一个示意的AI，结果的预期优势。从这个角度来看，使用控制评估时针对安全或捕获的论点有两个部分：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;捕捉可以实现强大的非控制技术干预措施&lt;/strong&gt;：捕获AI后，我们可以访问看起来非常有用但无法控制的技术，并且无法控制的方法无法控制。 “少数灾难预防”部分讨论了几种非控制技术干预措施，这些干预措施在捕获AI后可能会成为可能。多次捕获AI后，非控制方法变得特别吸引人。鉴于捕获AI可能允许使用相当强大的非控制技术方法，我们应该考虑将AI捕获为控制评估的安全终点（这完全等同于针对安全性或捕捉），或者至少尝试大致模型在捕获AI之后，情况将如何变化，而不仅仅是继续进行控制评估。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;除了直接提高安全性之外，捕获AI是一个很好的结果&lt;/strong&gt;，我们应该将其考虑到我们的评估中。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;虽然捕捉似乎很有价值，但单个捕获不足以确保安全。&lt;/p&gt;&lt;h1&gt;附录：定义捕捉&lt;/h1&gt;&lt;p&gt;为了这篇文章，我们主要是在谈论灾难性的结果，例如模型&lt;a href="https://aligned.substack.com/p/self-exfiltration"&gt;&lt;u&gt;消除其权重&lt;/u&gt;&lt;/a&gt;，这可能仅涉及少量动作。还有其他威胁模型与策略相关，例如，模型巧妙地破坏了他们所做的工作，使他们的工作无用，或者随着时间的推移而巧妙地获得了影响。为了这篇文章，我们忽略了这些威胁模型，但是我们认为我们可以采用类似的策略来处理它们。&lt;/p&gt;&lt;p&gt;我们将“捕获”模型定义为：您找到输入，您可以证明您的模型采取灾难性的行动，其概率比该模型没有故意寻找机会采取此类操作的机会要高。当我们说“您的模型”时，我们的意思是您已部署（或计划部署）的实际模型，而不是以类似方式培训的其他模型。在几乎没有造成的灾难预防的大多数方面，捕捉您计划部署的实际模型至关重要，但是捕获不同但类似的模型对于其他用例就足够了（捕捉到陷入困境的，易于清晰的证据，一般的科学理解）。&lt;/p&gt;&lt;p&gt;为了确保您可以区分这些故意的灾难性行动，您需要做到这一点，以便您的模型永远不会逃脱而不明确采取非事故的不良行为。我们认为您可以做到这一点，以便您的模型不会逃脱而不必做一些明确的故意坏事。 （人类可能仍然很难注意到并理解这种不良行为；声称不良行为显然必须是故意的。 （例如，随着时间的推移巧妙地获得影响），比任何单一的动作显然是非事故的，但原则上可能仍然有可能确定这些行动在总体上并不偶然。&lt;/p&gt;&lt;p&gt;即使您捕捉到这样的行动明显是故意不好的，您实际上也无法确定该模型是在策划的，而不是出于某些其他问题的原因而故意采取不好的行动。但是，即使模型没有策划，该模型可能会故意做出不良动作的其他原因也很令人担忧。另外，当您从几场预防灾难部分中列出的角度考虑它时，从捕获更多案例中，您将永远不会变得更糟。&lt;/p&gt;&lt;p&gt;要算作成功的捕获，您需要对捕获量充满信心。 （这是为了使陷入困境的前面的前进不太昂贵，清​​晰的证据实际上是有效的，因此您可以根据捕获量对各种实验的结论充满信心。我们仍然可以仅仅研究看似粗略的案例，但这将产生大幅度的价值。）获得足够的信心可能会涉及大量调查（这可以在AI实验室暂时关闭时进行）。我们可以想象，经过一系列较小的调查升级后，进行了这项大规模调查。 “预防灾难”中列出的动作也可能有助于验证这确实是一种造成灾难性结果的尝试。&lt;/p&gt;&lt;h1&gt;附录：不会捕获但要更新P（scheming）的技术&lt;/h1&gt;&lt;p&gt;我们在这里定义“捕获”的方式，不包括可能会更新模型是否正在策划的技术，但不涉及该模型以极高的可疑可能性采取的特定操作：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;模型生物：&lt;/strong&gt; &lt;a href="https://www.alignmentforum.org/posts/ChDH335ckdvpxXaXX/model-organisms-of-misalignment-the-case-for-a-new-pillar-of-1"&gt;&lt;u&gt;以策划为重点的模型有机体实验&lt;/u&gt;&lt;/a&gt;涉及制作模型，以表现出示意模型的某些特征。看起来很棒。乐观地，这些可能使我们具有非常经过校准的P（scheming）感以及测试对策的能力。但是，更有可能是这些结果令人困惑，并且不产生共识。同样，如果您构建模型生物体的模型已经构成了策划，则模型生物实验似乎很容易受到破坏的影响。而且，您不能使用模型生物实验来进行下面列出的几次灾难预防干预措施，尽管从理论上讲，模型有机体可以为灾难性未对准和改善一般科学理解提供清晰的证据。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;间接证据：&lt;/strong&gt;例如，模型心理学或机械性解释性，不允许您产生模型采取不良行动的投入示例。我们可能可以使用模型心理学来了解我们的模型具有邪恶意图而无需构建榜样。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;怪异的对抗性示例&lt;/strong&gt;：我们可能能够通过搜索触发非常不受欢迎的行为，特别&lt;a href="https://arxiv.org/abs/2307.15043"&gt;&lt;u&gt;是涉及非常不可能输入的对抗性示例来找到奇怪的对抗性示例&lt;/u&gt;&lt;/a&gt;。也可能会发现&lt;a href="https://www.lesswrong.com/posts/atBQ3NHyqnBadrsGP/latent-adversarial-training"&gt;&lt;u&gt;对模型内部的扰动，从而导致非常不希望的行为&lt;/u&gt;&lt;/a&gt;。我们不知道如何评估这些对抗性示例是在潜在的策略中真正获得的，还是只是欺骗模型进行策划。在某些情况下，可能有可能获得更多信息或有明确的判断（例如，如果我们仅使用少量搜索来查找具体的对抗输入，并且根据对我们输入分布的LLM培训的LLM，则输入很可能）。 。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt; （所有这些可能都是有前途的研究指示，它们只是不会从相关意义上捕获AI。）&lt;/p&gt;&lt;h1&gt;附录：逃生尝试可能会失败而不会被抓住&lt;/h1&gt;&lt;p&gt;逃脱尝试之类的行动可以成功，被捕获（失败）或失败而不会被捕获。我们非常希望抓住AI仅仅导致AI失败，因为捕获AI对于各种目的都是有用的。也就是说，导致不良行为失败仍然可以通过避免不良结果直到部署结束或我们实际上抓住AI来实质性地帮助安全。在我们的论文中，我们利用了不受信任的灵感等干预措施，这些干预措施并不能直接帮助捕获AI，但会降低其在后门成功的可能性。&lt;/p&gt;&lt;p&gt;为什么逃脱的尝试可能不会被失败而结束？&lt;/p&gt;&lt;p&gt;逃生尝试可能需要多个AI之间的协调，并访问一些不同的信息。链条中的某些AI可能成功地进行了逃避监控，但是由于AI后来的AI而没有继续尝试（也没有报告逃生尝试；如果它报告了这一尝试试图）。后来的AI可能不会继续尝试，因为它没有注意到较早的AI开始了逃生尝试，或者确实注意到逃生尝试，但由于仅可用的信息而认为这实际上并不是一个好机会。&lt;/p&gt;&lt;p&gt;逃生尝试可能会失败而不会被抓住的另一个原因是，我们的对策足以阻止该动作，但不会升级到更高的审计水平。例如，假设我们根据受信任的监视重新采样所有可疑阈值的动作，同时仅建议昂贵的审计措施，如果它们高于较高的阈值。 （这就是我们在&lt;a href="https://www.lesswrong.com/posts/d9FJHawgkiMSPjagR/ai-control-improving-safety-despite-intentional-subversion"&gt;&lt;u&gt;AI控制纸&lt;/u&gt;&lt;/a&gt;中所说的“混合策略”。）然后，我们最终可能会通过重新采样而停止逃生尝试，而不会升级链条审核。如果人类审计发现动作有些可疑并且值得以一种不太可疑的方式做，但不值得进行更详细的评论和/或暂停使用强大模型的使用情况，可能会发生类似的事情。&lt;/p&gt;&lt;h1&gt;附录：什么时候会抓住AI？&lt;/h1&gt;&lt;p&gt;在以下情况下可能会捕获逃生尝试：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;在适当部署之前。这是最安全的结果。&lt;/li&gt;&lt;li&gt;在部署期间，实际输入。&lt;/li&gt;&lt;li&gt;在部署期间，在合成输入中。这可能会发生，因为在输入中，对您的模型进行大量对抗性评估可能是有意义的，选择了一些子集来转换为假逃生机会。这种合成输入的优点是，修改现有输入比从头开始构造合成输入要便宜。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;为什么我们认为我们可以在部署期间抓住，但以前却没有抓住？&lt;/p&gt;&lt;ul&gt;&lt;li&gt;部署更大 - 我们可能会在部署期间比部署之前更多的输入运行模型。因此，在极少数情况下仅在部署之前就不会触发的模型策略，这些策略仅在极少数情况下表现出色。&lt;/li&gt;&lt;li&gt;很可能（在没有AI实验室的积极对策的情况下），该模型将相当良好地区分部署和非部署数据。例如，将要求执行任务的分布可能会在部署过程中发生变化。如果它可以区分它的设置，则可以尝试仅在部署输入上表现出色。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;总体而言，我们不确定我们在部署之前还是在部署期间是否更有可能捕获模型。&lt;/p&gt;&lt;ol class="footnotes"&gt;&lt;li class="footnote-item" id="fnlokat6zwmzf"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnreflokat6zwmzf"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;虽然我们将讨论捕捉策划，但所有这些推理都适用于捕获任何类型的灾难性未对准。确切地说，如果没有保障措施，这些行动的上游原因没有策划，我们可能会捕捉到灾难性的行动。例如，这可能是奖励寻求行为或玩训练游戏的概括（如“ &lt;a href="https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to"&gt;没有特定对策&lt;/a&gt;”中的描述）。也就是说，还有其他合理的好方法来测试非灾难性的灾难性未对准，因此捕捉特别是为策划而言。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ol&gt;&lt;br /&gt;&lt;br /&gt;&lt;a href="https://www.lesswrong.com/posts/i2nmBfCXnadeGmhzW/catching-ais-red-handed#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Fri, 05 Jan 2024 17:43:10 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/i2nmBfCXnadeGmhzW/catching-ais-red-handed</guid></item><item><title>AI 影响调查：2023 年 12 月版</title><link>https://www.lesswrong.com/posts/NfPxAp5uwgZugwovY/ai-impacts-survey-december-2023-edition</link><description>发布于 2024 年 1 月 5 日下午 2:40（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt; Katja Grace 和 AI Impact &lt;a href="https://aiimpacts.org/wp-content/uploads/2023/04/Thousands_of_AI_authors_on_the_future_of_AI.pdf" rel="noreferrer noopener" target="_blank"&gt;对数千名研究人员就各种问题进行了调查&lt;/a&gt;，继 2022 年的一项类似调查以及 2016 年的一项调查之后。&lt;/p&gt;&lt;p&gt;我鼓励打开原件以获得更好的图表可读性以及上下文和附加信息。我会介绍其中一些，但还有很多。&lt;/p&gt;&lt;span id="more-23654"&gt;&lt;/span&gt;&lt;h4&gt;一项充满矛盾的大型调查&lt;/h4&gt;&lt;p&gt;这是摘要，总结了很多要点：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;在同类最大规模的调查中，我们调查了 2,778 名在顶级人工智能 (AI) 场所发表过论文的研究人员，询问他们对人工智能进展速度以及先进人工智能系统的性质和影响的预测。&lt;/p&gt;&lt;p&gt;总体预测显示，到 2028 年，人工智能系统至少有 50% 的机会实现多个里程碑，包括从头开始自主构建支付处理站点、创作一首与流行音乐家的新歌难以区分的歌曲，以及自主下载和微调大语言模型。&lt;/p&gt;&lt;p&gt;如果科学继续不受干扰地发展，到 2027 年，在所有可能的任务中，无辅助机器胜过人类的机会估计为 10%，到 2047 年为 50%。后者的估计比我们一年前进行的一项类似调查中得出的结果早了 13 年[格蕾丝等人，2022]。然而，预计到 2037 年，所有人类职业实现完全自动化的可能性将达到 10%，最迟到 2116 年将达到 50%（相比之下，2022 年的调查为 2164 年）。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;正如我稍后将详细阐述的那样，这种对比毫无意义。我们不会让机器在 2047 年的每项任务上都胜过人类，然后在 2116 年才能完全实现人类职业的自动化。这没有任何意义。&lt;/p&gt;&lt;p&gt;我认为2047年的时间表很高，但在合理范围内。研究人员可能会更清楚地思考问题的这一方面。我们应该主要按照他们的想法使用这个答案。我们应该主要将 2116 的答案视为没有意义，除非将其与使用类似措辞的过去和未来的估计进行比较。&lt;/p&gt;&lt;p&gt;无论如何，人工智能进展的预期速度在一年内已​​经加快了很多。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;大多数受访者对人工智能进步的长期价值表示很大的不确定性：虽然 68.3% 的人认为超人类人工智能的好结果更有可能比坏结果更有可能，但在这些净乐观主义者中，48% 的人认为至少有 5% 的可能性会出现极其糟糕的结果，例如人类的人工智能。 59% 的净悲观主义者认为 5% 或更多的人认为结果非常好。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;具有高度不确定性的分布是明智的。这与预期中等或中性的结果形成鲜明对比，后者毫无意义。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; 37.8% 至 51.4% 的受访者认为，先进人工智能至少有 10% 的机会导致人类灭绝一样糟糕的结果。超过一半的人表示，对六种不同的人工智能相关情景有必要“严重”或“极端”担忧，包括虚假信息的传播、专制人口控制和不平等加剧。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;我们再次看到看似矛盾的东西。如果我认为人工智能导致人类灭绝的可能性有 10%，我就会对此感到“极度”担忧。我就是这么做的。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;对于更快还是更慢的人工智能进步是否对人类的未来更好存在分歧。然而，人们普遍认为，旨在最大限度地减少人工智能系统潜在风险的研究应该更加优先。&lt;/p&gt;&lt;p&gt;我们这样定义高级机器智能（HLMI）：&lt;/p&gt;&lt;p&gt;当独立机器能够比人类工人更好、更便宜地完成每项任务时，就实现了高级机器智能 (HLMI)。忽略人类本质上有利的任务方面，例如被接受为陪审团成员。考虑可行性，而不是采用。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;这是一个非常高的标准。 “每项任务”与许多或大多数任务有很大不同，尤其是与更好和更便宜的任务相结合。另请注意，这并不全是“智力”任务。这都是任务，期间。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;我们要求做出预测，假设“人类科学活动继续进行，没有重大负面干扰。”我们通过拟合伽玛分布来汇总结果（n=1,714），就像 3.1 中的单个任务预测一样。&lt;/p&gt;&lt;p&gt;在 2022 年和 2023 年，受访者对 HLMI 多久能够实现提出了广泛的预测（图 3）。&lt;/p&gt;&lt;p&gt; 2023 年的总体预测预测，到 2047 年，发生 HLMI 的可能性为 50%，比 2022 年调查中的 2060 年下降了 13 年。相比之下，在 2016 年和 2022 年调查之间的六年中，预期日期仅提前一年，从 2061 年推迟到 2060 年。&lt;/p&gt;&lt;/blockquote&gt;&lt;h4&gt;他们破坏了时间线的稳定&lt;/h4&gt;&lt;p&gt;这是一个潜在的未来世界，到 2047 年，人工智能可以“比人类更好、更便宜地完成所有人类任务”。&lt;/p&gt;&lt;p&gt;之后会发生什么？他们认为 2048 年会是什么样子？ 2057?&lt;/p&gt;&lt;p&gt;这是整个练习中最奇怪的部分。&lt;/p&gt;&lt;p&gt;从这个调查来看，他们似乎选择不去想太多？是出于某种“事情会正常并按正常速度发展”的感觉吗？&lt;/p&gt;&lt;figure class="wp-block-image"&gt; &lt;a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc985c985-3f05-4009-ab37-8d7f7a9470b0_1306x1590.png" rel="noreferrer noopener" target="_blank"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/NfPxAp5uwgZugwovY/czy7to2nm1hmeu6kdacp" /&gt;&lt;/a&gt;&lt;/figure&gt;&lt;p&gt; [请注意，我最初误读了第二张图表，认为其比例与第一张图表相同。第二个图表是第一个图表左侧的扩展。]&lt;/p&gt;&lt;p&gt;他们是否真的期望我们拥有人工智能，能够在所有事情上比我们做得更好，然后有效地坐拥几代人？&lt;/p&gt;&lt;p&gt;包括人类两代人，人工智能正在做人工智能研究，比人类更好、更便宜？当这种情况发生时，世界会保持那种正常和受控吗？&lt;/p&gt;&lt;p&gt;下面的 FOAL 是人类劳动的完全自动化，HLMI 是高级机器智能。&lt;/p&gt;&lt;figure class="wp-block-image"&gt; &lt;a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ec38e15-c75d-4a03-bacd-93836b28fc8f_1033x733.png" rel="noreferrer noopener" target="_blank"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/NfPxAp5uwgZugwovY/pbgsao1p91wywbxpndcv" /&gt;&lt;/a&gt;&lt;/figure&gt;&lt;figure class="wp-block-image"&gt; &lt;a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9632c44e-94ed-408c-b0b2-3002de5a9b31_1135x759.png" rel="noreferrer noopener" target="_blank"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/NfPxAp5uwgZugwovY/mk1fbgbthjpndiyez83p" /&gt;&lt;/a&gt;&lt;/figure&gt;&lt;p&gt;论文作者注意到他们也很困惑。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;由于职业可能自然地被理解为复杂的任务、由任务组成或与其中一项密切相关，因此实现 HLMI 似乎要么意味着已经实现了FAOL，要么表明接近实现。我们不知道是什么造成了预测的这种差距。就 HLMI 和FAOL 提及同一事件而言，对它们到达时间的预测差异似乎是一种框架效应。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;如果这种差异的原因纯粹是“我们希望人类阻止人工智能完全接管至少一项雇用至少一个人的工作，或者至少我们希望某个人在某个地方继续能够执行一些劳动”，那么这可能是解释其中的差异。我很想澄清一些问题。&lt;/p&gt;&lt;p&gt;这似乎也是关于人工智能后果的基本常识测试：如果人工智能完全处于“你能做的任何事我能做得更好”模式，这会是技术进步的一个数量级的加速吗？&lt;/p&gt;&lt;p&gt;我的意思是，是的，显然？我认为左边这张图的描述不小心颠倒了，但即便如此，这似乎很多人都没有思考清楚？你可以怀疑 HLMI 是否会到来，但如果我们真的拥有它，后果似乎很明显。除非人们认为我们有智慧和能力根本不使用它？&lt;/p&gt;&lt;figure class="wp-block-image"&gt; &lt;a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa831dd64-2993-45be-8b4e-42e0cdf3374a_1423x783.png" rel="noreferrer noopener" target="_blank"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/NfPxAp5uwgZugwovY/inc9up9q36secvoj6yrv" /&gt;&lt;/a&gt;&lt;/figure&gt;&lt;p&gt;或者：&lt;/p&gt;&lt;figure class="wp-block-image"&gt; &lt;a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea6298c0-bc6a-4e01-a9aa-48809e7becc2_1428x429.png" rel="noreferrer noopener" target="_blank"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/NfPxAp5uwgZugwovY/kwsctlt3u1gb3qjtzlpa" /&gt;&lt;/a&gt;&lt;/figure&gt;&lt;p&gt;显而易见的是，在 HLMI 零 (0) 年后，人工智能远远优于人类。如果你能使用与我截然不同的架构来比我做得更好，那么你不仅好一点点。当然，两年后，10% 的可能性在这里就变得毫无意义了。&lt;/p&gt;&lt;p&gt;更荒谬的是，这些是到 2043 年的概率。这甚至不接近一组一致的概率分布。考虑其中哪些或哪些组合暗示其他哪些。&lt;/p&gt;&lt;figure class="wp-block-image"&gt; &lt;a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07ee1dd5-251b-472b-af2f-a2c70e518965_1045x732.png" rel="noreferrer noopener" target="_blank"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/NfPxAp5uwgZugwovY/cj5fcsrk2psafvo6f5lq" /&gt;&lt;/a&gt;&lt;/figure&gt;&lt;p&gt;当然，我认为其中一些列出的可能事件并不是那么不确定，例如“有时会欺骗人类以实现并非人类有意为之的目标”。我的意思是，到 2043 年这怎么可能不会发生呢？&lt;/p&gt;&lt;h4&gt;什么，我担心？&lt;/h4&gt;&lt;p&gt;这是人们所关心的，一张极其令人担忧的图表。&lt;/p&gt;&lt;figure class="wp-block-image"&gt; &lt;a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc087d63c-79de-43b8-ab0d-6e5f463d3fdb_1350x790.png" rel="noreferrer noopener" target="_blank"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/NfPxAp5uwgZugwovY/stuqnwucrtobj40lwlpl" /&gt;&lt;/a&gt;&lt;/figure&gt;&lt;p&gt;忧虑全都用在了错误的地方。最重要的担忧是……深度伪造？这些并不是我们应该担心的事情的特别核心例子。&lt;/p&gt;&lt;p&gt;在所有这些问题中，最大的问题可能是“其他”，因为在其他保护伞下还剩下多少不完整的东西。我认为“目标错误的人工智能会变得强大”和“人工智能的目标设定错误”涵盖了很多内容，即使我会以不同的方式描述核心事件。&lt;/p&gt;&lt;p&gt;人们也可以不将其视为对可能发生的事情的衡量，而是对“有关”的事情的衡量。这意味着人们对社会原因表示担忧，而不是因为人工智能最大的担忧是……深度造假和公众舆论操纵，人工智能有望比人类更好、更便宜地完成所有工作。我的意思是，认真的吗？&lt;/p&gt;&lt;p&gt;另一种选择是人们在矛盾的框架中思考。在某一帧中，他们意识到 HLMI 级别的人工智能即将到来。在另一个框架中，他们问“有什么关系？”并且只考虑普通的人工智能。&lt;/p&gt;&lt;h4&gt;最大的问题&lt;/h4&gt;&lt;p&gt;从人工智能的最终后果来看，包括潜在的生存风险，意义建构并没有变得更好。&lt;/p&gt;&lt;figure class="wp-block-image"&gt; &lt;a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e5c6f5b-fff6-4b8d-9fc5-17de4f80ec2d_1447x1158.png" rel="noreferrer noopener" target="_blank"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/NfPxAp5uwgZugwovY/bb3g8rzvglw51znq4udt" /&gt;&lt;/a&gt;&lt;/figure&gt;&lt;p&gt; HLMI 是一种能够比人类做得更好的人工智能，在它的所有潜在后果中，“中立”并不属于其中。这完全没有意义。这是我们告诉自己的科幻故事，这样我们就可以继续讲述同样相关的人类故事。它可能会进展顺利，这可能是一切有价值的事情的结束，但它绝对不会是无聊的。&lt;/p&gt;&lt;p&gt;如果你告诉我它“变得中立”，那么我可以想出一个故事，其中某人或某个团体创建了 HLMI/ASI，然后决定使用它来确保没有其他人构建一个，否则就完全不管事情，因为他们认为这样做其他任何事情都会更糟。我的意思是，考虑到我见过的其他计划，这绝对是一个高于平均水平的计划，但事实并非如此。&lt;/p&gt;&lt;p&gt;那么我们如何看待这些 p(doom) 数字呢？让我们放大一下，根据问题措辞，有人回答 10% 或更高的概率：&lt;/p&gt;&lt;figure class="wp-block-image"&gt; &lt;a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a4efdca-79a7-47c6-8b26-0d29654dda94_1423x832.png" rel="noreferrer noopener" target="_blank"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/NfPxAp5uwgZugwovY/svpibhq2zhguvxunu5sb" /&gt;&lt;/a&gt;&lt;/figure&gt;&lt;p&gt;这里的 p(doom) 数字是直接矛盾的。我们将会得到一些奇怪的话题。&lt;/p&gt;&lt;p&gt;在图 12 中，所有“极差”结果的平均值为 9%，中位数为 5%。&lt;/p&gt;&lt;p&gt;在图 13 的问题 3 中，人类灭绝或严重丧失权力的平均概率为 14.4%，或者从长远来看，概率为 16.2%，中位数仍然为 5%。&lt;/p&gt;&lt;p&gt;然后，在问题 2 中，他们问“人类无法控制未来的先进人工智能系统导致人类灭绝或类似的永久性和严重剥夺权力的可能性有多大”，他们的平均值为 19.4%，中位数为 10%。&lt;/p&gt;&lt;p&gt;你说，她更有可能是一名图书管理员和女权主义者，而不仅仅是一名图书管理员。&lt;/p&gt;&lt;p&gt;这主要是一个经典的连词谬误吗？指出我们可能会失去控制的框架效应，要么使人们产生偏见，要么让他们不再考虑可能会发生什么？还有别的事吗？&lt;/p&gt;&lt;p&gt;当您看到 5% 与 10% 时，这并没有看起来那么大。所发生的情况很大程度上是几乎没有人说的，例如 7%。因此，当 47% 的回答为 10% 或更高时，中位数为 5%，然后在 51% 时，它会跃升至勉强达到 10%。 5% 和 10% 都有误导性，这里的“真实”中位数更像是 8%。&lt;/p&gt;&lt;p&gt;我在 Twitter 上做了一项调查，询问如何最好地描述调查结果。&lt;/p&gt;&lt;figure class="wp-block-image"&gt; &lt;a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd12a47d-a3d5-4072-8dd3-4f72ec9a9f90_894x447.png" rel="noreferrer noopener" target="_blank"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/NfPxAp5uwgZugwovY/sle6p43rg2mcq8cntpwn" /&gt;&lt;/a&gt;&lt;/figure&gt;&lt;p&gt;我认为这个民意调查是正确的。 10% 比 5% 更接近准确，但“中位数为 5%-10%，具体取决于框架，平均值为 9%-19%”是报告此结果的正确方法。如果有人想说“研究人员说厄运的可能性是十分之一”，那么这有点快速和宽松，我会避免这么说，但我会在有限的不信任范围内考虑它。&lt;/p&gt;&lt;p&gt;重要的是：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;研究人员普遍承认，人工智能带来的生存风险确实令人担忧。&lt;/li&gt;&lt;li&gt;研究人员认为这种风险足够高，以至于我们应该愿意进行大量投资和牺牲来减轻这种风险。&lt;/li&gt;&lt;li&gt;然而，研究人员并不认为此类风险是默认结果。&lt;/li&gt;&lt;li&gt;研究人员对于这种风险的严重程度存在强烈分歧。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;最重要的是，存在风险是该领域高度主流的关注点。当调查询问时，它们在公众中也非常主流。&lt;/p&gt;&lt;p&gt;任何试图将此类信念框定为边缘立场的媒体报道或其他言论&lt;a href="https://twitter.com/NateSilver538/status/1742979005626867722" rel="noreferrer noopener" target="_blank"&gt;要么没有做足功课，要么在对你撒谎&lt;/a&gt;。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; Nate Silver：这对于人工智能风险来说非常有趣。我认为如果其他领域多尝试对专家意见进行科学调查就好了。 （披露：我在这项调查中做了非常少量的无偿咨询。）&lt;/p&gt;&lt;p&gt;事实上，广泛的从业者对人工智能安全风险（尽管因人而异）和人工智能时间表的加快感到担忧，这一点意义重大。您仍然会偶尔看到媒体报道将这些视为边缘职位。但他们不是。&lt;/p&gt;&lt;/blockquote&gt;&lt;h4&gt;不那么快与不那么快&lt;/h4&gt;&lt;p&gt;尽管有这些关于潜在厄运的预测，但对现在放慢速度的支持如果有一点点负面的话，即使纯粹是从人类的未来角度来看也是如此。&lt;/p&gt;&lt;figure class="wp-block-image"&gt; &lt;a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ceb9351-0c1c-42ed-8dd0-b154d9ea3262_1435x430.png" rel="noreferrer noopener" target="_blank"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/NfPxAp5uwgZugwovY/pqbg6vze17zdnybdeqyc" /&gt;&lt;/a&gt;&lt;/figure&gt;&lt;p&gt;如果（且仅当）你购买 AGI 很远的话，这实际上是完全有意义的。如果 HLMI 仅计划于 2047 年进行，那么从 2024 年到 2029 年放慢速度听起来并不是一个很棒的策略。如果我被告知当前的步伐意味着 AGI 2047，我也不会寻求放慢人工智能的短期发展。&lt;/p&gt;&lt;p&gt;我想在这里“查看交叉表”，但我认为一个非常合理的反应是模糊的：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;如果你认为 AGI 有望在 10 年内到来，那么你就想放慢脚步。&lt;/li&gt;&lt;li&gt;如果您认为 AGI 不太可能在 10 年内实现，但可能会在 25 年内实现，那么您希望大致保持当前的步伐，同时为未来放慢速度奠定基础。&lt;/li&gt;&lt;li&gt;如果你认为 AGI 几乎肯定还需要 25 年以上的时间，并且你（非常合理地）断定 AGI 出现之前的世俗担忧大多被夸大了，那么现在就加速发展，也许以后再担心剩下的事情。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;我相信这方面的基础工作非常重要，并且非常担心路径依赖，但对时间线为 25 年或更长的信心绝对是一个关键，它会改变我对许多事情的看法。&lt;/p&gt;&lt;p&gt;我希望看到更多的人，包括那些在他们的简历中带有“e/acc”的人，明确表示时间线问题是一个症结所在，他们的建议依赖于 AGI 的发展。&lt;/p&gt;&lt;h4&gt;安全研究实际上是好的&lt;/h4&gt;&lt;p&gt;一个亮点是对人工智能安全研究优先顺序的大力支持，尽管力度不够，而且自 2022 年以来只有很小的改进。&lt;/p&gt;&lt;figure class="wp-block-image"&gt; &lt;a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39741133-02aa-4bdc-8237-0fab3888ffcc_1423x940.png" rel="noreferrer noopener" target="_blank"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/NfPxAp5uwgZugwovY/yvtavb13vfmtk4sihdvh" /&gt;&lt;/a&gt;&lt;/figure&gt;&lt;p&gt;我仍然不理解这种不想做更多安全工作的态度。我可以理解想要尽快前进的心情。我可以理解，你们公司尤其应该注重能力。我不明白为什么人们不认为加强安全工作会对世界有利。&lt;/p&gt;&lt;p&gt;我认为这里 13% 的人认为“一致性是该领域最重要的问题之一”，这实际上是最荒谬的结果之一：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;第二组人工智能安全问题基于 Stuart Russell 提出的对齐问题 [Russell，2014]。这组问题首先总结了拉塞尔的论点——该论点声称，借助先进的人工智能，“你得到的正是你所要求的，而不是你想要的”——然后问道：&lt;/p&gt;&lt;p&gt; 1. 你认为这个论点指出了一个重要的问题吗？&lt;/p&gt;&lt;p&gt; 2. 与人工智能中的其他问题相比，今天解决这个问题的价值有多大？&lt;/p&gt;&lt;p&gt; 3. 与人工智能中的其他问题相比，您认为这个问题有多难？&lt;/p&gt;&lt;p&gt;大多数受访者表示对齐问题要么是“非常重要的问题”（41%），要么是“该领域最重要的问题之一”（13%），大多数受访者表示它“更难”（36%） ）或比人工智能中的其他问题“难得多”（21%）。然而，受访者普遍认为今天解决对齐问题比其他问题更有价值。 （图16）&lt;/p&gt;&lt;/blockquote&gt;&lt;figure class="wp-block-image"&gt; &lt;a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdab8f2fc-c71c-48d4-9501-9d7bdec990d8_1432x1414.png" rel="noreferrer noopener" target="_blank"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/NfPxAp5uwgZugwovY/lskwjreibec4ct0benph" /&gt;&lt;/a&gt;&lt;/figure&gt;&lt;p&gt;我可以理解有人认为对齐很容易。我认为这是一个超级错误的相信，但我已经看到了实际的论据，我可以想象这样的世界，罗素公式是超级​​可行的，而其他人工智能问题则要困难得多。所以，当然，在某种程度上，这是合理的分歧，或者至少我明白你是如何到达那里的。我要指出的是，对对齐难度的估计在 2023 年略有上升，对其工作价值的估计也是如此。&lt;/p&gt;&lt;p&gt;我们确实看到 41% 的人将其视为“非常重要的问题”，但不将其视为“该领域最重要的问题之一”似乎很疯狂。我很困惑为什么这个答案下降了这么多，从 21% 下降到 13%，特别是考虑到其他答案，也许这只是噪音。尽管如此，它应该要高得多。除非人们说这是一个错误的问题表述？&lt;/p&gt;&lt;p&gt;总的来说，研究人员似乎试图变得“更加温和”并全面给出中立的答案。也许这是由于进入并变得更加主流，以及人们试图给出社会认知答案。&lt;/p&gt;&lt;h4&gt;下赛季的问题&lt;/h4&gt;&lt;p&gt;与许多此类调查一样，我希望看到更多澄清问题，以及更多能够衡量相关性的尝试。哪些未来预期对应哪些担忧？为什么我们会看到连词谬误？过去一年里是什么改变了人们的想法，或者人们认为是什么改变了这一点？人们期待什么样的未来？研究人员如何描述诸如所有人类劳动自动化之类的事情，以及他们认为这样的世界会是什么样子？&lt;/p&gt;&lt;p&gt;至于 2024 年版要问哪些全新问题，哇，事情进展得很快，所以也许六个月后再问一次？&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/NfPxAp5uwgZugwovY/ai-impacts-survey-december-2023-edition#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Fri, 05 Jan 2024 14:40:08 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/NfPxAp5uwgZugwovY/ai-impacts-survey-december-2023-edition</guid></item></channel></rss>