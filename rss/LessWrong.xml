<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>少错</title><link>https://www.lesswrong.com</link><description>致力于提炼理性艺术的社区博客</description><lastBuildDate>Sun, 14 Jan 2024 18:13:30 GMT</lastBuildDate><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>周二《纽约时报》头版关于民主生存的文章的批判性阅读</title><link>https://www.lesswrong.com/posts/fKLnv2ojPvr9LLaCn/critical-reading-of-tuesday-s-front-page-nyt-article-on-the-1</link><description>发布于 2024 年 1 月 14 日下午 5:01（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;这很有趣，因为人工智能政策已被告知所有人都极其重要，而且根据&lt;a href="https://www.statista.com/statistics/381569/leading-news-and-media-sites-usa-by-share-of-visits/"&gt;Statista 的数据&lt;/a&gt;，《纽约时报》现在可能是美国最大的新闻网站。&lt;/p&gt;&lt;p&gt; &lt;i&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iyEGwgtxLWgYByiYj/uki7f5rt0xluovpvxm9z" /&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;基本上，它概括了为什么我不再读《纽约时报》的原因——他们让自己的文章变得如此无聊，因为他们害怕说假话，同时每四句话就会说假话。如果您不知道什么是&lt;a href="https://www.goodreads.com/quotes/65213-briefly-stated-the-gell-mann-amnesia-effect-is-as-follows-you"&gt;盖尔曼失忆症&lt;/a&gt;，那么阅读主要新闻媒体的信息还不安全。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;数十亿人将在今年的重大选举中投票——据一些估计，这大约是全球人口的一半——这是人们记忆中规模最大、影响最深远的民主活动之一。结果将影响未来几十年世界的运行方式。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;进一步阅读：艾莉莎·万斯（Alyssa Vance）的&lt;a href="https://www.lesswrong.com/posts/pFaLqTHqBtAYfzAgx/the-dictatorship-problem"&gt;《独裁问题》&lt;/a&gt;及其评论，其中讨论了民主是否注定会失败并在未来约 10 年内崩溃。&lt;/p&gt;&lt;p&gt;根据记录，我认为民主在过去十年中消亡是合理的，而我们生活在民主的僵尸尸体中。国父们设计这个政府体系并不是为了&lt;a href="https://www.lesswrong.com/posts/jyAerr8txxhiKnxwA/5-reasons-why-governments-militaries-already-want-ai-for"&gt;与现代国家安全局在同一个星球上&lt;/a&gt;生存。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;与此同时，虚假叙述和阴谋论已演变成日益严重的全球威胁。&lt;/p&gt;&lt;p&gt;毫无根据的选举舞弊指控打击了人们对民主的信任。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;您确定对民主的信任受到打击吗？坦率地说，在19世纪和20世纪的整个事件中，总统们不断地承认失败并下台，甚至不做任何小题大做，这无疑大大打击了很多人对民主的信任。&lt;/p&gt;&lt;p&gt;自从 2000 年布什的胜利受到争议以来，2020 年的选举是我能想到的最真实的选举。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;外国影响力运动经常针对两极分化的国内挑战。人工智能加剧了虚假信息的传播并扭曲了对现实的看法。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;人工智能增强了数据科学的能力，而数据科学反过来&lt;a href="https://www.lesswrong.com/posts/LdEwDn5veAckEemi4/we-are-already-in-a-persuasion-transformed-world-and-must"&gt;又自然地增强了虚假信息和一般权力&lt;/a&gt;。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;与此同时，主要社交媒体公司已经缩减了保障措施并缩小了选举团队的规模。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;需要引用。&lt;/p&gt;&lt;p&gt; &lt;a href="https://www.lesswrong.com/posts/jyAerr8txxhiKnxwA/5-reasons-why-governments-militaries-already-want-ai-for"&gt;我的模型&lt;/a&gt;预测，五大科技公司将把这些能力转移到他们的安全部门或其他与军事相关的组织，而公众指责风险是他们商业模式的主要威胁之一。为什么他们的“选举团队”的存在甚至是公开的信息？&lt;/p&gt;&lt;p&gt;与此同时，埃隆至少似乎找到了银河大脑的方法来部署受预测市场启发的真相寻求算法，例如&lt;a href="https://twitter.com/ESYudkowsky/status/1712908282581631372"&gt;尤德科斯基喜欢&lt;/a&gt;但维基 &lt;a href="https://web.archive.org/web/20240109193736/https://en.wikipedia.org/wiki/Community_Notes"&gt;百科讨厌的&lt;/a&gt;社区笔记。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;布鲁金斯学会智囊团高级研究员达雷尔·M·韦斯特表示，“几乎每个民主国家都面临着压力，与技术无关。” “当你在此基础上添加虚假信息时，只会创造很多恶作剧的机会。”&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;世界已经发生了很大的变化，数百年前出现的民主现在已经不再分配了。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;他说，这是“一场完美的虚假信息风暴”。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;同意。嗯，实际上不是， &lt;a href="https://www.lesswrong.com/posts/Zvu6ZP47dMLHXMiG3/optimized-propaganda-with-bayesian-networks-comment-on"&gt;现代虚假信息是一个优化问题&lt;/a&gt;，完美是不可能实现的。&lt;/p&gt;&lt;p&gt;接下来是一个滚动动画信息图，它非常简洁，但远未达到简洁或有帮助足以弥补新闻标准下降的程度。&lt;/p&gt;&lt;p&gt; &lt;i&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iyEGwgtxLWgYByiYj/mqkgzuetgkjjpj0ydlwz" /&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt; &lt;i&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iyEGwgtxLWgYByiYj/mccgzcjj8zpzllcdxcaw" /&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;喜欢这个话题。如果你想对此感到厌烦，请私信我。&lt;/p&gt;&lt;p&gt; &lt;i&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iyEGwgtxLWgYByiYj/rb2fscsgrqd1cmiomcks" /&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;我上次检查（2021 年）时发现，巴基斯坦和印度尼西亚在中美关系中非常重要。远没有台湾那么重要（我上次查了一下，中国的优先顺序是美国、俄罗斯、台湾、日本/韩国、东南亚、中东和世界其他地区，尽管我不确定澳大利亚或欧盟在哪里）或印度适合）。&lt;/p&gt;&lt;p&gt; &lt;i&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iyEGwgtxLWgYByiYj/amxcrktxwh0qpsyfgh3n" /&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;值得注意的是，欧盟围绕的法国或德国没有举行选举。然而，英国将会举行选举，我认为这足以将 2024 年称为民主的大年（英语国家的民主倒退通常会影响其他英语国家的民主）&lt;/p&gt;&lt;p&gt; &lt;i&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iyEGwgtxLWgYByiYj/a2jp9mxbuypnrnaxfs1y" /&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;印度算不算&lt;a href="https://en.wikipedia.org/wiki/Hybrid_regime"&gt;混合政权&lt;/a&gt;？有些人对印度人民党感到愤怒，他们想将印度描述为一个混合政权，而有些人出于同样不诚实的原因，想将印度描述为一个非混合政权，所以我宁愿在这里问。&lt;/p&gt;&lt;p&gt; &lt;i&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iyEGwgtxLWgYByiYj/d93kmamtbnb0viivkqb4" /&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;我从未见过《纽约时报》如此接近地承认乌克兰不是一个成熟的民主国家。现在想来，在一个完全民主的国家里，曾经发生过冷战式的代理人战争吗？&lt;/p&gt;&lt;p&gt; &lt;i&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iyEGwgtxLWgYByiYj/ttxjiwbemtmqeequo4je" /&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;可能比所有其他选举的总和重要得多；我可以看到，即使世界上所有其他民主国家都垮台了，美国仍然是一个民主国家，但如果美国垮台，我真的不认为任何其他民主国家能够幸存。所有最好的科技公司和情报机构似乎都在这里。&lt;/p&gt;&lt;p&gt; &lt;i&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iyEGwgtxLWgYByiYj/dvodyyfrk7kjqs3bwhmx" /&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;南非很有趣，因为它最近公开表示支持中国/俄罗斯，反对西方（支持俄罗斯/中国的声明，与俄罗斯和中国的军事演习，以及反对以美国为基础的国际贸易机构，如国际货币基金组织，这是一个俄罗斯/中国阵营的主要内容）。当然，他们在非洲，所以他们可能只是表明他们认真对待双方的比赛。&lt;/p&gt;&lt;p&gt;这也很有趣，因为南非政权和军队仍然以欧洲白种人为主，这使得它成为俄罗斯/中国阵营中除俄罗斯以外唯一一个由欧洲人主导的国家（重要提示：我所说的“俄罗斯/中国阵营”）他们根本不是一个联盟或铁板一块，他们仍然会利用外交边缘政策来增加自己相对于其他成员的权力，并且只是通过反对西方和处于劣势地位而团结起来；随着西方变得更加强大，他们尝试将更多的差异放在一边）。&lt;/p&gt;&lt;p&gt; &lt;i&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iyEGwgtxLWgYByiYj/exwoirskjmcj8q1pz7ft" /&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt; 2010 年至 2020 年民主党倒退的地图。这不在《纽约时报》的文章中，我从维基百科中提取了它，它位于&lt;a href="https://en.wikipedia.org/wiki/Hybrid_regime"&gt;Hybrid Regimes&lt;/a&gt;的页面上。&lt;/p&gt;&lt;p&gt;值得注意的是，缅甸在 2021 年的一场政变中成为独裁国家（图片没有说明这一点），新政权将一切归咎于西方的影响力行动（众所周知，每当发生任何令人尴尬的事情时，独裁政权都会这样做，无论是实际上并非西方情报机构参与）。与此同时，从 2010 年至今，俄罗斯和中国变得更加专制，而新冠疫情并没有加强任何地方的民主。这张图应该更红。&lt;/p&gt;&lt;p&gt;回到纽约时报的文章：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;赌注是巨大的。&lt;/p&gt;&lt;p&gt;冷战结束后在全球传播的民主面临着世界范围内日益严峻的挑战——从大规模移民到气候破坏，从经济不平等到战争。许多国家为充分应对此类考验而付出的努力削弱了人们对自由、多元化社会的信心，为民粹主义者和强人领导人的呼吁打开了大门。&lt;/p&gt;&lt;p&gt;以俄罗斯和中国为首的独裁国家通常通过赞助虚假信息运动，利用政治不满情绪来推动破坏民主治理和领导力的言论。如果这些努力取得成功，选举可能会加速近期独裁领导人的崛起。&lt;/p&gt;&lt;p&gt;费奥多尔·A·卢基亚诺夫 (Fyodor A. Lukyanov) 是莫斯科与克里姆林宫结盟的智库外交与国防政策委员会的领导者，他最近表示，2024 年“可能是西方自由派精英失去对世界秩序控制的一年”。&lt;/p&gt;&lt;p&gt;技术政策公司 Anchor Change 的创始人、Facebook 管理选举的前公共政策总监凯蒂·哈巴斯 (Katie Harbath) 表示，许多国家的政治体制以及&lt;a href="https://www.nytimes.com/2023/09/09/world/europe/what-is-the-g20.html"&gt;20 国集团&lt;/a&gt;等政府间组织似乎即将发生剧变。虚假信息——通过社交媒体传播，也通过印刷品、广播、电视和口碑传播——有可能破坏政治进程的稳定。&lt;/p&gt;&lt;p&gt; “到了 2025 年，世界将会变得非常不同，”她说。&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;激进的国家特工&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;竞选活动中虚假信息的最大来源之一是独裁政府试图抹黑民主作为全球治理模式的声誉。&lt;/p&gt;&lt;p&gt;近几个月来，研究人员和美国政府都认为俄罗斯、 &lt;a href="https://www.nytimes.com/2023/12/14/business/media/pro-china-youtube-disinformation.html"&gt;中国&lt;/a&gt;和伊朗可能试图通过影响行动扰乱其他国家的选举，包括今年的美国总统选举。数字安全公司 Recorded Future 的分析师布赖恩·利斯顿 (Brian Liston) 表示，这些国家将未来的一年视为“在世界舞台上让我们难堪、利用社会分歧并破坏民主进程的真正机会”。Recorded Future 是一家数字安全公司，最近&lt;a href="https://www.recordedfuture.com/aggressive-malign-influence-threatens-us-2024-elections"&gt;报告了潜在威胁&lt;/a&gt;到美国种族。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;值得注意的是：这是我能想到的增加威权政权合法性的最有效方法之一。附近蓬勃发展的民主国家无疑是威权政权合法性最糟糕的事情之一。从这个意义上说，我们所看到的可能是威权政权领导人与美国反恐战争的某些人之间的某种价值握手（或 BATNA）。&lt;/p&gt;&lt;p&gt;不幸的是，这并没有改变这样一个事实：如果民主在这里消亡，那么它可能在任何地方都消亡。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;该公司还检查了 Meta &lt;a href="https://www.nytimes.com/2023/12/07/technology/russia-disinformation-mike-tyson-priscilla-presley.html"&gt;去年首次发现的&lt;/a&gt;俄罗斯影响力活动，该活动被称为“Doppelgänger”，该活动似乎冒充国际新闻机构并创建虚假账户，在美国和欧洲传播俄罗斯宣传。分身似乎使用了广泛使用的人工智能工具来创建专门报道美国政治的新闻媒体，其名称包括“选举观察”和“我的骄傲”。&lt;/p&gt;&lt;p&gt;像这样的虚假信息活动很容易跨越国界。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;事实上，穿越边境是一件非常困难的事情。你现在必须担心该国的情报机构，而不仅仅是民间社会。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;阴谋论——例如声称美国与不同国家的合作者策划当地权力转移的说法&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;不是阴谋论，抱歉。这有&lt;a href="https://en.wikipedia.org/wiki/Church_Committee"&gt;很长的历史&lt;/a&gt;。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;或者它在乌克兰&lt;a href="https://www.nytimes.com/2022/09/04/technology/russia-bioweapons-geneva.html"&gt;经营秘密生物武器&lt;/a&gt;工厂&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;错误等价。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; ——试图抹黑美国和欧洲在世界各地的政治和文化影响力。他们可能会&lt;a href="https://www.usip.org/sites/default/files/2022-11/20221117_sr_514-losing-facts-fiction-nationalism-misinformation-conspiracy-theories-pakistan.pdf"&gt;以乌尔都语&lt;/a&gt;出现在巴基斯坦，同时也会以不同的字符和语言出现&lt;a href="https://www.nytimes.com/2022/04/25/opinion/putin-russia-conspiracy-theories.html"&gt;在俄罗斯&lt;/a&gt;，从而使这些国家的公众舆论转向有利于反西方政客的观点。&lt;/p&gt;&lt;p&gt;世界各地流传的虚假叙述往往是由侨民社区分享或由国家支持的特工精心策划的。专家预测，选举舞弊的说法将继续演变并引起反响，就像 2022 年在美国和巴西以及 2023 年在阿根廷所发生的那样。&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;两极分化和极端主义的循环&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;日益两极分化和好斗的政治环境正在滋生仇恨言论和错误信息，这将选民进一步推入孤岛。少数有动机的极端声音，在强化用户偏见的社交媒体算法的帮助下，往往会淹没温和的大多数。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;请参阅&lt;a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks-and-mind"&gt;小丑攻击&lt;/a&gt;。这些人为不良行为者操纵舆论提供了很多程度的自由。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; “我们正在重新定义有关言论的社会规范，以及我们如何让人们对线上和线下的言论负责，”哈巴斯女士说。 “在这个国家，对于如何做到这一点有很多不同的观点，更不用说在全球范围内了。”&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;社区笔记和预测市场等新兴社交技术表明，如果您对社区笔记和预测市场等社交技术的可能性完全一无所知，那么很容易高估解决这个问题的难度。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;一些最极端的声音在其他社交媒体平台上互相寻找，例如 Telegram、 &lt;a href="https://www.pewresearch.org/short-reads/2023/02/17/key-facts-about-bitchute/"&gt;BitChute&lt;/a&gt;和&lt;a href="https://www.nytimes.com/2023/11/16/business/trump-media-truth-social-advertising-revenue.html"&gt;Truth Social&lt;/a&gt; 。据监测威胁和错误信息的公司 Pyrra 称，要求先发制人地制止选民欺诈的呼声最近在此类平台上流行起来，这种行为在历史上在统计上微不足道。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;现在我想起来，驳回选民欺诈真是令人毛骨悚然。就像，这正是一种可能以历史上前所未有的方式爆炸的事情，因为有人决定在上面花很多钱，尽管四年前没有人花太多钱。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; &lt;a href="https://www.pyrratech.com/articles/case-study-alt-socials-role-in-influencing-narratives-around-u-s-election-validity"&gt;皮拉在案例研究中发现，&lt;/a&gt; “这些叙述的普遍性和接受度只会越来越受关注”，甚至直接影响选举政策和立法。&lt;/p&gt;&lt;p&gt;该公司的研究人员写道：“这些阴谋正在政治精英中扎根，他们利用这些叙事来赢得公众的青睐，同时降低了他们本应维护的体系的透明度、制衡和平衡。”&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;人工智能的风险回报主张&lt;/strong&gt;&lt;/h2&gt;&lt;/blockquote&gt;&lt;p&gt;人们是否会读到/听到&lt;a href="https://www.lesswrong.com/posts/LTtNXM9shNM9AC2mp/superintelligence-faq"&gt;超级智能&lt;/a&gt;，然后在几个小时后完全忘记它？这就是为什么我们不断听到听起来像是人工智能风险的说法，但实际上却是完全不同的事情？&lt;/p&gt;&lt;p&gt;也许山姆·奥尔特曼（Sam Altman）上台说“人工智能有大量风险，但也有回报”，但没有进一步阐述，所以记者们只需要自己弄清楚这意味着什么？&lt;/p&gt;&lt;p&gt;这可能没什么，但也可能值得指出。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;根据&lt;a href="https://www.gsb.stanford.edu/sites/default/files/publication/pdfs/white-paper-2023-ai-and-elections-best-practices_0.pdf"&gt;一份报告&lt;/a&gt;，人工智能“为民主治理带来了希望”&lt;strong&gt; &lt;/strong&gt;芝加哥大学和斯坦福大学。以政治为重点的聊天机器人可以向选民通报关键问题，并更好地将选民与民选官员联系起来。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;的确。我们已经看到了通过社区笔记、 &lt;a href="https://news.manifold.markets/p/human-v-bots-forecasting-tournament"&gt;Manifold 的预测市场机器人实验&lt;/a&gt;、Google 搜索和 ChatGPT 来研究这一问题的努力（尽管现在判断 ChatGPT 燃烧剩余时间线的危害是否超过各种好处还为时过早）。该报告仅涵盖生成式人工智能；他们似乎不知道也不关心现代搜索引擎使用机器学习。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;该技术也可能成为虚假信息的载体。虚假的人工智能图像已经被用来传播阴谋论，例如毫无根据的断言，即全球存在一个用非白人&lt;a href="https://www.nytimes.com/2022/02/15/world/europe/france-elections-pecresse-great-replacement.html"&gt;移民&lt;/a&gt;&lt;a href="https://www.nytimes.com/2022/05/16/podcasts/the-daily/buffalo-shooting-replacement-theory.html"&gt;取代&lt;/a&gt;欧洲白人的阴谋。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;那么，你是说它“可能”或“已经”成为虚假信息的载体？我的意思是，无论如何，编剧们并没有足够的&lt;a href="https://www.lesswrong.com/posts/Zvu6ZP47dMLHXMiG3/optimized-propaganda-with-bayesian-networks-comment-on"&gt;数学知识&lt;/a&gt;来做出这样的判断。当《纽约时报》可以雇佣那些擅长发表模糊陈述且不会惹上麻烦的人时，为什么还要雇佣擅长数学的人呢？&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; 10 月，密歇根州国务卿乔斯林·本森 (Jocelyn Benson) 写信给纽约州民主党参议员、多数党领袖查克·舒默 (Chuck Schumer)，表示“人工智能生成的内容可能会增强高度本地化的错误信息的可信度。”&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;人工智能生成的错误信息不再有趣。基本上 EA 中的每个人都知道这一点，但 EA 中没有人知道如何使用人工智能来&lt;a href="https://www.lesswrong.com/posts/Zvu6ZP47dMLHXMiG3/optimized-propaganda-with-bayesian-networks-comment-on"&gt;研究人类信念的形成&lt;/a&gt;，或者&lt;a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks-and-mind"&gt;在将人们与特定信息进行匹配时优化可测量的说服力&lt;/a&gt;，或者&lt;a href="https://www.lesswrong.com/posts/Lw8enYm5EXyvbcjmt/sensor-exposure-can-compromise-the-human-brain-in-the-2020s"&gt;使用数十亿个人脸视频文件预测思想和行为（只要思想和行为是可测量的）&lt;/a&gt; 。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; “少数几个州——以及这些州内的特定选区——可能会决定总统职位，”她说。 “那些试图影响结果或制造混乱的人可能会利用人工智能工具在等待时间、关闭甚至特定投票地点的暴力行为方面误导选民。”&lt;/p&gt;&lt;p&gt;公共政策研究所布伦南司法中心负责选举和政府项目的劳伦斯·诺登补充说，人工智能可以模仿选举办公室的大量材料并将其广泛传播。或者，它可以制造后期&lt;strong&gt; &lt;/strong&gt;&lt;a href="https://www.nytimes.com/2023/03/25/us/politics/trump-october-surprise.html"&gt;十月的惊喜&lt;/a&gt;，比如在今年秋天斯洛伐克激烈的选举期间发布的带有人工智能干预迹象的&lt;a href="https://www.wired.com/story/slovakias-election-deepfakes-show-ai-is-a-danger-to-democracy/"&gt;音频&lt;/a&gt;。&lt;/p&gt;&lt;p&gt; “一段时间以来一直威胁我们民主的所有事情都可能因人工智能而变得更糟，”诺登先生在 11 月份参加一个在线小组讨论时表示。 （活动期间，组织者推出了诺登先生的&lt;a href="https://www.youtube.com/watch?v=VH1rOylsoMo&amp;amp;t=2s"&gt;人工操纵版本&lt;/a&gt;，以强调该技术的能力。）&lt;/p&gt;&lt;p&gt;一些专家担心， &lt;a href="https://www.nytimes.com/2023/10/28/business/media/ai-muddies-israel-hamas-war-in-unexpected-way.html"&gt;人工智能工具的存在&lt;/a&gt;可能会削弱人们对信息的信任，并使政治参与者忽视真实内容。其他人则表示，目前的担忧有些夸大。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;我还认为有些担忧被夸大了，例如&lt;a href="https://www.lesswrong.com/posts/aWPucqvJ4RWKKwKjH/4-min-read-an-intuitive-explanation-of-the-ai-influence?commentId=FeefDvsjMBLCxE3GS"&gt;深度造假&lt;/a&gt;。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;外交关系委员会智库高级副总裁詹姆斯·林赛 (James M. Lindsay) 表示，人工智能“只是众多威胁之一”。&lt;/p&gt;&lt;p&gt; “我不会忽视所有散播错误信息或虚假信息的老式方式，”他说。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;人工智能通过&lt;a href="https://www.lesswrong.com/posts/aWPucqvJ4RWKKwKjH/4-min-read-an-intuitive-explanation-of-the-ai-influence"&gt;将人与信息进行匹配并优化这些匹配以实现可衡量的信念变化，从而&lt;/a&gt;增强了散布虚假信息的老式方式的有效性。&lt;/p&gt;&lt;blockquote&gt;&lt;h2&gt;&lt;strong&gt;大型科技公司缩减保护规模&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;在计划于 2024 年举行大选的国家，虚假信息已成为联合国文化组织&lt;a href="https://www.unesco.org/sites/default/files/medias/fichiers/2023/11/unesco_ipsos_survey.pdf"&gt;教科文组织&lt;/a&gt;调查的绝大多数人的主要担忧。然而，社交媒体公司限制有毒内容的努力在 2016 年美国总统大选后升级，即使没有完全逆转，最近也逐渐&lt;a href="https://www.nytimes.com/2023/02/14/technology/disinformation-moderation-social-media.html"&gt;减弱&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;根据倡导组织 Free Press&lt;a href="https://www.freepress.net/big-tech-backslide-report"&gt;最近的一份报告，&lt;/a&gt; Meta、YouTube 和 X（前身为 Twitter）平台去年缩减或重组了负责 &lt;a href="https://www.nytimes.com/2022/11/28/technology/twitter-misinformation-experts-hiring.html"&gt;控制危险或不准确材料的&lt;/a&gt;团队。有些提供了特别难以监控的新功能，例如私人单向广播。&lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt;自由新闻公司的高级法律顾问诺拉·贝纳维德斯表示，这些公司在年初的时候“带宽很少，书面责任也很少，全世界有数十亿人转向这些平台获取信息”——这对于维护民主来说并不理想。&lt;/p&gt;&lt;p&gt; &lt;a href="https://www.nytimes.com/2022/08/14/business/media/on-tiktok-election-misinformation.html"&gt;TikTok&lt;/a&gt;等较新的平台很可能会开始在政治内容中发挥&lt;a href="https://www.nytimes.com/2023/12/13/business/media/tiktok-politicians.html"&gt;更大的作用&lt;/a&gt;。时事通讯初创公司 Substack 上个月表示，&lt;a href="https://www.nytimes.com/2023/12/22/business/substack-nazis-content-moderation.html"&gt;不会在其平台上禁止&lt;/a&gt;纳粹符号和极端主义言论，并希望 2024 年的投票季成为“ &lt;a href="https://on.substack.com/p/in-the-2024-us-elections-vote-for"&gt;Substack 选举&lt;/a&gt;”。政客们正在计划在 Twitch 上 &lt;a href="https://www.washingtonpost.com/technology/2023/09/28/live-stream-twitch-politics-khanna/"&gt;进行直播活动&lt;/a&gt;，Twitch 上还将举办人工智能生成的拜登总统和前总统唐纳德·J·特朗普之间的辩论。&lt;/p&gt;&lt;p&gt;拥有 Facebook、Instagram 和 WhatsApp 的 Meta 在 11 月的&lt;a href="https://about.fb.com/news/2023/11/how-meta-is-planning-for-elections-in-2024/"&gt;一篇博客文章&lt;/a&gt;中表示，它“处于有利地位，可以保护我们平台上明年选举的完整性”。 （上个月，公司指定的监督委员会对 Meta 的自动化工具及其对与以色列-哈马斯冲突相关的&lt;a href="https://oversightboard.com/news/1109713833718200-oversight-board-issues-first-expedited-decisions-about-israel-hamas-conflict/"&gt;两个视频&lt;/a&gt;的处理提出了质疑。）&lt;/p&gt;&lt;p&gt; YouTube 上个月&lt;a href="https://blog.youtube/inside-youtube/supporting-2024-united-states-election/"&gt;写道&lt;/a&gt;，其“专注于选举的团队一直在不停地工作，以确保我们拥有正确的政策和系统。”该平台今年夏天表示，将&lt;a href="https://www.npr.org/2023/06/02/1179864026/youtube-will-no-longer-take-down-false-claims-about-u-s-elections"&gt;停止删除虚假选民欺诈言论&lt;/a&gt;。 （YouTube 表示，它希望选民能够听到辩论的各个方面，但也指出“这并不是传播有害错误信息或宣扬仇恨言论的免费通行证。”）&lt;/p&gt;&lt;p&gt;亿万富翁埃隆·马斯克 (Elon Musk) 于 2022 年底上任后， &lt;a href="https://www.nytimes.com/interactive/2023/10/27/technology/twitter-x-elon-musk-anniversary.html"&gt;X 上的此类内容激增&lt;/a&gt;。几个月后，亚历山德拉·波普肯 (Alexandra Popken) 辞去了负责管理该平台信任和安全运营的职务。后来加入内容审核公司 WebPurify 的波普肯女士表示，许多社交媒体公司严重依赖不可靠的人工智能内容审核工具，导致精简人员不断处于救火状态。&lt;/p&gt;&lt;p&gt;她说：“选举公正性是一项艰巨的工作，你确实需要一项积极主动的战略、大量的人员、大脑和作战室。”&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;多人、大脑和作战室是目前处理&lt;a href="https://www.lesswrong.com/posts/jyAerr8txxhiKnxwA/5-reasons-why-governments-militaries-already-want-ai-for"&gt;由其他人、大脑和作战室故意造成的&lt;/a&gt;问题的最佳方式。&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/fKLnv2ojPvr9LLaCn/critical-reading-of-tuesday-s-front-page-nyt-article-on-the-1#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sun, 14 Jan 2024 17:01:42 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/fKLnv2ojPvr9LLaCn/critical-reading-of-tuesday-s-front-page-nyt-article-on-the-1</guid></item><item><title>对人工智能治理有影响的项目列表</title><link>https://www.lesswrong.com/posts/MPMsnpHkHoRQxarnG/list-of-projects-that-seem-impactful-for-ai-governance</link><description>发布于 2024 年 1 月 14 日下午 4:53（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt; （从&lt;a href="https://forum.effectivealtruism.org/posts/MWLyQxbAuP2Je3iCK/list-of-projects-that-seem-impactful-for-ai-governance"&gt;&lt;i&gt;EA 论坛&lt;/i&gt;&lt;/a&gt;&lt;i&gt;交叉发布&lt;/i&gt;&lt;i&gt;。）&lt;/i&gt;&lt;/p&gt;&lt;h1&gt;这篇文章的目标&lt;/h1&gt;&lt;p&gt;这篇文章的主要目标是阐明在人工智能治理的前沿可以完成哪些项目。间接目标是：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;激励人们致力于新的、富有成效的项目。&lt;/li&gt;&lt;li&gt;为新人提供可能的项目。&lt;/li&gt;&lt;li&gt;为&lt;a href="http://enais.co"&gt;&lt;u&gt;ENAIS&lt;/u&gt;&lt;/a&gt;放样新项目。&lt;/li&gt;&lt;li&gt;通过公开此列表来促进对项目优点的讨论。&lt;/li&gt;&lt;/ul&gt;&lt;h1&gt;我们如何收集列表&lt;/h1&gt;&lt;p&gt;我们筛选了过去 14 个月（有&lt;a href="https://www.lesswrong.com/posts/DJB82jKwgJE5NsWgT/some-cruxes-on-impactful-alternatives-to-ai-policy-work"&gt;&lt;u&gt;一个例外&lt;/u&gt;&lt;/a&gt;）至少有 15 个业力（通常 &amp;gt;100）的帖子。这篇文章主要是其他帖子/文章的想法的集合，但任何错误都是我们自己的。我们建议您阅读原始帖子以获取更多背景信息。我们没有过滤这些帖子中的项目；我们既没有根据我们的观点加入或排除项目，也没有删除可能有争议的项目。通过仅仅陈述这些项目，我们希望援引坎宁安定律来改进项目并提供其预期价值的背景。&lt;/p&gt;&lt;p&gt;此外，我们的收集方法意味着项目的范围和难度水平差异很大。此外，其中许多项目可能有许多子项目。&lt;/p&gt;&lt;p&gt;我们（来自 ENAIS 的 Jaime 和 Teun）在这篇文章上总共花费了不到 30 个小时。我们可能会决定根据帖子的接收方式进行更深入的研究。&lt;/p&gt;&lt;h1&gt;在开始项目之前&lt;/h1&gt;&lt;p&gt;如果您对任何项目感兴趣，我们建议您查看之前做过的事情。我们没有调查这一点，这些项目的状态可能在发布的帖子和您开始工作之间发生了变化。如果您发现某个项目已经在某种程度上得到解决，您可以在其基础上添加价值，审查已完成的工作并标记错误，在一份报告中收集不同的观点等。&lt;/p&gt;&lt;h1&gt;项目清单&lt;/h1&gt;&lt;h2&gt;写作&lt;/h2&gt;&lt;p&gt;在这里，我们建议任何类型的项目，其结果是书面的：帖子、论文、报告……显然，这些建议的不同范围的变体也是可能的。&lt;/p&gt;&lt;p&gt;本节背后的总体直觉是通过澄清相关概念来进一步推进人工智能治理，如下&lt;a href="https://www.lesswrong.com/posts/6FkWnktH3mjMAxdRT/what-i-would-do-if-i-wasn-t-at-arc-evals#Writing_blog_posts_or_takes_in_general"&gt;&lt;u&gt;所述&lt;/u&gt;&lt;/a&gt;：“&lt;i&gt;找出人们感到困惑的领域，提出可以让他们减少困惑的建议，或者找到在这些领域有良好看法的人，以及将它们写成清晰的博客文章。&lt;/i&gt; ”&lt;/p&gt;&lt;h3&gt;计算治理&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;对计算治理对人工智能进步的影响进行详细研究，并发表一系列文章，解释主要发现和对行业的影响 [1]。&lt;/li&gt;&lt;li&gt;制定关于人工智能治理计算监控标准设计的综合指南，详细介绍最佳实践、方法和案例研究 [2]。&lt;/li&gt;&lt;li&gt;创建一份白皮书或工具包，概述在各种组织环境中验证 AI 计算监控标准实施情况的方法 [2]。&lt;/li&gt;&lt;li&gt;设计一个框架或一套指南，以强制遵守人工智能计算监控标准，包括潜在的处罚和激励措施 [2]。&lt;/li&gt;&lt;li&gt;开发一份关于“GPU 中的防篡改日志记录”的白皮书：探索在 GPU 中实施防篡改日志记录系统的设计和影响，以增强安全性和可追溯性 [6]。&lt;/li&gt;&lt;li&gt;创建“GPU 全局跟踪”框架：设计一个系统来跟踪全球 GPU 的分布和使用情况，从而有可能监控和调节计算能力 [6]。&lt;/li&gt;&lt;li&gt; “学习证明算法”研究：研究和开发能够提供学习证明的算法，确保机器学习过程的完整性和可验证性[6]。&lt;/li&gt;&lt;li&gt; “模型现场检查”提案：制定一套对人工智能模型及其训练环境进行物理检查的指南和协议[6]。&lt;/li&gt;&lt;li&gt;开发“检测数据中心”：创建用于识别和监控大型数据中心的工具或方法，特别是用于密集计算任务的数据中心[6]。&lt;/li&gt;&lt;li&gt;构建“可验证推理套件”：设计一套可以验证人工智能模型做出的推理的工具或软件，确保它们准确且值得信赖[6]。&lt;/li&gt;&lt;li&gt;开展“衡量有效计算使用”的研究：探索衡量和控制算法进展的方法，以确保计算资源的高效和合乎道德的使用[6]。&lt;/li&gt;&lt;li&gt;怀特关于“规范大规模去中心化培训”的报告：调查挑战并提出规范去中心化人工智能培训的框架，特别是如果它成为中心化方法的竞争性替代方案[6]。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;评估/审核&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;撰写关于应该构建什么样的评估、未来几年可能出现哪些特定故障模式的文章，.. [2]。&lt;/li&gt;&lt;li&gt;一篇小文章提出了一个具体的可能想法，涉及如何在整个行业实施评估、什么样的协议是可行的、哪些利益相关者需要签署，以及这些利益相关者可能有哪些类型的需求/担忧[ 2]。&lt;/li&gt;&lt;li&gt;建议在部署前对模型进行第三方审核的标准化协议，重点关注安全性 [4]。&lt;/li&gt;&lt;li&gt;建立持续评估部署后危险能力模型的框架[4]。&lt;/li&gt;&lt;li&gt;在训练强大的模型之前创建用于进行风险评估的综合指南 [4]。&lt;/li&gt;&lt;li&gt;制定审计人工智能系统的详细建议，重点关注透明度和问责制[5]。&lt;/li&gt;&lt;li&gt;对人工智能训练运行中的篡改迹象和信号以及如何检测它们进行调查 [6]。&lt;/li&gt;&lt;li&gt;撰写一份关于通过互联网进行自主人工智能复制的可行性及其影响的报告 [6]。&lt;/li&gt;&lt;li&gt;制定一份关于美国政府可用于审计科技公司的现有工具和方法的白皮书，并提出改进建议[6]。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;蒸馏/通讯&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;创建一系列深入的文章或博客系列，为普通受众分解和阐明复杂的人工智能治理概念 [1]。&lt;/li&gt;&lt;li&gt;制作一系列高质量的播客或网络研讨会，旨在对不同的受众进行人工智能治理方面的教育，重点关注清晰和高保真的沟通 [2]。&lt;/li&gt;&lt;li&gt;为具有技术背景的个人组织研讨会或培训课程，学习如何有效地向政策制定者传达人工智能相关的想法[2]。&lt;/li&gt;&lt;li&gt;制定一套指南或一套最佳实践，用于沟通人工智能风险、解决常见误解并重点关注传播最重要的想法 [2]。&lt;/li&gt;&lt;li&gt;旨在提高认识、激励人才并通过资金和资源支持人工智能治理工作的战略计划或系列研讨会[3]。&lt;/li&gt;&lt;li&gt;比较并可视化人工智能实验室领导/员工关于他们对 AGI 风险和收益的看法的不同公开声明，包括他们愿意在其开发中承担的风险水平 [4]。&lt;/li&gt;&lt;li&gt;开始一系列博客文章，旨在澄清人工智能中令人困惑的领域，采访具有深刻见解的专家，并将其转化为更广泛受众易于理解、清晰的内容 [7]。&lt;/li&gt;&lt;li&gt;利用华盛顿当前的开放态度，制定教育材料和研讨会大纲，向国会和公众通报人工智能风险和政策考虑因素[9]。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;信息安全&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;建立一套专门为人工智能实验室及其供应商量身定制的信息安全最佳实践和标准。该项目可以包括创建安全措施认证流程、为不同级别的员工制定培训计划，以及组建快速响应团队以快速有效地应对任何新出现的威胁[1]。&lt;/li&gt;&lt;li&gt;比较人工智能实验室共享威胁情报和安全事件信息的举措[4]。&lt;/li&gt;&lt;li&gt;将人工智能实验室的信息安全计划与情报机构等其他组织的信息安全计划进行比较的文章[4]。&lt;/li&gt;&lt;li&gt;&lt;i&gt;制定防止神经网络权重泄露的指南或框架，概述潜在风险和缓解策略[6]。&lt;/i&gt;&lt;/li&gt;&lt;li&gt;&lt;i&gt;创建一份白皮书，探讨安全系统中编码助手不一致所带来的潜在特权升级风险 [6]。&lt;/i&gt;&lt;/li&gt;&lt;li&gt;&lt;i&gt;开发一个系统或一组协议来进行有效的数据中心监控，以检测未经授权的模型副本 [6]。&lt;/i&gt;&lt;/li&gt;&lt;li&gt;&lt;i&gt;研究并提出检测人工智能模型不同副本之间未经授权的通信通道的方法[6]。&lt;/i&gt;&lt;/li&gt;&lt;li&gt;&lt;i&gt;分析核指挥和控制系统面对人工智能威胁的脆弱性并提出保障措施[6]。&lt;/i&gt;&lt;/li&gt;&lt;li&gt;&lt;i&gt;设计一个可扩展的行为监控系统，可以聚合和分析来自数百万个人工智能的监控日志[6]。&lt;/i&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;其他写的&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;写出一份具体的建议，说明政府（或特定政府）应该采取哪些措施来使人工智能顺利发展[1]。&lt;/li&gt;&lt;li&gt;撰写一份全面的提案，概述主要人工智能实验室（或特定人工智能实验室：OpenAI、Anthropic 等）的具体行动，以确保符合道德的人工智能开发和使用 [1]。&lt;/li&gt;&lt;li&gt;为人工智能实验室制定动态政策行动计划，重点关注道德、安全和负责任的人工智能开发，包括实施路线图[1]。&lt;/li&gt;&lt;li&gt;对中国和西方的人工智能治理模式进行比较分析，重点关注计算和实验室治理，并撰写一篇文章，提出协调国际人工智能政策的建议[1]。&lt;/li&gt;&lt;li&gt;研究人工智能政策在领先机构中的实际应用及其理论框架。结果可能是一份报告，为弥合理论与实践之间的差距提供见解和建议[1]。&lt;/li&gt;&lt;li&gt;制定并发布关于人工智能威胁建模的综合指南，包括针对政策制定者和人工智能开发人员的案例研究和缓解策略[1]。&lt;/li&gt;&lt;li&gt;开发一个全面的框架来评估人工智能系统带来的潜在极端风险。该项目将涉及创建评估风险（包括最坏情况）的方法，并制定实施保障措施和监测机制的指南。它可以包括一系列研讨会的大纲以及与人工智能开发人员和风险分析师的合作，以验证和完善评估工具[1]。&lt;/li&gt;&lt;li&gt;研究用于人工智能训练运行的防篡改监控和验证的技术和方法。该项目可能涉及研究硬件解决方案、设计安全且可验证的训练过程协议，以及如何在现实环境中实施这些措施的建议 [1]。&lt;/li&gt;&lt;li&gt;法律分析或白皮书阐明了可用于训练高级人工智能系统的数据的当前法律状况，并为人工智能公司降低风险提出了建议[3]。&lt;/li&gt;&lt;li&gt;赏金和举报人保护制度的提案或试点项目，以激励负责任地报告人工智能实验室和政府的不负责任决策[3]。&lt;/li&gt;&lt;li&gt;为人工智能实验室提供框架建议，以便在部署之前识别、分析和评估强大模型的风险[4]。&lt;/li&gt;&lt;li&gt;制定一套指南或框架，为部署后的强大模型建立适当的安全限制[4]。&lt;/li&gt;&lt;li&gt;在部署强大的模型之前，审查人工智能实验室中委托外部红队的策略和服务[4]。&lt;/li&gt;&lt;li&gt;撰写一份关于如何使用人工智能实验室发布的不同模型（专有模型与开源模型……）及其社会影响的报告 [4]。&lt;/li&gt;&lt;li&gt;编译最先进的安全和对齐技术存储库，供人工智能实验室实施 [4]。&lt;/li&gt;&lt;li&gt;比较人工智能实验室的不同指标，即安全事件响应计划、应急响应计划、应对国家资助或工业间谍活动风险的一系列措施、对其治理结构的第三方审计、遏制模型（例如通过拳击或气隙），计划分阶段部署强大的模型.. [4]。&lt;/li&gt;&lt;li&gt;比较不同的协议，了解人工智能实验室何时以及如何暂停具有危险功能的模型的开发 [4]。&lt;/li&gt;&lt;li&gt;设计一个在人工智能实验室中实施错误赏金计划的框架 [4]。&lt;/li&gt;&lt;li&gt;就人工智能实验室何时以及如何开源强大的模型提出不同的指导方针 [4]。&lt;/li&gt;&lt;li&gt;写一篇文章，建议为人工智能治理领域的个人建立一个认证系统 [5]。&lt;/li&gt;&lt;li&gt;探索禁止开源大型语言模型的影响和可行性，并就此撰写报告 [5]。&lt;/li&gt;&lt;li&gt;关于美国政府内监督大型人工智能训练的最有效监管机构的研究和建议[6]。&lt;/li&gt;&lt;li&gt;美国对华人工智能技术出口管制差距及强化策略分析[6]。&lt;/li&gt;&lt;li&gt;预测人工智能应用或演示将引起社会最强烈反应的调查或研究[6]。&lt;/li&gt;&lt;li&gt;针对敏感任务（例如，为世界领导人提供建议）部署人工智能的场景分析，重点关注隐私和道德影响[6]。&lt;/li&gt;&lt;li&gt;研究围绕人工智能的政治话语如何使社会两极分化以及缓解这种两极分化的策略[6]。&lt;/li&gt;&lt;li&gt;利用人工智能技术实现工厂和武器等关键基础设施自动化的可行性研究和路线图[6]。&lt;/li&gt;&lt;li&gt;创建具有具体项目的研究议程，并通过在这些研究议程中发表早期工作来证明其学术可行性，这将极大地帮助招募学者[7]。&lt;/li&gt;&lt;li&gt;创建具有重型工程倾向的具体研究项目，并明确解释为什么这些项目与对齐相关，这似乎是招聘工程师的一个重要瓶颈[7]。&lt;/li&gt;&lt;li&gt;为AIS制定一个有前途的研究议程，并招募初级研究人员在高级研究人员的指导下做出贡献[7]。&lt;/li&gt;&lt;li&gt;撰写一篇深入的分析文章，讨论 Ryan Greenblatt 对雄心勃勃的机械解释的看法，概述为什么它比狭隘或有限的方法更受青睐，并阐述对人工智能开发的更广泛影响 [7]。&lt;/li&gt;&lt;li&gt;撰写一篇推测性文章，探讨未对齐的人工智能（称为“外星木星大脑”）控制数据中心的场景，讨论潜在的控制失败以及为什么这样的结果可能仍然是可控的 [7]。&lt;/li&gt;&lt;li&gt;对各种人工智能优化和微调方法（如 RLHF、BoN、量化、DPO 等）进行全面比较，分析它们的理论等效性、实际差异以及优化能力的影响 [7]。&lt;/li&gt;&lt;li&gt;写一篇有说服力的文章，强调在研究中纳入“相关工作”部分的重要性，讨论它如何使该领域受益、促进协作并避免多余的工作[7]。&lt;/li&gt;&lt;li&gt;创建一篇详细的帖子，讨论人工智能技术研究之外的各种关键角色，并就社区如何更好地鼓励和支持多样化的职业道路提供建议 [7]。&lt;/li&gt;&lt;li&gt;起草一篇意见文章，论证为什么人工智能社区应该减少对地位的重视，讨论地位考虑的负面影响，并建议更健康的话语的替代重点领域[7]。&lt;/li&gt;&lt;li&gt;研究并提出如何将长期规划更有效地纳入政治进程的策略，尽管当前的政治结构阻碍了长期思考[8]。&lt;/li&gt;&lt;li&gt;制作一系列政策简报或研讨会，讨论如何利用政府来实现大规模变革，包括历史实例和未来前景，特别是在人工智能和技术政策的背景下[8]。&lt;/li&gt;&lt;li&gt;对立法过程进行详细分析，以确定人工智能安全法案的战略干预点并制定两党支持策略[9]。&lt;/li&gt;&lt;li&gt;建立人工智能公司承担损害赔偿责任的法律先例[10]。&lt;/li&gt;&lt;li&gt;调查并公开论证支持/反对爆炸性增长的可能性和风险[12]。&lt;/li&gt;&lt;li&gt;描绘了一幅伟大成果的图画[12]。&lt;/li&gt;&lt;li&gt;对可能带来爆炸性技术增长的问题的政策分析[12]。&lt;/li&gt;&lt;li&gt;关于如何应对情报爆炸的规范/建议[12]。&lt;/li&gt;&lt;li&gt;分析：什么技术可以改变格局？[12]。&lt;/li&gt;&lt;li&gt;实验室应该有答案的一大堆问题&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;支持人工智能治理人员&lt;/h2&gt;&lt;p&gt;&lt;i&gt;注意：这个子列表完全是我们从 ENAIS 建议的。它比以前的大多数项目更加抽象，我们感谢您提供任何反馈来具体化这些项目想法。&lt;/i&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;将他们与从事类似主题的其他人联系起来&lt;/li&gt;&lt;li&gt;创建与欧盟治理相关的最新文件数据库&lt;/li&gt;&lt;li&gt;帮助他们找到新朋友&lt;ul&gt;&lt;li&gt;高级的&lt;/li&gt;&lt;li&gt;初级&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;促进美国与欧盟及其他国家之间的联系&lt;/li&gt;&lt;li&gt;为进入该领域的新人提供指导计划&lt;/li&gt;&lt;li&gt;在欧盟复制美国现有自动取款机的技能提升计划&lt;/li&gt;&lt;li&gt;帮助拨款/筹款&lt;/li&gt;&lt;li&gt;帮助他们填补技术知识空白&lt;ul&gt;&lt;li&gt;将他们与解释技术风险的人员联系起来&lt;/li&gt;&lt;li&gt;帮助他们拥有相关专业知识的人&lt;/li&gt;&lt;li&gt;帮助他们了解甚至需要哪些技术知识。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;行动主义&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;公开要求人工智能实验室公开其确保系统安全和一致的策略。&lt;/li&gt;&lt;li&gt;要求人工智能实验室发布内部风险评估结果或摘要。&lt;/li&gt;&lt;li&gt;要求人工智能实验室公布外部审查工作的结果或摘要，除非这会不适当地泄露专有信息或本身会产生重大风险。&lt;/li&gt;&lt;li&gt;要求人工智能实验室就他们如何做出有关模型开发和部署的高风险决策发表公开声明。&lt;/li&gt;&lt;li&gt;鼓励并促进社区成员公开声明有关人工智能风险和政策的立场，以提高透明度和问责制。&lt;/li&gt;&lt;li&gt;邮寄您的代表并要求他们阅读有关人工智能风险的信息并在议会/委员会中进行讨论[13]。&lt;/li&gt;&lt;li&gt;&lt;a href="https://pauseai.info/protests"&gt;&lt;u&gt;参加&lt;/u&gt;&lt;/a&gt;或&lt;a href="https://pauseai.info/organizing-a-protest"&gt;&lt;u&gt;组织抗议活动&lt;/u&gt;&lt;/a&gt;，以提高记者、公众和决策者的意识，并敦促他们实施合理的法规[13]。&lt;/li&gt;&lt;li&gt;筹备首尔人工智能安全峰会[13]&lt;ul&gt;&lt;li&gt;帮助与会者，以便他们在峰会前得到适当的通知。&lt;/li&gt;&lt;li&gt;致力于制定一项防止危险人工智能诞生的条约草案。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://pauseai.info/join"&gt;&lt;u&gt;帮助&lt;/u&gt;&lt;/a&gt;参与/招募志愿者[13]。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;责任&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;司法路径：建立人工智能企业承担损害赔偿责任的判例[10]。&lt;/li&gt;&lt;li&gt;监管路径：让维持责任相关规则​​的监管机构做出规则澄清，甚至制定新规则，明确人工智能公司对各种损害承担责任[10]。&lt;/li&gt;&lt;li&gt;立法路径：让州和/或联邦立法者通过法律，明确规定人工智能公司对各种损害承担责任[10]。&lt;/li&gt;&lt;li&gt;找到因幻觉而受到某种程度损害的广大人群，并对构建大型语言模型的公司提起集体诉讼[10]。&lt;/li&gt;&lt;li&gt;找到一些成为大量 Deepfake 对象的名人或政治家，并对模型制作了一堆 Deepfake 的公司提起诉讼[10]。&lt;/li&gt;&lt;li&gt;找到一些因员工/承包商使用大型语言模型伪造报告、文章等而受到严重损害的公司/组织，然后起诉其模型生成这些报告/文章等的公司[10]。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;监管调查和小规模干预&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;深入理解拟议的规则和规则制定者自己的目标[10]。&lt;/li&gt;&lt;li&gt; “解决平衡”他们创造的激励措施（特别是寻找公司可能利用的漏洞）[10]。&lt;/li&gt;&lt;li&gt;建议实施细节，例如堵住漏洞或以其他方式调整激励措施，既推进 X 风险降低目标又符合规则制定者自己的目标[10]。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;其他的&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;创建一个在线仪表板，聚合、综合和可视化来自 Metaculus 等各种预测平台以及与高级人工智能开发相关的其他预测市场的预测。该项目旨在使专家和社区预测的获取民主化，使包括政策制定者、研究人员和公众在内的更广泛受众更容易了解人工智能的潜在未来。它将涉及设计直观的用户界面并实现允许用户探索不同场景的交互功能。该项目还将包括推广计划，以促进使用仪表板作为人工智能治理和开发中知情决策的资源[1]。&lt;/li&gt;&lt;li&gt;开发针对人工智能领域的研究人员和专业人士的专门写作研讨会或在线课程，重点关注提高写作技能，以实现更清晰的沟通以及更好的政策和研究文档[1]。&lt;/li&gt;&lt;li&gt;制定专门针对技术研究人员的培训计划以提高沟通技能[5]。&lt;/li&gt;&lt;li&gt;创建一个导师计划，将初级研究人员与经验丰富的专业人士配对，以提高他们的分析和解决问题的能力，重点是分解和解决复杂的人工智能相关问题[1]。&lt;/li&gt;&lt;li&gt;一项研究或报告，对与人工智能发展相关的存在风险的主要来源进行识别和分类[3]。&lt;/li&gt;&lt;li&gt;一个分析项目，重点是了解中国的人工智能能力并评估其成为人工智能超级大国的可能性[3]。&lt;/li&gt;&lt;li&gt;一篇关于人工智能技术潜在的重大军事兴趣以及军事人工智能大型项目的可能性的帖子[3]。&lt;/li&gt;&lt;li&gt;一项全面的研究或一系列专家访谈，旨在预测随着更多利益团体加入辩论，人工智能治理领域将如何发展，同时考虑变革性经济应用、失业、虚假信息和关键决策自动化的影响[3​​]。&lt;/li&gt;&lt;li&gt;建立安全事故举报平台。国家行为者和其他实验室以及公众都可以使用它。任何人都可以添加事件。可以作为安全事件的数据库[4]。注：已经有类似的平台（ &lt;a href="https://incidentdatabase.ai/"&gt;&lt;u&gt;https://incidentdatabase.ai/&lt;/u&gt;&lt;/a&gt; ）&lt;/li&gt;&lt;li&gt;创建一个平台或一系列活动，旨在加强人工智能治理专业人员之间的网络[5]。&lt;/li&gt;&lt;li&gt;创建一个计划/调查/……积极让当前政策制定者参与有关人工智能安全和治理的讨论[5]。&lt;/li&gt;&lt;li&gt;创建计划，通过奖学金、实习或导师计划来识别和支持有前途的机器学习博士和研究生政策学生。在其他地方复制成功的计划，例如欧盟 [5]。&lt;/li&gt;&lt;li&gt;启动一个项目，旨在将不同观点纳入人工智能安全工作，超越传统上参与 EA 社区的观点 [5]。&lt;/li&gt;&lt;li&gt;制定材料和策略，将人工智能安全作为大多数 EA 外展工作的一个原因领域 [5]。&lt;/li&gt;&lt;li&gt;创建不采用 EA 框架的 AI 安全推广计划，让更广泛的受众能够接触到它们 [5]。&lt;/li&gt;&lt;li&gt; （如果您有学历并且相对资深）将高级学者和研究工程师 (RE) 与教授或其他高级 RE 联系起来，他们可以帮助回答更多问题，并且可能比没有太多清晰资历的初级人员更有说服力。请注意，我不建议这样做，除非[7]。&lt;/li&gt;&lt;li&gt;创建一个结构化的导师计划来指导人工智能安全（AIS）领域的年轻研究人员，重点是提供具体的项目和研究管理指导[7]。&lt;i&gt;注意：已经有垫子了。 （根据经验，Astra 并不真正适合初级人员）&lt;/i&gt;&lt;/li&gt;&lt;li&gt;建立支持体系，鼓励和帮助有前途的研究人员攻读博士学位，特别是在AIS相关领域[7]。&lt;/li&gt;&lt;li&gt;与学术界和 AIS 组织合作开发实习计划，旨在利用现有的指导能力并提供全职就业的途径 [7]。&lt;/li&gt;&lt;li&gt;作为一名博士生并影响你的教授/实验室伙伴。这里最大的影响可能是在对 AIS 感兴趣的研究人员较少的地方攻读博士学位，而不是去没有任何 AIS 存在的大学 [7]。&lt;/li&gt;&lt;li&gt;设计有效利用现有指导能力的举措，可能通过更直接的途径获得 AIS 组织的全职职位，以解决研究指导和管理能力的缺乏 [7]。&lt;/li&gt;&lt;li&gt;外展工作涉及 AI 安全社区与 (a) AI 实验室成员 + (b) ML 社区成员之间的互动 [11]。&lt;ul&gt;&lt;li&gt;更多的会议将来自不同小组的研究相似主题的研究人员聚集在一起（例如，Anthropic 最近与来自不同人工智能实验室和人工智能联盟组织的成员组织了一次可解释性务虚会）。&lt;/li&gt;&lt;li&gt;更多将来自不同群体的战略/治理思想家聚集在一起的会议（例如，Olivia 和 Akash 最近与人工智能实验室的一些成员和成员一起举办了一次小型的为期 1 天的战略务虚会）。&lt;/li&gt;&lt;li&gt;类似于&lt;a href="https://www.lesswrong.com/s/n945eovrA3oDueqtq"&gt;&lt;u&gt;MIRI 2021 对话的&lt;/u&gt;&lt;/a&gt;讨论，只不过更加强调通过直接触及主要人工智能实验室的研究人员和决策者的症结来与他们进行互动。&lt;/li&gt;&lt;li&gt;涉及与人工智能实验室协调的干预措施合作（例如，弄清楚是否有办法在研究项目上进行合作、努力实施出版政策和信息共享协议、努力监测正在开发通用人工智能的新参与者等）&lt;/li&gt;&lt;li&gt;更多 ML 社区推广。例如， &lt;a href="https://safe.ai/"&gt;&lt;u&gt;AI 安全中心&lt;/u&gt;&lt;/a&gt;（由 Dan Hendrycks 领导）和&lt;a href="https://forum.effectivealtruism.org/posts/ozm4SpiChfAAAGnw5/announcing-the-ai-safety-field-building-hub-a-new-effort-to"&gt;&lt;u&gt;AIS 现场建设中心&lt;/u&gt;&lt;/a&gt;（由 Vael Gates 领导）的项目。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;减少不良行为者的力量[12]。&lt;/li&gt;&lt;/ul&gt;&lt;h1&gt;参考&lt;/h1&gt;&lt;p&gt;[1] &lt;a href="https://forum.effectivealtruism.org/posts/gsPmsdXWFmkwezc5L/some-talent-needs-in-ai-governance"&gt;&lt;u&gt;EA——人工智能治理的一些人才需求&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[2] &lt;a href="https://forum.effectivealtruism.org/posts/ApiZbPmfiPSXXw8dR/ai-governance-and-strategy-priorities-talent-gaps-and"&gt;&lt;u&gt;EA - 人工智能治理与战略：优先事项、人才缺口和机遇&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[3] &lt;a href="https://forum.effectivealtruism.org/posts/8KhGio2rhgHgsBoZ6/a-summary-of-current-work-in-ai-governance"&gt;&lt;u&gt;EA——人工智能治理当前工作总结&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[4] &lt;a href="https://www.governance.ai/research-paper/towards-best-practices-in-agi-safety-and-governance"&gt;&lt;u&gt;GovAI - 迈向通用人工智能安全和治理的最佳实践：专家意见调查&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[5] &lt;a href="https://forum.effectivealtruism.org/posts/vzuqnPyfDFjtbCpgv/ai-safety-field-building-survey-talent-needs-infrastructure"&gt;&lt;u&gt;EA - 人工智能安全领域建设调查：人才需求、基础设施需求以及与 EA 的关系&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[6] &lt;a href="https://www.lesswrong.com/posts/ho63vCb2MNFijinzY/agi-safety-career-advice#List_of_governance_topics"&gt;&lt;u&gt;LW-AGI安全职业建议&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[7] &lt;a href="https://www.lesswrong.com/posts/6FkWnktH3mjMAxdRT/what-i-would-do-if-i-wasn-t-at-arc-evals"&gt;&lt;u&gt;LW - 如果我不在 ARC Evals 我会做什么&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[8] &lt;a href="https://www.lesswrong.com/posts/DJB82jKwgJE5NsWgT/some-cruxes-on-impactful-alternatives-to-ai-policy-work"&gt;&lt;u&gt;LW - 人工智能政策工作的影响力替代方案的一些症结&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[9] &lt;a href="https://www.lesswrong.com/posts/2sLwt2cSAag74nsdN/speaking-to-congressional-staffers-about-ai-risk"&gt;&lt;u&gt;LW - 与国会工作人员谈论人工智能风险&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[10] &lt;a href="https://www.lesswrong.com/posts/qCstMcRJcjNtdNW7r/what-i-would-do-if-i-were-working-on-ai-governance"&gt;&lt;u&gt;LW - 如果我从事人工智能治理我会做什么&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[11] &lt;a href="https://www.lesswrong.com/posts/BbM47qBPzdSRruY4z/instead-of-technical-research-more-people-should-focus-on"&gt;&lt;u&gt;LW——更多的人应该专注于争取时间，而不是技术研究&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[12]&lt;a href="https://forum.effectivealtruism.org/s/xvie6gNqdSi29r4MM/p/E4RgB9EFYnJf7qhMv"&gt;&lt;u&gt;项目思路：爆炸性技术增长期间的治理&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[13]来自PauseAI的项目想法&lt;/p&gt;&lt;h1&gt;致谢&lt;/h1&gt;&lt;p&gt;感谢 Lawrence Clan、Joep Meindertsma 和 Plex 提供有关该列表的反馈。&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/MPMsnpHkHoRQxarnG/list-of-projects-that-seem-impactful-for-ai-governance#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sun, 14 Jan 2024 17:25:25 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/MPMsnpHkHoRQxarnG/list-of-projects-that-seem-impactful-for-ai-governance</guid></item><item><title>利罗伊·詹金斯原则：错误的人工智能如何保证“警告射击”</title><link>https://www.lesswrong.com/posts/LZQm7osgjh5tBbEfF/the-leeroy-jenkins-principle-how-faulty-ai-could-guarantee</link><description>发布于 2024 年 1 月 14 日下午 3:03（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/LZQm7osgjh5tBbEfF/the-leeroy-jenkins-principle-how-faulty-ai-could-guarantee#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sun, 14 Jan 2024 15:03:21 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/LZQm7osgjh5tBbEfF/the-leeroy-jenkins-principle-how-faulty-ai-could-guarantee</guid></item><item><title>注意人们的方向何时正确</title><link>https://www.lesswrong.com/posts/TgKGpTvXQmPem9kcF/notice-when-people-are-directionally-correct</link><description>发布于 2024 年 1 月 14 日下午 2:12（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;我去年开始观看&lt;a href="https://www.youtube.com/@ZeihanonGeopolitics"&gt;Peter Zeihan 的&lt;/a&gt;视频。&lt;/p&gt;&lt;p&gt;他分享了很多有趣的信息，尽管他似乎对厄运和忧郁抱有非常强烈的偏见。&lt;/p&gt;&lt;p&gt;在我看来，有一件事特别荒谬：他声称，随着美国不再确保水域自由，全球贸易将因海盗行为而崩溃。&lt;/p&gt;&lt;p&gt;我的第一反应是：“这不是 17 世纪！海盗如今已不再是一个真正的问题。技术已经让他们过时了”。&lt;/p&gt;&lt;p&gt;鉴于此，当我听说胡塞叛军的导弹袭击导致大多数最大的航运公司决定避开曼德海峡并绕非洲航行时，我感到非常震惊。&lt;/p&gt;&lt;p&gt;这促使美国最近结成联盟以维护该地区的航运自由，并于最近进行了空袭作为报复。如果整个问题相对较快地得到解决，我不会感到惊讶，如果发生这种情况，那么最容易做的事情就是回到我最初的信念：“愚蠢的我，我有一秒钟担心彼得·泽汉可能会正确，但那只是我陷入了轰动效应。整个事件显然永远不会有什么结果。我应该忘记这一切”。&lt;/p&gt;&lt;p&gt;我相信这是一个错误。人们很容易忘记这一点，但胡塞武装能够造成尽可能多的破坏，这超出了我的模型。我可以将其标记为异常事件，或者可以查看我的原始模型中是否有任何需要调整的地方。&lt;/p&gt;&lt;p&gt;我进行了这个练习，并想到了以下想法，我将传达这些想法，因为它们具有说明性：&lt;/p&gt;&lt;p&gt; • 我听到一些人在不同的背景下提出，许多国家一直在沿海航行并依赖美国进行防御，但这只是在我脑海中浮现，人们说这可能是真的，也可能不是真的。我还没有真正深入研究这一点，但我开始怀疑我应该更加重视这个信念。&lt;br /&gt; • 我没有考虑到海军实力较弱的国家可能需要很长的时间来发展更强大的海军。&lt;br /&gt; • 我没有考虑过海盗可能与更大的原始国家行为体结盟的可能性，而不是个人犯罪分子。&lt;br /&gt; • 我没有考虑到非国家行为者可能阻碍航运的可能性，以及其他国家出于外交考虑至少不愿意对该行为者采取行动。&lt;br /&gt; • 我没有想到西方一些人可能出于政治原因支持这样的演员。&lt;br /&gt; • 尽管我多年前就意识到索马里海盗问题，但我没有正确考虑到这一点。当国家变得严肃起来时，这些海盗很容易被击败，这可能在我的预测中发挥了作用，但我还需要更新这曾经是一个问题。&lt;br /&gt; • 忘记环境可能发生巨大变化：曾经看似不可能的事件经常发生。&lt;/p&gt;&lt;p&gt;我的观点是，我可以从这次事件中学到很多东西，即使它最终很快得到解决。&lt;/p&gt;&lt;p&gt;我怀疑很少有人能够真正完全掌握从特定事件中学到的所有教训（相反，我怀疑大多数人只是从事件中汲取教训并宣称自己已经从该事件中学到了教训）。&lt;/p&gt;&lt;p&gt;如果您没有进行大量的小更新，则您可能错过了应该进行的更新。&lt;/p&gt;&lt;p&gt; （我只是想指出，我喜欢“方向正确”的手柄。这样说“我不认为 X 在所有点上都是正确的，但我认为他们的很多点都是正确的”要容易得多。 ）。&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/TgKGpTvXQmPem9kcF/notice-when-people-are-directionally-correct#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sun, 14 Jan 2024 14:12:41 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/TgKGpTvXQmPem9kcF/notice-when-people-are-directionally-correct</guid></item><item><title>与大多数人工智能风险类比相比</title><link>https://www.lesswrong.com/posts/SnfjK9ALrzFJB8x7B/against-most-ai-risk-analogies</link><description>发布于 2024 年 1 月 14 日凌晨 3:36（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;我不喜欢人们使用的大多数人工智能风险类比。虽然我认为类比有助于第一次向人们解释一个概念，但我认为类比经常被误用，而且常常是有害的。根本问题是，类比一直被错误地认为，并且经常被故意用作特定人工智能风险立场的&lt;i&gt;论据&lt;/i&gt;。大多数时候，当以这种方式使用类比时，我认为它们具有误导性和不精确性，通常会传达特定的、可信的人工智能模型的错误印象，即使不存在这样的可信模型。&lt;/p&gt;&lt;p&gt;以下是我在人工智能风险背景下发现的类比示例的随机列表：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;a href="https://people.eecs.berkeley.edu/~russell/papers/russell-bostonglobe23-AI"&gt;斯图尔特·拉塞尔&lt;/a&gt;：“这并不完全像邀请一个高级外星物种来永远成为我们的奴隶，但它有点像那样。”&lt;/li&gt;&lt;li&gt; &lt;a href="https://80000hours.org/podcast/episodes/ajeya-cotra-accidentally-teaching-ai-to-deceive-us/#aliens-and-other-analogies-013822"&gt;罗布·维布林&lt;/a&gt;：“这有点像试图了解章鱼如何思考或如何行为——只不过章鱼还不存在，我们要做的就是研究它们的祖先海蜗牛，然后我们必须从中弄清楚成为章鱼是什么感觉。”&lt;/li&gt;&lt;li&gt; &lt;a href="https://twitter.com/ESYudkowsky/status/1633219449724760065"&gt;Eliezer Yudkowsky&lt;/a&gt; ：“这个人工智能扮演的角色不是人工智能。人工智能是一个看不见的女演员，目前正在扮演这个角色。如果人工智能变得更聪明，这可能会适得其反。”&lt;/li&gt;&lt;li&gt; &lt;a href="https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization#Discussion_of_a_problem"&gt;Nate Soares&lt;/a&gt; ：“我对人工智能进展的猜测是，在某个时刻，一些团队得到的人工智能开始足够好地泛化，远远超出其训练分布，它可以掌握物理、生物工程和心理学等领域[...] 在其能力飞跃发展的同一过程中，其对齐属性被揭示为肤浅的，并且无法概括。&lt;strong&gt;这里的中心类比是，优化猿类的包容性遗传适应性（IGF）并不会使由此产生的人类在心理上针对 IGF 进行了优化。&lt;/strong&gt; ”&lt;/li&gt;&lt;li&gt;&lt;a href="https://lukemuehlhauser.com/wiener-on-the-ai-control-problem-in-1960/"&gt;诺伯特·维纳&lt;/a&gt;：“当我们建造的机器能够以我们无法跟上的速度对其输入数据进行操作时，我们可能不知道何时将其关闭，直到为时已晚。我们都知道魔法师学徒的寓言……”&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.datanami.com/2023/05/03/ai-threat-like-nuclear-weapons-hinton-says/"&gt;杰弗里·辛顿（Geoffry Hinton）&lt;/a&gt; ：“这就像核武器。如果发生核战争，我们都会失败。这些东西接管也是如此。”&lt;/li&gt;&lt;li&gt; &lt;a href="https://www.lesswrong.com/posts/76etTtAiKtZGGzkmi/video-and-transcript-of-presentation-on-existential-risk"&gt;乔·卡尔史密斯&lt;/a&gt;：“我认为人工智能的一个更好的比喻是一种工程病毒，如果它泄露出去，它会变得越来越难控制，这是一个越来越大的问题。”&lt;/li&gt;&lt;li&gt; &lt;a href="https://80000hours.org/podcast/episodes/ajeya-cotra-accidentally-teaching-ai-to-deceive-us/#aliens-and-other-analogies-013822"&gt;Ajeya Cotra&lt;/a&gt; ：“从某种意义上说，企业可能是比整个经济更好的类比：它们是由这些人类部分组成的，但最终往往追求的东西实际上并不是目标和目标的简单平均数。组成这台机器的人类的愿望，这台机器就是可口可乐公司之类的。”&lt;/li&gt;&lt;li&gt; &lt;a href="https://archive.is/K0Arj#selection-581.152-585.113"&gt;Ezra Klein&lt;/a&gt; ：“正如我的同事 Ross Douthat &lt;a href="https://archive.is/o/K0Arj/https://www.nytimes.com/2023/03/02/opinion/magic-science-ufo-ai.html"&gt;所写&lt;/a&gt;，这是一种召唤行为。施展这些咒语的程序员不知道什么会绊倒传送门。”&lt;/li&gt;&lt;li&gt; &lt;a href="https://forum.effectivealtruism.org/posts/zsFCj2mfnYZmSW2FF/ai-risk-is-like-terminator-stop-saying-it-s-not-1"&gt;SKLUUG：&lt;/a&gt; “人工智能风险就像&lt;i&gt;终结者&lt;/i&gt;！人工智能可能会变得非常聪明，并决定杀死我们所有人！我们需要对此采取一些措施！”&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;这些类比涵盖的范围很广，其中许多类比有时确实有助于传达有意义的信息。我的观点并不是说它们毫无用处，而是这些类比通常很肤浅且具有误导性。这些类比几乎没有对真实人工智能的行为和运作产生任何重要影响，但仍然给人留下了我们应该如何思考人工智能的模型的&lt;i&gt;印象&lt;/i&gt;。&lt;/p&gt;&lt;p&gt;并注意这些类比如何给人一种连贯的人工智能模型的印象，即使演讲​​者没有直接断言它&lt;i&gt;是&lt;/i&gt;一个模型。不管演讲者的意图如何，我认为&lt;i&gt;实际的&lt;/i&gt;效果往往是在听众的脑海中植入一幅详细但虚假的画面，从而引发关于真正的人工智能在未来如何运作的似是而非的想法。由于相似之处非常浅，因此从这些类比中得出的推理往往是不可靠的。&lt;/p&gt;&lt;p&gt;另外，这些类比经常是&lt;i&gt;有选择性地&lt;/i&gt;选择的——基于唤起特定的偏好图像而选择，而不是基于识别可能的最&lt;i&gt;自然&lt;/i&gt;的比较点。考虑&lt;a href="https://80000hours.org/podcast/episodes/ajeya-cotra-accidentally-teaching-ai-to-deceive-us/#aliens-and-other-analogies-013822"&gt;Ajeya Cotra 的这个例子&lt;/a&gt;，&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; &lt;strong&gt;Rob Wiblin：&lt;/strong&gt;我想花一点时间谈谈人们用来推理所有这些问题的不同类比和不同的心理图像。 [...]您认为还有其他值得强调的心理模型或类比吗？&lt;/p&gt;&lt;p&gt; &lt;strong&gt;Ajeya Cotra：&lt;/strong&gt;我听过的一个播客实际上做了另一个类比——这是一个艺术播客，随着人工智能艺术开始真正起飞，关于人工智能的一集也是如此——这就像你在抚养一只狮子幼崽，或者你有这些人抚养黑猩猩宝宝，而你正试图引导他们走向正确的方向。也许它非常可爱、迷人，但从根本上来说它与你格格不入。无论你多么努力地抚养它或引导它，它都可能在它成年后撕掉你的脸。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;当“金毛猎犬”同样有效时，科特拉选择“黑猩猩”作为比较点有什么原因吗？很难知道，但似乎她没有选择金毛猎犬，因为这会破坏她的一般论文。&lt;/p&gt;&lt;p&gt;我同意，如果她的目标是传达错位的&lt;i&gt;逻辑可能性&lt;/i&gt;，那么与金毛猎犬的类比可能行不通。但如果她的目标是传达错位的&lt;i&gt;合理性&lt;/i&gt;，或者类似我们应该如何看待人工智能的“心智模型”之类的东西，我认为没有充分的理由更喜欢其中一种。一个类比唤起负面形象，另一个类比唤起正面形象，这一事实本身似乎并没有任何使用偏好的基础。&lt;/p&gt;&lt;p&gt;或者考虑与人类进化的类比。如果你试图表达内在错位的&lt;i&gt;逻辑可能性&lt;/i&gt;，那么与人类进化的类比是有道理的。但是，如果你试图传达内在错位的&lt;i&gt;合理性&lt;/i&gt;，或者内在错位的心理模型，为什么不选择将这种情况与&lt;a href="https://www.lesswrong.com/posts/FyChg3kYG54tEN3u6/evolution-is-a-bad-analogy-for-agi-inner-alignment"&gt;人类一生中的学习&lt;/a&gt;进行类比呢？事实上，正如昆廷·波普（Quintin Pope） &lt;a href="https://www.lesswrong.com/posts/hvz9qjWyv8cLX9JJR/evolution-provides-no-evidence-for-the-sharp-left-turn#Human__misalignment__with_inclusive_genetic_fitness_provides_no_evidence_for_AI_misalignment"&gt;所解释的&lt;/a&gt;那样，进化论的类比似乎有一些很大的缺陷：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; “祖先环境中的人类行为”与“现代环境中的人类行为”并不是训练和部署环境之间行为差异的有效示例。人类并不是在祖先环境中“训练”，然后在现代环境中“部署”的。相反，人类在一生中不断地接受“训练”（通过奖励信号和感官预测错误信号）。人类在祖先和现代环境中的“训练运行”是不同的。&lt;/p&gt;&lt;p&gt;因此，人类进化不是以下例子：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;我们在环境 A 中训练系统。然后，经过训练的系统处理来自环境 B 的不同输入分布，现在系统的行为有所不同。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;这是一个例子：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;我们在环境 A 中训练了一个系统。然后，我们在环境 B 的不同输入分布上训练了同一系统的&lt;i&gt;新版本&lt;/i&gt;，现在这&lt;i&gt;两个不同的系统&lt;/i&gt;表现不同。&lt;/p&gt;&lt;/blockquote&gt;&lt;/blockquote&gt;&lt;p&gt;许多人工智能风险的支持者似乎很乐意在&lt;i&gt;不&lt;/i&gt;支持预期结论（例如&lt;a href="https://www.lesswrong.com/posts/RcZeZt8cPk48xxiQ8/anthropomorphic-optimism"&gt;拟人类比）&lt;/a&gt;时批评类比。有时，他们甚至会批评&lt;a href="https://80000hours.org/podcast/episodes/ajeya-cotra-accidentally-teaching-ai-to-deceive-us/#aliens-and-other-analogies-013822"&gt;他们想象的其他人使用的类比&lt;/a&gt;，例如“它就像烤面包机”或“它就像谷歌地图”。当然，在这些情况下，他们可以轻松识别缺陷：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; &lt;strong&gt;Ajeya Cotra：&lt;/strong&gt;我认为谷歌地图与所有这些东西和人工智能系统之间真正的不相似之处在于，我们没有以与生产谷歌地图相同的方式生产这些人工智能系统：由一些人坐下来思考它应该是什么样子就像，然后编写代码来确定它应该是什么样子。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;需要明确的是，我同意谷歌地图是一个糟糕的类比。但黑猩猩的比喻真的好得多吗？难道我们不应该对我们自己的类比应用同样程度的严格性吗？&lt;/p&gt;&lt;p&gt;我的观点不是“使用不同的类比”。我的观点是，我们&lt;i&gt;首先&lt;/i&gt;应该&lt;i&gt;停止依赖类比&lt;/i&gt;。请改用详细的对象级参数！&lt;/p&gt;&lt;p&gt;虽然类比的目的是提供知识来代替无知——解释一种见解或一个概念——但我相信许多人工智能风险类比主要是误导或迷惑人们，而不是启发他们；他们可以插入不必要的错误&lt;i&gt;假设&lt;/i&gt;来代替真正的理解。它们想要传达的基本概念可能很值得理解，但伴随着这个概念的是一大堆额外的猜测。&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;部分原因是我不会分享其他人对人工智能未来实际情况的看法。这只是我论点的一小部分，因为我的主要观点是，我们应该少用类比，而不是改用不同的类比来表达不同的画面。但我对未来看法的这种差异仍然在我对人工智能风险类比的使用感到沮丧的过程中发挥着重要作用。&lt;/p&gt;&lt;p&gt;例如，也许你认为外星人和动物的类比很棒，但我完全不明白其中的原因。但我仍然很难看到这一点。至少，让我比较一下我的照片，也许你能明白我来自哪里。&lt;/p&gt;&lt;p&gt;在我看来，默认的画面——在我看来，这就像对 2024 年当前趋势到中期未来的直接推断，因为人工智能匹配并开始略微超过人类智能——看起来与大多数人所描绘的漫画完全不同。标准类比。与人工智能将成为外星人的模型相反，我预计人工智能将直接诞生于我们的社会，由我们有意塑造，目的是填补我们世界中大部分人形的漏洞。他们将在社会上与我们融为一体，并可能在很大程度上分享我们关于社会和物理世界的概念，接受过我们的数据培训并能流利地使用我们的语言。他们数量众多、无处不在，不断地与我们互动、帮助我们、与我们合作，甚至为亿万人民提供友谊。人工智能将由我们评估、检查和选择，它们的行为将直接由我们的工程决定。&lt;/p&gt;&lt;p&gt;我觉得这张图是现有趋势的相对简单的延伸，法学硕士&lt;i&gt;已经&lt;/i&gt;接受了对我们友善和乐于助人的培训，并与我们合作，首先受到我们综合文化输出的影响。我预计，在可预见的未来，这种融入我们社会的趋势将会加剧，因为消费者对人们可以信任并愿意与之互动的人工智能会有需求。&lt;a href="https://aiimpacts.org/discontinuous-progress-investigation/"&gt;进展可能是渐进式的&lt;/a&gt;，而不是随着超级强大特工的到来而突然出现。也许最重要的是，我预计随着人工智能开始产生大规模影响&lt;a href="https://www.lesswrong.com/posts/EaZghEwcCJRAuee66/my-thoughts-on-the-social-response-to-ai-risk"&gt;，监督和监管将随着时间的推移而急剧增加&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;我无意在此描绘一幅一致乐观的景象。在我所呈现的场景中，仍然有很多可能出错的地方。其中大部分内容都没有具体说明，因为&lt;a href="https://forum.effectivealtruism.org/posts/zrSx3NRZEaJENazHK/why-i-think-it-s-important-to-work-on-ai-forecasting"&gt;我根本不知道未来会发生什么&lt;/a&gt;。但至少，也许你现在可以同情我的感觉，鉴于我的观点，大多数现有的人工智能风险类比都非常令人沮丧。&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;再次强调，我并不是说类比在人工智能风险讨论中没有地位。我自己也确实使用过它们很多次。但我认为它们可以，而且经常被粗心地使用，并且似乎经常将各种关于未来人工智能将如何表现&lt;i&gt;的错误&lt;/i&gt;说明放入人们的心理模型中，即使进行类比的人没有任何意图。在我看来，总体而言，如果我们减少对人工智能风险类比的依赖，并用特定的对象级点代替它们，情况会好得多。&lt;/p&gt;&lt;br /&gt;&lt;br /&gt;&lt;a href="https://www.lesswrong.com/posts/SnfjK9ALrzFJB8x7B/against-most-ai-risk-analogies#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sun, 14 Jan 2024 03:36:16 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/SnfjK9ALrzFJB8x7B/against-most-ai-risk-analogies</guid></item><item><title>用脸投票</title><link>https://www.lesswrong.com/posts/EitMbGASqTAuyWPJA/vote-with-your-face</link><description>发布于 2024 年 1 月 14 日凌晨 3:30（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;&lt;span&gt;从 7 月 2 日的舞蹈开始，&lt;/span&gt; &lt;a href="https://www.bidadance.org/"&gt;BIDA&lt;/a&gt;一半的舞蹈都是&lt;a href="https://blog.bidadance.org/2023/06/some-mask-optional-dances.html"&gt;不戴面具的&lt;/a&gt;。有些人只想在知道地板上的每个人都戴着&lt;a href="https://blog.bidadance.org/2022/05/requiring-high-filtration-masks.html"&gt;高过滤口罩的&lt;/a&gt;情况下参加，其他人只想在不需要口罩的情况下参加，当然其他人无论如何都会或不会参加。提供每种舞蹈中的一些，让人们选择适合自己喜好的舞蹈。&lt;/p&gt;&lt;p&gt;另外，作为一个喜欢实验的人，这几乎就像一个关于出勤戴口罩要求的随机对照试验！&lt;/p&gt;&lt;p&gt;在我们改为要​​求和可选佩戴口罩交替使用之前，我们不知道这会对出勤率产生什么影响：在可选佩戴口罩的夜晚（更有趣！）还是在需要佩戴口罩的夜晚（更安全！）会有更多的人。回顾一下出席情况，我发现：&lt;/p&gt;&lt;p&gt; &lt;a href="https://www.jefftk.com/bida-attendance-by-mask-status-big.png"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/EitMbGASqTAuyWPJA/jncoepfisn8qg4hz7ujn" /&gt;&lt;/a&gt;&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;请注意，除了只有十一种舞蹈外，您可能还需要调整某些舞蹈的一些独特方面：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt; 7/2 舞蹈是第一个可选面具舞蹈，因此可能会受到那些很高兴不戴面具跳舞但不会每次都来的人们的鼓舞。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt; 7/2 和 9/3 舞会是我们夏季前的最后一场舞会，也是回归后的第一场舞会，这可以增加上座率。但同时也是假期周末，这可能会减少出席人数。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt; 11/5和11/19的舞会都是双人舞，上座率普遍较高。但我们（大部分是偶然的）背靠背做了两次，每一次。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt; 12/3舞会是家庭舞会，这通常也意味着更高的出席率。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt; 12/17 的舞会在另一个地点萨默维尔军械库举行，这通常意味着出席率较低。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt; 1/7 舞会是在一场大暴风雪期间举行的，这通常意味着出席人数要少得多。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;总的来说，看起来不戴面具的夜晚更受欢迎，但考虑到所有的警告，这很难说。无论如何，差异显然没有大到足以成为停止其中之一的理由。我预计我们会继续轮流参加，因为我们仍然听到很多人说他们只想参加其中之一。&lt;/p&gt;&lt;p&gt; （所有这些都是代表我自己，而不是 BIDA 董事会。）&lt;/p&gt;&lt;br /&gt;&lt;br /&gt;&lt;a href="https://www.lesswrong.com/posts/EitMbGASqTAuyWPJA/vote-with-your-face#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sun, 14 Jan 2024 03:30:06 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/EitMbGASqTAuyWPJA/vote-with-your-face</guid></item><item><title>使用 MLP 线性化对稀疏自动编码器特征进行逆向工程的案例研究</title><link>https://www.lesswrong.com/posts/93nKtsDL6YY5fRbQv/case-studies-in-reverse-engineering-sparse-autoencoder</link><description>发布于 2024 年 1 月 14 日凌晨 2:06（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;&lt;i&gt;认知状态：初步/探索性。&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;作为 Neel Nanda 的 MATS 5.0（2023-2024 年冬季）研究冲刺的一部分进行的工作。&lt;/i&gt;&lt;/p&gt;&lt;p&gt; TL;DR：我们开发了一种方法，通过对 MLP 子层进行局部线性逼近，来理解 Transformer 模型中的稀疏自动编码器特征是如何从早期组件计算出来的。我们研究该特征如何在特定输入上激活，并采取措施通过检查模型权重来寻找与输入无关的解释。我们通过几个深入的案例研究演示了这种方法，以解释简单变压器（GELU-1L 和 GELU-2L）用于计算某些特定特征的机制，并验证它与因果方法的结果一致。&lt;/p&gt;&lt;h1&gt;介绍&lt;/h1&gt;&lt;p&gt;机械可解释性的核心目标是解决维度灾难，将神经网络的高维激活和参数分解为单独可理解的部分。&lt;a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html"&gt;稀疏自动编码器（SAE）&lt;/a&gt;是最近一项令人兴奋的发展，它使我们能够采用高维激活（可能处于叠加状态）并将它们分解为激活空间中代表（大部分）独立概念的有意义的方向。&lt;/p&gt;&lt;p&gt;当应用于 MLP 激活/输出时，SAE 的一个主要限制是很难研究如何根据早期模型组件的输出计算特征。有了有意义的神经元，我们可以直接查看它与早期组件的连接/&lt;a href="https://transformer-circuits.pub/2021/framework/index.html#residual-comms"&gt;虚拟权重&lt;/a&gt;——例如&lt;a href="https://distill.pub/2020/circuits/zoom-in/"&gt;，视觉模型中的汽车神经元是由车窗、车身和车轮神经元构建的&lt;/a&gt;——但 SAE 特征通常密集在神经元基础。这意味着，为了理解特征是如何计算的，我们需要理解数千个神经元激活的复杂非线性函数。维度的诅咒依然存在！&lt;/p&gt;&lt;p&gt;在这篇文章中，我们提出了一种技术来探索如何从前面的模型组件计算神经元密集的 SAE 特征。这种方法涉及采用 MLP 子层的导数，以获得 SAE 特征的局部线性逼近——这种技术在先前的工作（例如&lt;a href="https://www.neelnanda.io/mechanistic-interpretability/attribution-patching"&gt;属性修补）&lt;/a&gt;中已经取得了一些成功。重要的是，我们的方法更进一步，将这种线性近似与模型权重本身结合起来，以获得模型行为的&lt;i&gt;全局&lt;/i&gt;图像，而不是局限于特定的输入示例&lt;span class="footnote-reference" id="fnref5ns12vjb0f8"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn5ns12vjb0f8"&gt;[1]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。最终结果是一种获取有关模型如何计算 SAE 特征的独立于输入的信息的有效方法。&lt;/p&gt;&lt;p&gt;我们证明这种方法为一系列案例研究提供了有用的见解，使我们能够将特征逆向工程回到原始的令牌嵌入，并与因果干预的结果一致。我们研究这种近似的准确性和原则性。尽管它最终只是一个近似值，并且有时可能会崩溃，但我们相信这是一个有用的工具，可以让您更好地理解 SAE 特征的计算方式。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;免责声明&lt;/strong&gt;：这是非常初步的工作！我们认为这些结果都相当具有探索性；这篇文章并不试图对我们如何准确地理解这些 SAE 特征以及计算它们的机制做出强有力的声明。但我们希望我们的结果是有趣的，它们可以让其他人在它们的基础上进行构建，并且它们可能有助于为思考 SAE 提供更好的直觉。&lt;/p&gt;&lt;p&gt;这篇文章代表了我们作为 Neel Nanda 的 MATS 5.0 计划的一部分的两周冲刺的成果，我们将在该计划的其余部分继续以此为基础。如果您想以这些想法为基础，请联系我们！ （请随时在 LessWrong 上给我们留言，或者如果您愿意，请发送电子邮件至&lt;a href="mailto:jacob.dunefsky@yale.edu"&gt;jacob.dunefsky@yale.edu&lt;/a&gt; ）&lt;/p&gt;&lt;h2&gt;为什么这很重要？&lt;/h2&gt;&lt;p&gt;我们认为这是一个需要解决的重要问题，原因有很多：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;对 SAE 功能进行逆向工程使我们能够更有效地解释它们。&lt;/strong&gt;特别是，这样做可以揭示其他方法可能遗漏&lt;i&gt;的意外行为&lt;/i&gt;。例如，在我们的一个案例研究中，我们发现一个功能最初似乎只在令牌&lt;code&gt;(&amp;#39;&lt;/code&gt;上激活；然而，在应用逆向工程之后，我们发现该功能也在令牌&lt;code&gt;ह&lt;/code&gt; （&lt;a href="https://en.wiktionary.org/wiki/%E0%A4%B9"&gt;一个印地语字符）&lt;/a&gt;上激活我们对 SAE 功能理解上的差距限制了它们在预测和分析不安全或不需要的模型行为方面的效用，因此逆向工程对于帮助我们识别这些差距非常重要。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;对 SAE 特征进行逆向工程可能会揭示模型的新故障模式。&lt;/strong&gt;通过了解计算给定特征的算法，我们也许能够更好地理解导致模型表现出不良行为的原因。例如，如果我们可以在模型中找到重要的下游特征（例如候选人是否会在工作中表现良好）并将其追溯到输入的受保护特征（例如种族或性别），那么我们可以使用它来了解模型计算中的固有偏差。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;对 SAE 特征进行逆向工程可以帮助我们更好地提出关于特征通用性的理论主张。&lt;/strong&gt;例如，人们有兴趣了解不同的模型是否学习通用特征。 SAE 可以通过在不同模型的激活上训练 SAE 并比较它们学到的特征来解决这个问题（请参阅&lt;a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html#phenomenology-universality"&gt;Anthropic 的 SAE 论文中的“通用性”讨论&lt;/a&gt;）。由于逆向工程揭示了计算 SAE 特征的机制，因此它可以为评估通用性提供补充视角，让我们测试 SAE 是否不仅有助于学习通用特征，还有助于学习通用机制。它还可以帮助我们捕捉虚幻的“普遍性”，即特征表面上相似，但通过不同的机制计算，并在适当的情况下分开&lt;span class="footnote-reference" id="fnrefa1rxph5d74"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fna1rxph5d74"&gt;[2]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;对 SAE 功能进行逆向工程对于获得有关 SAE 一般功能和局限性的各种见解非常有用。&lt;/strong&gt;例如，在一个 2 层 Transformer 的实验中，我们使用我们的方法用第 0 层 SAE 特征来表达第 1 层 SAE 特征；我们发现第 0 层特征和第 1 层特征之间的连接很密集，这表明当前训练的 SAE 不容易产生有关同一模型不同层的特征如何相互关联的稀疏信息。这表明要么内部模型连接确实不稀疏，要么我们的 SAE 训练方法存在局限性，这两者都是有用的见解！我们预计，只有深入研究这些模型的内部结构以及它们如何组合在一起，我们才能发现有关模型和 SAE 的许多其他见解。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;如果没有逆向工程，SAE 功能就是一个不完整的故事。&lt;/strong&gt;仅在纯粹的美学层面上，我们发现对产生这些特征的计算一无所知是令人不满意的。&lt;/li&gt;&lt;/ul&gt;&lt;h1&gt;概述&lt;/h1&gt;&lt;p&gt;本文的其余部分分为以下部分：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;我们对用于进行逆向工程的方法进行了高级概述。&lt;/strong&gt;我们建议阅读本节，因为它为理解我们的案例研究中发生的情况提供了有用的背景。对明确的数学细节感兴趣的读者可能有兴趣阅读详细阐述此方法的附录部分。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;我们提供了许多案例研究，其中我们对特定的 SAE 功能进行了逆向工程。&lt;/strong&gt;这是我们文章的重点，我们希望大多数读者能够从中获得最大收益（尽管我们不希望所有读者都阅读所有案例研究）。这是一个很长的部分，我们在这里确实进行了非常深入的细节，但我们认为它可以帮助读者理解这些特征的计算方式以及逆向工程过程的总体工作原理。以下案例研究摘要可能会帮助您决定要进一步了解哪些案例：&lt;ul&gt;&lt;li&gt; GELU-1L 中的一项功能的案例研究，该功能主要在令牌上触发&lt;code&gt;(&amp;#39;&lt;/code&gt; .&lt;ul&gt;&lt;li&gt;在本案例研究中，我们使用逆向工程来揭示看似单义的特征有时会在不相关的标记上触发，例如&lt;code&gt;ह&lt;/code&gt; ，这是使用这种方法构建对抗性提示的概念验证，以及如何使用这种方法的示例了解意外行为。&lt;/li&gt;&lt;li&gt;我们还发现注意力头 0 和 3 通过触发指示代码相关或列表相关上下文的标记来贡献该功能。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt; GELU-1L 中一个特征的案例研究，当前面有标点符号时，该特征往往会触发二元词“it is”。&lt;ul&gt;&lt;li&gt;在此案例研究中，我们发现直接路径通过触发令牌来贡献该特征，这&lt;code&gt;is&lt;/code&gt;预期的那样，并且注意力头 0 触发了前面的&lt;code&gt;It&lt;/code&gt;令牌。这使我们能够理解计算二元特征的主题。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt; GELU-1L 中倾向于在令牌&lt;code&gt;&amp;#39;t&lt;/code&gt;上触发的功能的案例研究。&lt;ul&gt;&lt;li&gt;在这个案例研究中，我们发现直接路径非常容易解释。线性化表明注意力是无关紧要的，但更可靠的重采样消融因果干预表明它确实很重要，这表明线性化在这里具有误导性。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt; GELU-1L 中倾向于在代币上引发的功能的案例研究&lt;code&gt;is&lt;/code&gt;在神学/政治背景下进行的。&lt;ul&gt;&lt;li&gt;在这个案例研究中，我们研究了模型如何主要通过单个可解释的注意力头来计算上下文相关的特征。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt; GELU-2L 中某个功能的案例研究，该功能往往会在&lt;code&gt;{&amp;#39;name&amp;#39;: &amp;#39;&lt;/code&gt;等字符串上触发。&lt;ul&gt;&lt;li&gt;在本案例研究中，我们在两层模型中执行逆向工程，包括查看第 1 层 SAE 特征和第 0 层 SAE 特征之间的连接。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;我们更深入地讨论 MLP 线性化，提供实验结果，以便开始了解这种方法在哪里有效以及在哪里失败。&lt;/strong&gt;大多数读者可能会跳过本节，尽管对 Transformer 或该方法的有效性有更多理论兴趣的读者可能会从阅读中有所收获。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;与路径修补等因果方法相比，我们讨论了我们的方法的优点和缺点。&lt;/strong&gt;我们建议阅读本简短的部分，以便了解我们的方法与更广泛的机械可解释性方法的契合点。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;我们从整体上反思该方法及其优点和局限性。&lt;/strong&gt;我们建议您阅读本简短的部分，以校准您对该方法的用处以及未来的方向可能是什么样子的想法。&lt;/li&gt;&lt;/ul&gt;&lt;h1&gt;我们的方法&lt;/h1&gt;&lt;p&gt;在本节中，我们将简要介绍我们的 SAE 功能逆向工程方法。如需更详细的解释，感兴趣的读者应该查看“附录：我们方法的详细信息”部分。&lt;/p&gt;&lt;p&gt;我们应用并扩展了&lt;a href="https://www.lesswrong.com/posts/jDfjqu2qJLcPco9cf/automatically-finding-feature-vectors-in-the-ov-circuits-of"&gt;这篇文章&lt;/a&gt;和&lt;a href="https://arxiv.org/abs/2312.16291"&gt;本文&lt;/a&gt;中描述的方法，以实现逆向工程 SAE 功能。我们将首先了解它在 1 层变压器中的工作原理，然后了解如何扩展它。&lt;/p&gt;&lt;h2&gt;线性化：将 MLP 特征引入残差流&lt;/h2&gt;&lt;p&gt;计算 SAE 特征需要什么？回想一下，模型的残差流是所有先前模型组件的输出的总和。这意味着，如果该特征是残差流的线性函数（即，该特征是通过将线性流投影到给定特征向量上来计算的），那么我们可以应用&lt;a href="https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=disz2gTx-jooAcR0a5r8e7LZ"&gt;直接特征归因等&lt;/a&gt;技术来了解每个模型组件如何贡献该功能。&lt;/p&gt;&lt;p&gt;不幸的是，对于在 MLP 输出激活上训练的 SAE，特征在 MLP 输出空间而不是残余流中“实时”。因此，在进行任何进一步的分析之前，我们必须通过MLP拉回SAE特征向量，以获得残差流中与原始MLP输出特征向量相对应的特征向量。&lt;/p&gt;&lt;p&gt;如果 MLP 是线性的，那么这可以完全完成——但是 MLP 不是线性的！它们是由许多神经元组成的复杂非线性函数，并且大多数 SAE 特征都是密集的，这意味着我们需要了解每个神经元才能理解 SAE 特征。然而，&lt;strong&gt;我们可以通过获取 MLP 的梯度 来获得 MLP 的局部线性近似&lt;/strong&gt;。这使我们能够找到近似对应于 MLP 后特征向量的残差流特征向量。具体来说，我们在特定提示中的特定标记上，对输入到 MLP 层的残差流求 SAE 特征（ReLU 前）的导数。&lt;/p&gt;&lt;p&gt;请注意，这是一种基于激活的技术，而不是基于权重的技术；换句话说，获得的特征向量取决于特定的MLP激活，不同的输入会产生不同的线性化特征向量。我们稍后研究这些特征向量的一致性及其准确性。&lt;/p&gt;&lt;h3&gt;技术旁白：冻结 LayerNorm&lt;/h3&gt;&lt;p&gt;请注意，MLP 子层（与 Transformer 中的所有子层一样）前面有 LayerNorm，它由线性变换、非线性归一化操作和线性变换组成。人们可以通过采用 LayerNorm 的梯度以及 MLP 来解释这种非线性，但由于稍后讨论的原因，这并不总是能产生良好的结果。因此，有时需要通过忽略非线性并仅考虑线性变换来&lt;strong&gt;冻结&lt;/strong&gt;LayerNorms。 （这意味着，当我们将冻结的 LayerNorm 应用于残余流时，我们仍然将残余流除以估计残余流标准差的值——但是现在，我们将该值视为常数，而不是另一个函数的残余流。）&lt;/p&gt;&lt;h2&gt;术语说明：直接得分归因&lt;/h2&gt;&lt;p&gt;&lt;a href="https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=disz2gTx-jooAcR0a5r8e7LZ"&gt;直接 Logit 归因&lt;/a&gt;是一种众所周知的技术，用于了解模型组件对模型 Logit 的贡献。现在我们有了残差流特征向量，我们也可以应用它来了解模型组件对 SAE 特征得分的贡献。由于在查看 SAE 特征得分时谈论“logits”是没有意义的，因此我们在整篇文章中将 SAE 特征的直接 logit 归因称为&lt;strong&gt;直接得分归因&lt;/strong&gt;。核心思想是相同的：将残差流分解为分量之和，将每个分量与特征向量进行点积，看看哪些分量是重要的。&lt;/p&gt;&lt;h2&gt;直接路径和去嵌入&lt;/h2&gt;&lt;p&gt;现在我们有了残差流特征向量，我们可以用它来理解原始标记嵌入如何促进 SAE 特征激活。遵循&lt;a href="https://transformer-circuits.pub/2021/framework/index.html"&gt;Elhage 等人的&lt;/a&gt;观点，我们将这条从原始 token 嵌入到 SAE 特征的路径称为&lt;strong&gt;直接路径&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;解释直接路径的一种方法是使用一种我们称为&lt;strong&gt;“去嵌入”&lt;/strong&gt;的技术。去嵌入的思想是获取残差流特征向量，并与模型的输入嵌入矩阵中的每个向量取点积；这会在模型的词汇空间中产生一个特征向量。该向量中每个令牌的系数提供了该令牌通过直接路径对 SAE 功能贡献程度的近似值。&lt;i&gt;重要的是，我们可以查看该向量中系数最高的标记，以便了解模型词汇表中的哪些标记对于直接路径最重要。&lt;/i&gt;&lt;/p&gt;&lt;h2&gt;注意力&lt;/h2&gt;&lt;h3&gt;OV电路和QK电路分析&lt;/h3&gt;&lt;p&gt;我们还可以使用残差流特征向量来了解不同的注意力头如何对 SAE 特征激活做出贡献。回想一下，注意力头的功能可以分解为计算注意力分数的 QK 电路和转换 token 信息的 OV 电路。尽管注意力头是残差流的非线性函数，但如果我们孤立地看待 OV 电路，那么注意力输出只是每个注意力头和每个 token 的线性变换的加权和。&lt;/p&gt;&lt;p&gt;因此，我们可以通过 OV 矩阵拉回残余流特征向量来了解给定头的 OV 电路如何对 SAE 特征做出贡献。&lt;i&gt;请注意，完成此操作后，我们可以应用去嵌入等技术来了解模型词汇表中的哪些标记通过该注意头的 OV 电路对 SAE 功能贡献最大。&lt;/i&gt;换句话说，我们可以确定哪些令牌&lt;i&gt;如果&lt;/i&gt;受到该头的关注，最能激活该功能。然后可以通过查看哪些令牌具有最高的 QK 分数以及对 OV 电路很重要的令牌来单独分析 QK 电路。&lt;/p&gt;&lt;h3&gt; （头、源标记）对的直接分数归因&lt;/h3&gt;&lt;p&gt;我们在案例研究中经常使用的一种工具是对关注中的个体（头、源标记）对进行直接评分归因。我们可以这样做，因为注意力头的输出是每个源令牌贡献的加权和。这使我们能够了解每个源令牌通过每个注意力头对 SAE 功能的贡献有多大。&lt;/p&gt;&lt;h2&gt;多层模型&lt;/h2&gt;&lt;h3&gt;计算路径&lt;/h3&gt;&lt;p&gt;在多层变压器中，事情变得更加复杂。这是因为从输入到 SAE 特征的可能计算路径集随着层数的增加（呈指数级）增加。两层模型中的不同路径可能包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;令牌嵌入 → MLP1（直接路径）&lt;/li&gt;&lt;li&gt;令牌嵌入 → MLP0 → 注意力 1 head 5 → MLP1&lt;/li&gt;&lt;li&gt;令牌嵌入 → 注意力 0 头 3 → MLP0 → MLP1&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;然而，总体原理是相同的：通过路径中的每个组件不断拉回特征向量。&lt;/p&gt;&lt;p&gt;请注意，某些计算路径可能涉及&lt;i&gt;多个非线性&lt;/i&gt;，例如两个不同的 MLP 子层。在这种情况下，我们分别对每个非线性进行线性化。计算路径中存在的非线性越多，我们期望的近似误差就越大，但这仍然可以产生有用的结果。另一种选择是查看计算路径，而不是一直返回到原始标记嵌入，而是仅返回到前一层的 MLP。现在我们将看到这如何允许我们解释先前 MLP 中各个 SAE 特征的路径。&lt;/p&gt;&lt;h3&gt; SAE 虚拟重量&lt;/h3&gt;&lt;p&gt;多层模型中可用的一种有用的基于权重的技术是&lt;i&gt;能够根据前一层 SAE 特征在给定层编写 SAE 特征&lt;/i&gt;。我们将此称为查看&lt;strong&gt;SAE 虚拟权重&lt;/strong&gt;。为此，需要获取后续 SAE 特征的残差流特征向量，并通过前一层 SAE 的解码器矩阵将其拉回。就像去嵌入一样​​，作为此过程的结果，您将获得一个向量，该向量告诉您前一层 SAE 特征与后一层 SAE 特征的对应程度。&lt;/p&gt;&lt;h2&gt;其他标准特征解释技术&lt;/h2&gt;&lt;p&gt;除了尝试对特征的计算方式进行逆向工程之外，我们还遵循 Anthropic 的方法通过研究最大/均匀激活示例以及研究每个特征对模型词汇表中每个标记的 logits 的影响来理解这些特征。&lt;/p&gt;&lt;h3&gt;最大/均匀激活示例&lt;/h3&gt;&lt;p&gt;获得 SAE 功能的初步解释的一种方法是查看数据集中的哪些示例最能激活该功能。然而，按照&lt;a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html#setup-interface"&gt;Anthropic 的方法&lt;/a&gt;，在整个特征激活分数范围内采样示例有时也是有用的，以便更广泛地了解特征所代表的含义。在这种情况下，我们找到特征分数均匀分布的样本（尽可能）。例如，如果某个特征在某个数据集上被激活在 0.0 到 10.0 之间，并且我们想要查看 10 个均匀间隔的示例，那么我们将尝试找到一个特征得分为 1.0 的示例，一个具有某个特征的示例分数为2.0等&lt;/p&gt;&lt;h3&gt;Logit 权重&lt;/h3&gt;&lt;p&gt;Anthropic 使用的另一种方法是检查 SAE 特征对模型词汇表中每个标记的 logits 的影响。直观上，这让我们了解模型期望哪些令牌遵循高度激活该功能的令牌。其原理类似于 Logit 透镜：通过获取 SAE 特征的解码器向量并将其乘以模型的非嵌入矩阵，然后查看模型词汇表中在结果向量中得分最高的标记来完成此操作。有时，这可以让我们对模型如何使用该特征有一个很好的初步了解，但情况并非总是如此；因此，将我们通过查看 logit 权重获得的理解与通过执行逆向工程获得的理解进行比较是有用的。&lt;/p&gt;&lt;h1&gt;实例探究&lt;/h1&gt;&lt;h2&gt;实验装置&lt;/h2&gt;&lt;p&gt;我们研究的两个模型来自 Neel Nanda 的 ToyLM 系列，特别是&lt;a href="https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=U-XWl8ddNkvId7o4ElrpehQ0"&gt;GELU-1L 和 GELU-2L 模型&lt;/a&gt;，（顾名思义）它们分别是 1 层和 2 层模型。前四个案例研究针对 GELU-1L，最后一个案例研究针对 GELU-2L。这些模型“在 22B 个数据标记上进行训练，其中 80% 来自 C4（网络文本），20% 来自 Python 代码”；他们的模型维度&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;d&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;m&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;o&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;d&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;e&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;l&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;是 512，他们的 MLP 维度&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;d&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;m&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;l&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;p&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;是 1,024，他们每个注意力子层有 8 个注意力头，他们的词汇表包含 48,262 个标记。&lt;/p&gt;&lt;p&gt;当我们使用数据集（例如查看最大激活示例）时，我们使用&lt;a href="https://NeelNanda/c4-code-20k"&gt;c4-code-20k&lt;/a&gt;中的 1,638,400 个标记，其中包含与训练模型的数据集相同的数据分布。该语料库分为每个提示 128 个标记。&lt;/p&gt;&lt;p&gt;我们针对 GELU-1L 调查的 SAE 可&lt;a href="https://huggingface.co/NeelNanda/sparse_autoencoder"&gt;通过此链接&lt;/a&gt;作为&lt;a href="https://huggingface.co/NeelNanda/sparse_autoencoder"&gt;SAE&lt;/a&gt; &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;#&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 25 获得，这是单个 SAE 训练运行的最终检查点，由 Neel Nanda 在 GELU-1L 激活上进行训练。我们针对 GELU-2L 调查的 SAE 可作为&lt;a href="https://huggingface.co/NeelNanda/sparse_autoencoder"&gt;前缀为“gelu-2l”的 SAE 从此链接中&lt;/a&gt;获取。所有 SAE 都有 16,384 个功能。 GELU-1L SAE 在模型的&lt;code&gt;mlp_post&lt;/code&gt;激活（维度 1,024）上进行训练，而 GELU-2L SAE 在模型的&lt;code&gt;mlp_output&lt;/code&gt;激活（维度 512）上进行训练。您可以&lt;a href="https://colab.research.google.com/drive/1u8larhpxy8w4mMsJiSBddNOzFGj7_RTn#scrollTo=riilpvs-CgoQ"&gt;在此处找到使用这些 SAE 的代码。&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;关于研究特征的选择&lt;/h2&gt;&lt;p&gt;案例研究所涉及的特征是以相对无原则的方式选择的，很大程度上是基于我们认为研究有趣的内容。指导我们选择的是特征审核，其目的是通过计算特征标记对的 F1 分数来确定所有特征表现出上下文依赖性的程度。选择每个功能的原因如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;选择&lt;code&gt;(&amp;#39;&lt;/code&gt;特征是因为它是最早的高频特征之一（按特征索引排序，没有内在含义）。&lt;/li&gt;&lt;li&gt;选择&lt;code&gt;it is&lt;/code&gt;功能是因为功能审核表明它主要在单个令牌上激活，并且仅在非常有限的上下文中激活，因此我们认为进一步研究这一点会很酷。&lt;/li&gt;&lt;li&gt;选择&lt;code&gt;&amp;#39;t&lt;/code&gt;功能是因为功能审核表明该功能在单个令牌上以及几乎所有出现此令牌的情况下都被高度激活。因此，我们想要研究这个看似与上下文无关的功能。&lt;/li&gt;&lt;li&gt; “神学/政治背景下的‘是’”特征很大程度上是随机选择的。&lt;/li&gt;&lt;li&gt;选择 GELU-2L 特征是因为它是最早的高频特征之一。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;请注意，一旦我们开始案例研究，我们就不会放弃它。因此，您看到的结果考虑了我们调查的所有功能。&lt;/p&gt;&lt;h2&gt; A &lt;code&gt;(&amp;#39;&lt;/code&gt; GELU-1L 中的功能&lt;/h2&gt;&lt;h3&gt;最大激活示例&lt;/h3&gt;&lt;p&gt;我们要研究的第一个功能是 GELU-1L SAE 中的功能 8。查看顶部激活示例可以立即解释此功能：主要在令牌&lt;code&gt;(&amp;#39;&lt;/code&gt; . &lt;/p&gt;&lt;figure class="image image_resized" style="width: 76.43%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/93nKtsDL6YY5fRbQv/cg14g3ktiocvo6wibwrh" /&gt;&lt;figcaption&gt;相关 SAE 功能的热门激活示例。有趣的是，大多数这些顶级激活令牌似乎都后面跟着相同的“django”令牌，即使模型看不到下一个令牌。请注意，↩ 表示换行符，· 表示空格&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Logit 权重&lt;/h3&gt;&lt;p&gt;这个 SAE 功能最有力地提升了&lt;code&gt;django&lt;/code&gt;代币的 logits，这反映了我们在顶级激活示例中看到的情况。它还提高了其他与代码相关的标记的 logits，例如&lt;code&gt;&amp;lt;&lt;/code&gt;和&lt;code&gt;utf&lt;/code&gt; 。 &lt;/p&gt;&lt;figure class="image image_resized" style="width: 19.85%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/93nKtsDL6YY5fRbQv/mjd51l7bp6g7xxexrpvw" /&gt;&lt;figcaption&gt;此功能具有最高 Logit 权重的令牌&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;直接路径和意外发现&lt;/h3&gt;&lt;p&gt;现在，我们执行“去嵌入”，以便了解哪些令牌通过直接路径对此功能贡献最大。具体来说：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;我们通过 MLP0 区分该 SAE 功能在特定高激活示例上的激活。这给了我们残差流&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;中&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;长度&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;为&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;d&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;model&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;的&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;特征向量&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;v&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 。&lt;/li&gt;&lt;li&gt;我们将此特征向量乘以嵌入矩阵，以查看哪些标记可以通过直接路径激活它，即查看长度为&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;d&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;v&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;o&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;c&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;的向量&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;W&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;E&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.12em;"&gt;T&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;v&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 。自然地，我们预测&lt;code&gt;(&amp;#39;&lt;/code&gt;会得分很高。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;结果如下： &lt;/p&gt;&lt;figure class="image image_resized" style="width: 44.02%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/93nKtsDL6YY5fRbQv/tjrongai1hr8kq5ix77s" /&gt;&lt;figcaption&gt;去嵌入标记分数以获得线性化 SAE 特征的直接路径&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;果然，顶部的标记是我们在顶部激活示例中看到的&lt;code&gt;(&amp;#39;&lt;/code&gt;标记。但是还有一些其他意想不到的标记，例如标记&lt;code&gt;ह&lt;/code&gt; 。这让我们感到惊讶；为了了解这是否是一个由于我们方法中的错误，我们在包含此标记的对抗性提示上运行模型，并记录每个标记的原始特征激活（不考虑 SAE 偏差或 ReLU）： &lt;/p&gt;&lt;figure class="image image_resized" style="width: 30.95%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/93nKtsDL6YY5fRbQv/h1nqcwu5y2kc0jjpk38m" /&gt;&lt;figcaption&gt;对抗性提示的 SAE 原始特征评分&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;正如我们所看到的， &lt;i&gt;SAE 功能实际上确实在此令牌上激活&lt;/i&gt;，尽管其程度与在&lt;code&gt;(&amp;#39;&lt;/code&gt;令牌上激活的程度不同。这对我们来说是令人惊讶和兴奋的，因为这从标准方法中根本不明显查看顶级激活示例。我们认为这是我们方法的令人兴奋的概念证明，帮助我们构建 SAE 特征的对抗性示例！ &lt;span class="footnote-reference" id="fnref36xy0nowkoe"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn36xy0nowkoe"&gt;[3]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;h3&gt;注意力&lt;/h3&gt;&lt;p&gt;对注意力头进行直接得分归因似乎表明头 0 和 3 很重要。在下面的示例中，我们看到 head 0 通过&lt;code&gt;&amp;#39;:&lt;/code&gt;标记和&lt;code&gt;(&amp;#39;&lt;/code&gt;标记贡献了该功能。Head 3 通过右括号标记&lt;code&gt;&amp;quot;}),&lt;/code&gt;贡献了该功能。 &lt;/p&gt;&lt;figure class="image image_resized" style="width: 80.02%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/93nKtsDL6YY5fRbQv/evdzdjejy64k8hvv3ho9" /&gt;&lt;figcaption&gt;注意力的直接评分归因&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;查看 head 0 &lt;span class="footnote-reference" id="fnref1tpfjhj7ezr"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn1tpfjhj7ezr"&gt;[4]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;的 OV 电路去嵌入表明，顶部标记往往包括各种开头字符串标记，例如&lt;code&gt;&amp;quot;&lt;/code&gt; 、 &lt;code&gt;([&amp;#39;&lt;/code&gt;和标记&lt;code&gt;(&amp;#39;&lt;/code&gt;本身，但也包括后跟空格的换行符的各种排列。有趣的是，尽管标记&lt;code&gt;&amp;#39;&lt;/code&gt;的去嵌入分数很高，但上面的直接分数归因示例表明该标记对特征激活贡献不大。这似乎是因为头 0 对该标记的关注程度不高。事实上，来自 token &lt;code&gt;(&amp;#39;&lt;/code&gt; to the token &lt;code&gt;&amp;#39;&lt;/code&gt;的 pre-softmax 注意力得分为 51.15，小于来自 token &lt;code&gt;(&amp;#39;&lt;/code&gt; to the token &lt;code&gt;&amp;#39;:&lt;/code&gt;的 softmax 之前的注意力得分 63.86)。 &lt;/p&gt;&lt;figure class="image image_resized" style="width: 36.41%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/93nKtsDL6YY5fRbQv/ogt3x7bfvfln4mpnaxeg" /&gt;&lt;figcaption&gt;模型词汇表中头 0 的 OV 电路去嵌入得分最高的标记&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;头 3 的 OV 电路去嵌入中的顶部标记中有许多右大括号标记，例如&lt;code&gt;})&lt;/code&gt; 、 &lt;code&gt;)})&lt;/code&gt;和&lt;code&gt;&amp;#39;)&lt;/code&gt; 。这表明，除了初始空白标记（正如我们在直接得分归因结果中看到的那样）之外，头 3 还通过这些右大括号标记对 SAE 功能做出了贡献。然而，使这张图片稍微复杂化的是，去嵌入中的顶部标记是不相关的标记&lt;code&gt;Illustration&lt;/code&gt; ，在测试一些初始对抗性提示时，它似乎对特征分数没有影响。我们后来对线性化的探索表明，像这样的 token 的存在可能是线性逼近 MLP 的产物。 &lt;/p&gt;&lt;figure class="image image_resized" style="width: 28.23%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/93nKtsDL6YY5fRbQv/tnya7vpo5ch3jynxwien" /&gt;&lt;figcaption&gt;头 3 的 OV 电路的最佳去嵌入结果。&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;概括&lt;/h3&gt;&lt;p&gt;我们的逆向工程和去嵌入，结合通过查看最大激活示例获得的证据，表明该功能具有以下解释：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;该功能主要在代码上下文中的标记&lt;code&gt;(&amp;#39;&lt;/code&gt;上触发，特别是在涉及包含字符串的列表或元组的上下文中。&lt;ul&gt;&lt;li&gt;该功能的直接路径强烈触发&lt;code&gt;(&amp;#39;&lt;/code&gt; .&lt;/li&gt;&lt;li&gt;头 0 和头 3 通过触发初始空白标记来建立代码上下文。&lt;/li&gt;&lt;li&gt; Head 0 通过触发表示此类列表和元组开头的标记（例如标记&lt;code&gt;[&amp;quot;&lt;/code&gt; ）来建立涉及包含字符串的列表和元组的上下文。Head 3 通过触发右大括号标记（例如&lt;code&gt;})&lt;/code&gt; , &lt;code&gt;)})&lt;/code&gt;来建立此上下文， 和&lt;code&gt;&amp;#39;)&lt;/code&gt; 。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;但该功能也会在代币&lt;code&gt;ह&lt;/code&gt;上触发，尽管没有那么强烈！&lt;ul&gt;&lt;li&gt;这是在查看此功能的直接路径去嵌入后得出的意外发现。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;仍然存在一些悬而未决的问题，以及一些难以解释的结果。尤其：&lt;/p&gt;&lt;ul&gt;&lt;li&gt; Head 3 OV 去嵌入的最重要标记是&lt;code&gt;Illustration&lt;/code&gt; ，它似乎对对抗性提示中的 SAE 功能没有贡献。这是该方法的缺点，还是在某些上下文中该令牌确实对该功能有所贡献？&lt;/li&gt;&lt;li&gt;在本案例研究中，我们没有研究 QK 电路。这是否可以揭示更复杂的行为，或者可以解释&lt;code&gt;Illustration&lt;/code&gt;令牌发生了什么？&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; GELU-1L 中“[标点符号] it is”的功能&lt;/h2&gt;&lt;p&gt;这是我们正在研究的 SAE 中的功能 4542。&lt;/p&gt;&lt;h3&gt;最大激活示例&lt;/h3&gt;&lt;p&gt;当标记&lt;code&gt;it&lt;/code&gt; （或其大写变体）前面有标记时，此功能&lt;code&gt;is&lt;/code&gt;会在标记上最大程度地激活，并且标记&lt;code&gt;it&lt;/code&gt;前面通常带有标点符号。&lt;/p&gt;&lt;h3&gt; Logit 权重&lt;/h3&gt;&lt;p&gt;查看 SAE 功能的 logit 权重，该功能最能提高标记&lt;code&gt;advis&lt;/code&gt; 、 &lt;code&gt;advised&lt;/code&gt; 、 &lt;code&gt;conceivable&lt;/code&gt; 、 &lt;code&gt;recommended&lt;/code&gt;和类似标记的 logit ——所有这些标记都倾向于用在“It is”之后的非人称结构中，例如作为“建议......” &lt;/p&gt;&lt;figure class="image image_resized" style="width: 26.98%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/93nKtsDL6YY5fRbQv/o0mvbdliwhsfmr6fhvqj" /&gt;&lt;figcaption&gt;此功能具有最高 Logit 权重的令牌&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;直接路径&lt;/h3&gt;&lt;p&gt;在直接路径去嵌入中，当不考虑预MLP LayerNorm时，该特征的顶部标记是标记&lt;code&gt;is&lt;/code&gt; 。 &lt;/p&gt;&lt;figure class="image image_resized" style="width: 28.36%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/93nKtsDL6YY5fRbQv/hb9vnjiftl3i7kyzyirj" /&gt;&lt;figcaption&gt;直接路径去嵌入中的顶级标记，不考虑 pre-MLP LayerNorm&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;请注意，具有第二高去嵌入分数的标记&lt;code&gt;are&lt;/code&gt; ，在对抗性提示中使用时会稍微激活该功能： 提示&lt;code&gt;. It are&lt;/code&gt;导致该功能以 0.151 分触发。&lt;/p&gt;&lt;p&gt;然而，当线性化 pre-MLP LayerNorm 时，此功能的顶部标记更加难以解释（尽管&lt;code&gt;Is&lt;/code&gt;是第二高激活标记）。关于线性化的部分给出了为什么线性化预 MLP LayerNorm 可能会导致这些更糟糕的结果的潜在理论基础。 &lt;/p&gt;&lt;figure class="image image_resized" style="width: 28.69%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/93nKtsDL6YY5fRbQv/zgsqi973ntbldne2pyzj" /&gt;&lt;figcaption&gt;考虑预 MLP LayerNorm 时，直接路径去嵌入中的顶级标记&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;注意力&lt;/h3&gt;&lt;p&gt;对注意力进行直接评分归因表明，头 0 以及较小程度上的头 1 通过触发“it”等标记来对该特征做出贡献。我们还看到 head 1 在某种程度上对标点符号进行触发，尽管远低于最初的最大激活示例可能暗示的情况。 &lt;/p&gt;&lt;figure class="image image_resized" style="width: 81.7%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/93nKtsDL6YY5fRbQv/jh67o1jlpwx3nwwj53pg" /&gt;&lt;figcaption&gt;注意力头/标记对的直接得分归因&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;对头 0 的 OV 电路执行去嵌入，毫无疑问，揭示了前三个标记是“it”的变体。 &lt;/p&gt;&lt;figure class="image image_resized" style="width: 29.01%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/93nKtsDL6YY5fRbQv/uhmbhm7tiqz41onlexw5" /&gt;&lt;figcaption&gt;注意头 0 OV 电路去嵌入顶部令牌&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;然而，头 1 的去嵌入顶级标记更令人费解：所有前 20 个标记都很难解释，并且似乎与该特征无关，例如&lt;code&gt;stitial&lt;/code&gt; 、 &lt;code&gt;undes&lt;/code&gt;和&lt;code&gt;consc&lt;/code&gt; 。有趣的是，这些令牌似乎可以用于构建激活该功能的对抗提示。例如，提示“然后是”使该功能以1.390的分数触发，但是对抗性提示”是“ stitial”是“导致功能以分数为1.521发射，而对抗性提示则是“ undes”，它使功能是“使功能”使该功能降低。得分为1.626。 （请注意，提示“是”使该功能以1.733的分数触发，高于这些对抗性提示。）&lt;/p&gt;&lt;p&gt;同样令人惊讶的是，我们在令牌中没有看到任何标点令牌，因为头部1-令牌是最高的&lt;code&gt;.&lt;/code&gt;例如，仅是第9077位得分最高的令牌。尽管这个令牌提高了特征分数：提示&lt;code&gt;. It is&lt;/code&gt;由于该功能以分数1.903激活的功能，而提示&lt;code&gt;It is&lt;/code&gt;导致该功能以分数1.820激活。&lt;/p&gt;&lt;h3&gt;概括&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;该功能似乎在令牌上激活的&lt;code&gt;is&lt;/code&gt;在像&lt;code&gt;It&lt;/code&gt;和&lt;code&gt;it&lt;/code&gt; &lt;code&gt;it&lt;/code&gt;的代币之前。&lt;ul&gt;&lt;li&gt;在没有线性化前MLP分层的情况下获得的直接路径去除膜显示出很高的&lt;code&gt;is&lt;/code&gt;分数。但是，当我们将前MLP分层线性化时，这会产生更多无法解释的令牌。这反映了我们稍后讨论的与线性分层相关的某些行为。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt; Head 0通过像&lt;code&gt;it&lt;/code&gt; &lt;code&gt;It&lt;/code&gt;向代币射击来为功能做出贡献。肯定的， &lt;code&gt;It&lt;/code&gt; ， &lt;code&gt;it&lt;/code&gt;和&lt;code&gt;It&lt;/code&gt;是拆卸中的顶级令牌。&lt;/li&gt;&lt;li&gt;直接分数归因表明，Head 1通过在标点令牌上稍微触发来促进该功能。但是，这并不反映在头1的拆卸中，这是完全无法解释的。更深入的调查可能会阐明标点符号正在发生的事情。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; gelu-1l中的&lt;code&gt;&amp;#39;t&lt;/code&gt;功能&lt;/h2&gt;&lt;p&gt;这是我们正在研究的S​​AE中的功能编号10996。&lt;/p&gt;&lt;h3&gt;统一的激活示例&lt;/h3&gt;&lt;p&gt;查看此功能激活的示例，似乎此功能主要在诸如“ not”，“ not”，“ not”之类的单词的末尾激活在&lt;code&gt;&amp;#39;t&lt;/code&gt;上，并且类似&lt;span class="footnote-reference" id="fnref2belxv37guh"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn2belxv37guh"&gt;[5 ]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。在较低的激活水平下，此功能还会在诸如“ dont”和“ dot”之类的拼写错误上发射。 &lt;/p&gt;&lt;figure class="image image_resized" style="width: 77.92%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/93nKtsDL6YY5fRbQv/umgpxcigopmzht3jpco9" /&gt;&lt;figcaption&gt;统一激活的例子&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;logit重量&lt;/h3&gt;&lt;p&gt;此功能最强烈地增强了由标点符号组成的&lt;code&gt;s&lt;/code&gt;和令牌的逻辑，然后是引号。这并没有反映在统一的激活示例中，这表明观察该功能的计算方式（例如，通过拆卸等方法）可能比一旦计算出该功能的下游效应，可能会带来更多的果实。 &lt;/p&gt;&lt;figure class="image image_resized" style="width: 19.82%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/93nKtsDL6YY5fRbQv/uzydvausyd0bz9jhc9o1" /&gt;&lt;figcaption&gt;该功能的logit权重最高的令牌&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;直接路径&lt;/h3&gt;&lt;p&gt;直接路径中得分最高的令牌为&lt;code&gt;&amp;#39;t&lt;/code&gt; 。得分很高的其他代币是上述宫缩的拼写错误，例如&lt;code&gt;wont&lt;/code&gt;和&lt;code&gt;didnt&lt;/code&gt; ，以及像&lt;code&gt;not&lt;/code&gt;和&lt;code&gt;Not&lt;/code&gt;负面因素。 &lt;/p&gt;&lt;figure class="image image_resized" style="width: 29.45%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/93nKtsDL6YY5fRbQv/ubomlalnhgnqrtlb8xx9" /&gt;&lt;figcaption&gt;直接路径在模型的词汇量中删除顶级令牌&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;注意力&lt;/h3&gt;&lt;p&gt;执行直接分数归因似乎表明注意力并没有发挥太大作用。在一个例子上，除&lt;code&gt;&amp;lt;|BOS|&amp;gt;&lt;/code&gt;令牌以外的其他代币的总贡献（忽略注意力偏见）仅为0.34。 Head 1似乎在&lt;code&gt;&amp;#39;t&lt;/code&gt;代币上略有激活，看着这个头部的OV去除确实表明，在48k代币词汇中， &lt;code&gt;&amp;#39;t&lt;/code&gt;令牌”的消除得分是第35位。&lt;/p&gt;&lt;p&gt;但是，重新样本消融注意力头的输出（一种因果干预）讲述了另一个故事。在更换注意力输出的提示/令牌时，该功能通过其注意力输出发射的提示/令牌，该功能未发射该功能，并比较清洁和损坏的运行之间的SAE功能激活，激活的差异是1.1226。这表明注意力在这里做一些有用的事情，尽管我们说什么还为时过早。请注意，重塑消融结果与直接分数归因结果之间差异的一种可能性是，作为因果干预，重新样品消融融合了从MLP中忽略的非线性效应，这些效应在直接分数归因中被忽略。&lt;/p&gt;&lt;h3&gt;概括&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;该功能似乎主要在&lt;code&gt;&amp;#39;t&lt;/code&gt;代币上激活，例如“不”。直接路径隔离反映了这一点， &lt;code&gt;&amp;#39;t&lt;/code&gt;得分最高 - 尽管拼写错误的单词也&lt;code&gt;didnt&lt;/code&gt;得分很高。&lt;/li&gt;&lt;li&gt;直接得分归因表明，尽管Head 1似乎通过激活&lt;code&gt;&amp;#39;t&lt;/code&gt;令牌”而做出了一些贡献，但对此功能并不重要。但是重新样本消融以引起注意，这表明注意力确实有效。这表明线性化在这里具有误导性。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; GELU-1L中的上下文依赖性“ IS”功能&lt;/h2&gt;&lt;p&gt;此功能是我们正在研究的S​​AE中的功能编号4958。&lt;/p&gt;&lt;h3&gt;最大激活示例&lt;/h3&gt;&lt;figure class="image image_resized" style="width: 73.2%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/93nKtsDL6YY5fRbQv/od0nhmfvsdlshzec13e1" /&gt;&lt;figcaption&gt;该功能的最大激活示例&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;查看最大激活示例，该功能主要在令牌上&lt;code&gt;is&lt;/code&gt; （并且偶尔在动词“要”的其他形式上）。但是这个功能似乎更多：它似乎在涉及神学和政治的上下文中激活。因此，此功能让人联想到拟人化的SAE纸中讨论的特征，例如“在抽象代数的背景下的令牌&lt;code&gt;a&lt;/code&gt; ”。&lt;/p&gt;&lt;h3&gt; logit重量&lt;/h3&gt;&lt;p&gt;该功能最大程度地提高逻辑的令牌没有立即解释。 &lt;/p&gt;&lt;figure class="image image_resized" style="width: 24.3%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/93nKtsDL6YY5fRbQv/qks7eoejz2ymyuqvvhat" /&gt;&lt;figcaption&gt;该功能的logit权重最高的令牌&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;最高的代币&lt;code&gt;rael&lt;/code&gt;可能与代币结合在一起&lt;code&gt;is&lt;/code&gt;形成“以色列”，这与一些最大的激活例子中的宗教主题保持一致。 &lt;code&gt;manifested&lt;/code&gt;和&lt;code&gt;violated&lt;/code&gt;代币也暗示了圣经的含义。但是，很难看到&lt;code&gt;aroused&lt;/code&gt;和&lt;code&gt;assertEquals&lt;/code&gt;地方发挥作用。&lt;/p&gt;&lt;h3&gt;直接路径&lt;/h3&gt;&lt;figure class="image image_resized" style="width: 30.75%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/93nKtsDL6YY5fRbQv/jpccjcqsatmoomvnutcy" /&gt;&lt;figcaption&gt;直接路径在模型的词汇量中删除顶级令牌&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;直接路径去除得分证实了最大激活示例：得分最高的令牌&lt;code&gt;is&lt;/code&gt; ，随后是动词的其他形式的“要”。&lt;/p&gt;&lt;h3&gt;注意力&lt;/h3&gt;&lt;p&gt;由于最大激活的示例表明此功能与上下文有关，因此我们希望注意力将发挥相当重要的作用。执行直接分数归因表明两个重要的注意力头：头0和头4。特别是，我们发现头部0倾向于自我介绍为&lt;code&gt;is&lt;/code&gt;代币，并在该令牌上发射，而诸如ISM等令牌上有4个Head 4点火（如&lt;code&gt;ism&lt;/code&gt; （例如用“原教旨主义”， &lt;code&gt;spirit&lt;/code&gt;甚至&lt;code&gt;Plato&lt;/code&gt;之类的词。 &lt;/p&gt;&lt;figure class="image"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/93nKtsDL6YY5fRbQv/a9wkhhycups1mqdnktpf" /&gt;&lt;figcaption&gt;在与柏拉图有关的示例中关注的直接分数归因&lt;/figcaption&gt;&lt;/figure&gt;&lt;figure class="image"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/93nKtsDL6YY5fRbQv/rcm85t951nlfytyvxvss" /&gt;&lt;figcaption&gt;在包含令牌&lt;code&gt;ism&lt;/code&gt;的示例上关注的直接分数归因&lt;/figcaption&gt;&lt;/figure&gt;&lt;figure class="image image_resized" style="width: 89.35%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/93nKtsDL6YY5fRbQv/bwyo50tif8ywl037umof" /&gt;&lt;figcaption&gt;直接分数归因于包含令牌&lt;code&gt;spirit&lt;/code&gt;的示例的关注&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;查看头部0的ov de缩放得分，顶部令牌是动词的各种形式的“要”（ &lt;code&gt;be&lt;/code&gt; ， &lt;code&gt;was&lt;/code&gt; ，， &lt;code&gt;is&lt;/code&gt; &lt;code&gt;s&lt;/code&gt;可能是clitic &lt;code&gt;&amp;#39;s&lt;/code&gt; ），又&lt;code&gt;been&lt;/code&gt; &lt;code&gt;&amp;#39;s&lt;/code&gt; ）。 &lt;/p&gt;&lt;figure class="image image_resized" style="width: 29.97%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/93nKtsDL6YY5fRbQv/yiwqopwdtlub3rabgvzj" /&gt;&lt;figcaption&gt;拆卸头0 OV电路&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Head 4的OV De-ebedding分数非常有启发性：顶级令牌都是&lt;code&gt;mythology&lt;/code&gt; ， &lt;code&gt;soul&lt;/code&gt; ， &lt;code&gt;urrection&lt;/code&gt; ， &lt;code&gt;existential&lt;/code&gt; ，死亡， &lt;code&gt;Divine&lt;/code&gt; ， &lt;code&gt;psy&lt;/code&gt; ，以及类似的标记等神话，灵魂，尿道， &lt;code&gt;Death&lt;/code&gt; ，死亡，神灵。 &lt;/p&gt;&lt;figure class="image image_resized" style="width: 30.85%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/93nKtsDL6YY5fRbQv/mtvh431o6bhi2nvoms8m" /&gt;&lt;figcaption&gt;拆卸4 OV电路的头部&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;概括&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;该功能似乎主要是在代币上发射的&lt;code&gt;is&lt;/code&gt;在神学和政治的背景下。&lt;/li&gt;&lt;li&gt;直接路径的动词形式为“ be”。&lt;/li&gt;&lt;li&gt;注意力0似乎在令牌&lt;code&gt;is&lt;/code&gt;强烈发射，而Head 4似乎负责合并环境。 OV De-Abedding得分进一步支持了这一点。&lt;/li&gt;&lt;li&gt;此功能运行的机制暗示了一种计算这些“在某个上下文中的令牌”特征的一般机制：主令牌上的直接路径触发，而稀疏的注意力头则负责从一个从一个令牌上触发的令牌。共享语义字段。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; Python词典的“值”字符串中的开口撇号的Gelu-2l中的一个功能&lt;/h2&gt;&lt;p&gt;此功能 -  MLP1 SAE的功能8是我们将在GELU-2L中进行调查的第一个功能。随着更多的层的复杂性，此案例研究是对这种特征反向工程是否可以扩展到多层模型的测试。&lt;/p&gt;&lt;p&gt;特别是，由于此功能是MLP1的功能，即变压器第二层中的MLP，因此有更多有助于特征激活的计算路径。在此案例研究中，我们将研究这些计算路径。&lt;/p&gt;&lt;h3&gt;统一的激活示例&lt;/h3&gt;&lt;p&gt;查看此功能的均匀激活示例，我们看到它往往会激活（尤其是在更高的激活时）在令牌上&lt;code&gt;&amp;#39;&lt;/code&gt;在代币之前&lt;code&gt;&amp;#39;:&lt;/code&gt; 。这可以识别为撇号启动键值字典中的“值”字符串。 &lt;/p&gt;&lt;figure class="image image_resized" style="width: 70.45%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/93nKtsDL6YY5fRbQv/xcuhmxq8iecp5m8doikj" /&gt;&lt;figcaption&gt;我们正在研究的MLP1 SAE功能的均匀激活示例&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;logit重量&lt;/h3&gt;&lt;p&gt;该特征最大程度地提高逻辑的令牌是&lt;code&gt;Male&lt;/code&gt;和&lt;code&gt;Female&lt;/code&gt; ，这可能是&lt;code&gt;{&amp;#39;gender: &amp;#39;Male&amp;#39;}&lt;/code&gt;字典中的值。同样，大多数顶级令牌始于大写字母。虽然有趣，但这并不能告诉我们我们正在寻找的信息。 &lt;/p&gt;&lt;figure class="image image_resized" style="width: 23.26%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/93nKtsDL6YY5fRbQv/f7d9edi8dpollkmud8zf" /&gt;&lt;figcaption&gt;该功能的logit权重最高的令牌&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;直接通往MLP1的路径&lt;/h3&gt;&lt;p&gt;首先，让我们研究从输入到MLP1的直接路径。有理由期望这种直接路径可能不像输入到MLP0的路径那样解释，因为MLP1可能正在处理更高级别的抽象。 &lt;span class="footnote-reference" id="fnref0hyqmgltk21m"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn0hyqmgltk21m"&gt;[6]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;然而，值得一看的是这一直接路径是值得的，因为我们不能&lt;i&gt;确定&lt;/i&gt;这一直接路径对特征激活不负责。&lt;/p&gt;&lt;p&gt;查看具有最高拆卸得分的令牌，前十个令牌都是无法解释的令牌，例如&lt;code&gt;inex&lt;/code&gt; ， &lt;code&gt;έ&lt;/code&gt;和&lt;code&gt;immer&lt;/code&gt; 。也就是说，值得注意的是，“预期”代币&lt;code&gt;&amp;#39;&lt;/code&gt;是得分最高的标记，是超过48k代币的词汇。&lt;/p&gt;&lt;p&gt;回想一下，为了获得残留流特征向量，我们在一个特定示例上对MLP sublayer进行线性化，这意味着每个示例都会产生不同的特征向量。因为我们发现的不可解释的代币数量令我们令人惊讶，所以我们想探索这种不可解释的令牌现象的程度是我们在该特定示例中线性化MLP的特定示例的伪像。因此，我们在100个顶部激活的示例中采用了MLP1梯度的平均值，并研究了该平均特征向量。再一次，顶级令牌是无法解释的，例如&lt;code&gt;corro&lt;/code&gt; ， &lt;code&gt;deton&lt;/code&gt; &lt;code&gt;VERY&lt;/code&gt; &lt;code&gt;έ&lt;/code&gt; ，尽管现在，预期的令牌&lt;code&gt;&amp;#39;&lt;/code&gt;是分数最高的标记。&lt;/p&gt;&lt;p&gt;为了进一步衡量这些线性化特征向量的解放结果的依赖性依赖性的程度，我们查看了平均特征矢量和单个示例特征向量的最高&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;k&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;最高解放得分的令牌；然后，我们改变了&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;K&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; ，并查看了这些得分最高令牌的交点中令牌的比例。当&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;k&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;200&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;时，有119个令牌（即59.5％的令牌。这似乎表明了中等程度的示例依赖性。 &lt;/p&gt;&lt;figure class="image image_resized" style="width: 74.56%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/93nKtsDL6YY5fRbQv/lsieldpkxyq1pd2nhq5t" /&gt;&lt;figcaption&gt;关于用单个示例获得的特征矢量与特征向量与通过在100个示例上取出平均特征向量获得的特征向量获得的特征矢量相似性的结果。&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;这些结果在多大程度上准确地反映了模型的行为？&lt;/strong&gt;我们在该直接路径上进行了路径修补，从令牌&lt;code&gt;&amp;#39;&lt;/code&gt;到令牌&lt;code&gt;corro&lt;/code&gt; （即MLP1将其输入视为令牌&lt;code&gt;corro&lt;/code&gt; ，而所有其他模型组件仍然将输入令牌视为&lt;code&gt;&amp;#39;&lt;/code&gt; ）。我们发现这样做&lt;i&gt;实际上将特征激活略微增加了&lt;/i&gt;+0.1079。在这种情况下，意外结果实际上确实反映了模型的行为。但这不是其他令牌。在这些情况下，似乎线性近似过程中的错误是罪魁祸首。例如，当&lt;code&gt;VERY&lt;/code&gt;向量很小时，从&lt;code&gt;&amp;#39;&lt;/code&gt;方向上的路径弥补。但是，随着修补的激活越来越近&lt;code&gt;&amp;#39;&lt;/code&gt;并且距离越来越&lt;code&gt;VERY&lt;/code&gt; ，功能激活停止增加，然后开始减少。我们的直觉是，令牌嵌入的空间是一个离散的空间，而不是连续的空间。由于该模型永远不会在&lt;code&gt;VERY&lt;/code&gt;和&lt;code&gt;&amp;#39;&lt;/code&gt;之间的一半之间看到嵌入，因此在它们之间线性插值可能没有太多含义。 &lt;/p&gt;&lt;figure class="image image_resized" style="width: 71.55%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/93nKtsDL6YY5fRbQv/fu0bit1gzzlagtwtg3h2" /&gt;&lt;figcaption&gt;路径修补结果， &lt;code&gt;VERY&lt;/code&gt;在干净的&lt;code&gt;&amp;#39;&lt;/code&gt;和肮脏的令牌之间插值。&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;从MLP0到MLP1的路径&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;MLP0 SAE功能与MLP1 SAE功能之间的连接&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;因为我们正在处理多层变压器，所以我们现在可以查看从MLP0到MLP1的路径。结果之一是，使用与拆卸相同的原理，我们可以&lt;i&gt;根据MLP0 SAE功能直接表达我们的MLP1功能&lt;/i&gt;。为此，将MLP1特征乘以MLP0 SAE解码器矩阵的转置。重要的是，这是一个纯粹基于权重的操作，没有参考我们的特定示例上的内部模型激活（除了区分MLP1以获取初始特征向量）。这使我们可以看到哪些MLP0 SAE功能对MLP1功能贡献最大。 &lt;/p&gt;&lt;figure class="image image_resized" style="width: 39.39%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/93nKtsDL6YY5fRbQv/ati0xyol2s8qadnpak2y" /&gt;&lt;figcaption&gt;根据其标准化分数&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;在进行此实验之前，我们希望最佳功能会&lt;i&gt;很少&lt;/i&gt;-  MLP1功能可以用很少的MLP0功能表示。不幸的是，情况并非如此：有512 MLP0功能具有MLP1特征得分大于平均值的两个标准偏差。 &lt;/p&gt;&lt;figure class="image image_resized" style="width: 77.24%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/93nKtsDL6YY5fRbQv/akmpx0yzwxhsvaxn8wji" /&gt;&lt;figcaption&gt;线性化MLP1特征的MLP0归一化特征得分的直方图。&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;但是，从这个过程中可以获得有趣的见解。例如，如果我们查看最佳功能的统一激活示例，功能81，我们发现此功能似乎在非常相似的示例上与原始的MLP1 SAE功能一起激活，该功能由令牌&lt;code&gt;&amp;#39;&lt;/code&gt;由标记之前&lt;code&gt;&amp;#39;:&lt;/code&gt;组成。 : &lt;code&gt;&amp;#39;:&lt;/code&gt;但是，在MLP1功能和MLP0功能之间，这些示例的功能得分通常在功能分数上差异。换句话说，尽管这些功能似乎在类似类型的输入上激活，但MLP1功能通常会激活高度，以使MLP0功能在该输入中激活低，反之亦然。 &lt;/p&gt;&lt;figure class="image image_resized" style="width: 84.57%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/93nKtsDL6YY5fRbQv/owcrrfmj6ult1mu81djs" /&gt;&lt;figcaption&gt; MLP0特征81的均匀激活示例&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;其他最高得分的功能更难解释。一方面，特征11265在令牌&lt;code&gt;=&amp;quot;&lt;/code&gt;中触发，这是在我们之前讨论过的MLP1功能的均匀激活示例之一中发现的。激活在我们的代码中呈现为Gibberish的令牌。&lt;/p&gt;&lt;p&gt;这里的要点是，尽管从MLP0 SAE功能方面可以从查看MLP1 SAE功能中获得一些见解，但仍有许多密集的计算可能会阻止一种幼稚的解释。一个有趣的未来研究领域是研究是否可以同时在不同层进行训练，以鼓励其特征之间的稀疏连接。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;从MLP0到MLP1的路径的特征解释&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;现在，让我们看一下从令牌嵌入的计算路径如何通过MLP0，然后通过MLP1启用SAE功能。重要的是，由于该计算路径涉及两个连续的MLP，&lt;i&gt;因此我们采用MLP0和MLP1的线性近似&lt;/i&gt;。 （特别是，一旦我们拥有MLP1的线性化特征向量向量，然后我们就MLP0的此特征向量进行线性性化。）我们期望随着我们近似更多的非线性，线性化的错误会变得复合，但是尽管如此，我们认为我们可能会认为我们可以能够在这里获得有趣的结果。&lt;/p&gt;&lt;p&gt;在执行了这种双线性化后，当查看此计算路径的功能的解释时，顶部令牌包括&lt;code&gt;=”&lt;/code&gt; &lt;code&gt;&amp;#39;:&amp;#39;&lt;/code&gt; and&amp;#39;and &lt;code&gt;=&amp;quot;&lt;/code&gt; 。有趣的是，这些令牌具有与我们期望的&lt;code&gt;&amp;#39;&lt;/code&gt;代币”相似的语义要找到：所有这些顶级令牌都介绍了“键值”构造的“值”部分，例如&lt;code&gt;&amp;#39;name&amp;#39;:&amp;#39;John&amp;#39;&lt;/code&gt;或&lt;code&gt;&amp;quot;address&amp;quot;=&amp;quot;123 Greenfield Lane&amp;quot;&lt;/code&gt; 。&lt;/p&gt;&lt;p&gt;请注意，“预期”令牌&lt;code&gt;&amp;#39;&lt;/code&gt;的脱水得分为第102高。总体而言，这些结果与我们的期望更一致，而不是对MLP1的直接途径的拆卸结果，尽管令牌&lt;code&gt;&amp;#39;&lt;/code&gt;位置仍然比我们预期的要低。&lt;/p&gt;&lt;h3&gt;从ATTN0到MLP1的路径&lt;/h3&gt;&lt;p&gt;给定一个高度激活的示例，在MLP1功能上执行ATTN0上的直接分数归因表明，对该功能的主要贡献来自Head 2，该功能在&amp;#39;：&amp;#39;：在&lt;code&gt;&amp;#39;:&lt;/code&gt;在&amp;#39;：在&amp;#39;&amp;#39;&amp;#39;&amp;#39; &lt;code&gt;&amp;#39;&lt;/code&gt;上之前的doken上强烈启动，例如&lt;code&gt;{&amp;#39;name&amp;#39;: &amp;#39;John&amp;#39;}&lt;/code&gt; 。 &lt;/p&gt;&lt;figure class="image"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/93nKtsDL6YY5fRbQv/hqhusm3quxpld1gd6lox" /&gt;&lt;figcaption&gt;第0层的MLP1功能的直接分数归因&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;此外，在目的地令牌时，查看&lt;code&gt;&amp;#39;&lt;/code&gt; &lt;code&gt;&amp;#39;:&lt;/code&gt;令牌”的QK分数表明，当源代币远离目的地时，注意力评分非常急剧下降。&lt;/p&gt;&lt;p&gt;将HEAD 2的OV功能解释为&lt;code&gt;:&amp;quot;&lt;/code&gt;和&lt;code&gt;:&amp;#39;&lt;/code&gt;是第四和第5个得分最高的令牌，这符合我们对头部功能的直觉。但是，排名最高的最高得分标记是出乎意料的：出乎意料的是： &lt;code&gt;Î&lt;/code&gt; ， &lt;code&gt;)=\&lt;/code&gt; ，和&lt;code&gt;))**(&lt;/code&gt;分别。我们在提示中使用了这些令牌，以查看它们是否激活了该特征；对于参考，提示&lt;code&gt;&amp;#39;: &amp;#39;&lt;/code&gt;产生了4.535的特征激活。我们发现提示&lt;code&gt;Î &amp;#39;&lt;/code&gt;时， &lt;code&gt;Î &amp;#39;&lt;/code&gt;根本没有激活该功能，提示&lt;code&gt;))**( &amp;#39;&lt;/code&gt;弱激活该功能，得分为1.038。&lt;/p&gt;&lt;h3&gt;从ATTN0到MLP0再到MLP1的路径&lt;/h3&gt;&lt;p&gt;在一个高度激活的示例上的直接分数归因表明，大多数贡献再次来自&lt;code&gt;&amp;#39;:&lt;/code&gt; ”令牌之前的“ &lt;code&gt;&amp;#39;&lt;/code&gt;令牌”。此路径的Head 2的OV De插度得分最高的令牌为&lt;code&gt;&amp;#39;:&lt;/code&gt; ，”，例如&lt;code&gt;&amp;quot;:&lt;/code&gt; and &lt;code&gt;&amp;#39;):&lt;/code&gt;也存在于前十个令牌中。&lt;/p&gt;&lt;p&gt;有趣的是，令牌&lt;code&gt;perhaps&lt;/code&gt;是第四高的分数，并在提示中使用它的&lt;code&gt;&amp;#39;&lt;/code&gt;令牌”弱激活了原始的MLP1 SAE功能（激活为0.787）。&lt;/p&gt;&lt;h3&gt;涉及ATTN1的路径&lt;/h3&gt;&lt;p&gt;在模型的所有子层上执行直接得分归因，表明ATTN1对特征分数有负贡献，这在很大程度上是由于注意力输出偏置向量。因此，我们没有对涉及ATTN1的路径进行非常彻底的研究。对其头的OV脱落得分的初步研究是无法解释的。也就是说，对此SAE功能的全面调查将花费更多时间查看ATTN1。&lt;/p&gt;&lt;h3&gt;概括&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;均匀的激活示例表明，MLP1特征似乎在Bigram &lt;code&gt;&amp;#39;: &amp;#39;&lt;/code&gt;上激活。&lt;/li&gt;&lt;li&gt;在MLP1的直接路径上执行去除力会产生大量难以理解的令牌，作为得分最高的令牌。也就是说，预期的令牌&lt;code&gt;&amp;#39;&lt;/code&gt;得分排名第159。使用一些难以理解的令牌进行路径修补实际上确实增加了特征激活，但是其他难以理解的令牌的存在似乎是线性化过程的伪像。&lt;/li&gt;&lt;li&gt;用MLP0 SAE功能表达MLP1 SAE功能表明，大量MLP0功能有助于MLP1功能。最重要的MLP0功能具有顶部激活示例，看起来与MLP1功能的顶级激活示例非常相似，但是这些功能分数之间通常存在差异。&lt;/li&gt;&lt;li&gt;查看从MLP0到MLP1的路径的解释，顶部令牌包括&lt;code&gt;=”&lt;/code&gt; and &lt;code&gt;&amp;#39;:&amp;#39;&lt;/code&gt; and &lt;code&gt;=&amp;quot;&lt;/code&gt; ;“预期”令牌&lt;code&gt;&amp;#39;&lt;/code&gt;分数为102级最高分数。&lt;/li&gt;&lt;li&gt;查看从ATTN0到MLP1的路径以及从MLP0到MLP0再到MLP1的路径表明，注意力头2似乎通过&lt;code&gt;&amp;#39;:&lt;/code&gt; &lt;code&gt;&amp;#39;&lt;/code&gt; De-bedding支持了这一点，但也揭示了一些意外的令牌，当在提示中使用时，它们会弱激活原始的SAE功能。&lt;/li&gt;&lt;/ul&gt;&lt;h1&gt;线性化实验&lt;/h1&gt;&lt;p&gt;由于反向工程过程通过采用梯度线性近似MLP，而MLP是高度非线性的，因此这提出了该方法的准确性。为了获得一些初始直觉，我们进行了一些实验测试这种线性化方法。我们的初步发现是，线性化倾向于为激活SAE特征的输入提供良好的近似值，但其准确性在非激活输入方面却少得多。&lt;/p&gt;&lt;h2&gt; gelu-1L中的&lt;code&gt;&amp;#39;t&lt;/code&gt;功能”的激活转向&lt;/h2&gt;&lt;p&gt;回想我们先前研究的GELU-1L中的&lt;code&gt;&amp;#39;t&lt;/code&gt;功能”。为了了解通过线性化MLP获得的特征向量是否有用，我们执行了激活转向实验，在该实验中，我们将线性化的特征向量添加到模型的MLP激活中，并查看SAE特征得分的变化程度。&lt;/p&gt;&lt;p&gt;特别是，我们查看了“快速棕狐&lt;code&gt;[TOKEN]&lt;/code&gt;狐”）的提示，在那里&lt;code&gt;[TOKEN]&lt;/code&gt;被“测试”，“测试”，“饮食”，“ will”和“ not”代替。 SAE RAW功能分数（没有Relu或偏见；这使我们可以在每个提示中记录最终令牌的SAE功能的部分激活；然后，当在每个提示中添加了线性化的特征向量（带系数1）时，将SAE RAW特征分数记录在MLP激活中时。&lt;/p&gt;&lt;figure class="table"&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;代币&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;干净分数&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;肮脏的分数&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;“测试”&lt;/td&gt;&lt;td&gt; -1.4542&lt;/td&gt;&lt;td&gt; 1.4392&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt; “测试”&lt;/td&gt;&lt;td&gt; -0.9500&lt;/td&gt;&lt;td&gt; 2.4638&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt; “吃”&lt;/td&gt;&lt;td&gt; -0.6423&lt;/td&gt;&lt;td&gt; 3.7210&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt; “将要”&lt;/td&gt;&lt;td&gt; 1.1101&lt;/td&gt;&lt;td&gt; 6.1618&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt; “不”&lt;/td&gt;&lt;td&gt; 2.5207&lt;/td&gt;&lt;td&gt; 8.6271&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;结果可以在上表中找到。我们看到，在所有情况下，用线性化特征向量的激活转向确实会提高原始特征得分。但特别是，随着原始令牌会激活SAE功能，激活转向变得更加有效。现在，我们通过在一个高激活的示例上区分了功能向量，并且直观地，示例与示例相比，该功能火灾更相似，因此，该功能矢量却不奇怪，因此此功能向量的工作原理越多，越多，越好原始令牌激活该功能。&lt;/p&gt;&lt;h2&gt; Gelu-1L中的&lt;code&gt;&amp;#39;t&lt;/code&gt;特征”的线性化余弦相似性&lt;/h2&gt;&lt;p&gt;再次，我们查看了Gelu-1L中的&lt;code&gt;&amp;#39;t&lt;/code&gt;功能”，以及“快速棕狐&lt;code&gt;[TOKEN]&lt;/code&gt; ”形式的提示，在该形式中， &lt;code&gt;[TOKEN]&lt;/code&gt;被“测试”，“测试”，“ EAT”，“ WILL WILL”代替。 ”和“不”。我们计算了这些提示中每个提示的最后一个令牌计算的线性化特征与在最后一个令牌上计算的线性化功能之间的余弦相似性，以提示“快速棕色狐狸没有”。我们通过冻结分层和通过分层区分来做到这一点。结果在下表中提供。&lt;/p&gt;&lt;figure class="table"&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;代币&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;冷冻分层&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;分化的分层&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;“测试”&lt;/td&gt;&lt;td&gt; 0.7745&lt;/td&gt;&lt;td&gt; 0.3155&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt; “测试”&lt;/td&gt;&lt;td&gt; 0.7897&lt;/td&gt;&lt;td&gt; 0.3500&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt; “吃”&lt;/td&gt;&lt;td&gt; 0.8373&lt;/td&gt;&lt;td&gt; 0.4508&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt; “将要”&lt;/td&gt;&lt;td&gt; 0.9095&lt;/td&gt;&lt;td&gt; 0.6253&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt; “不”&lt;/td&gt;&lt;td&gt; 0.9570&lt;/td&gt;&lt;td&gt; 0.8189&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;有两个含义跳出来。首先，当我们将MLP线性化的令牌激活SAE功能时，余弦相似性变得更高。我们假设这是因为激活SAE特征的示例往往位于MLP具有相似行为的激活空间的相似区域。&lt;/p&gt;&lt;p&gt;其次，当余弦相似性与分层线性化时，余弦相似性之间存在鲜明的对比，尤其是对于激活SAE的代币而言，它的特征最少。这违背了最初的直觉，即分层不会极大地影响特征向量方向。&lt;/p&gt;&lt;p&gt; Neel建议这可能是因为在数学上，地图&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;V&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;|&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mfrac MJXc-space3"&gt;&lt;span class="mjx-box MJXc-stacked" style="width: 0.878em; padding: 0px 0.12em;"&gt;&lt;span class="mjx-numerator"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;V&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-denominator"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;|。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;V&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;|&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-line"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-vsize"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 。通过此区分将平行于&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;V的&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;方向设置为零，并且所有其他方向不变，因此在这里，它将特征向量的组件平行于与残留流相平行。如果“ true”特征向量是残差流的重要组成部分，则将删除特征向量的大部分，从而产生错误。参见&lt;a href="https://www.neelnanda.io/mechanistic-interpretability/attribution-patching#layernorm"&gt;&lt;u&gt;Nanda等。&lt;/u&gt;&lt;/a&gt;有关在归因修补程序中有关此效果的更多讨论。鉴于这些结果，我们建议在可能的情况下冻结分层，而不是通过它区分。&lt;/p&gt;&lt;h2&gt; GELU-2L MLP1特征的线性化余弦相似性&lt;/h2&gt;&lt;p&gt;在此实验中，我们研究了前面讨论的GELU-2L MLP1功能。我们采用了48个示例，该示例大约在MLP1 SAE特征原始得分范围内分布（即不考虑偏见和relu）。然后，我们采用了通过在每个示例中取梯度来获得的线性化MLP1特征的成对余弦相似性。结果可以在以下图中找到。 &lt;/p&gt;&lt;figure class="image image_resized" style="width: 58.95%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/93nKtsDL6YY5fRbQv/hper8jjqxbu9ziniwbih" /&gt;&lt;figcaption&gt; MLP1梯度之间的成对余弦相似性&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;此外，我们有以下结果：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;最低激活的24个示例和自身之间的平均成对余弦相似性： &lt;strong&gt;0.3089&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;最低激活的24个示例与最高激活的24个示例之间的平均成对余弦相似性： &lt;strong&gt;0.1097&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;最高激活的24个示例与自身之间的平均成对余弦相似性： &lt;strong&gt;0.7037&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;该实验中的一些有趣的收获：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;我们看到，较高激活的例子彼此具有较高的成对余弦相似性。这令人兴奋，因为这表明我们不需要每个示例获得单独的功能向量，我们可以在许多高激活示例中平均每个功能获得一个一致的矢量，这似乎更强大和可靠！&lt;/li&gt;&lt;li&gt;我们似乎观察到不连续的行为：SAE功能开始发射（绘图中途），所有成对余弦的相似性都会更高。请注意，我们在区分时会忽略SAE的依赖，因此它不能成为这种不连续性的来源。&lt;/li&gt;&lt;li&gt;激活最低的示例的梯度都比彼此更相似，而不是吸引最大的示例梯度。&lt;ul&gt;&lt;li&gt;令人惊讶的是，这似乎很弱暗示了最低激活的例子中的某种聚类行为，而不仅仅是激活的例子。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; GELU-2L的线性化特征系数与注射结果&lt;/h2&gt;&lt;p&gt;再次，我们正在考虑GELU-2L案例研究。回想一下，我们以MLP0 SAE功能来表达线性化的MLP1特征向量。这提供了系数的向量，该矢量表示每个MLP0 SAE特征在多大程度上有助于线性化的MLP1特征。让我们称此矢量&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;C&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; ；让MLP0功能的&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;C&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;系数&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;表示&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;为&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;C&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 。&lt;/p&gt;&lt;p&gt;现在，考虑一下如果我们一次将每个MLP0 SAE功能注入每个MLP0 SAE功能，也就是说，我们将MLP0 SAE功能&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;I I&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;添加到模型的Pre-MLP1残差流中，并查看这如何影响这MLP1 SAE原始功能分数（即忽略偏差术语和relu）。如果MLP1是线性的，那么SAE原始特征分数的变化将完全等于&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;C&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 。因此，测量MLP1线性近似的准确性的一种方法是查看&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;C&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;与每个MLP0特征的注入，C与SAE原始特征分数的变化之间的相关性。&lt;/p&gt;&lt;p&gt;首先，我们使用提示&lt;code&gt;{&amp;#39;name&amp;#39;: &amp;#39;&lt;/code&gt;作为基本提示进行了剩余流的编辑。请注意，MLP1 SAE功能在此提示的最后一个令牌上高度激活。结果可以在以下图中找到。 &lt;/p&gt;&lt;figure class="image image_resized" style="width: 62.42%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/93nKtsDL6YY5fRbQv/pvjtobymrlftw26ccozv" /&gt;&lt;figcaption&gt;在高度激活的示例与线性特征系数上注射结果&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;接下来，我们使用提示&lt;code&gt;{&amp;#39;name&amp;#39;: testing testing&lt;/code&gt;作为基本提示，对其进行了编辑。请注意，MLP1 SAE功能不会在此提示的最后一个令牌上激活。结果可以在以下图中找到。 &lt;/p&gt;&lt;figure class="image image_resized" style="width: 64.95%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/93nKtsDL6YY5fRbQv/o6od0kltshhrzukl6kbt" /&gt;&lt;figcaption&gt;注射示例与线性化特征系数的注射结果&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;这些结果产生以下含义：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;线性化非常有效地近似于高度激活的示例，但根本没有有效的例子。&lt;/li&gt;&lt;li&gt;当基本提示弱或未激活时，近似值较弱。&lt;/li&gt;&lt;li&gt;在弱激活或非激活的示例上，基本提示和编辑提示的原始特征得分的差异较小。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;组件的直接分数归因与零消融&lt;/h2&gt;&lt;p&gt;研究线性化疗效的另一种方法如下。给定一组提示，对于每个提示，将不同模型组件的输出（即原始的令牌嵌入和注意Sublayer）的输出为零，并查看这如何改变SAE功能的激活。然后，对于相同的提示，将直接分数归因与线性化SAE功能一起使用，以估计每个组件对SAE功能的重要性。线性化越准确，直接分数归因结果与零消融结果之间的相关性越大。&lt;/p&gt;&lt;p&gt;我们使用前200个激活示例和均匀分布的200个激活示例对每个案例研究的特征进行了此实验。对于每个功能，我们进行了两次实验：一次，通过前MLP分层区分以获得线性化特征向量，而不是通过PRE-MLP分层进行区分（仅考虑其实现的线性转换， ）。&lt;/p&gt;&lt;p&gt;我们发现，总体而言，在激活示例时，具有线性化特征的直接得分归因与零消融得分相关。正如我们先前的线性化结果所表明的那样，我们发现，在顶部激活示例上的相关性比均匀激活示例更大。但是，对于某些特征，通过分层线性化导致相关性较小，而对于其他人来说，冻结分层导致相关性较小。&lt;/p&gt;&lt;p&gt;每个案例研究的具体结果在下面给出。&lt;/p&gt;&lt;h3&gt; gelu-1l &lt;code&gt;(&amp;#39;&lt;/code&gt;功能&lt;/h3&gt;&lt;figure class="image image_resized" style="width: 80.02%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/93nKtsDL6YY5fRbQv/qsttjn1itkpi2g0e5j04" /&gt;&lt;/figure&gt;&lt;p&gt;对于此功能，即使在统一激活的示例（而不仅仅是最高的激活示例），直接得分归因的结果与零消融的结果之间也存在很强的相关性。这表明线性化在此设置中的性能很好。另请注意，使用分层和冻结分层之间的性能似乎没有太大差异。&lt;/p&gt;&lt;h3&gt; gelu-1l“是”功能&lt;/h3&gt;&lt;figure class="image image_resized" style="width: 84.15%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/93nKtsDL6YY5fRbQv/fx08tqlfjvmkz3rjubyf" /&gt;&lt;/figure&gt;&lt;p&gt;对于此功能，我们可以看到，当通过分层区分分化时，直接得分归因与零消融结果高度相关。但是，当冻结分层时，直接得分归因完全停止工作。&lt;/p&gt;&lt;h3&gt; gelu- &lt;code&gt;&amp;#39;t&lt;/code&gt;功能&lt;/h3&gt;&lt;figure class="image image_resized" style="width: 84.21%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/93nKtsDL6YY5fRbQv/yuvm5beqpt1jiw5vp3he" /&gt;&lt;/figure&gt;&lt;p&gt;对于此功能，与线性化特征向量的直接得分归因似乎与通过分化通过LaiseNorm区分获得特征向量时的零消融结果非常吻合。然而，冻结分层对于直接得分归因而产生的性能明显较差，尤其是在统一分布的激活示例集中。&lt;/p&gt;&lt;h3&gt; GELU-1L上下文依赖性“ IS”功能&lt;/h3&gt;&lt;figure class="image image_resized" style="width: 78.71%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/93nKtsDL6YY5fRbQv/emoeshjjvhytgqiouhcx" /&gt;&lt;/figure&gt;&lt;p&gt;对于此功能，直接分数归因结果与零消融结果之间的相关性小于以前的特征，尽管仍然存在不错的相关性。有趣的是，在统一分布的激活示例测试时，冻结分层可以明显地获得直接得分归因的更好的结果。&lt;/p&gt;&lt;h3&gt; Gelu-2l Python词典功能&lt;/h3&gt;&lt;figure class="image image_resized" style="width: 84.79%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/93nKtsDL6YY5fRbQv/zs8h05rwibtjivwkd8xd" /&gt;&lt;/figure&gt;&lt;p&gt; For this feature, we see decent correlation between the results of direct score attribution and zero ablation on the top 200 highest-activating examples, with freezing LayerNorm yielding somewhat better results than differentiating through LayerNorm. However, when testing on the broader set of uniformly-activating examples, the performance of direct score attribution drops precipitously.&lt;/p&gt;&lt;h2&gt; Linearization experiments: overall takeaways and hypotheses&lt;/h2&gt;&lt;ul&gt;&lt;li&gt; Taking the gradient of an MLP&amp;#39;s dot product with an SAE feature seems to be an OK approximation of the MLP&amp;#39;s behavior on inputs that highly activate the SAE feature. However, on inputs that don&amp;#39;t activate the SAE feature, the gradient is not a good approximation of MLP behavior.&lt;ul&gt;&lt;li&gt; A hypothesis as to why this is the case is that inputs that highly activate the SAE feature tend to lie within a cluster in activation space in which the MLP has similar behavior for all points in the cluster. Additionally, the pairwise cosine similarity results that we obtained suggest that there might also be some sort of weak clustering behavior for non-activating examples as well.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt; LayerNorm does not play nice with linearization in small models, where individual tokens&amp;#39; representations take up large portions of the residual stream. We recommend freezing LayerNorm rather than differentiating through it.&lt;/li&gt;&lt;/ul&gt;&lt;h1&gt; Discussion: comparison to causal methods&lt;/h1&gt;&lt;p&gt; A natural question to ask is what our method adds over causal interventions such as path patching. For example, looking at the projection of attention heads onto a given feature vector is an approximation to just zero-ablating the path from that head into the MLP layer for just this SAE feature. Even the weights-based techniques applied, such as de-embedding, may be done causally by path patching the direct path from the embedding into the MLP layer (for just this SAE feature) for each token in the vocab, one at a time.&lt;/p&gt;&lt;p&gt; We think that MLP linearization presents significant advantages in speed, especially for weights-based approaches on larger models, where a forward pass for each token in the vocabulary may be prohibitive! MLP linearization is very mathematically similar to &lt;a href="https://www.neelnanda.io/mechanistic-interpretability/attribution-patching#path-patching"&gt;attribution path patching&lt;/a&gt; , and as such, this relationship between MLP linearization and causal interventions is analogous to that between &lt;a href="https://neelnanda.io/attribution-patching"&gt;&lt;u&gt;attribution patching and activation patching&lt;/u&gt;&lt;/a&gt; . Indeed, attribution patching also takes a gradient-based approximation to a causal intervention, yielding a substantial speed-up at the cost of some reliability ( &lt;a href="https://arxiv.org/abs/2310.10348"&gt;&lt;u&gt;but note that it is surprisingly useful&lt;/u&gt;&lt;/a&gt; &lt;u&gt;!&lt;/u&gt; ).&lt;/p&gt;&lt;p&gt; We also think that MLP linearization has promise for better understanding the SAE features on a more general level than path patching: if the feature vectors obtained via MLP linearization point in similar directions across many examples where the feature fires, then this provides a significant hint about the mechanism underpinning the feature. And we can also use such an averaged feature vector to try to understand the SAE feature on a more input-independent level. However, we also think there are many situations where path patching is sufficient and more reliable, and the feature vectors obtained by MLP linearization may be very different on different examples! As such, we think this is a promising technique that needs further investigation, but it&amp;#39;s not yet a slam dunk.&lt;/p&gt;&lt;h1&gt; &lt;strong&gt;Discussion: Is this approach useful?&lt;/strong&gt;&lt;/h1&gt;&lt;p&gt; We&amp;#39;ve presented 5 case studies of applying MLP linearization to reverse-engineer SAE features. But fundamentally, this approach involves taking linear approximations of highly nonlinear transformations, so there are naturally some major limitations to this method. As such, can we take away an idea of whether this approach is useful, and if so, when?&lt;/p&gt;&lt;h2&gt;好的&lt;/h2&gt;&lt;p&gt;In favor of this approach, we find that it can yield rich, weights-based, input-independent information about what causes a given SAE feature to activate:&lt;/p&gt;&lt;ul&gt;&lt;li&gt; The information provided by de-embeddings is rich in that it allows us to interpret SAE features at the token level according to how different computational paths use these tokens; we personally found this new way of interpreting features to be very cool.&lt;/li&gt;&lt;li&gt; This method is largely weights-based because, with the exception of taking derivatives of MLP sublayers, it relies on the fixed trained model weights rather than internal activations on a given prompt. As a result, this approach is more naturally faithful to the model&amp;#39;s computation than to probing-based methods and is faster and more scalable than causal methods.&lt;/li&gt;&lt;li&gt; This method is largely input-independent in that, unlike traditional attribution methods, it provides information about the model&amp;#39;s computation on all inputs rather than information locally relevant to a single input.&lt;/li&gt;&lt;li&gt; Note that there is some input dependence when we take derivatives of MLP sublayers. However, our linearization experiments suggest that the derivatives of MLP sublayers are very similar across different inputs that highly activate an SAE feature.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt; Using this approach allowed us to construct adversarial prompts that revealed unexpected polysemanticity in certain SAE features; this suggests that this approach can complement existing techniques by picking up on behaviors that they might miss. And with regard to the accuracy of this approach, we found that, particularly on highly-activating examples, this approach often agreed with the results obtained by causal interventions.&lt;/p&gt;&lt;h2&gt;不好的&lt;/h2&gt;&lt;p&gt;Among the limitations of this approach, it still remains to be seen whether this can scale to larger, far more complex models. Additionally, some of the information obtained by this approach can be opaque: in particular, looking at the importance of layer 0 SAE features for a layer 1 SAE feature, the layer 0 features didn&amp;#39;t seem very sparse, limiting our ability to understand later-layer features in terms of earlier-layer ones. (Note that this might also just reflect an unavoidable reality of how models compute using SAE features, but if this is the case, then this still hampers our ability to understand the model using our approach.) Most importantly, the linear approximations of MLPs are not always accurate: in our case study for the &lt;code&gt;&amp;#39;t&lt;/code&gt; feature in GELU-1L, direct score attribution with the linearized feature indicated that attention was not important for the feature, but this was contradicted by a causal attention ablation. Indeed, right now, it&amp;#39;s hard to tell whether the method will be reliable for a given context (although preliminary results suggest greater reliability on highly-activating prompts), and in theory, the linearized feature directions can be totally different for each example.&lt;/p&gt;&lt;h1&gt;结论&lt;/h1&gt;&lt;p&gt;Overall, we think that this is a useful but limited technique; thus, we have high uncertainty on how far it can be applicable. In our preliminary experiments, this approach seems valuable for getting a sense of how a feature is computed, finding hypotheses for feature behavior that other methods might miss, and iterating fast, but it seems like it will be more difficult to get to a point where the approach is fully robust or reliable. We hope to spend the rest of the MATS program exploring the strengths and limitations of this approach to reverse-engineering SAE features.&lt;/p&gt;&lt;h1&gt; Citing this work&lt;/h1&gt;&lt;p&gt; This is ongoing research. If you want to reference any of our current findings or code, we would appreciate reference to&lt;/p&gt;&lt;pre&gt; &lt;code&gt;@misc{dunefsky2024linearization, author= {Dunefsky, Jacob and Chlenski, Philippe, and Rajamanoharan, Senthooran and Nanda, Neel}, url = {https://www.alignmentforum.org/posts/93nKtsDL6YY5fRbQv/case-studies-in-reverse-engineering-sparse-autoencoder}, year = {2024}, howpublished = {Alignment Forum}, title = {Case Studies in Reverse-Engineering Sparse Autoencoder Features by Using MLP Linearization}, }&lt;/code&gt;&lt;/pre&gt;&lt;h1&gt;作者贡献声明&lt;/h1&gt;&lt;p&gt;Jacob and Philippe were core contributors on this project and both contributed equally. Jacob formulated the original reverse-engineering method and wrote the original reverse-engineering code; carried out the case studies for the &lt;code&gt;&amp;#39;{&lt;/code&gt; feature, the &lt;code&gt;&amp;#39;t&lt;/code&gt; feature, the context-dependent &amp;quot;is&amp;quot; feature, and the GELU-2L feature; and carried out the linearization experiments. Philippe performed a feature audit, calculating F1 scores to guide our selection of interesting features to investigate; carried out the case study for the &amp;quot;it is&amp;quot; feature; and refactored and organized code. Sen and Neel gave guidance and feedback throughout the project, including suggesting ideas for causal experiments to test the efficacy of linearization. The original project idea was suggested by Neel.&lt;/p&gt;&lt;h1&gt; Appendix: mathematical details on our method&lt;/h1&gt;&lt;p&gt; In this section, we elaborate with ample mathematical details upon the explanation of our method provided earlier in the post.&lt;/p&gt;&lt;h2&gt; 1-Layer Transformers&lt;/h2&gt;&lt;h3&gt; Finding a feature vector in MLP input space&lt;/h3&gt;&lt;p&gt; Let&amp;#39;s say that we have an SAE feature trained on &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;MLP_out&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;∈&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mtext MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-ams-R"&gt;R&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.615em; padding-left: 0px;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;d&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;model&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; , the output of the MLP sublayer, and we want to understand what causes that feature to activate. Then, the activation of the &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; -th SAE feature on &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;MLP_out&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; is given by &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;ReLU&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-size1-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;w&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%; padding-left: 0px;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;enc&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;MLP_out&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;−&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%; padding-left: 0px;"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;enc&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-size1-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; , where &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;w&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%; padding-left: 0px;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;enc&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; is the &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; -th row of the SAE encoder weight matrix and &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%; padding-left: 0px;"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;enc&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; is the &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; -th value in the encoder bias vector &lt;span class="footnote-reference" id="fnrefszr2mm7vzrh"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnszr2mm7vzrh"&gt;[7]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; . This means that the SAE feature activation is determined by the dot product &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;w&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%; padding-left: 0px;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;enc&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;MLP_out&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; . As such, we can consider &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;w&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%; padding-left: 0px;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;enc&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; to be the relevant feature vector in the output space of the MLP.&lt;/p&gt;&lt;p&gt; Our first task is to determine a feature vector &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;mid&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; in the input space of the MLP that corresponds to &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;w&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%; padding-left: 0px;"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;enc&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; . What this means is that if &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;mid&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; is the input to the MLP, then we want &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;mid&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.12em;"&gt;T&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;mid&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;≈&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space3"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;w&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%; padding-left: 0px;"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;enc&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;MLP_out&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; , which is the same as &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;mid&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.12em;"&gt;T&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;mid&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;≈&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space3"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;w&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%; padding-left: 0px;"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;enc&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;MLP&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;mid&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; . If the MLP were linear, then we could write &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;MLP&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;mid&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; as &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;W&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;mid&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; for some matrix &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;W&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; . In this hypothetical, we would have that &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;w&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%; padding-left: 0px;"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;enc&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;MLP&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;mid&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space3"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;w&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%; padding-left: 0px;"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;enc&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;W&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;mid&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; , implying that &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;mid&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;w&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%; padding-left: 0px;"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;enc&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;W&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.12em;"&gt;T&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; .&lt;/p&gt;&lt;p&gt; Unfortunately, MLPs are not linear in real life! But we can &lt;i&gt;linearly approximate&lt;/i&gt; &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;W&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;i&gt;by taking the gradient of the MLP&lt;/i&gt; . As such, our feature vector in MLP input space, &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;mid&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; , is given by &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;∇&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;w&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%; padding-left: 0px;"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;enc&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;MLP&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;（&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;X&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;）&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;）&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 。&lt;/p&gt;&lt;p&gt; An important question is this: to what extent is this linearization accurate, given that MLPs are in fact highly nonlinear? We have performed some initial investigations into this, which can be found in the section on linearization experiments; we intend to look deeply into this question as we continue our research.&lt;/p&gt;&lt;h3&gt; Different paths&lt;/h3&gt;&lt;p&gt; Now, we have a feature vector &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;mid&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; in the MLP input space, ie the residual stream prior to the MLP sublayer.我们该怎么办？ The first thing that we have to understand is that the residual stream at this point is the sum of two different computational paths in the model: the path directly from the input tokens and the path from the input tokens through the attention sublayer. &lt;span class="footnote-reference" id="fnreff7rrxo2wv7k"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnf7rrxo2wv7k"&gt;[8]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; As such, the activation of &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;mid&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; is given by the sum of the contributions of each path. This means that we can analyze and find feature vectors for each path separately.&lt;/p&gt;&lt;h3&gt; The direct path and de-embedding&lt;/h3&gt;&lt;p&gt; First, let&amp;#39;s look at the direct path. This is the path that implements the computation &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;W&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;E&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;token&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; , where &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;token&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; is the one-hot vector for the token at the current position, and &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;W&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;E&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; is the embedding matrix that maps each token to its embedding. At this point, the activation of &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;mid&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; due to the direct path is given by &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;mid&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.12em;"&gt;T&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;W&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;E&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;token&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; , which is equal to &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-size1-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;W&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;E&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.12em;"&gt;T&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;mid&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-size1-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.851em; padding-left: 0px;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.12em;"&gt;T&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;token&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; . As such, the feature vector in token input space for the direct path is given by &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;W&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;E&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.12em;"&gt;T&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;mid&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; .&lt;/p&gt;&lt;p&gt; Now, &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;W&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;E&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.12em;"&gt;T&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;mid&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; is a vector whose dimension is equal to the number of tokens in the model vocabulary, where the &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; -th entry in the vector represents the amount that token &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; contributes to activating the feature &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;mid&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; . And since &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;mid&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; is just an approximation for the original SAE feature, this means that &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;W&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;E&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.12em;"&gt;T&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;mid&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;i&gt;is an approximation of how much each token in the model&amp;#39;s vocabulary contributes to activating the original SAE feature&lt;/i&gt; .&lt;/p&gt;&lt;p&gt; We refer to this process of obtaining a vector of token scores for a given residual stream feature as &lt;strong&gt;de-embedding&lt;/strong&gt; . De-embedding forms a key part of the reverse-engineering process, as it allows us to analyze at a concrete token level the extent to which each token contributes to the feature. Importantly, this process works for any feature that lives in the residual stream of the model. This means that de-embedding can be used for understanding not just pre-MLP features, but also pre-attention features, and features at different layers of multi-layer models.&lt;/p&gt;&lt;h3&gt;注意力&lt;/h3&gt;&lt;p&gt;Now, let&amp;#39;s look at the path from the tokens to the attention sublayer. The first step is to note that the output of attention, for the destination token at position &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;j&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; , is given by&lt;/p&gt; &lt;span&gt;&lt;span class="mjpage mjpage__block"&gt;&lt;span class="mjx-chtml MJXc-display" style="text-align: center;"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%; padding-left: 0px;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;j&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;attn_out&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-munderover MJXc-space3"&gt;&lt;span class="mjx-itable"&gt;&lt;span class="mjx-row"&gt;&lt;span class="mjx-cell"&gt;&lt;span class="mjx-op" style="padding-left: 1.8em;"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-size2-R" style="padding-top: 0.74em; padding-bottom: 0.74em;"&gt;∑&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-row"&gt;&lt;span class="mjx-under" style="font-size: 70.7%;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;attention head&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-munderover MJXc-space1"&gt;&lt;span class="mjx-itable"&gt;&lt;span class="mjx-row"&gt;&lt;span class="mjx-cell"&gt;&lt;span class="mjx-op" style="padding-left: 2.32em;"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-size2-R" style="padding-top: 0.74em; padding-bottom: 0.74em;"&gt;∑&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-row"&gt;&lt;span class="mjx-under" style="font-size: 70.7%;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;source token index&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space1"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;score&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%; padding-left: 0px;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;pre&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space1"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%; padding-left: 0px;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;j&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;pre&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;W&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;O&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;V&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%; padding-left: 0px;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;pre&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt; where &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%; padding-left: 0px;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;pre&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; is the residual stream before attention (ie after token and positional embeddings) for token &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; , &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;W&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;O&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;V&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; is the OV matrix for head &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; , and &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;score&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%; padding-left: 0px;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;pre&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space1"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%; padding-left: 0px;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;j&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;pre&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; is the attention weight for head &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; from the source token at position &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; to the destination token at position &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;j&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; . &lt;span class="footnote-reference" id="fnref2e6edu41698"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn2e6edu41698"&gt;[9]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt; If we treat attention scores as a constant, only focusing on the OV circuit, then this output is just the sum of linear functions on the source t​okens, one for each head, given by &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;↦&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space3"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;W&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;O&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;V&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; . This means that the feature vector for the OV circuit of head &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; is given by &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%; padding-left: 0px;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;pre&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;W&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;O&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;V&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.12em;"&gt;T&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;mid&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; . De-embedding can be applied directly to this feature vector too, in order to understand which tokens contribute the most (through the OV circuit for this head) to the overall SAE feature.&lt;/p&gt;&lt;p&gt; As for analyzing the QK circuits of attention (ie the part of attention responsible for determining attention scores between tokens), we found that the best way to do this was to directly calculate the QK scores for pairs of tokens relevant in the OV circuit, or between different token positions. Examples of this can be found in our case studies.&lt;/p&gt;&lt;h3&gt; Direct score attribution for individual heads and tokens in attention sublayers&lt;/h3&gt;&lt;p&gt; Recall again the equation for attention sublayers, which explains that &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%; padding-left: 0px;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;j&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;attn_out&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; , the output of attention for the destination token at position &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;j&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; is given by &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%; padding-left: 0px;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;j&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;attn_out&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-munderover MJXc-space3"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-size1-R"&gt;∑&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;attention head&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-munderover MJXc-space1"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-size1-R"&gt;∑&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;source token index&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space1"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;score&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%; padding-left: 0px;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;pre&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space1"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%; padding-left: 0px;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;j&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;pre&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;W&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;O&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;V&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%; padding-left: 0px;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;pre&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt; where &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%; padding-left: 0px;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;pre&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; is the residual stream before attention for token &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; , &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;W&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;O&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;V&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; is the OV matrix for head &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; , and &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;score&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%; padding-left: 0px;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;pre&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space1"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%; padding-left: 0px;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;j&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;pre&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; is the attention weight for head &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; from the source token at position &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; to the destination token at position &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;j&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; .&lt;/p&gt;&lt;p&gt; Applying the idea of direct score attribution to this equation, this means that the contribution of head &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; and source token &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; to feature vector &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; at destination token &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;j&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; is given by &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;score&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%; padding-left: 0px;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;pre&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space1"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%; padding-left: 0px;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;j&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;pre&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mrow MJXc-space1"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-size1-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.12em;"&gt;T&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;W&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;O&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;V&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%; padding-left: 0px;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-type-R"&gt;pre&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-size1-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; .&lt;/p&gt;&lt;p&gt; &lt;strong&gt;The takeaway:&lt;/strong&gt; it&amp;#39;s possible to see how much each token, at each attention head, contributes to a given feature.&lt;/p&gt;&lt;h3&gt; Wait, what about LayerNorms?&lt;/h3&gt;&lt;p&gt; One thing that we haven&amp;#39;t yet mentioned is the ubiquitous presence of LayerNorm nonlinearities in the model. These can be handled by linearizing them in the same way that we approximate MLPs. But, as is discussed in the &lt;a href="https://www.neelnanda.io/mechanistic-interpretability/attribution-patching"&gt;attribution patching post&lt;/a&gt; , LayerNorms, in general, shouldn&amp;#39;t affect the direction of feature vectors, so there is a theoretical basis for them to be ignored. However, we find that this doesn&amp;#39;t always hold in our &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;实验。 Hypotheses as for why this is the case, along with further discussion, can be found in our section on linearization.&lt;/p&gt;&lt;h2&gt; Multi-layer transformers&lt;/h2&gt;&lt;p&gt; Although the above exposition only discusses 1-layer transformers, it is straightforward to extend this to multi-layer transformers. After all, we now know how to propagate feature vectors through every type of sublayer found in a transformer. As such, given a computational path in a multi-layer transformer, we can simply propagate the SAE feature through this computational path by iteratively propagating it through each sublayer in the computational path, as described above.&lt;/p&gt;&lt;ol class="footnotes"&gt;&lt;li class="footnote-item" id="fn5ns12vjb0f8"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnref5ns12vjb0f8"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt; Though the feature vector obtained still depends on the input we differentiated on, so it&amp;#39;s not fully input independent. Each input has a different local linear approximation.&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fna1rxph5d74"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefa1rxph5d74"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt; We note that this may be catchable via non-mechanistic approaches, such as running the models on a large corpus of data and analysing whether the feature activations are highly correlated, as in &lt;a href="https://arxiv.org/abs/2306.09346"&gt;Dravid et al.&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn36xy0nowkoe"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnref36xy0nowkoe"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt; We&amp;#39;re not sure this counts as an adversarial example. Possibly this is a genuinely polysemantic feature that &amp;quot;wants&amp;quot; to fire on &lt;code&gt;ह&lt;/code&gt; too, or possibly it&amp;#39;s undesirable but hard to disentangle. We speculated there might be superposition where the token embeddings were highly similar, but found that other tokens are more similar to &lt;code&gt;(&amp;#39;&lt;/code&gt; than &lt;code&gt;ह&lt;/code&gt; is, but these do not trigger the SAE feature.&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn1tpfjhj7ezr"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnref1tpfjhj7ezr"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt; That is, &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-size1-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;W&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;E&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;W&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%;"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;V&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;W&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%;"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;O&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-size1-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.852em; padding-left: 0px;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.12em;"&gt;T&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;v&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; . Looking at the greatest components in this vector answers the following question: given that head 0 is attending to a position, which tokens, if they were at that position, will activate the feature the most?&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn2belxv37guh"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnref2belxv37guh"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt; Note that &lt;code&gt;&amp;#39;t&lt;/code&gt; basically never appears &lt;i&gt;without&lt;/i&gt; don/can/won/etc as a prefix, so it&amp;#39;s unclear from just the maximum activating examples whether these matter for the SAE feature.&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn0hyqmgltk21m"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnref0hyqmgltk21m"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt; For the sake of intuition regarding why we might expect this: imagine a limit case where we&amp;#39;re looking at a feature at layer 48 of some gigantic transformer. In this exaggerated case, we would probably not expect that this feature can be directly interpreted in terms of the original token embeddings.&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnszr2mm7vzrh"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefszr2mm7vzrh"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt; This formulation assumes that the SAE encoder weight matrix &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;W&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;enc&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; is a &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-ams-R"&gt;R&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.615em; padding-left: 0px;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;d&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;SAE&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;×&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;d&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;model&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; matrix, where &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;d&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;SAE&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; is the hidden dimension of the SAE and &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;d&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;model&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; is the dimension of the input to the SAE (this could be the hidden dimension of the model or the dimensionality of MLP sublayers, depending on which activations the SAE is trained on). In this formulation, vectors multiply on the right of matrices. However, note that certain libraries, such as TransformerLens, use the opposite convention, in which column vectors multiply on the left of matrices.&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnf7rrxo2wv7k"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnreff7rrxo2wv7k"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt; For a clearer explanation of this, refer to the &lt;a href="https://transformer-circuits.pub/2021/framework/index.html#one-layer-attention-only-transformers"&gt;seminal Transformer Circuits paper&lt;/a&gt; .&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn2e6edu41698"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnref2e6edu41698"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt; Note that we ignore bias terms here. We can get away with this when we care about understanding which inputs cause a feature to activate; this is because bias terms merely add a constant to the feature score that&amp;#39;s independent of all inputs. However, when analyzing the different contributions that sublayers have to feature scores (eg when performing analysis via direct score attribution), bias terms should be taken into account.&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnaeodi9xhson"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefaeodi9xhson"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt; The discrepancy between the top tokens when linearizing through LayerNorm and the top tokens without taking into account LayerNorm is explored further in our section on discussing linearization.&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ol&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/93nKtsDL6YY5fRbQv/case-studies-in-reverse-engineering-sparse-autoencoder#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sun, 14 Jan 2024 02:06:00 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/93nKtsDL6YY5fRbQv/case-studies-in-reverse-engineering-sparse-autoencoder</guid></item><item><title>D&amp;amp;D.Sci 超球面分析第 1 部分：数据字段和初步分析</title><link>https://www.lesswrong.com/posts/rAnS5jCQ5r87eMvus/d-and-d-sci-hypersphere-analysis-part-1-datafields-and</link><description>发布于 2024 年 1 月 13 日晚上 8:16（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;这是一篇文章（希望最终是一个简短的系列文章），详细介绍了我对 Abstractapplic &lt;a href="https://www.lesswrong.com/posts/T3iG4MQ76988JBfkq/d-and-d-sci-fi-colonizing-the-superhypersphere"&gt;最近发布的 D&amp;amp;D.Sci 场景&lt;/a&gt;的分析。我决定记录一下我所做的事情 - 如果您打算在没有帮助的情况下自己玩这个场景，那么您应该在阅读本文之前这样做。如果您想在解决方案中使用此信息，请继续。&lt;/p&gt;&lt;h2&gt;原始数据和列&lt;/h2&gt;&lt;p&gt;我首先获取原始数据并将其保存为 csv，然后将其导入到&lt;a href="https://raw.githubusercontent.com/aphyer1992/dndsci_hypersphere/main/dndsci_zppg.py"&gt;Python&lt;/a&gt;和 Excel 中进行使用。我现在不想做任何特定的分析，只是想熟悉数据并看看是否有什么发现。&lt;/p&gt;&lt;p&gt;我们将一一浏览这些列，并在 Excel 中制作一些图表以进行可视化&lt;span class="footnote-reference" id="fnrefxtxl79an65"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnxtxl79an65"&gt;[1]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;经度&lt;/strong&gt;在-180 到+180 之间相对均匀分布。 &lt;/p&gt;&lt;figure class="image"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rAnS5jCQ5r87eMvus/gsxtboyyggz2mxhkjf2f" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;纬度&lt;/strong&gt;可以从 -90 到 +90 变化，但是是双峰的，通常取 45 左右的值 - 我们很少降落在赤道附近或两极附近。 &lt;/p&gt;&lt;figure class="image"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rAnS5jCQ5r87eMvus/qoczvq1wh3qqroqqdqnr" /&gt;&lt;/figure&gt;&lt;p&gt; Shortitude 和 Deltitude 看起来与 Latitude 相同： &lt;/p&gt;&lt;figure class="image"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rAnS5jCQ5r87eMvus/ysq41ourfgbxn7mhb2vb" /&gt;&lt;/figure&gt;&lt;figure class="image"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rAnS5jCQ5r87eMvus/wvowp0jltzgnarvxdgxy" /&gt;&lt;/figure&gt;&lt;p&gt;这可能是您在 4D 球体上的随机点着陆时自然期望的结果 - 当然，+-90 附近的值不太可能自然发生，因为这些是“极点”并且占用的面积较小。我没有足够好的直觉来知道一旦添加更多维度，接近 0 的值是否自然也很少见，或者我们是否出于某种原因避免使用“赤道”。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;奇怪的气味&lt;/strong&gt;通常会有些存在，但很少会非常存在： &lt;/p&gt;&lt;figure class="image"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rAnS5jCQ5r87eMvus/peeleskulznbxszrcn9m" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;空气味道&lt;/strong&gt;有一些不同的值，一些常见，一些罕见： &lt;/p&gt;&lt;figure class="image"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rAnS5jCQ5r87eMvus/wt1p7m3qlzk12qmscuoo" /&gt;&lt;/figure&gt;&lt;p&gt;风水通常是充足的，有时不好，很少有好的： &lt;/p&gt;&lt;figure class="image"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rAnS5jCQ5r87eMvus/u4iqlmilkvafcph6dgoa" /&gt;&lt;/figure&gt;&lt;p&gt;奇怪的声音有五个可能的值，我们一次最多可以看到三个：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;诡异的沉默&lt;/li&gt;&lt;li&gt;不可能的嗡嗡声&lt;/li&gt;&lt;li&gt;超凡脱俗的掠过&lt;/li&gt;&lt;li&gt;异常压制&lt;/li&gt;&lt;li&gt;不自然的嗡嗡声&lt;/li&gt;&lt;/ul&gt;&lt;figure class="image"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rAnS5jCQ5r87eMvus/y09cf6krublcyreepjxe" /&gt;&lt;/figure&gt;&lt;p&gt;当没有其他声音出现时，诡异的寂静就出现了。&lt;/p&gt;&lt;p&gt;掠过声和嗡嗡声永远不会同时发生（尽管它们分别是最常见的声音）。&lt;/p&gt;&lt;p&gt; Pi 的局部值的分布看起来非常整齐： &lt;/p&gt;&lt;figure class="image"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rAnS5jCQ5r87eMvus/es5olev4slmim6cnbmi0" /&gt;&lt;/figure&gt;&lt;p&gt; 3.141 + (1d41-1d41)/1000 从字面上看并不准确，因为我们看到的小数位数比这更多，但这可能是一种合理的思考方式？&lt;/p&gt;&lt;p&gt;墨菲常数有一个非常奇怪的分布： &lt;/p&gt;&lt;figure class="image"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rAnS5jCQ5r87eMvus/cbhhf3e2xbk5cdouyymz" /&gt;&lt;/figure&gt;&lt;p&gt;我会猜测这样的事情：&lt;/p&gt;&lt;ul&gt;&lt;li&gt; MC 的分布与我们看到 Pi 的分布类型相同，概率从 0 线性增加到 ~5.5，然后从 ~5.5 线性增加到 ~11。&lt;/li&gt;&lt;li&gt;但我们光辉帝国从来不选择MC&amp;gt;6的地盘。&lt;/li&gt;&lt;li&gt;此外，我们在 MC=4 和 MC=5 处设置了一些限制或条件，使得感知频率在这些点处下降。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;性能看起来不太好： &lt;/p&gt;&lt;figure class="image"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rAnS5jCQ5r87eMvus/humjat6wv5du0g3yyf2p" /&gt;&lt;/figure&gt;&lt;p&gt;在 10,000 多个站点中，有&lt;strong&gt;2 个&lt;/strong&gt;站点的性能 &amp;gt;=100%。&lt;/p&gt;&lt;p&gt;诚然，我们确实有 11 万个预先批准的站点可供选择。这表明在全部可能的数据中大约有 20 个站点的性能可以接受——这反过来又表明我们的任务不会有太多余地。如果我们不能几乎完美地识别影响性能的&lt;strong&gt;所有&lt;/strong&gt;因素，我们实际上就无法找到 12 100%+ 的站点。&lt;/p&gt;&lt;h2&gt;重新格式化以供使用&lt;/h2&gt;&lt;p&gt;我调整了一些列以使其更加用户友好：&lt;/p&gt;&lt;ul&gt;&lt;li&gt; “Weird Sounds”变成了 4 个独立的布尔列来表示不同的噪音（忽略了当没有其他噪音时我们得到的“Silence”）。&lt;/li&gt;&lt;li&gt; “奇怪的气味？”而“周边风水”则变成从0到2的数字：0表示没有气味/风水不好，1表示有气味/风水好，2表示气味极重/风水好。&lt;/li&gt;&lt;li&gt; “Air Tastes Like” 变成 4 个独立的布尔列（适用于除“Nothing”之外的所有气味）&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;然后将结果输出为一个我认为更方便的&lt;a href="https://raw.githubusercontent.com/aphyer1992/dndsci_hypersphere/main/dndsci_zppg_formatted.csv"&gt;新文件&lt;/a&gt;。&lt;/p&gt;&lt;h2&gt;相关矩阵&lt;/h2&gt;&lt;p&gt;我们可以利用这些数据做的最简单有用的事情之一就是构建一个相关矩阵。 &lt;span class="footnote-reference" id="fnrefegy9fnduwep"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnegy9fnduwep"&gt;[2]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;figure class="table"&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;洛&lt;/td&gt;&lt;td&gt;拉&lt;/td&gt;&lt;td&gt;什&lt;/td&gt;&lt;td&gt;德&lt;/td&gt;&lt;td&gt;圆周率&lt;/td&gt;&lt;td&gt;亩&lt;/td&gt;&lt;td&gt;闻&lt;/td&gt;&lt;td&gt;冯&lt;/td&gt;&lt;td&gt;应用程序&lt;/td&gt;&lt;td&gt;烧伤&lt;/td&gt;&lt;td&gt;科普&lt;/td&gt;&lt;td&gt;薄荷&lt;/td&gt;&lt;td&gt;哼&lt;/td&gt;&lt;td&gt;短剧&lt;/td&gt;&lt;td&gt;斯奎&lt;/td&gt;&lt;td&gt;嗡嗡声&lt;/td&gt;&lt;td&gt;性能&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;洛&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td style="background-color: #F8E984; text-align: right;"&gt;0.01&lt;/td&gt;&lt;td style="background-color: #FEE883; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #FCEA84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FEEB84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FEEA83; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FCEB84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FEE983; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #FEE983; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #FEE983; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #F3E884; text-align: right;"&gt; 0.02&lt;/td&gt;&lt;td style="background-color: #FCEB84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FCEA84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FCEB84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FEE983; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #FDEB84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #D2DE82; text-align: right;"&gt; 0.07&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;拉&lt;/td&gt;&lt;td style="background-color: #F8E984; text-align: right;"&gt;0.01&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td style="background-color: #FEEA83; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FEE983; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #FEEB84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FCEA84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #F3E884; text-align: right;"&gt; 0.02&lt;/td&gt;&lt;td style="background-color: #FAEA84; text-align: right;"&gt; 0.01&lt;/td&gt;&lt;td style="background-color: #F6E984; text-align: right;"&gt; 0.01&lt;/td&gt;&lt;td style="background-color: #FEE783; text-align: right;"&gt; -0.02&lt;/td&gt;&lt;td style="background-color: #FDEB84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FCEB84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FDEB84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #F4E884; text-align: right;"&gt; 0.02&lt;/td&gt;&lt;td style="background-color: #FEE883; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #FEE783; text-align: right;"&gt; -0.02&lt;/td&gt;&lt;td style="background-color: #FBEA84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;什&lt;/td&gt;&lt;td style="background-color: #FEE883; text-align: right;"&gt;-0.01&lt;/td&gt;&lt;td style="background-color: #FEEA83; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td style="background-color: #F9EA84; text-align: right;"&gt; 0.01&lt;/td&gt;&lt;td style="background-color: #FEE983; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #FEEB84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FEEA83; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #FEE783; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #FEEA83; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FEE883; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #FEE983; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #F7E984; text-align: right;"&gt; 0.01&lt;/td&gt;&lt;td style="background-color: #FEE783; text-align: right;"&gt; -0.02&lt;/td&gt;&lt;td style="background-color: #F8E984; text-align: right;"&gt; 0.01&lt;/td&gt;&lt;td style="background-color: #FEE983; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #FEE983; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #FBEA84; text-align: right;"&gt; 0.01&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;德&lt;/td&gt;&lt;td style="background-color: #FCEA84; text-align: right;"&gt;0.00&lt;/td&gt;&lt;td style="background-color: #FEE983; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #F9EA84; text-align: right;"&gt; 0.01&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td style="background-color: #FEEA83; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #F0E784; text-align: right;"&gt; 0.02&lt;/td&gt;&lt;td style="background-color: #FEEB84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FEEA83; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FEE983; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #F6E984; text-align: right;"&gt; 0.01&lt;/td&gt;&lt;td style="background-color: #F5E984; text-align: right;"&gt; 0.02&lt;/td&gt;&lt;td style="background-color: #FEE983; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #FEE983; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #FEE983; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #FEEA83; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #FBEA84; text-align: right;"&gt; 0.01&lt;/td&gt;&lt;td style="background-color: #FDEB84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;圆周率&lt;/td&gt;&lt;td style="background-color: #FEEB84; text-align: right;"&gt;0.00&lt;/td&gt;&lt;td style="background-color: #FEEB84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FEE983; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #FEEA83; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td style="background-color: #FEE983; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #FEEA83; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FEEA83; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #FEE683; text-align: right;"&gt; -0.02&lt;/td&gt;&lt;td style="background-color: #FBEA84; text-align: right;"&gt; 0.01&lt;/td&gt;&lt;td style="background-color: #FEEA83; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FEEA83; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FEEB84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #F7E984; text-align: right;"&gt; 0.01&lt;/td&gt;&lt;td style="background-color: #FEEB84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FEE983; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #BDD881; text-align: right;"&gt; 0.11&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;亩&lt;/td&gt;&lt;td style="background-color: #FEEA83; text-align: right;"&gt;0.00&lt;/td&gt;&lt;td style="background-color: #FCEA84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FEEB84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #F0E784; text-align: right;"&gt; 0.02&lt;/td&gt;&lt;td style="background-color: #FEE983; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td style="background-color: #FEEA83; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #F9EA84; text-align: right;"&gt; 0.01&lt;/td&gt;&lt;td style="background-color: #FFEB84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FBEA84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FEEB84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FFEB84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FBEA84; text-align: right;"&gt; 0.01&lt;/td&gt;&lt;td style="background-color: #FEEA83; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FEE783; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #FEEA83; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #F98871; text-align: right;"&gt; -0.40&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;闻&lt;/td&gt;&lt;td style="background-color: #FCEB84; text-align: right;"&gt;0.00&lt;/td&gt;&lt;td style="background-color: #F3E884; text-align: right;"&gt; 0.02&lt;/td&gt;&lt;td style="background-color: #FEEA83; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #FEEB84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FEEA83; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FEEA83; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td style="background-color: #FEE983; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #FEEA83; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FEEA83; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FEEA83; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FCEB84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #F5E984; text-align: right;"&gt; 0.01&lt;/td&gt;&lt;td style="background-color: #FEE683; text-align: right;"&gt; -0.02&lt;/td&gt;&lt;td style="background-color: #FEE983; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #F7E984; text-align: right;"&gt; 0.01&lt;/td&gt;&lt;td style="background-color: #FCC37C; text-align: right;"&gt; -0.16&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;冯&lt;/td&gt;&lt;td style="background-color: #FEE983; text-align: right;"&gt;-0.01&lt;/td&gt;&lt;td style="background-color: #FAEA84; text-align: right;"&gt; 0.01&lt;/td&gt;&lt;td style="background-color: #FEE783; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #FEEA83; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FEEA83; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #F9EA84; text-align: right;"&gt; 0.01&lt;/td&gt;&lt;td style="background-color: #FEE983; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td style="background-color: #F9EA84; text-align: right;"&gt; 0.01&lt;/td&gt;&lt;td style="background-color: #FEE783; text-align: right;"&gt; -0.02&lt;/td&gt;&lt;td style="background-color: #FEE883; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #FAEA84; text-align: right;"&gt; 0.01&lt;/td&gt;&lt;td style="background-color: #FEE883; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #FBEA84; text-align: right;"&gt; 0.01&lt;/td&gt;&lt;td style="background-color: #FFEB84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FEE883; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #C0D981; text-align: right;"&gt; 0.10&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;应用程序&lt;/td&gt;&lt;td style="background-color: #FEE983; text-align: right;"&gt;-0.01&lt;/td&gt;&lt;td style="background-color: #F6E984; text-align: right;"&gt; 0.01&lt;/td&gt;&lt;td style="background-color: #FEEA83; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FEE983; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #FEE683; text-align: right;"&gt; -0.02&lt;/td&gt;&lt;td style="background-color: #FFEB84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FEEA83; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #F9EA84; text-align: right;"&gt; 0.01&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td style="background-color: #FDC67C; text-align: right;"&gt; -0.15&lt;/td&gt;&lt;td style="background-color: #FDD17F; text-align: right;"&gt; -0.11&lt;/td&gt;&lt;td style="background-color: #F97C6E; text-align: right;"&gt; -0.45&lt;/td&gt;&lt;td style="background-color: #FEE983; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #FEEA83; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FDEB84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FCEB84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FED980; text-align: right;"&gt; -0.07&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;烧伤&lt;/td&gt;&lt;td style="background-color: #FEE983; text-align: right;"&gt;-0.01&lt;/td&gt;&lt;td style="background-color: #FEE783; text-align: right;"&gt; -0.02&lt;/td&gt;&lt;td style="background-color: #FEE883; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #F6E984; text-align: right;"&gt; 0.01&lt;/td&gt;&lt;td style="background-color: #FBEA84; text-align: right;"&gt; 0.01&lt;/td&gt;&lt;td style="background-color: #FBEA84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FEEA83; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FEE783; text-align: right;"&gt; -0.02&lt;/td&gt;&lt;td style="background-color: #FDC67C; text-align: right;"&gt; -0.15&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td style="background-color: #FEDE81; text-align: right;"&gt; -0.05&lt;/td&gt;&lt;td style="background-color: #FCB379; text-align: right;"&gt; -0.23&lt;/td&gt;&lt;td style="background-color: #FDEB84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FEEA83; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #FCEA84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FAEA84; text-align: right;"&gt; 0.01&lt;/td&gt;&lt;td style="background-color: #C5DB81; text-align: right;"&gt; 0.09&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;科普&lt;/td&gt;&lt;td style="background-color: #F3E884; text-align: right;"&gt;0.02&lt;/td&gt;&lt;td style="background-color: #FDEB84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FEE983; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #F5E984; text-align: right;"&gt; 0.02&lt;/td&gt;&lt;td style="background-color: #FEEA83; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FEEB84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FEEA83; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FEE883; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #FDD17F; text-align: right;"&gt; -0.11&lt;/td&gt;&lt;td style="background-color: #FEDE81; text-align: right;"&gt; -0.05&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td style="background-color: #FCC37C; text-align: right;"&gt; -0.16&lt;/td&gt;&lt;td style="background-color: #F2E884; text-align: right;"&gt; 0.02&lt;/td&gt;&lt;td style="background-color: #F7E984; text-align: right;"&gt; 0.01&lt;/td&gt;&lt;td style="background-color: #FDEB84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FFEB84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #E8E583; text-align: right;"&gt; 0.04&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;薄荷&lt;/td&gt;&lt;td style="background-color: #FCEB84; text-align: right;"&gt;0.00&lt;/td&gt;&lt;td style="background-color: #FCEB84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #F7E984; text-align: right;"&gt; 0.01&lt;/td&gt;&lt;td style="background-color: #FEE983; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #FEEA83; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FFEB84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FCEB84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FAEA84; text-align: right;"&gt; 0.01&lt;/td&gt;&lt;td style="background-color: #F97C6E; text-align: right;"&gt; -0.45&lt;/td&gt;&lt;td style="background-color: #FCB379; text-align: right;"&gt; -0.23&lt;/td&gt;&lt;td style="background-color: #FCC37C; text-align: right;"&gt; -0.16&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td style="background-color: #FEEA83; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #FBEA84; text-align: right;"&gt; 0.01&lt;/td&gt;&lt;td style="background-color: #FEEA83; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FEE983; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #63BE7B; text-align: right;"&gt; 0.26&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;哼&lt;/td&gt;&lt;td style="background-color: #FCEA84; text-align: right;"&gt;0.00&lt;/td&gt;&lt;td style="background-color: #FDEB84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FEE783; text-align: right;"&gt; -0.02&lt;/td&gt;&lt;td style="background-color: #FEE983; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #FEEB84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FBEA84; text-align: right;"&gt; 0.01&lt;/td&gt;&lt;td style="background-color: #F5E984; text-align: right;"&gt; 0.01&lt;/td&gt;&lt;td style="background-color: #FEE883; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #FEE983; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #FDEB84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #F2E884; text-align: right;"&gt; 0.02&lt;/td&gt;&lt;td style="background-color: #FEEA83; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td style="background-color: #F5E984; text-align: right;"&gt; 0.01&lt;/td&gt;&lt;td style="background-color: #FBEA84; text-align: right;"&gt; 0.01&lt;/td&gt;&lt;td style="background-color: #FEEA83; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #F8786D; text-align: right;"&gt; -0.47&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;短剧&lt;/td&gt;&lt;td style="background-color: #FCEB84; text-align: right;"&gt;0.00&lt;/td&gt;&lt;td style="background-color: #F4E884; text-align: right;"&gt; 0.02&lt;/td&gt;&lt;td style="background-color: #F8E984; text-align: right;"&gt; 0.01&lt;/td&gt;&lt;td style="background-color: #FEE983; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #F7E984; text-align: right;"&gt; 0.01&lt;/td&gt;&lt;td style="background-color: #FEEA83; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FEE683; text-align: right;"&gt; -0.02&lt;/td&gt;&lt;td style="background-color: #FBEA84; text-align: right;"&gt; 0.01&lt;/td&gt;&lt;td style="background-color: #FEEA83; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FEEA83; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #F7E984; text-align: right;"&gt; 0.01&lt;/td&gt;&lt;td style="background-color: #FBEA84; text-align: right;"&gt; 0.01&lt;/td&gt;&lt;td style="background-color: #F5E984; text-align: right;"&gt; 0.01&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td style="background-color: #FFEB84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #F8696B; text-align: right;"&gt; -0.53&lt;/td&gt;&lt;td style="background-color: #D7E082; text-align: right;"&gt; 0.06&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;斯奎&lt;/td&gt;&lt;td style="background-color: #FEE983; text-align: right;"&gt;-0.01&lt;/td&gt;&lt;td style="background-color: #FEE883; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #FEE983; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #FEEA83; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #FEEB84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FEE783; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #FEE983; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #FFEB84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FDEB84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FCEA84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FDEB84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FEEA83; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FBEA84; text-align: right;"&gt; 0.01&lt;/td&gt;&lt;td style="background-color: #FFEB84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td style="background-color: #F6E984; text-align: right;"&gt; 0.01&lt;/td&gt;&lt;td style="background-color: #FBA276; text-align: right;"&gt; -0.29&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;嗡嗡声&lt;/td&gt;&lt;td style="background-color: #FDEB84; text-align: right;"&gt;0.00&lt;/td&gt;&lt;td style="background-color: #FEE783; text-align: right;"&gt; -0.02&lt;/td&gt;&lt;td style="background-color: #FEE983; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #FBEA84; text-align: right;"&gt; 0.01&lt;/td&gt;&lt;td style="background-color: #FEE983; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #FEEA83; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #F7E984; text-align: right;"&gt; 0.01&lt;/td&gt;&lt;td style="background-color: #FEE883; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #FCEB84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FAEA84; text-align: right;"&gt; 0.01&lt;/td&gt;&lt;td style="background-color: #FFEB84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FEE983; text-align: right;"&gt; -0.01&lt;/td&gt;&lt;td style="background-color: #FEEA83; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #F8696B; text-align: right;"&gt; -0.53&lt;/td&gt;&lt;td style="background-color: #F6E984; text-align: right;"&gt; 0.01&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td style="background-color: #FCC17B; text-align: right;"&gt; -0.17&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;性能&lt;/td&gt;&lt;td style="background-color: #D2DE82; text-align: right;"&gt;0.07&lt;/td&gt;&lt;td style="background-color: #FBEA84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #FBEA84; text-align: right;"&gt; 0.01&lt;/td&gt;&lt;td style="background-color: #FDEB84; text-align: right;"&gt; 0.00&lt;/td&gt;&lt;td style="background-color: #BDD881; text-align: right;"&gt; 0.11&lt;/td&gt;&lt;td style="background-color: #F98871; text-align: right;"&gt; -0.40&lt;/td&gt;&lt;td style="background-color: #FCC37C; text-align: right;"&gt; -0.16&lt;/td&gt;&lt;td style="background-color: #C0D981; text-align: right;"&gt; 0.10&lt;/td&gt;&lt;td style="background-color: #FED980; text-align: right;"&gt; -0.07&lt;/td&gt;&lt;td style="background-color: #C5DB81; text-align: right;"&gt; 0.09&lt;/td&gt;&lt;td style="background-color: #E8E583; text-align: right;"&gt; 0.04&lt;/td&gt;&lt;td style="background-color: #63BE7B; text-align: right;"&gt; 0.26&lt;/td&gt;&lt;td style="background-color: #F8786D; text-align: right;"&gt; -0.47&lt;/td&gt;&lt;td style="background-color: #D7E082; text-align: right;"&gt; 0.06&lt;/td&gt;&lt;td style="background-color: #FBA276; text-align: right;"&gt; -0.29&lt;/td&gt;&lt;td style="background-color: #FCC17B; text-align: right;"&gt; -0.17&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有几点值得注意：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;大多数条目对几乎没有相关性。这使得诸如“邪恶的压制生物有铜的气味”或“薄荷味驱赶蜂鸣器”之类的理论变得非常难以置信。&lt;/li&gt;&lt;li&gt;所有空气味道都是负相关的（显然，因为它们是相互排斥的）。&lt;/li&gt;&lt;li&gt;掠过和嗡嗡声是非常负相关的，如上所述，它们永远不会同时发生。 （飞掠者吃掉蜂鸣器？）&lt;/li&gt;&lt;li&gt;性能与几个不同的变量相关。乍一看显示：&lt;ul&gt;&lt;li&gt;受到墨菲常数的严重伤害（我猜这个常数衡量了他的定律的强度？）&lt;/li&gt;&lt;li&gt;受到嗡嗡声、静噪，尤其是嗡嗡声的伤害。&lt;/li&gt;&lt;li&gt;薄荷的空气味道很好，其他一些味道稍好或稍差。&lt;/li&gt;&lt;li&gt;较高的 Pi 值和较好的风水略好。&lt;/li&gt;&lt;li&gt;气味较浓则稍差。&lt;/li&gt;&lt;li&gt;奇怪的是，经度越高似乎越好。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;该分析有两个主要局限性。&lt;/p&gt;&lt;ol&gt;&lt;li&gt;毫无疑问，您已经被告知了一千万次，相关性并不意味着因果关系。 &lt;span class="footnote-reference" id="fnrefcao6y67v2tf"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fncao6y67v2tf"&gt;[3]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;特别是，我们看到 Skittering 看起来有一点好处，但我怀疑这是海市蜃楼。 Skittering 和 Buzzing 是互斥的，而 Buzzing 是相当糟糕的。掠过者可能很糟糕，但不像蜂鸣器那么糟糕：这表明我们也应该避免掠过，并支持沉默。&lt;/li&gt;&lt;li&gt;数据中可能存在的某些模式不会正确反映在这些数字中。&lt;ol&gt;&lt;li&gt;如果在经度 +83 处有一片美妙的绿洲，我们会发现经度和性能之间存在非常轻微的正相关关系。&lt;/li&gt;&lt;li&gt;如果风水有一点是好的，但很多是坏的，我们就不会跟踪得那么好。&lt;/li&gt;&lt;li&gt;如果有一种相互作用，悍马在赤道附近很平静，但在远离赤道的地方很凶猛，我们会完全错过它。&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h2&gt;不充分的初步猜测&lt;/h2&gt;&lt;p&gt;如果我尝试使用此处的信息进行初步猜测：&lt;/p&gt;&lt;p&gt;我们有&lt;strong&gt;大量&lt;/strong&gt;可供尝试的网站。即使在几个要求之后：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;一定是沉默。&lt;/li&gt;&lt;li&gt;必须没有任何奇怪的气味。&lt;/li&gt;&lt;li&gt;空气中一定有薄荷的味道。&lt;/li&gt;&lt;li&gt;风水至少必须是足够的。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;我们还有 2628 个条目。我根据墨菲常数/pi/一点经度的猜测得分建议尝试以下网站：&lt;/p&gt;&lt;p&gt; 6123&lt;br /&gt; 10709&lt;br /&gt; 11789&lt;br /&gt; 16118&lt;br /&gt; 23695&lt;br /&gt; 24728&lt;br /&gt; 29720&lt;br /&gt; 33672&lt;br /&gt; 36008&lt;br /&gt; 48703&lt;br /&gt; 53187&lt;br /&gt; 61818&lt;/p&gt;&lt;p&gt;然而，当我将相同的逻辑应用于主数据集并查看此类网站中的生成器实际得分情况时，它们往往在 50-90% 的范围内。这比 23% 的总体平均水平要好得多，但显然还不够好，以至于我应该冒险去尝试。&lt;/p&gt;&lt;p&gt;据推测，深入研究经度/纬度/短度/纬度将提供更多细节。我会在某个时候这样做，并尝试将我所做的事情写下来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;ol class="footnotes"&gt;&lt;li class="footnote-item" id="fnxtxl79an65"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefxtxl79an65"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt; （个人喜好说明：Excel 是一种非常糟糕的编程语言，对于那些真正应该只使用 Python 的人来说，它习惯于解决简单的编程任务，但它是一个非常好的数据可视化工具。）&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnegy9fnduwep"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefegy9fnduwep"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;这些非常有用，如果您使用粘贴转置和混合绝对引用，那么在 Excel 中制作起来确实非常容易。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fncao6y67v2tf"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefcao6y67v2tf"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt; （我讨厌这种特殊的措辞，因为“暗示”这个词可以用来表示“证明”或“建议”，虽然相关性并不能证明因果关系，但它确实表明了因果关系。）&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ol&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/rAnS5jCQ5r87eMvus/d-and-d-sci-hypersphere-analysis-part-1-datafields-and#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sat, 13 Jan 2024 20:16:39 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/rAnS5jCQ5r87eMvus/d-and-d-sci-hypersphere-analysis-part-1-datafields-and</guid></item><item><title>一些额外的 SAE 想法</title><link>https://www.lesswrong.com/posts/fqgn56tS5AgjmDpnX/some-additional-sae-thoughts</link><description>发布于 2024 年 1 月 13 日晚上 7:31（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;&lt;i&gt;感谢 Lee Sharkey 对第一部分的反馈，感谢 Lee Sharkey、Jake Mendel、Kaarel Hänni 和 LISA 办公室的其他人围绕本文进行的对话。&lt;/i&gt;&lt;/p&gt;&lt;p&gt;这篇文章是过去几个月的一些小实验和想法的集合，这些实验和想法从未真正变成更大的东西，但有助于澄清我对一些与 SAE 相关的事情的想法。我现在已经加入了 Anthropic 的可解释性团队，但这里写的所有内容都来自该日期之前。&lt;/p&gt;&lt;h2&gt; MLP 中的分布式特征如何发挥作用？&lt;/h2&gt;&lt;h3&gt;概括&lt;/h3&gt;&lt;p&gt;在撰写最初的 SAE 论文时，我对以下论点感到困扰：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;要了解 MLP 层的作用，我们需要了解非线性如何作用于数据，因为这就是新颖的计算所在。&lt;/li&gt;&lt;li&gt;稀疏自动编码器在变压器 MLP 的后非线性中发现的特征似乎在神经元之间分布非常紧密。&lt;/li&gt;&lt;li&gt;分布在大量神经元上的特征只会对每个单独的神经元产生微小的影响，并且神经元在如此小的变化上将近似线性。&lt;/li&gt;&lt;li&gt;因此，这些分布式特征不可能是真正的动作所在，我们需要以某种方式将神经元基础纳入我们理解 MLP 特征的方式中。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;我不再相信这一点，最重要的是，因为我意识到，&lt;strong&gt;当一个特征分布在&lt;/strong&gt;&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;n 个&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;神经元上时，可以通过将输入特征的大小放大&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msqrt"&gt;&lt;span class="mjx-box"&gt;&lt;span class="mjx-surd"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;√&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-box" style="border-top: 1px solid;"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;strong&gt;倍来恢复原始非线性&lt;/strong&gt;&lt;strong&gt;。&lt;/strong&gt;事实上，&lt;i&gt;它只&lt;/i&gt;需要缩放&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msqrt"&gt;&lt;span class="mjx-box"&gt;&lt;span class="mjx-surd"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;√&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-box" style="border-top: 1px solid;"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; ，而不是隐含在我的非正式论证中的&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; （当我考虑缩放的可能性时......）使得使用多个高度分布的特征变得可行，只要少数功能处于活动状态。&lt;/p&gt;&lt;p&gt;代码可&lt;a href="https://github.com/HoagyC/distrib_feats/blob/main/streamlit_page.py"&gt;在 GitHub 上&lt;/a&gt;获取。&lt;/p&gt;&lt;h3&gt;非线性的需要&lt;/h3&gt;&lt;p&gt;多层模型作用于输入数据，将其在每一层处理成新的结构。如果我们查看每个连续层中存在的信息总量，我们知道信息量只会下降，因为较高层中存在的任何信息都是直接从较低层计算出来的，因此信息也必须是存在于那些较低层中。&lt;/p&gt;&lt;p&gt;然而，模型&lt;i&gt;可以&lt;/i&gt;做的是通过多层的操作使数据的特定功能&lt;i&gt;更容易访问&lt;/i&gt;。特别是，该模型使得线性探针能够提取丰富的特征，例如最近在&lt;a href="https://arxiv.org/abs/2310.02207"&gt;代表空间和时间的语言模型&lt;/a&gt;以及许多以前的作品中看到的那样。&lt;/p&gt;&lt;p&gt;线性探针是对哪些信息真正&lt;i&gt;可用的&lt;/i&gt;自然测试，而不是隐藏在高维数据流形中，因为线性可用允许后续神经元根据数据的此特征的存在或不存在进行调节。&lt;/p&gt;&lt;p&gt;重要的是，数据的线性变换不能线性地提供新信息。如果我们有一个线性变换&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;A&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; ，它允许我们使用线性探针&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;σ&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;p&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;⋅&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;A&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;从特征激活&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;中提取信息，那么我们可以通过定义新的探针&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;σ&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;p&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px;"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;′&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;⋅&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;来消除对&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;A&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;的需要使用&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;p&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px;"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;′&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space3"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;A&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.12em;"&gt;T&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;p&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 。因此，如果我们想要在这个意义上提供新信息，我们将需要对其进行非线性变换，这就是变压器的 MLP 层的用武之地。&lt;/p&gt;&lt;h3&gt;基础对齐和分布式特征&lt;/h3&gt;&lt;p&gt;考虑到这种非线性需求，我想探索如何在模型中构建特征，在该模型中，我们从将 MLP 中的特征视为单个神经元的输出，转变为分布在多个神经元的方向。神经元。 &lt;span class="footnote-reference" id="fnref6iedrpq2n4a"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn6iedrpq2n4a"&gt;[1]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;在本节中，我将使用 100 个神经元 MLP 和两个潜在特征，一个与单个神经元对齐，另一个分布在所有 100 个神经元中。这些将有助于理解基础对齐特征和分布式特征之间的一些简单差异。&lt;/p&gt;&lt;p&gt;特征被定义为具有单位范数的方向，因此&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;"&gt;f&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;∈&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space3"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-ams-R"&gt;R&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.615em; padding-left: 0px;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;100&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; , &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;|&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;|&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;"&gt;f&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;|&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;|&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 。&lt;/p&gt;&lt;p&gt;特征 1 就是第一个神经&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;元&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;，&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;"&gt;f&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em;"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;...&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;特征 2 最大程度地分布在神经元&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;上&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;，&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;"&gt;f&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em;"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0.1&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0.1&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0.1&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;...&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;请注意，我们将&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;L&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px;"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;范数设置为 1（不是&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;L&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px;"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;范数），因此特征方向的计算为&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-munderover"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-size1-R"&gt;Σ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space1"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;"&gt;if&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%;"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;所以&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;"&gt;f&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%;"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0.01&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space1"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;"&gt;fi&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;=&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0.1&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;我们可以将这些解释为前非线性空间和后非线性空间中的特征，并查看这些特征如何相互影响。我们可以看一下以下关系，其中&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;"&gt;f&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;∈&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-texatom MJXc-space3"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;"&gt;f&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em;"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space1"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;"&gt;f&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em;"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;是所选的输入特征， &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;"&gt;f&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;o&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;∈&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;{&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;"&gt;f&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em;"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space1"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;"&gt;f&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em;"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;}&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;是所选的输出特征， &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;是该特征的输入标量级别， &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;y&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;是该特征的输出标量级别。&lt;/p&gt;&lt;p&gt; &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;y&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space3"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;"&gt;f&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;o&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;⋅&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;G&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;E&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;L&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;U&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;"&gt;f&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;下面绘制了四种可能的输入输出关系 - 在查看之前，我鼓励您思考这些关系会是什么样子，尤其是扩展输入和扩展输出之间的关系。 &lt;/p&gt;&lt;hr /&gt;&lt;figure class="image"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/fqgn56tS5AgjmDpnX/o8spsuwfuqygxgc1y04m" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;请注意，每个图的比例不同 - y 轴重新调整比例以显示形状的相似性。&lt;/strong&gt;在左上角我们只看到 GELU 非线性。在右上角，我们看到相同的形状，但正如我们所期望的，输出的大小是使用单个输出时的 1/10&lt;/p&gt;&lt;p&gt;在左下角，我们看到单输入单输出情况下非线性的放大版本，因为我们只将我们关心的神经元更改为预期数量的十分之一。不仅变化的幅度变小，而且非线性现在几乎无法察觉。&lt;/p&gt;&lt;p&gt;右下角是一个有趣的案例。重要的是，尽管输入和输出特征相同，但我们不会复制左上角的行为。相反，我们看到的是左下角的行为，但在所有 100 个神经元上都得到了复制。输出的规模扩大了 10 倍，因为我们将单个 (0.1 x 1) 计算替换为 100 x (0.1 x 0.1)。&lt;/p&gt;&lt;h3&gt;按 sqrt(n) 放大可恢复非线性&lt;/h3&gt;&lt;p&gt;右下角的图表非常接近线性，并且作为非线性函数没有多大用处。相反，如果我们想要恢复相同的非线性，我们需要将输入方向的变化规模增加&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msqrt"&gt;&lt;span class="mjx-box"&gt;&lt;span class="mjx-surd"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;√&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-box" style="border-top: 1px solid;"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msqrt MJXc-space3"&gt;&lt;span class="mjx-box"&gt;&lt;span class="mjx-surd"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;√&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-box" style="padding-top: 0.13em; border-top: 1px solid;"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;00&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;10&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; ，在这种情况下我们得到以下结果： &lt;/p&gt;&lt;figure class="image"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/fqgn56tS5AgjmDpnX/aplpbetyg9ha5zv8x5go" /&gt;&lt;/figure&gt;&lt;p&gt;这是我们所说的 MLP 具有首选基础并且对于旋转不是不变的意思的一个例子 - 为了使分布在&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;n 个&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;神经元&lt;strong&gt;上的特征获得相同的非线性，&lt;/strong&gt;&lt;strong&gt;我们必须扩大输入特征为&lt;/strong&gt;&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msqrt"&gt;&lt;span class="mjx-box"&gt;&lt;span class="mjx-surd"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;√&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-box" style="border-top: 1px solid;"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;strong&gt;。&lt;/strong&gt;请注意，输出规模现在也增加了相同的数量。&lt;/p&gt;&lt;p&gt;我们还可以在某种程度上看到这如何产生额外的鲁棒性 - 如果我们强烈打开另一个完全相同方向的特征，那么根本就没有额外的非线性可以使用，而在这里我们很可能处于“膝盖” &amp;#39;许多神经元的非线性，因此将得到非线性响应。&lt;/p&gt;&lt;h3&gt;正向和负向特征方向&lt;/h3&gt;&lt;p&gt;请注意，上述情况依赖于所有向量元素均为正。如果我们有偶数个正负元素，那么它们就会相互抵消，无法产生很大的非线性，如果我们有一些不平衡的组合，那么我们会得到非线性程度有所减弱的中间情况。&lt;/p&gt;&lt;p&gt;因此&lt;strong&gt;，如果我们要使新信息线性可用，我们应该期望输入的特征向量会严重偏向正分量或负分量。&lt;/strong&gt; &lt;/p&gt;&lt;figure class="image"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/fqgn56tS5AgjmDpnX/aq0ehqfgsujvdk1smc7y" /&gt;&lt;/figure&gt;&lt;p&gt;这也意味着 MLP 中可能的特征数量受到一些限制。 Johnson-Lindenstrauss 告诉我们，高维空间中近正交特征的数量在该空间的维度中呈指数增长，但这些维度中的大多数将具有几乎平衡数量的正分量和负分量，因此不会有用非线性，因为该方向上的输出将是该方向上输入的齐次函数。&lt;/p&gt;&lt;p&gt;我们可以看到，使用这些分布式函数，我们仍然可以将网络视为将它们想要放大的东西放在神经元的正方向上，但在多个维度上进行此操作。&lt;/p&gt;&lt;h3&gt;干涉与维数之间的交易&lt;/h3&gt;&lt;p&gt;通过扩大输入特征的大小，我们恢复了单个神经元的原始非线性行为。然而，直觉上我们预计，使输入的幅度更大，输入具有固定比例的非线性，应该会对我们在任何时候可以激活的特征数量产生影响，因为干扰水平会更大。&lt;/p&gt;&lt;p&gt;术语中的一个重要注释（感谢 Jake Mendel）：&lt;i&gt;线性&lt;/i&gt;是&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;"&gt;f&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;y&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;"&gt;f&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;y&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;的条件，而同质性是&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;"&gt;f&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;的较弱条件&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;"&gt;f&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 。虽然线性度最终是我感兴趣的东西，但在这种情况下我测量的是特定输入方向的同质性，因此我将在适当的情况下切换到这个术语。&lt;/p&gt;&lt;p&gt;为了了解这是否以及何时成为问题，我绘制了特定方向的输入输出响应曲线。算法如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;我生成一组特征向量，每个特征向量都是输入/输出空间中的一个方向，仅包含固定数量的非零元素， &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;code&gt;dims_per_feature&lt;/code&gt;&lt;/li&gt;&lt;li&gt;我设置了固定数量的功能&lt;code&gt;n_on&lt;/code&gt; ，除了感兴趣的功能之外，该功能将在任何时候打开&lt;/li&gt;&lt;li&gt;我选择一个随机输入特征，并生成一组非线性输入，其中该特征和&lt;code&gt;n_on&lt;/code&gt;其他特征均处于活动状态，其大小在&lt;code&gt;-np.sqrt(dims_per_feature) &amp;lt; mag &amp;lt; np.sqrt(dims_per_feature)&lt;/code&gt;&lt;/li&gt;&lt;li&gt;我将这些输入向量通过非线性传递以获得输出向量，并将这些输出向量投影到感兴趣的特征上，以获得输出中的特征级别。&lt;/li&gt;&lt;li&gt;我运行回归来预测输出中所选特征的水平，作为输入中特征水平的函数&lt;/li&gt;&lt;li&gt;我通过获取非线性之前和之后的特征激活程度并计算简单回归来根据非线性之前的特征值预测非线性之后的特征值来测量非均匀程度。第一个回归只有这个单一的输入参数。第二个包含二次项 - 只是输入值的平方。 &lt;/li&gt;&lt;/ul&gt;&lt;figure class="image image_resized" style="width: 65.53%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/fqgn56tS5AgjmDpnX/borwpoinwulmvc5w029s" /&gt;&lt;figcaption&gt;在存在来自重叠特征的噪声的情况下将输入和输出向量投影到特征方向的示例也处于活动状态。&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;如果存在明显的非同质性，那么二次回归应该能够比线性回归更好地预测输出值。&lt;/p&gt;&lt;p&gt;输出指标是由 20% 的数据组成的测试集上的二次回归和线性回归之间的&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;R&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px;"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;差异。这为我们提供了一个简单的衡量方法，可以衡量我们因改变输入而看到的非均匀响应的程度。 &lt;/p&gt;&lt;figure class="image image_resized" style="width: 64.08%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/fqgn56tS5AgjmDpnX/binxxaqzulpbcfuhzn68" /&gt;&lt;figcaption&gt;示例：如何将非均匀性得分计算为特征输入级别与投影到特征方向上的输出向量（有干扰）之间的线性回归和二次回归的&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;R&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px;"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;得分之间的差异。&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;然后我们可以做的是采用固定的网络宽度，这里仍然是&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;n&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;100&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; ，并改变每个特征分布的维度数。我们可以为“非同质性水平”设置一个阈值，并在我们认为关系大致同质之前查看有多少特征可以在任何时候处于活动状态。这里我随意把这个阈值设置为0.1。&lt;/p&gt;&lt;p&gt;同时，对于每个特征的给定维数，我们可以绘制出我们可以拥有多少个近乎正交的特征，并使用一组随机特征向量的平均最大余弦相似度（MMCS）作为代理。我们将近正交特征的数量作为 MMCS 超过某个阈值的点，这里选择 0.3。这当然是低估的，因为网络可以将特征排列得比随机子集更精确地正交，但它给出了一个粗略的想法。&lt;/p&gt;&lt;p&gt;完成所有这些都是为了我们可以看到，实际上存在一种看似合理的权衡，即通过增加每个特征的维度数，我们允许自己有更多几乎正交的特征向量，但代价是能够拥有很少的活动特征在干扰淹没非线性并消除我们对层进行有趣工作的能力之前（至少通过这种简单的非均匀性测量）。 &lt;/p&gt;&lt;figure class="image"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/fqgn56tS5AgjmDpnX/rncb5iv09hrnh6inhzqi" /&gt;&lt;figcaption&gt;该图分别使用任意阈值来表示“可接受的”非均匀性和干扰的最小和最大水平，并不是声称这些是总特征或活动特征的真实数量，而只是为了证明两者之间存在权衡，因为我们变化&lt;code&gt;dims_per_feature&lt;/code&gt; 。&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;这密切反映了&lt;a href="https://transformer-circuits.pub/2022/toy_model/index.html"&gt;叠加玩具模型论文&lt;/a&gt;中以更详细的方式得出的结论，该论文探讨了叠加在理想化残差流中的作用，并为如何在 MLP 中找到分布式特征提供了一些理论支持。&lt;/p&gt;&lt;h2&gt;多层特征的问题&lt;/h2&gt;&lt;h3&gt;关注点&lt;/h3&gt;&lt;p&gt;稀疏自动编码器可能无法兑现其承诺的一种方式是，我们有“已完成的功能”和“正在进行的功能”之类的东西。例如，要在第 20 层读取特征，而先决条件元素在第 10 层就位，但是模型发现增量构建特征比在 (10, 20) 中的单层中学习特征更有利，也许是因为这增加了可以同时构建的功能的数量，或者可能只是因为它并不昂贵，而且创建功能的分布式方法比非分布式方法多得多。&lt;/p&gt;&lt;p&gt;这里的一个激励性例子是&lt;a href="https://arxiv.org/abs/2310.02207"&gt;Gurnee 和 Tegmark (2023) 的&lt;/a&gt;论文之一，该论文探讨了 Llama 模型各层的经度和纬度表示。&lt;/p&gt;&lt;p&gt;如果我们通过稀疏自动编码器或任何其他基础查找方法的镜头来看待这个问题，一旦它“完全构建”，我们可能会找到一个纬度方向 - 但是当这些层正在构建时我们期望找到什么？ &lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;figure class="image"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/KsWo4gCuZaX8fJNjG/zktyihbbwuqxakcm6ziq" /&gt;&lt;figcaption&gt; Gurnee 和 Tegmark 2023 的图 2 显示，纬度和经度的探测质量有所平稳增加，特别是在早期层&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;人们可能会做一些不同的实验来研究这些问题 - 在这里，我刚刚运行了最简单的潜在测试之一，看看实际上是否存在将特定连接的学习集中到单个层的倾向。&lt;/p&gt;&lt;h3&gt;一个简单的实验&lt;/h3&gt;&lt;p&gt;该设置是仅 MLP 变压器的简单模型，由一系列隐藏层组成，每层都有剩余连接。输入是一对 one-hot 向量，这意味着如果向量维度为 500，则输入将是一个 1000 维向量，其中前 500 个活动维度中有一个活动维度，第二个 500 个活动维度中有一个活动维度。这些是投影的隐藏维度与学习向量的输出宽度相同，然后是一系列具有循环连接的 MLP 层，每个层具有相同的宽度。&lt;/p&gt;&lt;p&gt;首先，我们可以检查每层之后的总体损失，评估每层之后输出的损失。我们发现，至少在后面的层中，接近线性减少，这表明模型正在迭代地使残差流更接近所需的输出。 &lt;/p&gt;&lt;figure class="image image_resized" style="width: 54.59%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/fqgn56tS5AgjmDpnX/orjagthy0756ovbumwjv" /&gt;&lt;figcaption&gt;对于 3 个不同数量的数据点，设计任务上的 logit-lens 损失作为层的函数，显示出类似的、一致的损失减少，并在接近结束时加速。&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;您可以想象模型计算此问题的两种相反方式 - 在一端，每一层都会计算输入的特定子集的正确输出。在这个范围的另一端，输入将不断转换，直到每个数据点的答案最终在最后一层得到正确，每个层都会进行一定程度的中间处理。&lt;/p&gt;&lt;p&gt;为了简单测试这两张图片中哪一张更接近真实情况，然后我在每个层之后获取各个数据点的 Logit 透镜输出，并仅获取一些随机选择的数据点，我得到如下图： &lt;/p&gt;&lt;figure class="image image_resized" style="width: 67.64%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/fqgn56tS5AgjmDpnX/flgiunnm9i8ftej5xw0r" /&gt;&lt;figcaption&gt; 5 个随机选择的数据点的分层 Logit 透镜损失，即使对于单个数据点也显示出大致连续的损失减少。&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;这已经为我们提供了所需的信息，但为了防止这些异常值，我们可以采用一些损失阈值作为某个层是否正确学习输出的代理，然后绘制所学习的数据点的比例，我们得到以下结果（阈值=0.1）： &lt;/p&gt;&lt;figure class="image image_resized" style="width: 86.97%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/KsWo4gCuZaX8fJNjG/fpxqwtlvluntxz5r6ct2" /&gt;&lt;figcaption&gt;图表绘制了每层正确学习的输入示例的比例（通过 Logit 透镜测量），其中“正确学习”意味着损失小于&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;我们再次看到第一张图中已经很明显的内容 - 不同的数据点只有在接近结束时才被完全学习 - 尽管存在一定程度的变化。更有趣的是，当唯一数据点的数量越大时，成功满足阈值的数据点的比例就越高，即使它们最终都可以被学习。&lt;strong&gt;这证明以相对于层的分布式方式计算层对于模型来说比在单独的层中计算它们是更有效的解决方案。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;我还查看了对于各个功能，在决定哪个输出时是否仍然存在类似“决定性”层的东西。对于各个数据点，我查看了每层权重矩阵的梯度范数，当梯度是根据正确的输出而不是损失计算时。&lt;/p&gt;&lt;pre&gt; &lt;code&gt;optimizer.zero_grad() output = model(inputs[i]) correct_output = output[targets[i]] # getting the target for a single datapoint correct_output.backward() for layer_n in range(n_hidden_layers): wandb.log( { f&amp;quot;grads/layer_grad_{i}_output&amp;quot;: torch.norm( model.hidden_layers[layer_n].weight.grad ), &amp;quot;layer_n&amp;quot;: layer_n, } )&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;我们再次没有看到任何特定的峰值： &lt;/p&gt;&lt;figure class="image image_resized" style="width: 65.54%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/fqgn56tS5AgjmDpnX/lcryndqszctjmeiekgyn" /&gt;&lt;figcaption&gt;每层梯度矩阵相对于输出向量的正确元素的2-范数。&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;这进一步证明，没有单个层可以学习单个输出，而是它们平滑地分布在全套可用层中。&lt;/p&gt;&lt;p&gt;我认为这个简单的实验无论如何都不是决定性的，但对我来说，它使得真实模型中的特征更有可能在很大程度上是逐层迭代地细化的，（更推测性地）中间部分没有任何特征特别自然的表现。这并不是说不存在层次结构特征，但如果层次结构的深度远小于层数，那么您就不会期望能够单独解释模型的 MLP 层。&lt;/p&gt;&lt;p&gt;顺便说一句，我真的很感兴趣看到人们对这些简单的算法任务进行机械解释，以扩展模型如何执行这些超级基本任务的库。&lt;/p&gt;&lt;p&gt;人们可以做的另一件事是为模型的所有层训练 SAE，他们在其上找到经度和纬度向量，并检查学习维度的程度与 Gurnee 论文中发现的维度相似。&lt;/p&gt;&lt;h2&gt;为什么我们应该期望 SAE 发挥作用？&lt;/h2&gt;&lt;h3&gt;为什么 L1 惩罚会起作用？&lt;/h3&gt;&lt;p&gt;虽然我从经验上知道 L1 损失是 L0 惩罚的一个很好的可微分替代品，但长期以来我脑子里的图像不正确，无法解释&lt;i&gt;为什么&lt;/i&gt;会出现这种情况。&lt;/p&gt;&lt;p&gt;关于 L1 惩罚的一个基本问题是“等等，但是如果我们有大小为 5.0 的向量 X，以及基向量&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;Y&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;、&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space1"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;Y&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;、……&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;”。&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;。&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;。&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space1"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;Y&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;我们可以用&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;5&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;Y&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;或&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;Y&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;Y&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;3&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;Y&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;4&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;Y&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;5&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;Y&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;6&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;构建，然后（忽略系数）L1 损失在这两种情况下都是 5，所以看起来这并不&amp;#39;实际上不会激励稀疏性吗？ &lt;/p&gt;&lt;figure class="image image_resized" style="width: 42.07%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/fqgn56tS5AgjmDpnX/t26dkg8u31o21qpgtfnc" /&gt;&lt;figcaption&gt;以图表的形式提出问题&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;当我们有多个指向同样相似方向的基向量时，L1 惩罚无法激励稀疏性的反对意见是正确的，但这是一种特殊情况，仅当两个字典元素指向相同方向时才会发生。只需要相似程度的微小差异即可打破相等性并支持更稀疏的解决方案。如果我们有一个 2D 空间和三个方向，其中每对都是线性独立的，那么为了选择最有效的方法，我们将在可能的情况下选择 1 特征解决方案，以及 2 特征解决方案，对更接近的解决方案给予更大的权重，当没有一维解时。 &lt;/p&gt;&lt;figure class="image image_resized" style="width: 45.12%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/fqgn56tS5AgjmDpnX/c7ctze9qj9dwb7wodpuv" /&gt;&lt;figcaption&gt;支持稀疏解决方案&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;换句话说，假设我们完美地重建了输入向量，L1 损失将与用于重建特征的特征的平均余弦相似度成反比，并根据每个特征的活跃程度进行加权，因此它会朝着尽可能使用最相似的余弦特征。&lt;/p&gt;&lt;p&gt;当然，稀疏自动编码器无法进行优化以找到最佳可用解决方案，但它应该像使用简单的线性编码器一样近似此过程，因此仍应具有相同的近似属性。&lt;/p&gt;&lt;p&gt;同时，我们不应该期望稀疏自动编码器首先学习两个高度相似的方向，因为在同一方向上拥有两个向量在同时使用时不会带来任何好处，同时错过了合并的机会字典中其他一些有用的方向。&lt;/p&gt;&lt;h3&gt;为什么我认为稀疏自动编码器在查找特征向量的任务中比文献中更复杂的方法具有更好的先验&lt;/h3&gt;&lt;p&gt;稀疏自动编码器是解决更一般的稀疏编码问题的一种非常简单的方法。在一般设置中，我们有一个特征字典，我们想要计算哪个小特征子集用于创建每个示例，以及达到何种程度（系数，又称特征向量）。&lt;/p&gt;&lt;p&gt;在标准稀疏编码方法中，稀疏编码是自由优化的，输入向量和特征级别之间没有封闭形式的关系。这种自由度在许多信号处理情况下很有帮助。例如，图像是更详细的现实的低维表示。两种截然不同的现实配置在照片中看起来几乎相同并没有什么特别的原因，因此可能需要进行大量仔细的侦探工作才能了解两种可能的配置中的哪一种实际上产生了照片。 &lt;/p&gt;&lt;figure class="image image_resized" style="width: 50.94%;"&gt;&lt;img alt="MC埃舍尔是迷幻视错觉之王，但他值得更多的赞誉" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/fqgn56tS5AgjmDpnX/x238u6bj5vvtcursvxfb" /&gt;&lt;figcaption&gt;不适合 SAE&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;神经网络的情况非常不同，因为如果两组完全不同的特征产生相同的残余流，那么网络本身将很难将两者分开。相反，网络希望使用其拥有的工具轻松读取信息，这些工具主要是线性映射&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;（&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;W&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;in&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;、&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space1"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;W&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;Q&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 、 &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;W&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;V&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 、 &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;W&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.04em;"&gt;K&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; ）。&lt;/p&gt;&lt;p&gt; Aidan Ewart 尝试使用多层编码器，而不是标准 SAE 的单个 Linear+ReLU。他发现很难匹配性能，并且最终的功能似乎无法解释。我认为这是因为它消除了先验如何读取特征的好处，而这种读取特征与网络读取特征的方式很接近。&lt;/p&gt;&lt;p&gt;但这并不意味着当前的 SAE 是最佳的。有一个非常明显的问题，编码器很难有足够的能力来正确预测输出的幅度。例如，如果您有特征 (1, 0) 和 (0, 1)，并且它们通常单独存在，因此正确的偏差为 0，则它很难重新创建向量 (1, 1) - 它想用数量级为 2，而不是&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msqrt"&gt;&lt;span class="mjx-box"&gt;&lt;span class="mjx-surd"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;√&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-box" style="padding-top: 0.13em; border-top: 1px solid;"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; ！&lt;/p&gt;&lt;p&gt;对于权值绑定的自动编码器来说，这尤其是一个问题，但即使是未绑定的自动编码器也仅具有有限的灵活性。&lt;/p&gt;&lt;p&gt;因此，为了做出改进，我们应该考虑如何让 SAE 在学习到正确的特征后更容易地重建输出，同时仍然保留有用的线性先验特征（或者如果我们可以更好地理解网络，则更好的先验） 。&lt;/p&gt;&lt;ol class="footnotes"&gt;&lt;li class="footnote-item" id="fn6iedrpq2n4a"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnref6iedrpq2n4a"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;关于特征的注释 - 我不认为网络确实是由固定数量的离散特征组成的，相反，我认为它是对连续现实的离散近似，我发现这对于如何思考模型内部结构很有帮助，但可能有严重的局限性。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ol&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/fqgn56tS5AgjmDpnX/some-additional-sae-thoughts#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sat, 13 Jan 2024 19:31:40 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/fqgn56tS5AgjmDpnX/some-additional-sae-thoughts</guid></item><item><title>（阅读 4 分钟）AI 影响情况的直观解释</title><link>https://www.lesswrong.com/posts/aWPucqvJ4RWKKwKjH/4-min-read-an-intuitive-explanation-of-the-ai-influence</link><description>发布于 2024 年 1 月 13 日下午 5:34（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;阅读时间为 4 分钟，灵感来自 Eukaryote 的&lt;a href="https://www.lesswrong.com/posts/NQgWL7tvAPgN2LTLn/spaghetti-towers"&gt;Spaghetti Towers&lt;/a&gt;帖子中的优化写作。&lt;/p&gt;&lt;p&gt;我的想法是，生成式人工智能有可能被严重操纵，但 2010 年代社交媒体新闻源和其他自动化系统中使用的人工智能是一个更大的威胁，这项技术告诉我们更多关于&lt;a href="https://www.lesswrong.com/posts/jyAerr8txxhiKnxwA/5-reasons-why-governments-militaries-already-want-ai-for"&gt;国际事务的未来以及政府的激励措施加速人工智能的竞赛&lt;/a&gt;，意识到这一点的人较少， &lt;a href="https://www.lesswrong.com/posts/LdEwDn5veAckEemi4/we-are-already-in-a-persuasion-transformed-world-and-must"&gt;被用来攻击人工智能安全社区的风险很大&lt;/a&gt;，而且&lt;a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks-and-mind#How_to_protect_yourself_and_others__"&gt;防御措施很容易部署，而且是强制性的&lt;/a&gt;。这篇文章解释了为什么这项技术足够强大，可以成为人们世界模型的核心。&lt;/p&gt;&lt;p&gt; Tristan Harris 的&lt;a href="https://www.netflix.com/title/81254224"&gt;《社会困境》&lt;/a&gt; （2020）中的人们以快速而&lt;a href="https://www.lesswrong.com/posts/RryyWNmJNnLowbhfC/please-don-t-throw-your-mind-away"&gt;有趣的&lt;/a&gt;方式出色地描述了自动优化机制（&lt;a href="https://scrapsfromtheloft.com/movies/the-social-dilemma-movie-transcript/"&gt;文字记录&lt;/a&gt;）。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; [特里斯坦]一个[舞台]魔术师了解一些我们不知道的你心灵的某些部分。这就是[魔术]幻觉发挥作用的原因。医生、律师、知道如何制造 747 或核导弹的人，他们并不知道自己的思想是多么脆弱。这是一个单独的学科。这是一门适用于全人类的学科……&lt;/p&gt;&lt;p&gt; [Shoshana] 我们如何利用 Facebook 页面上的潜意识线索让更多人在中期选举中投票？他们发现他们能够做到这一点。&lt;/p&gt;&lt;p&gt;他们得出的结论是，我们现在知道我们可以影响现实世界的行为和情绪，而无需触发用户的意识。他们完全一无所知。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;重要提示：这里的所有优化都高度依赖于可测量性，触发用户的意识是一个高度可测量的事情。 &lt;/p&gt;&lt;figure class="image"&gt;&lt;img alt="114501_1_09dreyfuss-video_wg_720p.mp4 [视频转 gif 输出图像]" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/aWPucqvJ4RWKKwKjH/x3ba8x6vdij4y7h3lu4s" /&gt;&lt;/figure&gt;&lt;p&gt;如果有什么事情让某人感到害怕，他们就会减少使用该平台；这样的事情非常容易测量和&lt;a href="https://www.lesswrong.com/posts/Zvu6ZP47dMLHXMiG3/optimized-propaganda-with-bayesian-networks-comment-on"&gt;隔离因果关系&lt;/a&gt;。为了获得足够的数据，这些平台&lt;a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks-and-mind#:~:text=These%20zero%20days,workforce%20or%20less."&gt;必须自动重塑自身，以确保使用起来安全&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;这显然包括让你&lt;i&gt;感觉&lt;/i&gt;被操纵的广告；毫不奇怪，我们最终进入一个超过 95% 的广告遇到不匹配的系统。 &lt;/p&gt;&lt;figure class="image image_resized" style="width: 29.17%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/aWPucqvJ4RWKKwKjH/wntc01wcithal8mx0nwm" /&gt;&lt;/figure&gt;&lt;p&gt;这为研究人员提供了足够的自由度来&lt;a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks-and-mind#:~:text=A%20big%20element,generated%20every%20day."&gt;尝试不同类型的角度并看看什么有效&lt;/a&gt;，甚至使该过程自动化。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; [特里斯坦]我们将这些人工智能引擎指向我们自己，以对我们的反应进行逆向工程。几乎就像你在刺激蜘蛛上的神经细胞，看看是什么导致它的腿做出反应。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;重要提示：我的模型表明很大一部分数据来自滚动。使用触摸屏/板或鼠标滚轮（不是箭头键）滚动新闻源上的某些内容的运动实际上会生成一条曲线。&lt;/p&gt;&lt;p&gt;它是插入机器学习的完美生物数据； 2010 年代的社交媒体新闻源范式根据不同的人对他们滚动浏览的不同概念和想法的反应，生成了数万亿个线性代数实例。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;所以，这确实是一种监狱实验，我们只是，你知道，把人们拉进矩阵，我们只是从他们的所有活动中收获所有这些钱和……以及数据来从中获利。我们甚至不知道它正在发生。&lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt; [Chamath] 所以，我们想从心理上弄清楚如何尽快操纵你，然后给你带来多巴胺的刺激......&lt;/p&gt;&lt;p&gt; [Sean Parker] 我的意思是，这正是像我这样的黑客会想出的东西，因为你正在利用人类心理学中的漏洞......&lt;/p&gt;&lt;p&gt; [特里斯坦]当&lt;strong&gt;自行车&lt;/strong&gt;出现时，没有人感到不安。正确的？就像，如果每个人都开始骑自行车出行，没有人会说：“哦，天哪，我们刚刚毁了社会。就像自行车正在影响人们一样。他们正在把人们从他们的孩子身边拉开。他们正在破坏民主的结构。人们无法分辨什么是真实的。”&lt;/p&gt;&lt;p&gt;就像，我们从来没有说过任何关于自行车的事情。如果某物是一种工具，那么它确实只是坐在那里，耐心等待。如果某样东西不是工具，它就会向你索要东西……它会向你索要东西。我们已经从基于工具的技术环境转向基于成瘾和操纵的技术环境。&lt;/p&gt;&lt;p&gt;这就是改变的地方。社交媒体并不是一个等待使用的工具。它有自己的目标，也有自己的手段，通过利用你的心理来对抗你。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;在纪录片中，哈里斯描述了他在科技公司工作期间三个独立的自动优化方向之间的平衡：参与度目标，提高使用量并让人们不断滚动，增长目标，让人们回来并鼓励朋友，以及广告目标，支付服务器使用费用。&lt;/p&gt;&lt;p&gt;事实上，还有第四个槽点：在任何可衡量的方向上优化人们的思维，比如让人们在乌克兰这样的代理战争中狂热地支持美国一方，反对俄罗斯一方。这四个优化方向之间的权衡是巨大的，平衡/优先级分配取决于公司的偏好以及公司与其政府和军队的联系程度（据我所知，这里主要与美国和中国有关）。&lt;/p&gt;&lt;p&gt;小丑攻击是适合第四个位置的一个很好的例子：工程师通过向某人​​展示低地位的小丑在谈论它，而高地位的人忽视或批评它来认为某个主题（例如实验室泄漏假设）是低地位的。&lt;/p&gt;&lt;p&gt;值得注意的是，这个问题远没有&lt;a href="https://www.lesswrong.com/posts/LTtNXM9shNM9AC2mp/superintelligence-faq"&gt;超级智能（人类的终点线）&lt;/a&gt;那么重要。但这对于世界建模至关重要；人类文明过去一万年的发展之所以如此，是因为当时还不存在这种程度的操纵能力。&lt;/p&gt;&lt;p&gt;在 20 世纪，信息战相对于军事力量来说不太重要，因为它的实力较弱。随着信息战变得更加强大，投资回报不断增长，更多的政府和军​​队对信息战的投资比历史先例所暗示的要多，我们最终进入了信息战时间表。&lt;/p&gt;&lt;p&gt; 2020年代， &lt;a href="https://www.lesswrong.com/posts/Lw8enYm5EXyvbcjmt/sensor-exposure-can-compromise-the-human-brain-in-the-2020s"&gt;计算机视觉使眼球追踪和大规模面部微表情识别/研究可能成为最大的威胁&lt;/a&gt;。与保护操作系统或在智能手机附近进行重要对话的绝望不同，对于人工智能安全社区的人们来说，解决方案是简单且值得的（这就是为什么我发布此内容是积极的）。它只需要一小块胶带和铝箔（这对我来说很容易剥离大部分并随后更换）。&lt;/p&gt;&lt;p&gt; &lt;i&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/aWPucqvJ4RWKKwKjH/opzndlemyx3fvwhegupi" /&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt; &lt;i&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/aWPucqvJ4RWKKwKjH/nyt6nfqdwo2tqwgjb4l0" /&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt; &lt;i&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/aWPucqvJ4RWKKwKjH/cdgndotfa5mfeuhcrplp" /&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt; &lt;i&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/aWPucqvJ4RWKKwKjH/kkihhkjwowi3tidnebqa" /&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/aWPucqvJ4RWKKwKjH/c7ge0vpvvdtyjw9bqlrr" /&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;我已经写过&lt;a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks-and-mind#How_to_protect_yourself_and_others__"&gt;其他修复方法&lt;/a&gt;以及攻击者入侵人工智能安全社区并使其针对自身的&lt;a href="https://www.lesswrong.com/posts/F7sp7rQg3zfD4totA/helpful-examples-to-get-a-sense-of-modern-automated"&gt;各种方法示例&lt;/a&gt;。请不要将&lt;a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks-and-mind#:~:text=Thus%2C%20you%20should,something%20like%20zero."&gt;自己&lt;/a&gt;和&lt;a href="https://www.lesswrong.com/posts/c5oyHuHaw4AcWy4tf/information-warfare-historically-revolved-around-human"&gt;他人&lt;/a&gt;置于危险之中。&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/aWPucqvJ4RWKKwKjH/4-min-read-an-intuitive-explanation-of-the-ai-influence#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sat, 13 Jan 2024 17:34:36 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/aWPucqvJ4RWKKwKjH/4-min-read-an-intuitive-explanation-of-the-ai-influence</guid></item></channel></rss>