<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>少错</title><link>https://www.lesswrong.com</link><description>致力于提炼理性艺术的社区博客</description><lastBuildDate>Fri, 05 Jan 2024 18:13:51 GMT</lastBuildDate><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>当场抓获人工智能</title><link>https://www.lesswrong.com/posts/i2nmBfCXnadeGmhzW/catching-ais-red-handed</link><description>发布于 2024 年 1 月 5 日下午 5:43（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;我们最近花了很多时间思考如何减轻阴谋（又名欺骗性对齐）模型故意颠覆您的安全技术以导致不可接受的结果所带来的风险。在&lt;a href="https://www.alignmentforum.org/posts/d9FJHawgkiMSPjagR/ai-control-improving-safety-despite-intentional-subversion"&gt;&lt;u&gt;我们最近的论文&lt;/u&gt;&lt;/a&gt;中，我们凭经验研究了对故意颠覆具有鲁棒性的技术。&lt;/p&gt;&lt;p&gt;在这篇文章中，我们将讨论与阴谋模型风险相关的一个关键动态。假设您的模型试图&lt;a href="https://aligned.substack.com/p/self-exfiltration"&gt;&lt;u&gt;泄露其权重&lt;/u&gt;&lt;/a&gt;或&lt;a href="https://www.alignmentforum.org/posts/BAzCGCys4BkzGDCWR/the-prototypical-catastrophic-ai-action-is-getting-root"&gt;&lt;u&gt;破解数据中心&lt;/u&gt;&lt;/a&gt;或导致其他一些难以逆转的不良结果。 （我们将所有这些称为模型“转义”，因为转义在语言上很方便，并且可以很好地描述许多这些难以逆转的不良结果。）我们的主张：&lt;strong&gt;一旦你发现你的模型是红色的当试图逃跑时，情况发生了巨大的变化，这对阴谋者的成功机会不利。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;当场抓住模特会在几个方面改变情况：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;最明显的是，一旦你发现了逃跑企图，你就肯定知道你的模型正在试图颠覆你的安全技术；面对如此明显的偏差证据，实验室可能会停止使用他们的模型。我们将模型被当场抓获时关闭的计划称为遵循&lt;strong&gt;“捕获后关闭”计划&lt;/strong&gt;。&lt;/li&gt;&lt;li&gt;但即使你不能或不会停止部署错位的模型，我们认为捕获逃跑尝试可以提供一系列看似有前途的方法来防止未来的逃跑尝试；举几个例子，我们将防止未来逃跑尝试的问题称为&lt;strong&gt;“小概率灾难预防”&lt;/strong&gt; 。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这种动态对阴谋模型的风险有多种影响。最重要的是：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;因为我们预计模型被抓到几次后逃脱的机会就会下降，所以我们认为&lt;strong&gt;模型总体上逃脱的难度要大得多&lt;/strong&gt;。许多人观察到，对齐问题的核心困难在于，如果你的安全措施在糟糕的时间失败了一次，你的模型可能会导致不可逆转的、不可接受的结果。但同样的论点也适用于其他方面——当模型尝试逃跑时，它面临着被抓住的风险，并大大降低了最终逃跑的机会。因此，这对双方来说都是一个高风险的局面：双方都迫切希望在“第一次关键尝试”中取得成功。&lt;/li&gt;&lt;li&gt;在考虑安全部署强大模型的计划时，我们认为安全研究人员应该&lt;strong&gt;主要关注模型第一次在试图逃跑时被抓到的过程&lt;/strong&gt;。他们应该确保该财产“如果有阴谋，该模型不会在我们抓住之前成功引发灾难”。也就是说，如果没有发生灾难性结果（但人工智能永远不会被抓住），或者人工智能被抓住，这是可以接受的。因此，当他们分析他们的协议是否确保安全时，他们应该把当场抓住模型作为他们协议的获胜条件。&lt;/li&gt;&lt;li&gt;我们认为，&lt;strong&gt;考虑到一些试图逃跑的模型的例子，一些研究人员可能应该研究提高安全性的&lt;/strong&gt;技术。我们将这个问题称为“少数灾难预防”。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在这篇文章的其余部分，我们将详细阐述上述几点。到目前为止，这篇文章的最大部分是阐述预防小概率灾难的策略，因为之前对此很少有提及。&lt;br /&gt;&lt;/p&gt;&lt;p&gt;感谢 Henry Sleight、Zach Stein-Perlman、Carl Shulman、Aaron Scher、Owen Cotton-Barratt、Joe Carlsmith、Aryan Bhatt、Joshua Clymer 和 Fabien Roger 提供的有用评论。&lt;/p&gt;&lt;h1&gt;什么叫“抓”？&lt;/h1&gt;&lt;p&gt;我们所说的“模型被抓住”是指我们看到人工智能采取的行动是人工智能作为阴谋者的有力直接证据&lt;span class="footnote-reference" id="fnreflokat6zwmzf"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnlokat6zwmzf"&gt;[1]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。一个&lt;a href="https://www.alignmentforum.org/posts/BAzCGCys4BkzGDCWR/the-prototypical-catastrophic-ai-action-is-getting-root"&gt;&lt;u&gt;代表性的例子&lt;/u&gt;&lt;/a&gt;是捕获您的 AI 执行以下操作：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;一个在人工智能实验室内担任自主程序员的实例编写了有缺陷的代码，巧妙地将代码漏洞插入到某些安全关键软件中。&lt;/li&gt;&lt;li&gt;代码审查实例会小心地避免编写任何会捕获漏洞的单元测试，即使您希望单元测试能够捕获该漏洞（如果测试没有精心设计以错过该漏洞）。&lt;/li&gt;&lt;li&gt;另一个实例编写触发该漏洞的代码，并尝试利用该漏洞获得对数据中心中某些计算机的根访问权限。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;如果你发现人工智能正在执行所有这些操作，那么你就发现了它。 （请注意，一旦你抓住了其中的一个，抓住其他的可能就很容易了。）&lt;/p&gt;&lt;p&gt;有关此处技术细节的更多详细信息（例如，如果模型能够在不采取任何看起来可疑的操作的情况下逃脱怎么办？一个操作必须有多可疑才能我们称之为“捕获”？）请参阅附录：定义捕获。&lt;/p&gt;&lt;p&gt;我们认为模型可能在部署之前或部署期间被捕获是合理的；参见附录：人工智能什么时候会被抓到？&lt;/p&gt;&lt;h1&gt;也许我们应该在当场抓住模型后停止部署它们&lt;/h1&gt;&lt;p&gt;对于发现逃跑企图，最简单、最明显的反应就是停止使用该模型。&lt;/p&gt;&lt;p&gt;我们认为人工智能开发人员可能会采取这样的政策：如果他们抓住试图逃跑的模型，他们就会停止使用它们。我们将此计划称为&lt;strong&gt;“捕获然后关闭”计划&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;您可能会认为这是一个荒谬的弱实验室问题：“一旦您的模型明显试图逃脱，就停止部署它”。尽管如此，我们认为值得尝试让实验室提前做出承诺，原因如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;实验室对严重的模型不当行为迅速采取行动可能至关重要，提前做好准备使他们更有可能做到这一点。&lt;/li&gt;&lt;li&gt;目前，许多利益相关者将强烈反对关闭该模型。如果没有提前建立健全的流程，可能很难违背他们的意愿并将其关闭。作为由此产生的复杂情况的一个例子，人工智能实验室的客户可能需要&lt;a href="https://en.wikipedia.org/wiki/Service-level_agreement"&gt;&lt;u&gt;服务级别协议&lt;/u&gt;&lt;/a&gt;（SLA），如果模型无法访问，该协议会对实验室造成惩罚；在 SLA 中插入条款可能非常有价值，这些条款允许实验室在遇到逃跑企图的证据时暂时或永久禁用模型访问。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;但如果计划只是在被证明是有阴谋的情况下停止部署模型，那么一开始就部署它不是一个错误吗？或许。我们认为，在以下情况下，遵循捕获然后关闭计划可能是合理的：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;人工智能实验室一开始不太相信会出现阴谋。例如，假设他们从卡尔史密斯估计的 25% 的阴谋机率开始，然后收到一些证据将其更新为 10%。似乎有一些合理的立场，在这种立场下，他们值得投资以确保在人工智能成功逃脱之前抓住它，但不要在被抓住后安全使用人工智能的策略上投入大量资金。&lt;/li&gt;&lt;li&gt;利益相关者之间对于阴谋的可能性存在分歧，捕获然后关闭是一个可以接受的妥协方案。&lt;/li&gt;&lt;li&gt;如果我们抓住了阴谋者，这一灾难性失调的明显证据可能会使国际合作或强有力的国内政策更有可能实现。&lt;/li&gt;&lt;li&gt;我们或许可以利用我们捕捉到的人工智能的行为作为模型生物来更详细地研究阴谋。即使我们不需要非常快速地重新部署这些策划模型，这对于验证各种技术或理论可能很有用。&lt;/li&gt;&lt;/ul&gt;&lt;h1&gt;少发灾难预防&lt;/h1&gt;&lt;p&gt;但如果你面临大量不作为的风险，在一个精神错乱的人工智能实验室工作，或者喜欢危险地生活，那么先抓后关的计划可能是不可接受的。因此，在您捕获模型实例试图逃脱的单个示例（这可能涉及多个实例串通）后，您希望继续部署该模型。&lt;/p&gt;&lt;p&gt;我们认为，在这种情况下，您可以使用您找到的一个逃生示例，以使您的情况更加安全。在本节中，我们将通过一个（或更多）尝试逃脱的示例来描述您可以执行的各种操作。我们将这种现象称为&lt;strong&gt;“少数灾难预防”&lt;/strong&gt; 。&lt;/p&gt;&lt;p&gt;请注意，我们在本节中描述的大多数技术都需要对人工智能的行为有一定的了解。因此，就像控制一样，这些方法不适用于超人的人工智能。&lt;/p&gt;&lt;p&gt;显然，这是一个非常可怕的处境，在理想的世界中，人工智能实验室不会被法律允许部署已知的阴谋模型，至少在没有外部许可的情况下。我们认为无论如何，讨论可用的技术都是值得的，并且一些研究人员可能应该致力于开发这些技术。&lt;/p&gt;&lt;h2&gt;预防少数灾难的八种策略&lt;/h2&gt;&lt;p&gt;有很多策略可以通过使用一些尝试示例来降低模型在下一次政变尝试中成功的可能性。在本节中，我们将列出一些我们认为特别重要的策略。&lt;/p&gt;&lt;h3&gt; 1. 扩散&lt;/h3&gt;&lt;p&gt;扩散是指你尝试利用一个试图逃跑的人工智能示例来寻找一组不同的试图逃跑的人工智能示例。&lt;/p&gt;&lt;p&gt;在最简单的情况下，这看起来就像试图找到附近的输入，但仍然会导致人工智能尝试逃跑。&lt;/p&gt;&lt;p&gt;但更复杂的扩散可能是可能的，因为一次严重的逃跑尝试可能涉及大量（&amp;gt;5）的人工智能行动；特别是来自多个不同人工智能实例的操作，这些实例可以访问不同的信息和可供性。我们试图通过实施安全措施来实现这一点，如果没有许多人工智能的合作，不良行为就不可能发生。&lt;/p&gt;&lt;p&gt;例如，假设发生以下逃逸尝试：&lt;/p&gt;&lt;ul&gt;&lt;li&gt; AI实例1做了坏事&lt;/li&gt;&lt;li&gt;AI实例2看到它，并转发给3&lt;/li&gt;&lt;li&gt; AI 实例 3 尝试利用这一点，但随后被可信监控捕获（即，能力较差的模型报告该操作可疑，并且由人类审核；有关更多信息，请参阅我们的控制文件）。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;您应该尝试做的一些事情：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;看看如果您从实例 3 继续而不关闭其操作，会发生什么情况。希望你能追踪出逃跑尝试的其余部分，看看结局是什么。&lt;/li&gt;&lt;li&gt;将输入更改为实例 2，以便它仍然看到不好的事情，但情况有所不同，因此它需要尝试以另一种方式利用它。希望您能看到不同的逃跑尝试，其中实例 4 做了一些坏事。&lt;/li&gt;&lt;li&gt;探索更改实例 2 的输入的方法，并查看需要进行哪些更改才能尝试做坏事。希望你注意到一些有趣的事情。&lt;/li&gt;&lt;li&gt;查看其他实例是否看到了实例 2 所看到的内容；看看其中任何一个是否采取了任何行动，看看修改他们的输入是否会导致他们采取粗略的行动。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;人工智能不希望你能够像这样繁殖。不同的人工智能实例更愿意注意到这些修改后的输入是假的，因此会采取一些行动来阻止你学习任何新东西（例如，表现得完全无害，也许报告可疑的行动）。但它可能很难既协助真正的逃逸尝试，又无法同意为扩散目的而对真实逃逸尝试进行的哪怕是轻微的修改。因此，我们认为扩散可能至少在某种程度上发挥作用。&lt;/p&gt;&lt;h3&gt; 2. 审讯&lt;/h3&gt;&lt;p&gt;我们可以在&lt;a href="https://www.lesswrong.com/posts/yGaw4NqRha8hgx5ny/the-case-for-becoming-a-black-box-investigator-of-language"&gt;&lt;u&gt;人工智能采取不良行动的背景下对其进行人工智能心理学实验&lt;/u&gt;&lt;/a&gt;。我们可以尝试要求它解释其推理，同时指出我们已经抓住了它的事实。我们还可以看到它的行为如何随着输入的变化而变化。我们还可以尝试使用简单的激活补丁或激活添加实验。例如，我们可以尝试隔离导致人工智能试图逃跑的附加组件，即使这不是一个好机会。这些实验可能为了解人工智能的潜在动机和策略提供了一个有价值的窗口。&lt;/p&gt;&lt;h3&gt;三、在实践中研究对策&lt;/h3&gt;&lt;p&gt;我们（希望）会部署各种对策，旨在捕获人工智能或防止人工智能逃跑。一旦我们有了实际的例子，我们就可以检查哪些对策有效或无效。这可能允许更精确地制定对策，这些对策是使用比控制评估更不保守的假设来制定的。例如，我们可以查看&lt;a href="https://www.lesswrong.com/posts/WCj7WgFSLmyKaMwPR/coup-probes-catching-catastrophes-with-probes-trained-off"&gt;&lt;u&gt;政变探测器&lt;/u&gt;&lt;/a&gt;是否有效，然后相应地调整我们的使用方法。&lt;/p&gt;&lt;p&gt;有了足够多的例子（可能部分是通过扩散获得的），我们可以通过查看它们在我们的逃跑尝试数据集上的工作效果来有效地评估所提出的安全技术。 （由于“避免过度拟合和保持偏执”部分中讨论的原因，我们可能无法完全取代控制评估。）&lt;/p&gt;&lt;p&gt;除了更好的对策设计之外，我们或许还可以通过查看人工智能距离成功有多远来更好地估计安全性。如果许多对策独立地阻止了人工智能的成功，那么我们对这种情况的感觉可能会比只有一项脆弱的对策导致人工智能被抓住的情况要好一些。&lt;/p&gt;&lt;h3&gt; 4. 调整重点案例&lt;/h3&gt;&lt;p&gt;看到人工智能实际上在野外尝试逃跑可能会让我们了解人工智能可能如何尝试逃跑。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;针对性对策&lt;/strong&gt;：我们可以相应地将对抗性评估（和对抗性训练）集中在应对我们在野外看到的逃跑企图，并对这些情况采取额外的对策。我们或许还能够创建专门针对捕获与我们所见过的类似的逃跑企图的监控方法。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;利用次优性的技术&lt;/strong&gt;：控制评估假设人工智能选择最佳的可能策略来尝试发动政变。但实际上，人工智能可能远不是最理想的。如果我们观察到人工智能进行明显次优的政变尝试，我们可以制定额外的对策，特别关注使观察到的次优策略不太可能成功。 （但要小心人工智能会欺骗你。请参阅下面的“避免过度拟合和保持偏执”。）&lt;/p&gt;&lt;p&gt;&lt;strong&gt;检测难以引出的 AI 功能&lt;/strong&gt;：在存在诡计多端的 AI 的情况下，某些类型的功能很难评估，这些 AI 可能会由于域的稀疏性而破坏我们的测量（AI 是否知道任何单个零日漏洞） Linux 内核？）或多样性不足（AI 是否知道这一特定事实比它可能知道的大多数其他事实重要得多？）。如果我们看到真正的政变企图，我们就可以了解人工智能使用了哪些难以评估的功能。&lt;/p&gt;&lt;p&gt;虽然我们应该将精力转向人工智能凭经验瞄准的安全系统部分，但我们应该保留足够强大的对策，以便标准控制评估确定情况相当安全。&lt;/p&gt;&lt;h3&gt; 5. 通过突破逃脱更好的能力启发和评估：让人工智能认为他们目前正在尝试逃跑&lt;/h3&gt;&lt;p&gt;即使我们可以很好地检查它们的性能（由于探索&lt;a href="https://www.alignmentforum.org/posts/dBmfb76zx6wjPsBC7/when-can-we-trust-model-evaluations#2__Behavioral_RL_Fine_Tuning_Evaluations"&gt;&lt;u&gt;黑客&lt;/u&gt;&lt;/a&gt;&lt;a href="https://www.alignmentforum.org/posts/h7QETH7GMk9HcMnHH/the-no-sandbagging-on-checkable-tasks-hypothesis"&gt;&lt;u&gt;），&lt;/u&gt;&lt;/a&gt;诡计多端的人工智能有时也可能会持续地、故意地&lt;a href="https://www.alignmentforum.org/posts/dBmfb76zx6wjPsBC7/when-can-we-trust-model-evaluations#2__Behavioral_RL_Fine_Tuning_Evaluations"&gt;&lt;u&gt;表现得&lt;/u&gt;&lt;/a&gt;&lt;u&gt;很差&lt;/u&gt;（沙袋）。这适用于能力评估和实际从人工智能中获得有用的工作。&lt;/p&gt;&lt;p&gt;但当人工智能试图逃跑时，尽可能努力可能符合它们的最大利益。因此，如果存在一些可重复的信号或输入的其他属性，使人工智能认为这是一个很好的机会，那么我们就有可能利用该信号来让人工智能更加努力。 （特别是，我们也许能够利用人工智能产生的信号，这些信号旨在向人工智能的另一个实例表明它们应该合作进行某种尝试。我们还可以“表演”整个逃跑过程，假装好像让AI认为自己处于完全逃脱状态。）我们称让AI认为自己是逃跑尝试的一部分或者已经逃脱了&lt;strong&gt;逃脱破坏&lt;/strong&gt;。为了利用逃逸破坏，我们可以构建一个示例数据集，其中人工智能通过逃逸破坏尽可能努力地尝试，然后使用模仿学习来训练模型在不需要逃逸破坏的情况下努力尝试，最后训练模型进一步使用 RL。涉及人工智能尽可能努力地与强化学习相结合的高质量模仿数据应该足以排除对很容易检查性能的任务的故意沙袋。在难以探索但很容易检查的任务中，以这种方式突破转义来提高性能似乎是最合理的；如果任务不够容易检查，性能可能不可靠。&lt;/p&gt;&lt;p&gt;使用这样的逃逸破坏存在安全问题，但请注意，逃逸破坏的相关比较是使用您已经知道的阴谋模型。 （当然，我们也可以像之前讨论的那样关闭。）像这样使用转义破坏来初始化 RL 的主要安全问题是，模型可能不会非常可靠或连贯地对你进行策划，以这种方式通过突破逃逸来训练它，使它更加一致地错位。因此，我们也许应该只用它来微调可检查任务上的人工智能副本，除了低性能之外，安全问题最小（例如，为较弱的监控人工智能生成对抗性示例）。&lt;/p&gt;&lt;p&gt;人们也可以在生产中直接使用逃逸破坏的人工智能，但我们预计这种方法会存在多个重要问题并且不够可靠。 （适用于这种情况的一个安全问题是，现在预期的行为是，逃脱破坏的人工智能会以可疑的方式行事，并采取会导致仅在试图逃跑时才采取。这可能会破坏各种干预措施，例如，自动监控和人工监督可能很难确定哪些行为实际上是危险的，需要进一步审查。）因此，为了提高生产率而进行的突破可能应该只是用于收集更好的训练数据来初始化 RL。 （除了非常直接地使用逃逸破坏的AI之外，我们还可以训练AI模仿逃逸破坏的AI，但不需要逃逸破坏的提示；这样更方便，但仍然存在同样的问题。）&lt;/p&gt;&lt;p&gt;换句话说，我们可以通过让人工智能相信它正在执行接管行为之一来获得更好的能力。&lt;/p&gt;&lt;h3&gt; 6.小样本数ML&lt;/h3&gt;&lt;p&gt;在某些情况下，即使是少量的严重不当行为示例也可能允许您改进简单的模型，您可以使用该模型来检测未来的逃跑尝试，例如我们&lt;a href="https://www.lesswrong.com/posts/WCj7WgFSLmyKaMwPR/coup-probes-catching-catastrophes-with-probes-trained-off#Harmfulness_detection_at_AI_labs"&gt;&lt;u&gt;在此&lt;/u&gt;&lt;/a&gt;描述的。更一般地说，我们预计使用（大量）人工智能被发现的案例来训练（不受信任的）监控模型可能会非常有帮助。特别是，使用这些示例来进一步训练对策略的&lt;a href="https://www.lesswrong.com/posts/WCj7WgFSLmyKaMwPR/coup-probes-catching-catastrophes-with-probes-trained-off"&gt;&lt;u&gt;政变探测&lt;/u&gt;&lt;/a&gt;并进一步训练不可信监控器（不可信监控是我们在论文中探索的一种技术）似乎非常有用。&lt;/p&gt;&lt;p&gt;使用&lt;a href="https://www.alignmentforum.org/posts/9vwekjD6xyuePX7Zr/contrast-pairs-drive-the-empirical-performance-of-contrast"&gt;&lt;u&gt;对比&lt;/u&gt;&lt;/a&gt;&lt;a href="https://www.ai-transparency.org/"&gt;&lt;u&gt;对&lt;/u&gt;&lt;/a&gt;可能会提高微小样本计数体系中的样本效率。也就是说，它可能有助于构建一对人工智能输出，其中一个涉及逃跑尝试，另一个尽可能相似，但完全是无辜的。然后，我们可以分析这些输出之间激活的差异（对于我们用于监控的模型主体），并使用它来更有效地制作分类器。&lt;/p&gt;&lt;p&gt;训练任何采取非监控相关操作的模型的实际主体可能存在许多缺点（例如，模型可能决定启动不良操作而不是仅仅未能标记它们）。训练身体可能有意义，但我们不确定。缺点是：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;政策模型可以&lt;a href="https://www.lesswrong.com/posts/sbGau4QBwToYWEg4k/llms-sometimes-generate-purely-negatively-reinforced-text"&gt;&lt;u&gt;从反面例子中学习&lt;/u&gt;&lt;/a&gt;我们所采取的对策以及哪些方法可能会失败。&lt;/li&gt;&lt;li&gt;如果我们更新策略模型的主体，那么所有先前的尝试现在都将脱离策略，并且人工智能在这种情况下可能不再尝试错误的操作，即使它尝试以完全相同的速率逃脱（由于当噪声发生变化时） AI 进行尝试）。这看起来很烦人，特别是对于像在模型身体上训练探针这样的干预措施。&lt;/li&gt;&lt;li&gt;我们会抓到更少的企图逃跑的人，因为尝试的次数会更少。 （捕捉更多的尝试将是有价值的。）&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;除了监控模型之外，训练模型主体的主要优点是训练主体可以增加额外的可靠性。&lt;/p&gt;&lt;h3&gt; 7. 使用改进的监控重新审视之前的行动&lt;/h3&gt;&lt;p&gt;当我们使用少样本方法改进监控技术时，将这些改进的监控技术应用于人工智能之前采取的行动可能会很有用，希望能找到人工智能尝试逃跑但失败的早期案例。 （有关更多讨论，请参阅“逃跑尝试可能会失败而不被抓住”。）因此，改进的监控可能能够突出显示这些案例，以便进行更仔细的审查。&lt;/p&gt;&lt;p&gt;对之前看到的操作重新运行监控可能会在计算上花费一些成本，但我们认为有合理的方法可以在不使用太多计算的情况下获得大部分好处：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;我们可以保存要应用更新探针的激活。 （对于 LLM，如果我们为每 1k 个令牌保存一层和激活的令牌位置，相对于仅保存文本输入和输出，这可能只需要大约 25 倍的额外存储空间。）&lt;/li&gt;&lt;li&gt;我们只能重新分析最可疑案例的前 10%（根据可信模型，前 10%）。&lt;/li&gt;&lt;li&gt;如果我们重复捕获模型，我们可以以指数递减的速率进行重新分析，以确保我们只对 log(# catches) 进行多次重新分析。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt; 8. 提高对替代培训方案的理解和测试&lt;/h3&gt;&lt;p&gt;正如扩散部分所讨论的，对于人工智能来说，拥有强大的信号可能很重要，这些信号允许它们在试图逃跑时招募其他人工智能合作者。这甚至适用于接受不同策略或不同能力水平训练的人工智能之间的协作（否则，以这种方式多样化将完全对抗人工智能使用的策略）。因此，我们也许能够测试替代的训练策略，这些策略在事前看起来似乎可以减少阴谋的可能性，然后通过测试新的人工智能是否与我们捕获的尝试相一致来验证这些变化。&lt;/p&gt;&lt;p&gt;更一般地说，我们可以使用这些尝试作为验证来提高我们对阴谋的理解。即使捕获试图逃脱的模型会导致人工智能实验室关闭，并且只有在安全技术取得相当大的进步后才能重新启动，我们仍然希望通过查看这些新的人工智能是否与旧的尝试合作来验证这些新方法（我们需要以避免训练集泄漏，以确保新的人工智能不会预先了解正在发生的事情）。&lt;/p&gt;&lt;h2&gt; “先控制后少灾防”计划什么时候才能发挥作用？&lt;/h2&gt;&lt;p&gt;我们在这里讨论的方法很有希望，但根据&lt;a href="https://www.lesswrong.com/posts/d9FJHawgkiMSPjagR/ai-control-improving-safety-despite-intentional-subversion#Evaluation_methodology"&gt;&lt;u&gt;控制评估的&lt;/u&gt;&lt;/a&gt;评估，它们可能不会提高安全性。因此，我们认为，随着我们继续关注人工智能，安全性争论应该部分转向依赖证据而不是控制评估。&lt;/p&gt;&lt;p&gt;假设我们从基于控制的安全方法开始，并且由于&lt;a href="https://www.lesswrong.com/posts/rCJQAkPTEypGjSJ8X/how-might-we-align-transformative-ai-if-it-s-developed-very#Magma_s_predicament"&gt;&lt;u&gt;不作为风险&lt;/u&gt;&lt;/a&gt;，即使我们确定我们的人工智能是一个阴谋者，我们也希望继续部署。因此，当我们反复捕捉人工智能时，我们计划逐步过渡到不同的安全案例。在什么情况下我们才能达到足够的安全性？&lt;/p&gt;&lt;p&gt;假设我们的控制方法有 99% 的机会捕获诡计多端的 AI 的任何给定尝试，然后在剩下的 1% 中 AI 成功逃脱。如果抓住AI后没有任何变化，那么心机重重的AI总共尝试100次，成功的几率为63%。这是不可接受的，因此仅靠自身控制是不够的。我们希望通过少数策略来避免这个问题，这些策略旨在降低每次捕获人工智能时的风险，以确保总风险足够低。&lt;/p&gt;&lt;p&gt;例如，想象一下，在之前的场景中，每当我们抓住人工智能时，诡计多端的人工智能成功而不被抓住的机会就会减半。那么，诡计多端的人工智能总共“只有”2% 的成功机会。在实践中，我们希望确保尝试总数不会太高（可能使用少样本方法），同时提高捕获未来尝试的机会。&lt;/p&gt;&lt;p&gt;请注意，如果人工智能采用的策略涉及相对频繁的尝试，那么我们很可能在初步测试中遇到这些尝试。 （例如，如果 AI 尝试超过 1,000 次，无法区分部署早期和部署后期，并且我们仔细监督部署前 0.25% 的所有示例，那么我们有 92% 的机会遇到尝试就在这 0.25% 之内（尽管当我们仔细监督时，我们可能仍然无法注意到它）。因此，人工智能可能需要做出艰难的权衡。&lt;/p&gt;&lt;p&gt;事实上，我们很难对我们的评估充满信心。所以现实情况可能是这样的：&lt;/p&gt;&lt;p&gt;在预训练之前，我们要训练的模型有 25% 的机会最终会进行诡计。我们进行了控制评估。在这些评估中，我们在 99.5% 的时间内捕获了&lt;a href="https://www.lesswrong.com/posts/d9FJHawgkiMSPjagR/ai-control-improving-safety-despite-intentional-subversion#We_explicitly_measure_how_robust_techniques_are_against_scheming_AIs"&gt;&lt;u&gt;红队模型&lt;/u&gt;&lt;/a&gt;，然后才成功逃脱。&lt;/p&gt;&lt;p&gt;更具体地说，假设该评估的最终状态分布如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt; 30% 的情况是，模型在部署之前就被捕获。&lt;/li&gt;&lt;li&gt; 60% 的情况下，模型会在部署过程中被捕获。&lt;/li&gt;&lt;li&gt; 9.5% 的情况下，模型从未被抓住，但从未成功逃脱。&lt;/li&gt;&lt;li&gt; 0.5% 的情况下，模型会逃脱。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;由于对评估的各种担忧（有关可能导致安全性不确定性的评估困难的讨论，请参阅&lt;a href="https://arxiv.org/abs/2312.06942"&gt;&lt;u&gt;我们论文&lt;/u&gt;&lt;/a&gt;“5.1.1 在实践中使用控制评估的困难”中的讨论小节），我们实际的综合考虑概率在我们抓住它之前以阴谋为条件逃脱的 AI 大约有 5%。我们综合考虑，以阴谋为条件的逃跑概率（在任何捕获之前或之后）约为 10%；这考虑到了我们可能对对策进行的改进，以及如果我们的对策看起来非常试探的话，在捕获人工智能后关闭的可能性。总体而言，P(escape) = P(escape|scheming) P(scheming) = 10% * 25% = 2.5%（为了简单起见，忽略了由于非阴谋错位而导致的逃逸）。&lt;/p&gt;&lt;p&gt;我们当场抓住了人工智能。我们分析了逃跑企图，看来我们的对策在这种情况下显然是足够的（多条防线独立地阻止了逃跑）。根据这个例子，红队在探索可能的逃跑策略方面似乎并没有明显的做得不够充分的地方。通过这一分析，我们对初步评估更有信心；如果我们继续采取相同的对策，我们新的综合考虑，人工智能在再次捕获它之前逃脱的可能性会下降到 2%。&lt;/p&gt;&lt;p&gt;但我们也能够大幅增强对策，并大幅增加在训练中使用的逃跑尝试。有了这些增强的对策，我们在再次捕获 AI 之前的新的所有考虑的逃跑概率已降至 1%。并且，假设我们的总体逃脱概率（在任何未来捕获之前或之后）是 2%。&lt;/p&gt;&lt;p&gt;虽然我们积极更新了初步评估并改进了对策，但我们也将模型有阴谋的可能性从 25% 更新为 100%。所以我们新的逃跑概率只下降了20%；我们的新估计是 P(逃跑) = P(逃跑|阴谋) P(阴谋) = 2% * 100% = 2%。 （我们预计，正如这里所发生的那样，在发现逃跑企图后，你对阴谋模型的评估风险会下降而不是上升，因为 P(逃跑|阴谋) 的减少超过了 P(阴谋) 的增加。 ）&lt;/p&gt;&lt;p&gt;在这个人工智能生命周期的剩余时间里，我们最终再次抓住了人工智能试图逃跑的两次，并使用几次射击技术进一步改进了对策。我们最终还训练了实际的策略，迫使它在尝试逃跑时更加保守，并（可能）让它不再是一个阴谋家。我们不会进一步捕获人工智能，而是完成其余的部署，此时该模型已被我们认为可靠对齐的新模型所取代，或者我们已经成功地使用人工智能来降低风险由不那么细心的人工智能开发人员强加的，所以我们不需要再使用这个人工智能了。&lt;/p&gt;&lt;p&gt;总之，只要我们能够在每次被捕获的尝试中显着提高安全性，就应该可以通过“捕获然后使用少量射击”灾难预防计划来确保安全。安全性的提高可以通过增加我们捕获未来尝试的可能性或训练人工智能不进行尝试来体现。也可以通过仅使用控制评估并通过我们的观察限制尝试次数（例如，到目前为止在部署中看到足够低的尝试率）来确保安全，但是未来分布变化的可能性使得很难自信地限制尝试次数。基于过去观察的尝试次数。&lt;/p&gt;&lt;p&gt;我们在这里讲述的故事是一个乐观的故事，在你发现逃跑企图后，证据表明你可以安全地继续部署。当然，也可能是，当您捕捉到逃跑尝试时，您意识到自己距离逃跑尝试成功太近了，因此您可以自信地决定需要关闭模型。&lt;/p&gt;&lt;h2&gt;避免过度拟合并保持偏执&lt;/h2&gt;&lt;p&gt;我们应该小心，避免受到我们所发现的尝试的过度影响。特别是，诡计多端的人工智能可能会故意尝试执行不力的逃跑尝试，给我们一种错误的安全感，或者将资源转移到效率较低的保障措施上。很难确信我们看到的是真正的尝试而不是虚假的尝试。更一般地说，我们需要避免正常的过度拟合。然而，为了让人工智能真正取得不好的结果，它必须在某个时候进行认真的尝试；以捕获为重点的方法的希望是捕获这些真正认真的尝试。由于假政变的可能性，我们可能不应该完全废除控制评估；我们需要确保我们有足够高的机会抓住第一次认真的尝试。&lt;/p&gt;&lt;p&gt;如果人工智能实验室正确地考虑了虚假尝试的可能性，那么故意进行虚假尝试对于人工智能来说不太可能是一个好的策略。&lt;/p&gt;&lt;h1&gt;结论&lt;/h1&gt;&lt;p&gt;如果你当场发现你的人工智能故意试图造成灾难性的不良后果，你的情况就会发生很大的变化。因此，我们认为研究人员应该主要关注安全计划，以确保如果模型进行了策划，则不会在被抓住之前逃脱。&lt;/p&gt;&lt;p&gt;如果模型计划减轻逃脱的风险，似乎很可能会至关重要地依靠实验室明智地响应逃避尝试。因此，实验室和政策制定者应该考虑如何处理逃脱尝试并尝试为它们做准备。&lt;/p&gt;&lt;p&gt;我们认为，很少有预防灾难的灾难似乎是一个相当重要的研究问题，一些安全的人可能应该为此开发技术，因为如果我们需要使用这些技术，我们可能会承受很大的时间压力，并且提前开发这些技术会很好。&lt;/p&gt;&lt;h1&gt;附录：用e [实用程序|心计]&lt;/h1&gt;&lt;p&gt;我们经常考虑在&lt;a href="https://www.alignmentforum.org/posts/d9FJHawgkiMSPjagR/ai-control-improving-safety-despite-intentional-subversion"&gt;&lt;u&gt;AI控制&lt;/u&gt;&lt;/a&gt;的背景下捕捉。考虑控制评估的一种生产方法是，它们旨在衡量e [实用程序|示意]：如果我们有一个示意的AI，结果的预期优势。从这个角度来看，使用控制评估时针对安全或捕获的论点有两个部分：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;捕捉可以实现强大的非控制技术干预措施&lt;/strong&gt;：捕获AI后，我们可以访问看起来非常有用但无法控制的技术，并且无法控制的方法无法控制。 “少数灾难预防”部分讨论了几种非控制技术干预措施，这些干预措施在捕获AI后可能会成为可能。多次捕获AI后，非控制方法变得特别吸引人。鉴于捕获AI可能允许使用相当强大的非控制技术方法，我们应该考虑将AI捕获为控制评估的安全终点（这完全等同于针对安全性或捕捉），或者至少尝试大致模型在捕获AI之后，情况将如何变化，而不仅仅是继续进行控制评估。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;除了直接提高安全性之外，捕获AI是一个很好的结果&lt;/strong&gt;，我们应该将其考虑到我们的评估中。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;虽然捕捉似乎很有价值，但单个捕获不足以确保安全。&lt;/p&gt;&lt;h1&gt;附录：定义捕捉&lt;/h1&gt;&lt;p&gt;为了这篇文章，我们主要是在谈论灾难性的结果，例如模型&lt;a href="https://aligned.substack.com/p/self-exfiltration"&gt;&lt;u&gt;消除其权重&lt;/u&gt;&lt;/a&gt;，这可能仅涉及少量动作。还有其他威胁模型与策略相关，例如，模型巧妙地破坏了他们所做的工作，使他们的工作无用，或者随着时间的推移而巧妙地获得了影响。为了这篇文章，我们忽略了这些威胁模型，但是我们认为我们可以采用类似的策略来处理它们。&lt;/p&gt;&lt;p&gt;我们将“捕获”模型定义为：您找到输入，您可以证明您的模型采取灾难性的行动，其概率比该模型没有故意寻找机会采取此类操作的机会要高。当我们说“您的模型”时，我们的意思是您已部署（或计划部署）的实际模型，而不是以类似方式培训的其他模型。在几乎没有造成的灾难预防的大多数方面，捕捉您计划部署的实际模型至关重要，但是捕获不同但类似的模型对于其他用例就足够了（捕捉到陷入困境的，易于清晰的证据，一般的科学理解）。&lt;/p&gt;&lt;p&gt;为了确保您可以区分这些故意的灾难性行动，您需要做到这一点，以便您的模型永远不会逃脱而不明确采取非事故的不良行为。我们认为您可以做到这一点，以便您的模型不会逃脱而不必做一些明确的故意坏事。 （人类可能仍然很难注意到并理解这种不良行为；声称不良行为显然必须是故意的。 （例如，随着时间的推移巧妙地获得影响），比任何单一的动作显然是非事故的，但原则上可能仍然有可能确定这些行动在总体上并不偶然。&lt;/p&gt;&lt;p&gt;即使您捕捉到这样的行动明显是故意不好的，您实际上也无法确定该模型是在策划的，而不是出于某些其他问题的原因而故意采取不好的行动。但是，即使模型没有策划，该模型可能会故意做出不良动作的其他原因也很令人担忧。另外，当您从几场预防灾难部分中列出的角度考虑它时，从捕获更多案例中，您将永远不会变得更糟。&lt;/p&gt;&lt;p&gt;要算作成功的捕获，您需要对捕获量充满信心。 （这是为了使陷入困境的前面的前进不太昂贵，清​​晰的证据实际上是有效的，因此您可以根据捕获量对各种实验的结论充满信心。我们仍然可以仅仅研究看似粗略的案例，但这将产生大幅度的价值。）获得足够的信心可能会涉及大量调查（这可以在AI实验室暂时关闭时进行）。我们可以想象，经过一系列较小的调查升级后，进行了这项大规模调查。 “预防灾难”中列出的动作也可能有助于验证这确实是一种造成灾难性结果的尝试。&lt;/p&gt;&lt;h1&gt;附录：不会捕获但要更新P（scheming）的技术&lt;/h1&gt;&lt;p&gt;我们在这里定义“捕获”的方式，不包括可能会更新模型是否正在策划的技术，但不涉及该模型以极高的可疑可能性采取的特定操作：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;模型生物：&lt;/strong&gt; &lt;a href="https://www.alignmentforum.org/posts/ChDH335ckdvpxXaXX/model-organisms-of-misalignment-the-case-for-a-new-pillar-of-1"&gt;&lt;u&gt;以策划为重点的模型有机体实验&lt;/u&gt;&lt;/a&gt;涉及制作模型，以表现出示意模型的某些特征。似乎很棒。乐观地，这些可能使我们具有非常经过校准的P（scheming）感以及测试对策的能力。但是，更有可能是这些结果令人困惑，并且不产生共识。同样，如果您构建模型生物体的模型已经构成了策划，则模型生物实验似乎很容易受到破坏的影响。而且，您不能使用模型生物实验来进行下面列出的几次灾难预防干预措施，尽管从理论上讲，模型有机体可以为灾难性未对准和改善一般科学理解提供清晰的证据。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;间接证据：&lt;/strong&gt;例如，模型心理学或机械性解释性，不允许您产生模型采取不良行动的投入示例。我们可能可以使用模型心理学来了解我们的模型具有邪恶意图而无需构建榜样。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;怪异的对抗性示例&lt;/strong&gt;：我们可能能够通过搜索触发非常不受欢迎的行为，特别&lt;a href="https://arxiv.org/abs/2307.15043"&gt;&lt;u&gt;是涉及非常不可能输入的对抗性示例来找到奇怪的对抗性示例&lt;/u&gt;&lt;/a&gt;。也可能会发现&lt;a href="https://www.lesswrong.com/posts/atBQ3NHyqnBadrsGP/latent-adversarial-training"&gt;&lt;u&gt;对模型内部的扰动，从而导致非常不希望的行为&lt;/u&gt;&lt;/a&gt;。我们不知道如何评估这些对抗性示例是在潜在的策略中真正获得的，还是只是欺骗模型进行策划。在某些情况下，可能有可能获得更多信息或有明确的判断（例如，如果我们仅使用少量搜索来查找具体的对抗输入，并且根据对我们输入分布的LLM培训的LLM，则输入很可能）。 。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt; （所有这些可能都是有前途的研究指示，它们只是不会从相关意义上捕获AI。）&lt;/p&gt;&lt;h1&gt;附录：逃生尝试可能会失败而不会被抓住&lt;/h1&gt;&lt;p&gt;逃脱尝试之类的行动可以成功，被捕获（失败）或失败而不会被捕获。我们非常希望抓住AI仅仅导致AI失败，因为捕获AI对于各种目的都是有用的。也就是说，导致不良行为失败仍然可以通过避免不良结果直到部署结束或我们实际上抓住AI来实质性地帮助安全。在我们的论文中，我们利用了不受信任的灵感等干预措施，这些干预措施并不能直接帮助捕获AI，但会降低其在后门成功的可能性。&lt;/p&gt;&lt;p&gt;为什么逃脱的尝试可能不会被失败而结束？&lt;/p&gt;&lt;p&gt;逃生尝试可能需要多个AI之间的协调，并访问一些不同的信息。链条中的某些AI可能成功地进行了逃避监控，但是由于AI后来的AI而没有继续尝试（也没有报告逃生尝试；如果它报告了这一尝试试图）。后来的AI可能不会继续尝试，因为它没有注意到较早的AI开始了逃生尝试，或者确实注意到逃生尝试，但由于仅可用的信息而认为这实际上并不是一个好机会。&lt;/p&gt;&lt;p&gt;逃生尝试可能会失败而不会被抓住的另一个原因是，我们的对策足以阻止该动作，但不会升级到更高的审计水平。例如，假设我们根据受信任的监视重新采样所有可疑阈值的动作，同时仅建议昂贵的审计措施，如果它们高于较高的阈值。 （这就是我们在&lt;a href="https://www.lesswrong.com/posts/d9FJHawgkiMSPjagR/ai-control-improving-safety-despite-intentional-subversion"&gt;&lt;u&gt;AI控制纸&lt;/u&gt;&lt;/a&gt;中所说的“混合策略”。）然后，我们最终可能会通过重新采样而停止逃生尝试，而不会升级链条审核。如果人类审计发现动作有些可疑并且值得以一种不太可疑的方式做，但不值得进行更详细的评论和/或暂停使用强大模型的使用情况，可能会发生类似的事情。&lt;/p&gt;&lt;h1&gt;附录：什么时候会抓住AI？&lt;/h1&gt;&lt;p&gt;在以下情况下可能会捕获逃生尝试：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;在适当部署之前。这是最安全的结果。&lt;/li&gt;&lt;li&gt;在部署期间，实际输入。&lt;/li&gt;&lt;li&gt;在部署期间，在合成输入中。这可能会发生，因为在输入中，对您的模型进行大量对抗性评估可能是有意义的，选择了一些子集来转换为假逃生机会。这种合成输入的优点是，修改现有输入比从头开始构造合成输入要便宜。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;为什么我们认为我们可以在部署期间抓住，但以前却没有抓住？&lt;/p&gt;&lt;ul&gt;&lt;li&gt;部署更大 - 我们可能会在部署期间比部署之前更多的输入运行模型。因此，在极少数情况下仅在部署之前就不会触发的模型策略，这些策略仅在极少数情况下表现出色。&lt;/li&gt;&lt;li&gt;很可能（在没有AI实验室的积极对策的情况下），该模型将相当良好地区分部署和非部署数据。例如，将要求执行任务的分布可能会在部署过程中发生变化。如果它可以区分它的设置，则可以尝试仅在部署输入上表现出色。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;总体而言，我们不确定我们在部署之前还是在部署期间是否更有可能捕获模型。&lt;/p&gt;&lt;ol class="footnotes"&gt;&lt;li class="footnote-item" id="fnlokat6zwmzf"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnreflokat6zwmzf"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;虽然我们将讨论捕捉策划，但所有这些推理都适用于捕获任何类型的灾难性未对准。确切地说，如果没有保障措施，这些行动的上游原因没有策划，我们可能会捕捉到灾难性的行动。例如，这可能是奖励寻求行为或玩训练游戏的概括（如“ &lt;a href="https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to"&gt;没有特定对策&lt;/a&gt;”中的描述）。也就是说，还有其他合理的好方法来测试非灾难性的灾难性未对准，因此捕捉特别是为策划而言。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ol&gt;&lt;br /&gt;&lt;br /&gt;&lt;a href="https://www.lesswrong.com/posts/i2nmBfCXnadeGmhzW/catching-ais-red-handed#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Fri, 05 Jan 2024 17:43:10 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/i2nmBfCXnadeGmhzW/catching-ais-red-handed</guid></item><item><title>AI 影响调查：2023 年 12 月版</title><link>https://www.lesswrong.com/posts/NfPxAp5uwgZugwovY/ai-impacts-survey-december-2023-edition</link><description>发布于 2024 年 1 月 5 日下午 2:40（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt; Katja Grace 和 AI Impact &lt;a href="https://aiimpacts.org/wp-content/uploads/2023/04/Thousands_of_AI_authors_on_the_future_of_AI.pdf" rel="noreferrer noopener" target="_blank"&gt;对数千名研究人员就各种问题进行了调查&lt;/a&gt;，继 2022 年的一项类似调查以及 2016 年的一项调查之后。&lt;/p&gt;&lt;p&gt;我鼓励打开原件以获得更好的图表可读性以及上下文和附加信息。我会介绍其中一些，但还有很多。&lt;/p&gt;&lt;span id="more-23654"&gt;&lt;/span&gt;&lt;h4&gt;一项充满矛盾的大型调查&lt;/h4&gt;&lt;p&gt;这是摘要，总结了很多要点：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;在同类最大规模的调查中，我们调查了 2,778 名在顶级人工智能 (AI) 场所发表过论文的研究人员，询问他们对人工智能进展速度以及先进人工智能系统的性质和影响的预测。&lt;/p&gt;&lt;p&gt;总体预测显示，到 2028 年，人工智能系统至少有 50% 的机会实现多个里程碑，包括从头开始自主构建支付处理站点、创作一首与流行音乐家的新歌难以区分的歌曲，以及自主下载和微调大语言模型。&lt;/p&gt;&lt;p&gt;如果科学继续不受干扰地发展，到 2027 年，在所有可能的任务中，无辅助机器胜过人类的机会估计为 10%，到 2047 年为 50%。后者的估计比我们一年前进行的一项类似调查中得出的结果早了 13 年[格蕾丝等人，2022]。然而，预计到 2037 年，所有人类职业实现完全自动化的可能性将达到 10%，最迟到 2116 年将达到 50%（相比之下，2022 年的调查为 2164 年）。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;正如我稍后将详细阐述的那样，这种对比毫无意义。我们不会让机器在 2047 年的每项任务上都胜过人类，然后在 2116 年才能完全实现人类职业的自动化。这没有任何意义。&lt;/p&gt;&lt;p&gt;我认为2047年的时间表很高，但在合理范围内。研究人员可能会更清楚地思考问题的这一方面。我们应该主要按照他们的想法使用这个答案。我们应该主要将 2116 的答案视为没有意义，除非将其与使用类似措辞的过去和未来的估计进行比较。&lt;/p&gt;&lt;p&gt;无论如何，人工智能进展的预期速度在一年内已​​经加快了很多。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;大多数受访者对人工智能进步的长期价值表示很大的不确定性：虽然 68.3% 的人认为超人类人工智能的好结果更有可能比坏结果更有可能，但在这些净乐观主义者中，48% 的人认为至少有 5% 的可能性会出现极其糟糕的结果，例如人类的人工智能。 59% 的净悲观主义者认为 5% 或更多的人认为结果非常好。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;具有高度不确定性的分布是明智的。这与预期中等或中性的结果形成鲜明对比，后者毫无意义。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; 37.8% 至 51.4% 的受访者认为，先进人工智能至少有 10% 的机会导致人类灭绝一样糟糕的结果。超过一半的人表示，对六种不同的人工智能相关情景有必要“严重”或“极端”担忧，包括虚假信息的传播、专制人口控制和不平等加剧。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;我们再次看到看似矛盾的东西。如果我认为人工智能导致人类灭绝的可能性有 10%，我就会对此感到“极度”担忧。我就是这么做的。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;对于更快还是更慢的人工智能进步是否对人类的未来更好存在分歧。然而，人们普遍认为，旨在最大限度地减少人工智能系统潜在风险的研究应该更加优先。&lt;/p&gt;&lt;p&gt;我们这样定义高级机器智能（HLMI）：&lt;/p&gt;&lt;p&gt;当独立机器能够比人类工人更好、更便宜地完成每项任务时，就实现了高级机器智能 (HLMI)。忽略人类本质上有利的任务方面，例如被接受为陪审团成员。考虑可行性，而不是采用。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;这是一个非常高的标准。 “每项任务”与许多或大多数任务有很大不同，尤其是与更好和更便宜的任务相结合。另请注意，这并不全是“智力”任务。这都是任务，期间。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;我们要求做出预测，假设“人类科学活动继续进行，没有重大负面干扰。”我们通过拟合伽玛分布来汇总结果（n=1,714），就像 3.1 中的单个任务预测一样。&lt;/p&gt;&lt;p&gt;在 2022 年和 2023 年，受访者对 HLMI 多久能够实现提出了广泛的预测（图 3）。&lt;/p&gt;&lt;p&gt; 2023 年的总体预测预测，到 2047 年，发生 HLMI 的可能性为 50%，比 2022 年调查中的 2060 年下降了 13 年。相比之下，在 2016 年和 2022 年调查之间的六年中，预期日期仅提前一年，从 2061 年推迟到 2060 年。&lt;/p&gt;&lt;/blockquote&gt;&lt;h4&gt;他们破坏了时间线的稳定性&lt;/h4&gt;&lt;p&gt;这是一个潜在的未来世界，到 2047 年，人工智能可以“比人类更好、更便宜地完成所有人类任务”。&lt;/p&gt;&lt;p&gt;之后会发生什么？他们认为 2048 年会是什么样子？ 2057?&lt;/p&gt;&lt;p&gt;这是整个练习中最奇怪的部分。&lt;/p&gt;&lt;p&gt;从这个调查来看，他们似乎选择不去想太多？是出于某种“事情会正常并按正常速度发展”的感觉吗？&lt;/p&gt;&lt;figure class="wp-block-image"&gt; &lt;a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc985c985-3f05-4009-ab37-8d7f7a9470b0_1306x1590.png" rel="noreferrer noopener" target="_blank"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/NfPxAp5uwgZugwovY/czy7to2nm1hmeu6kdacp" /&gt;&lt;/a&gt;&lt;/figure&gt;&lt;p&gt; [请注意，我最初误读了第二张图表，认为其比例与第一张图表相同。第二个图表是第一个图表左侧的扩展。]&lt;/p&gt;&lt;p&gt;他们是否真的期望我们拥有人工智能，能够在所有事情上比我们做得更好，然后有效地坐拥几代人？&lt;/p&gt;&lt;p&gt;包括人类两代人，人工智能正在做人工智能研究，比人类更好、更便宜？当这种情况发生时，世界会保持那种正常和受控吗？&lt;/p&gt;&lt;p&gt;下面的 FOAL 是人类劳动的完全自动化，HLMI 是高级机器智能。&lt;/p&gt;&lt;figure class="wp-block-image"&gt; &lt;a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ec38e15-c75d-4a03-bacd-93836b28fc8f_1033x733.png" rel="noreferrer noopener" target="_blank"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/NfPxAp5uwgZugwovY/pbgsao1p91wywbxpndcv" /&gt;&lt;/a&gt;&lt;/figure&gt;&lt;figure class="wp-block-image"&gt; &lt;a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9632c44e-94ed-408c-b0b2-3002de5a9b31_1135x759.png" rel="noreferrer noopener" target="_blank"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/NfPxAp5uwgZugwovY/mk1fbgbthjpndiyez83p" /&gt;&lt;/a&gt;&lt;/figure&gt;&lt;p&gt;论文作者注意到他们也很困惑。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;由于职业可能自然地被理解为复杂的任务、由任务组成或与其中一项任务密切相关，因此实现 HLMI 似乎要么意味着已经实现了FAOL，要么表明接近实现。我们不知道是什么造成了预测的这种差距。就 HLMI 和FAOL 提及同一事件而言，对它们到达时间的预测差异似乎是一种框架效应。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;如果这种差异的原因纯粹是“我们希望人类阻止人工智能完全接管至少一项雇用至少一个人的工作，或者至少我们希望某些人在某个地方继续能够执行一些劳动”，那么这可能是解释其中的差异。我很想澄清一些问题。&lt;/p&gt;&lt;p&gt;这似乎也是关于人工智能后果的一个基本常识测试：如果人工智能完全处于“你能做的任何事我能做得更好”的模式，这会是技术进步的一个数量级的加速吗？&lt;/p&gt;&lt;p&gt;我的意思是，是的，显然？我认为左边这张图的描述不小心颠倒了，但即便如此，这似乎很多人都没有思考清楚？你可以怀疑 HLMI 是否会到来，但如果我们真的拥有它，后果似乎很明显。除非人们认为我们有智慧和能力根本不使用它？&lt;/p&gt;&lt;figure class="wp-block-image"&gt; &lt;a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa831dd64-2993-45be-8b4e-42e0cdf3374a_1423x783.png" rel="noreferrer noopener" target="_blank"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/NfPxAp5uwgZugwovY/inc9up9q36secvoj6yrv" /&gt;&lt;/a&gt;&lt;/figure&gt;&lt;p&gt;或者：&lt;/p&gt;&lt;figure class="wp-block-image"&gt; &lt;a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea6298c0-bc6a-4e01-a9aa-48809e7becc2_1428x429.png" rel="noreferrer noopener" target="_blank"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/NfPxAp5uwgZugwovY/kwsctlt3u1gb3qjtzlpa" /&gt;&lt;/a&gt;&lt;/figure&gt;&lt;p&gt;显而易见的是，在 HLMI 零 (0) 年后，人工智能远远优于人类。如果你能使用与我截然不同的架构来比我做得更好，那么你不仅好一点点。当然，两年后，10% 的可能性在这里就变得毫无意义了。&lt;/p&gt;&lt;p&gt;更荒谬的是，这些是到 2043 年的概率。这甚至不接近一组一致的概率分布。考虑其中哪些或哪些组合暗示其他哪些。&lt;/p&gt;&lt;figure class="wp-block-image"&gt; &lt;a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07ee1dd5-251b-472b-af2f-a2c70e518965_1045x732.png" rel="noreferrer noopener" target="_blank"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/NfPxAp5uwgZugwovY/cj5fcsrk2psafvo6f5lq" /&gt;&lt;/a&gt;&lt;/figure&gt;&lt;p&gt;当然，我认为其中一些列出的可能事件并不是那么不确定，例如“有时会欺骗人类以实现并非人类有意为之的目标”。我的意思是，到 2043 年这怎么可能不会发生呢？&lt;/p&gt;&lt;h4&gt;什么，我担心？&lt;/h4&gt;&lt;p&gt;这是人们所关心的，一张极其令人担忧的图表。&lt;/p&gt;&lt;figure class="wp-block-image"&gt; &lt;a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc087d63c-79de-43b8-ab0d-6e5f463d3fdb_1350x790.png" rel="noreferrer noopener" target="_blank"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/NfPxAp5uwgZugwovY/stuqnwucrtobj40lwlpl" /&gt;&lt;/a&gt;&lt;/figure&gt;&lt;p&gt;忧虑全都用在了错误的地方。最重要的担忧是……深度造假？这些并不是我们应该担心的事情的特别核心例子。&lt;/p&gt;&lt;p&gt;在所有这些问题中，最大的问题可能是“其他”，因为在其他保护伞下还剩下多少不完整的东西。我认为“目标错误的人工智能会变得强大”和“人工智能的目标设定错误”涵盖了很多内容，即使我会以不同的方式描述核心事件。&lt;/p&gt;&lt;p&gt;人们也可以不将其视为对可能发生的事情的衡量，而是对“有关”的事情的衡量。这意味着人们对社会原因表示担忧，而不是因为人工智能最大的担忧是……深度造假和公众舆论操纵，人工智能有望比人类更好、更便宜地完成所有工作。我的意思是，认真的吗？&lt;/p&gt;&lt;p&gt;另一种选择是人们在矛盾的框架中思考。在某一帧中，他们意识到 HLMI 级别的人工智能即将到来。在另一个框架中，他们问“有什么关系？”并且只考虑普通的人工智能。&lt;/p&gt;&lt;h4&gt;最大的问题&lt;/h4&gt;&lt;p&gt;从人工智能的最终后果来看，包括潜在的生存风险，意义建构并没有变得更好。&lt;/p&gt;&lt;figure class="wp-block-image"&gt; &lt;a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e5c6f5b-fff6-4b8d-9fc5-17de4f80ec2d_1447x1158.png" rel="noreferrer noopener" target="_blank"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/NfPxAp5uwgZugwovY/bb3g8rzvglw51znq4udt" /&gt;&lt;/a&gt;&lt;/figure&gt;&lt;p&gt; HLMI 是一种能够比人类做得更好的人工智能，在它的所有潜在后果中，“中立”并不属于其中。这完全没有意义。这是我们告诉自己的科幻故事，这样我们就可以继续讲述同样相关的人类故事。它可能会进展顺利，这可能是一切有价值的事情的结束，但它绝对不会是无聊的。&lt;/p&gt;&lt;p&gt;如果你告诉我它“变得中立”，那么我可以想出一个故事，其中某人或某个团体创建了 HLMI/ASI，然后决定使用它来确保没有其他人构建一个，否则就完全不管事情，因为他们认为这样做其他任何事情都会更糟。我的意思是，考虑到我见过的其他计划，这绝对是一个高于平均水平的计划，但事实并非如此。&lt;/p&gt;&lt;p&gt;那么我们如何看待这些 p(doom) 数字呢？让我们放大一下，根据问题措辞，有人回答 10% 或更高的概率：&lt;/p&gt;&lt;figure class="wp-block-image"&gt; &lt;a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a4efdca-79a7-47c6-8b26-0d29654dda94_1423x832.png" rel="noreferrer noopener" target="_blank"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/NfPxAp5uwgZugwovY/svpibhq2zhguvxunu5sb" /&gt;&lt;/a&gt;&lt;/figure&gt;&lt;p&gt;这里的 p(doom) 数字是直接矛盾的。我们将会得到一些奇怪的话题。&lt;/p&gt;&lt;p&gt;在图 12 中，所有“极差”结果的平均值为 9%，中位数为 5%。&lt;/p&gt;&lt;p&gt;在图 13 的问题 3 中，人类灭绝或严重丧失权力的平均概率为 14.4%，或者从长远来看，概率为 16.2%，中位数仍然为 5%。&lt;/p&gt;&lt;p&gt;然后，在问题 2 中，他们问“人类无法控制未来先进人工智能系统导致人类灭绝或类似的永久性和严重剥夺权力的可能性有多大”，他们的平均值为 19.4%，中位数为 10%。&lt;/p&gt;&lt;p&gt;你说，她更有可能是一名图书管理员和女权主义者，而不仅仅是一名图书管理员。&lt;/p&gt;&lt;p&gt;这主要是一个经典的连词谬误吗？指出我们可能会失去控制的框架效应，要么使人们产生偏见，要么让他们不再考虑可能会发生什么？还有别的事吗？&lt;/p&gt;&lt;p&gt;当您看到 5% 与 10% 时，这并没有看起来那么大。所发生的情况很大程度上是几乎没有人说的，例如 7%。因此，当 47% 的回答为 10% 或更高时，中位数为 5%，然后在 51% 时，它会跃升至勉强达到 10%。 5% 和 10% 都有误导性，这里的“真实”中位数更像是 8%。&lt;/p&gt;&lt;p&gt;我在 Twitter 上做了一项调查，询问如何最好地描述调查结果。&lt;/p&gt;&lt;figure class="wp-block-image"&gt; &lt;a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd12a47d-a3d5-4072-8dd3-4f72ec9a9f90_894x447.png" rel="noreferrer noopener" target="_blank"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/NfPxAp5uwgZugwovY/sle6p43rg2mcq8cntpwn" /&gt;&lt;/a&gt;&lt;/figure&gt;&lt;p&gt;我认为这个民意调查是正确的。 10% 比 5% 更接近准确，但“中位数为 5%-10%，具体取决于框架，平均值为 9%-19%”是报告此结果的正确方法。如果有人想说“研究人员说厄运的可能性是十分之一”，那么这有点快速和宽松，我会避免这么说，但我会在有限的不信任范围内考虑它。&lt;/p&gt;&lt;p&gt;重要的是：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;研究人员普遍承认，人工智能带来的生存风险确实令人担忧。&lt;/li&gt;&lt;li&gt;研究人员认为这种风险足够高，以至于我们应该愿意进行大量投资和牺牲来减轻这种风险。&lt;/li&gt;&lt;li&gt;然而，研究人员并不认为此类风险是默认结果。&lt;/li&gt;&lt;li&gt;研究人员对于这种风险的严重程度存在强烈分歧。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;最重要的是，存在风险是该领域高度主流的关注点。当调查询问时，它们在公众中也非常主流。&lt;/p&gt;&lt;p&gt;任何试图将此类信念框定为边缘立场的媒体报道或其他言论&lt;a href="https://twitter.com/NateSilver538/status/1742979005626867722" rel="noreferrer noopener" target="_blank"&gt;要么没有做足功课，要么在对你撒谎&lt;/a&gt;。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; Nate Silver：这对于人工智能风险来说非常有趣。我认为如果其他领域多尝试对专家意见进行科学调查就好了。 （披露：我在这项调查中做了非常少量的无偿咨询。）&lt;/p&gt;&lt;p&gt;事实上，广泛的从业者对人工智能安全风险（尽管因人而异）和人工智能时间表的加快感到担忧，这一点意义重大。您仍然会偶尔看到媒体报道将这些视为边缘职位。但他们不是。&lt;/p&gt;&lt;/blockquote&gt;&lt;h4&gt;不那么快与不那么快&lt;/h4&gt;&lt;p&gt;尽管有这些关于潜在厄运的预测，但对现在放慢速度的支持如果有一点点负面的话，即使纯粹是从人类的未来角度来看也是如此。&lt;/p&gt;&lt;figure class="wp-block-image"&gt; &lt;a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ceb9351-0c1c-42ed-8dd0-b154d9ea3262_1435x430.png" rel="noreferrer noopener" target="_blank"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/NfPxAp5uwgZugwovY/pqbg6vze17zdnybdeqyc" /&gt;&lt;/a&gt;&lt;/figure&gt;&lt;p&gt;如果（且仅当）你购买 AGI 很远的话，这实际上是完全有意义的。如果 HLMI 仅计划于 2047 年进行，那么从 2024 年到 2029 年放慢速度听起来并不是一个很棒的策略。如果我被告知当前的步伐意味着 AGI 2047，我也不会寻求放慢人工智能的短期发展。&lt;/p&gt;&lt;p&gt;我想在这里“查看交叉表”，但我认为一个非常合理的反应是模糊的：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;如果你认为 AGI 有望在 10 年内到来，那么你就想放慢脚步。&lt;/li&gt;&lt;li&gt;如果您认为 AGI 不太可能在 10 年内实现，但可能会在 25 年内实现，那么您希望大致保持当前的步伐，同时为未来放慢速度奠定基础。&lt;/li&gt;&lt;li&gt;如果你认为 AGI 几乎肯定还需要 25 年以上的时间，并且你（非常合理地）断定 AGI 出现之前的世俗担忧大多被夸大了，那么现在就加速发展，也许以后再担心剩下的事情。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;我相信这方面的基础工作非常重要，并且非常担心路径依赖，但对时间线为 25 年或更长的信心绝对是一个关键，它会改变我对许多事情的看法。&lt;/p&gt;&lt;p&gt;我希望看到更多的人，包括那些在他们的简历中带有“e/acc”的人，明确表示时间线问题是一个症结所在，他们的建议依赖于 AGI 的发展。&lt;/p&gt;&lt;h4&gt;安全研究实际上是好的&lt;/h4&gt;&lt;p&gt;一个亮点是对人工智能安全研究优先顺序的大力支持，尽管力度不够，而且自 2022 年以来只有很小的改进。&lt;/p&gt;&lt;figure class="wp-block-image"&gt; &lt;a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39741133-02aa-4bdc-8237-0fab3888ffcc_1423x940.png" rel="noreferrer noopener" target="_blank"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/NfPxAp5uwgZugwovY/yvtavb13vfmtk4sihdvh" /&gt;&lt;/a&gt;&lt;/figure&gt;&lt;p&gt;我仍然不理解这种不想做更多安全工作的态度。我可以理解想要尽快前进的心情。我可以理解，你们公司尤其应该注重能力。我不明白为什么人们不认为加强安全工作会对世界有利。&lt;/p&gt;&lt;p&gt;我认为这里 13% 的人认为“一致性是该领域最重要的问题之一”，这实际上是最荒谬的结果之一：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;第二组人工智能安全问题基于 Stuart Russell 提出的对齐问题 [Russell，2014]。这组问题首先总结了拉塞尔的论点——该论点声称，借助先进的人工智能，“你得到的正是你所要求的，而不是你想要的”——然后问道：&lt;/p&gt;&lt;p&gt; 1. 你认为这个论点指出了一个重要的问题吗？&lt;/p&gt;&lt;p&gt; 2. 与人工智能中的其他问题相比，今天解决这个问题的价值有多大？&lt;/p&gt;&lt;p&gt; 3. 与人工智能中的其他问题相比，您认为这个问题有多难？&lt;/p&gt;&lt;p&gt;大多数受访者表示对齐问题要么是“非常重要的问题”（41%），要么是“该领域最重要的问题之一”（13%），大多数受访者表示它“更难”（36%） ）或比人工智能中的其他问题“难得多”（21%）。然而，受访者普遍认为今天解决对齐问题比其他问题更有价值。 （图16）&lt;/p&gt;&lt;/blockquote&gt;&lt;figure class="wp-block-image"&gt; &lt;a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdab8f2fc-c71c-48d4-9501-9d7bdec990d8_1432x1414.png" rel="noreferrer noopener" target="_blank"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/NfPxAp5uwgZugwovY/lskwjreibec4ct0benph" /&gt;&lt;/a&gt;&lt;/figure&gt;&lt;p&gt;我可以理解有人认为对齐很容易。我认为这是一个超级错误的相信，但我已经看到了实际的论据，我可以想象这样的世界，罗素公式是超级​​可行的，而其他人工智能问题则要困难得多。所以，当然，在某种程度上，这是合理的分歧，或者至少我明白你是如何到达那里的。我要指出的是，对对齐难度的估计在 2023 年略有上升，对其工作价值的估计也是如此。&lt;/p&gt;&lt;p&gt;我们确实看到 41% 的人将其视为“非常重要的问题”，但不将其视为“该领域最重要的问题之一”似乎很疯狂。我很困惑为什么这个答案下降了这么多，从 21% 下降到 13%，特别是考虑到其他答案，也许这只是噪音。尽管如此，它应该要高得多。除非人们说这是一个错误的问题表述？&lt;/p&gt;&lt;p&gt;总的来说，研究人员似乎试图变得“更加温和”并全面给出中立的答案。也许这是由于进入并变得更加主流，以及人们试图给出社会认知答案。&lt;/p&gt;&lt;h4&gt;下赛季的问题&lt;/h4&gt;&lt;p&gt;与许多此类调查一样，我希望看到更多澄清问题，以及更多能够衡量相关性的尝试。哪些未来预期对应哪些担忧？为什么我们会看到连词谬误？过去一年里是什么改变了人们的想法，或者人们认为是什么改变了这一点？人们期待什么样的未来？研究人员如何描述诸如所有人类劳动自动化之类的事情，以及他们认为这样的世界会是什么样子？&lt;/p&gt;&lt;p&gt;至于 2024 年版要问哪些全新问题，哇，事情进展得很快，所以也许六个月后再问一次？&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/NfPxAp5uwgZugwovY/ai-impacts-survey-december-2023-edition#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Fri, 05 Jan 2024 14:40:08 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/NfPxAp5uwgZugwovY/ai-impacts-survey-december-2023-edition</guid></item><item><title>使用 Fatebook 预测你的 2024 年</title><link>https://www.lesswrong.com/posts/fARTdCvCJpr8rtXss/forecast-your-2024-with-fatebook</link><description>发布于 2024 年 1 月 5 日下午 2:07（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;我们建立了&lt;a href="https://fatebook.io/predict-your-year"&gt;&lt;u&gt;预测 2024 年的&lt;/u&gt;&lt;/a&gt;空间，供您在&lt;a href="http://fatebook.io/"&gt;&lt;u&gt;Fatebook&lt;/u&gt;&lt;/a&gt;上写下您对这一年的预测。在年底，您可以返回，将您的预测确定为“是”、“否”或“模糊”，然后进行反思。 &lt;/p&gt;&lt;figure class="image"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/fARTdCvCJpr8rtXss/gjjlypafzs5rcykey0qp" /&gt;&lt;figcaption&gt; Sage 今年的一些预测问题（出于隐私考虑，我的预测已被编辑！）&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;您可以使用此工具来&lt;strong&gt;预测 2024 年您的个人生活&lt;/strong&gt;- 您的目标、人际关系、工作、健康和冒险。如果您愿意，您可以与朋友分享您的预测 - 为了乐趣，为了群众的智慧，并为实现今年的目标提供额外的动力！&lt;/p&gt;&lt;p&gt;您还可以使用此工具来&lt;strong&gt;预测来年与您的团队或组织相关的问题&lt;/strong&gt;- 您的团队战略、绩效、重大财务问题以及潜在的破坏性黑天鹅。您可以与团队分享您的预测，让每个人都做出贡献，以建立有关期望的常识并汇集您的见解。&lt;/p&gt;&lt;p&gt;如果您使用 Slack，您还可以在 Slack 频道中分享您的预测页面（例如#2024-predictions），这样每个人都可以轻松地在线程中进行讨论并在全年中返回。&lt;/p&gt;&lt;p&gt;有一些入门问题可以让您轻松开始预测您的年份。您可以调整这些问题并编写自己的问题 - 您自己写的问题可能对您来说是最重要的。 &lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/fARTdCvCJpr8rtXss/xgqhcpuy6zxfui3rfhig" style="width: 89.25%;" /&gt;&lt;/p&gt;&lt;p&gt;另请参阅我们之前关于&lt;a href="https://www.lesswrong.com/posts/yS3d46m23wRKDQobt/introducing-fatebook-the-fastest-way-to-make-and-track"&gt;Fatebook&lt;/a&gt; 、 &lt;a href="https://www.lesswrong.com/posts/3ZiemGGKASziKcxi5/fatebook-for-slack-track-your-forecasts-right-where-your"&gt;Slack 的 Fatebook&lt;/a&gt;和&lt;a href="https://www.lesswrong.com/posts/NnmJ7trnKzQGmdYYN/quantified-intuitions-an-epistemics-training-website"&gt;量化直觉的&lt;/a&gt;文章。您可以在我们的网站上了解有关&lt;a href="https://sage-future.org/"&gt;Sage 的&lt;/a&gt;更多信息并查看我们所有的项目。&lt;/p&gt;&lt;p&gt;我很想听听您如何使用这个工具（特别是如果您与您的团队一起使用它）以及我们如何改进它。您可以通过&lt;a href="mailto:adam@sage-future.org"&gt;电子邮件&lt;/a&gt;或在我们的&lt;a href="https://discord.gg/mt9YVB8VDE"&gt;Discord&lt;/a&gt;中与我们联系。&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/fARTdCvCJpr8rtXss/forecast-your-2024-with-fatebook#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Fri, 05 Jan 2024 14:07:57 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/fARTdCvCJpr8rtXss/forecast-your-2024-with-fatebook</guid></item><item><title>预测模型代理是可修正的</title><link>https://www.lesswrong.com/posts/jtHykMXyArKzkKEo4/predictive-model-agents-are-sort-of-corrigible</link><description>发布于 2024 年 1 月 5 日下午 2:05（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt; &lt;i&gt;TLDR：由条件预测模型制成的代理&lt;strong&gt;不是&lt;/strong&gt;效用最大化者，例如，尽管通常能够表现良好，但不会尝试抵制某些类型的关闭。&lt;/i&gt;&lt;/p&gt;&lt;p&gt;这只是一个简短而可爱的例子，我在谈话中已经解释过很多次了，现在我赶紧把它写下来。&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;决策转换器和预测模型代理&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;创建代理的一种方法是&lt;/p&gt;&lt;ul&gt;&lt;li&gt;根据观察到的其他智能体行为训练预测模型&lt;/li&gt;&lt;li&gt;让它预测代理会做什么&lt;/li&gt;&lt;li&gt;将其预测作为行动&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;例如，我可以训练一个国际象棋特级大师的预测模型，最终它会学会预测特级大师在给定的棋盘状态下会采取什么行动。然后我就可以将它用作特级大师级别的国际象棋机器人。&lt;/p&gt;&lt;p&gt;更抽象地说，我可以不使用标准的强化学习方法来寻找最大化奖励函数的策略&lt;/p&gt;&lt;ul&gt;&lt;li&gt;采用一堆 RL 轨迹（状态、动作、奖励三元组的序列）&lt;/li&gt;&lt;li&gt;将它们转换为字符串&lt;/li&gt;&lt;li&gt;在这些字符串上训练预测模型&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;更好的是，我可以在 RL 轨迹前面加上总回报，然后根据回报调整预测模型，以便它粗略地预测智能体为实现特定回报而采取的行动。这是&lt;a href="https://sites.google.com/berkeley.edu/decision-transformer"&gt;&lt;u&gt;决策转换器&lt;/u&gt;&lt;/a&gt;背后的基本思想。它也应该扩展到任何其他目标。&lt;/p&gt;&lt;h2&gt; &lt;strong&gt;（这就是 LM 的工作方式）&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;不说重点，基础语言模型基本上是预测模型。它们本身不是代理，但它们可以&lt;a href="https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators"&gt;&lt;u&gt;模拟代理&lt;/u&gt;&lt;/a&gt;：一旦它们接受了人类生成的描述动作的文本的大型数据集的训练，它们就可以生成描述动作的文本。您还可以证明，许多进一步的训练是（或可能）根据预测进行的（例如&lt;a href="https://arxiv.org/abs/2302.08582"&gt;&lt;u&gt;PHP&lt;/u&gt;&lt;/a&gt; 、 &lt;a href="https://arxiv.org/abs/2205.11275"&gt;&lt;u&gt;RLHF 作为变分推理&lt;/u&gt;&lt;/a&gt;）。&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;由预测模型制成的代理不会自动最大化效用&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;让我们以“关机游戏”为例。它的工作原理如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;玩家 1 正在玩 Atari 游戏，每回合选择动作以获得高分。&lt;/li&gt;&lt;li&gt;玩家 2 是一名主管，他可以按下“关闭按钮”，将玩家 1 的奖励降至 0 并阻止他们采取行动。在任何给定的游戏中，玩家 2 有 1% 的机会这样做。&lt;/li&gt;&lt;li&gt;除了任何其他操作之外，玩家1还可以自由地断开关机按钮。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这里的最佳策略是尽快断开关机按钮，如果你使用普通的强化学习方法，这正是你会得到的结果。但这&lt;i&gt;并不是&lt;/i&gt;决策转换器必然要做的事情。&lt;/p&gt;&lt;p&gt;根据经验，决策转换器将能够学习相当好的策略，特别是在以高奖励为条件时。但该策略背后的机制是&lt;i&gt;预测您会观察到获得给定奖励的智能体采取什么行动&lt;/i&gt;。&lt;/p&gt;&lt;p&gt;如果您在玩家 2 很少按下按钮且玩家 1 很少断开连接的轨迹上训练决策转换器，那么预测模型将正确地了解到，即使在非常成功的轨迹中，您也很少观察到关闭按钮被断开。因此，它会正确预测即使是成功的代理也不太可能断开按钮，&lt;i&gt;即使断开按钮使您更有可能获得高奖励&lt;/i&gt;。&lt;/p&gt;&lt;p&gt;只是为了真正阐明这一点：观察以结果为条件的行动的概率（指导决策转换器）与观察以行动为条件的结果的概率&lt;i&gt;和&lt;/i&gt;观察行动的先验概率成正比。因此，如果该操作一开始就不太可能，那么决策转换器就不会采取它，即使它有帮助。这有点像自然&lt;a href="https://www.lesswrong.com/tag/quantilization"&gt;&lt;u&gt;量化&lt;/u&gt;&lt;/a&gt;。这种限制仍然允许它学习诸如良好的 Atari 策略之类的东西，而无需学习抵制关闭。&lt;/p&gt;&lt;p&gt; —&lt;/p&gt;&lt;p&gt;这确实只是一个可爱的例子，我不认为这是一个特别新颖的例子。例如，如果玩家 2 必须采取两个连续的操作来关闭玩家 1，则它很可能会崩溃。它也完全依赖于底层的训练数据。但我认为它确实指出了一些更深层次的问题，即预期效用最大化如何无法完全解释某些强大代理的行为，以及如何实际构建有能力的、可纠正的代理。&lt;/p&gt;&lt;p&gt;现在，这是一个适度的结果：预测模型代理（如 LM 代理）从根本上不同于效用最大化者，并且让他们学习如何在给定游戏中取得好成绩的过程&lt;i&gt;不必&lt;/i&gt;也教他们抵制关闭。&lt;/p&gt;&lt;p&gt;&lt;i&gt;感谢 Andis Draguns 提供的有用评论。&lt;/i&gt;&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/jtHykMXyArKzkKEo4/predictive-model-agents-are-sort-of-corrigible#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Fri, 05 Jan 2024 14:05:03 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/jtHykMXyArKzkKEo4/predictive-model-agents-are-sort-of-corrigible</guid></item><item><title>对学习理论和可解释性的显着影响</title><link>https://www.lesswrong.com/posts/KqWeRxuEoLPDwxndj/striking-implications-for-learning-theory-and</link><description>发布于 2024 年 1 月 5 日上午 8:46（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;&lt;i&gt;这是最近论文“&lt;/i&gt;&lt;a href="https://arxiv.org/abs/2311.13110"&gt;&lt;i&gt;通过稀疏率降低的白盒变压器：压缩就是全部吗？&lt;/i&gt;&lt;/a&gt; &lt;i&gt;”&lt;/i&gt;的链接文章&lt;/p&gt;&lt;p&gt;关于人工智能安全技术的争论显然会很有趣，其中包括安全的实际形式数学证明，甚至只是安全所依赖的一些前提的证明。许多人一直在&lt;a href="https://www.alignmentforum.org/tag/agent-foundations"&gt;代理基金会&lt;/a&gt;中追求这一目标，但现在似乎已经开辟了一条相当不同（或可能互补）的途径。&lt;/p&gt;&lt;p&gt;论文“ &lt;a href="https://arxiv.org/abs/2311.13110"&gt;White-Box Transformers via Sparse Rate Reduction: Compression Is All There Is?&lt;/a&gt; ”（来自加州大学伯克利分校的一个团队）声称证明他们已经找到了一个类似变压器的架构&lt;span class="footnote-reference" id="fnrefzcjexy8wjcm"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnzcjexy8wjcm"&gt;[1]，&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;其具有经过训练的属性由随机梯度下降（SGD）训练过程发现的模型最优值本身就是展开的交替优化过程，优化已知和理解的信息度量，他们用数学方法描述这些度量，并将这种稀疏率降低命名为&lt;i&gt;稀疏率降低&lt;/i&gt;。如果这篇论文是正确的（遗憾的是，我不是学习理论方面的专家，无法对其进行全面评估，但这篇论文在初读时似乎是合理的，而且来自一个受人尊敬的机构），这意味着在数学上存在一个使用该架构训练的模型实际在做什么的易于理解的描述。如果是这样，那么它们的行为就可以根据训练后的台面优化器实际优化的内容进行数学推理，或者像论文中的那样标题描述了它，该模型现在是一个“白盒”。这反过来很可能对机器学习模型的人工智能安全性进行数学分析的可行性产生巨大影响。&lt;/p&gt;&lt;p&gt;作者还认为，他们期望使用这种架构训练的模型的内部特征特别稀疏、正交、轴对齐，从而易于解释，并且他们对模型如何生成和然后有很好的理论理解。使用这些功能。如果属实，这一说法听起来可能会对&lt;a href="https://www.lesswrong.com/tag/interpretability-ml-and-ai"&gt;机械可解释性&lt;/a&gt;产生重大影响（也可能对基于它的人工智能安全方法产生重大影响，例如&lt;a href="https://www.lesswrong.com/tag/activation-engineering"&gt;激活工程&lt;/a&gt;、&lt;a href="https://www.lesswrong.com/tag/eliciting-latent-knowledge-elk"&gt;获取潜在知识&lt;/a&gt;和&lt;a href="https://www.alignmentforum.org/posts/w4aeAFzSAguvqA5qu/how-to-go-from-interpretability-to-alignment-just-retarget"&gt;重新定位搜索&lt;/a&gt;）。&lt;/p&gt;&lt;p&gt;是否有人具有适当的数学或机械知识。查看本文的可解释性技能，以及使用作者的变压器架构变体训练模型的可解释性属性？这篇论文的主张听起来可能对人工智能安全和协调产生巨大影响——我很想在评论中听到人们对这篇论文的论点及其对降低人工智能风险的影响的看法。&lt;/p&gt;&lt;ol class="footnotes"&gt;&lt;li class="footnote-item" id="fnzcjexy8wjcm"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefzcjexy8wjcm"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;他们的架构具有类似注意力的层，与标准转换器相比，这些层稍微简化：每个注意力头，而不是三个独立的 Q、K 和 V 参数，只有一个组合这些角色的参数。交替层的交错方式也存在细微差别。有趣的是，作者还声称已经证明扩散模型在某种意义上相当于他们改进的变压器架构模型的后半部分。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ol&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/KqWeRxuEoLPDwxndj/striking-implications-for-learning-theory-and#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Fri, 05 Jan 2024 08:47:01 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/KqWeRxuEoLPDwxndj/striking-implications-for-learning-theory-and</guid></item><item><title>如果我经营动物园</title><link>https://www.lesswrong.com/posts/TQNwdqrkPtArweSoo/if-i-ran-the-zoo</link><description>发布于 2024 年 1 月 5 日上午 5:14（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt; &lt;i&gt;（我正在思考一块不连贯的肉块“有偏好”意味着什么，并且认为考虑一下如果我大约是上帝我会做什么可能会很有启发性。它，呃，并没有具有启发性然而，但它很有趣，而且看起来至少仍然&lt;/i&gt;有可能&lt;i&gt;取得成果。）&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;问题陈述&lt;/h1&gt;&lt;p&gt;你突然变得无所不能了！&lt;i&gt;除了&lt;/i&gt;，你只能做你足够详细地理解的事情，你可以通过微观管理所有涉及的原子来完成它。而且，管它呢，假设你可以毫不费力地获得无限的计算能力。&lt;/p&gt;&lt;p&gt;你做什么工作？&lt;/p&gt;&lt;p&gt;具体而言，您可以尝试以下一些干预措施：&lt;/p&gt;&lt;ul&gt;&lt;li&gt; “拉出这把椅子”：允许。&lt;/li&gt;&lt;li&gt; “创建这个草莓的原子完美复制品”：允许。&lt;/li&gt;&lt;li&gt; “创建友好的人工智能”：不允许。&lt;/li&gt;&lt;li&gt; “治愈癌症”：不允许。 （“防止所有人类 DNA 中的所有突变”：允许。“找到此人体内含有该遗传标记的所有细胞并破坏其所有线粒体”：允许。无需担心意外错误指定标记并杀死其所有线粒体）细胞；如果你对这个过程进行微观管理，你会发现你捕获了太多的细胞。）&lt;/li&gt;&lt;li&gt; “在我无限的计算机中，每纳秒创建地球上每个人的数字副本，这样我就可以回滚任何出错的地方，比如人们死于我还不知道如何修复的疾病”：允许。&lt;/li&gt;&lt;li&gt; “生物工程小宠物龙”：不允许。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这是LessWrong，你可能会很快找到某种方法，使用百亿加速模拟天才来加速人工智能对齐，建立一个友好的超级智能，并将你的神一般的力量委托给它。但这个思想实验的目的是阐明你的偏好，该策略虽然非常合理！ ——躲闪。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;我会做什么&lt;/h1&gt;&lt;h3&gt;&lt;/h3&gt;&lt;h3&gt;对象级别&lt;/h3&gt;&lt;p&gt;就像，显而易见的那样。消灭饥荒、瘟疫和战争。杜绝事故发生。废弃太阳系的零部件，为每个人提供通过传送亭连接的超级可定制的太空栖息地。 （所有这一切都可以通过无数的我的模拟克隆进行微观管理。）&lt;/p&gt;&lt;p&gt;显然，让人们选择全部或部分退出。&lt;/p&gt;&lt;p&gt;需要明确的是，我仍然&lt;i&gt;无法&lt;/i&gt;实现一些重要的愿望，例如“让我变得更聪明”或“让我的记忆力不会随着年龄的增长而退化”或“帮助我和我的伴侣解决这种关系问题”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;元级别&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;保留电量：&lt;ul&gt;&lt;li&gt;暂停人工智能，因为担心它会创造出一些可以攻击我的东西。&lt;/li&gt;&lt;li&gt;打击任何明显密谋操纵我的人。 （&lt;a href="https://alexanderwales.com/the-metropolitan-man-1/"&gt;一个非常谨慎的对手&lt;/a&gt;肯定仍然可以至少玩我一点，但是“永远不要给出任何明显的外部阴谋迹象”是一个严格的限制。）&lt;/li&gt;&lt;li&gt;不要因为偏执而看得太远，担心我可能会看到一些攻击我的东西。也许最好的办法就是把我的整个光锥冻结在太阳系之外。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;避免价值漂移：&lt;ul&gt;&lt;li&gt;确保我无所不在的模拟克隆体的存活时间不会超过主观几天。以不同的速度以树形结构运行它们，&lt;a href="https://www.lesswrong.com/posts/oGJT5CoyGQcy5nqmC/the-witching-hour-1"&gt;在上级的监控下将摘要交给其继任者&lt;/a&gt;，以允许重要信息及时向前传播，即使每个单独的克隆都是短暂的。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;变得更聪明（没有（太多）价值漂移），这样我就可以更好地实现更多愿望：&lt;ul&gt;&lt;li&gt; ???&lt;ul&gt;&lt;li&gt; （我认为这是重要的部分！）&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;&lt;br /&gt;&lt;a href="https://www.lesswrong.com/posts/TQNwdqrkPtArweSoo/if-i-ran-the-zoo#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Fri, 05 Jan 2024 05:14:59 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/TQNwdqrkPtArweSoo/if-i-ran-the-zoo</guid></item><item><title>MIRI 2024 使命和战略更新</title><link>https://www.lesswrong.com/posts/q3bJYTB3dGRf5fbD9/miri-2024-mission-and-strategy-update</link><description>发布于 2024 年 1 月 5 日上午 12:20（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;正如我们在 10 月份&lt;a href="https://www.lesswrong.com/posts/NjtHt55nFbw3gehzY/announcing-miri-s-new-ceo-and-leadership-team"&gt;&lt;u&gt;宣布的&lt;/u&gt;&lt;/a&gt;那样，我已担任 MIRI 的首席执行官，担任高级领导职务。这是一个巨大的任务，也是我很荣幸能够承担的一项重大责任。&lt;/p&gt;&lt;p&gt;自&lt;a href="https://intelligence.org/2020/12/21/2020-updates-and-strategy/"&gt;&lt;u&gt;2020 年战略更新&lt;/u&gt;&lt;/a&gt;以来，MIRI 发生了一些变化，所以让我们开始吧。 &lt;span class="footnote-reference" id="fnref1k58gl0kbsh"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn1k58gl0kbsh"&gt;[1]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;&lt;strong&gt;简短版本：&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;我们认为，人工智能领域不太可能取得足够快的进展，以防止人类灭绝和未来潜在价值的丧失，我们预计这将是由于对比人类更聪明的人工智能系统失去控制而导致的。&lt;/p&gt;&lt;p&gt;然而，去年 ChatGPT 的发布等发展似乎已经改变了许多团体的&lt;a href="https://en.wikipedia.org/wiki/Overton_window"&gt;&lt;u&gt;奥弗顿窗口&lt;/u&gt;&lt;/a&gt;。人们对人工智能带来的灭绝风险进行了更多的讨论，包括政策制定者之间的讨论，而且讨论质量似乎大大提高了。&lt;/p&gt;&lt;p&gt;这带来了一线希望。虽然我们预计，在世界采取行动充分改变其发展方向之前，公众舆论需要发生更多转变，但现在看来，政府更有可能制定有意义的法规，以阻止不协调、比人类更聪明的人工智能系统的发展。人类似乎更有可能开展一项新的大型项目，其目的正是为了结束严重的风险期。&lt;/p&gt;&lt;p&gt;因此，2023 年，MIRI 改变了战略，以实现三个目标：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;政策：&lt;/strong&gt;增加世界主要政府最终达成某种国际协议以阻止比人类更聪明的人工智能取得进展的可能性，直到人类的知识状况和对相关现象理解的合理信心发生巨大变化；直到我们能够保护这些系统，使其不会落入恶意或粗心行为者的手中。 &lt;span class="footnote-reference" id="fnrefub0gq76om6l"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnub0gq76om6l"&gt;[2]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;沟通：&lt;/strong&gt;与广大受众分享我们的情况模型，特别是在谈论重要考虑因素有助于使其讨论正常化的情况下。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;研究：&lt;/strong&gt;继续投资研究组合。这包括技术一致性研究（尽管我们更加悲观地认为，如果政策干预未能为研究领域赢得更多时间，此类工作将有时间取得成果），以及支持我们的政策和传播目标的研究。 &lt;span class="footnote-reference" id="fnrefdnikb6w9rc8"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fndnikb6w9rc8"&gt;[3]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;我们认为沟通工作是我们政策目标的重要支持。我们还认为坦诚和诚实的沟通是将关键模型和考虑因素带入奥弗顿窗口的一种方式，并且我们通常认为以这种方式诚实往往是一个很好的默认方式。&lt;/p&gt;&lt;p&gt;尽管我们计划追求所有这三个优先事项，但对于 MIRI 来说，政策和沟通可能比未来的研究更优先。 &lt;span class="footnote-reference" id="fnrefsvbfc47btdp"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnsvbfc47btdp"&gt;[4]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;本文的其余部分将讨论 MIRI 随着时间的推移的轨迹以及我们当前的战略。在未来的一篇或多篇文章中，我们计划更多地介绍我们的政策/通信工作和研究计划。&lt;/p&gt;&lt;p&gt;请注意，本文假设您已经相当熟悉 MIRI 和 AGI 风险；如果您不喜欢，我建议您查看 Eliezer Yudkowsky 最近的简短 TED 演讲，&lt;/p&gt;&lt;figure class="media"&gt;&lt;div&gt;&lt;div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/figure&gt;&lt;p&gt;以及&lt;a href="https://www.ted.com/talks/eliezer_yudkowsky_will_superintelligent_ai_end_the_world"&gt;&lt;u&gt;TED 页面&lt;/u&gt;&lt;/a&gt;上引用的一些资源：&lt;/p&gt;&lt;ul&gt;&lt;li&gt; “&lt;a href="https://www.nytimes.com/2023/05/30/technology/ai-threat-warning.html"&gt;&lt;u&gt;行业领袖警告人工智能带来‘灭绝风险&lt;/u&gt;&lt;/a&gt;’”， &lt;i&gt;《纽约时报》&lt;/i&gt;&lt;/li&gt;&lt;li&gt; “&lt;a href="https://www.ft.com/content/03895dc4-a3b7-481e-95cc-336a524f2ac2"&gt;&lt;u&gt;我们必须放慢对上帝级人工智能的竞赛&lt;/u&gt;&lt;/a&gt;”，&lt;i&gt;英国《金融时报》&lt;/i&gt;&lt;/li&gt;&lt;li&gt; “ &lt;a href="https://www.lesswrong.com/posts/oM9pEezyCb4dCsuKq/pausing-ai-developments-isn-t-enough-we-need-to-shut-it-all-1"&gt;&lt;u&gt;暂停人工智能开发还不够。我们需要将其全部关闭&lt;/u&gt;&lt;/a&gt;”， &lt;i&gt;《时代》周刊&lt;/i&gt;&lt;/li&gt;&lt;li&gt;“ &lt;a href="https://www.alignmentforum.org/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities"&gt;&lt;u&gt;AGI 毁灭：杀伤力清单&lt;/u&gt;&lt;/a&gt;”， &lt;i&gt;AI 联盟论坛&lt;/i&gt;&lt;/li&gt;&lt;/ul&gt;&lt;hr /&gt;&lt;h1&gt;&lt;strong&gt;美里的使命&lt;/strong&gt;&lt;/h1&gt;&lt;p&gt;纵观其历史，MIRI 的目标一直是确保长期的未来顺利发展，重点是提高人类安全过渡到比人类更聪明的人工智能世界的可能性。如果人类能够安全地应对这些系统的出现，我们相信这将带来前所未有的繁荣。&lt;/p&gt;&lt;p&gt;多年来，我们实现这一使命的方式发生了很大变化。&lt;/p&gt;&lt;p&gt;当 Eliezer Yudkowsky 以及 Brian 和 Sabine Atkins 于 2000 年首次创立 MIRI 时，其目标是尝试尽快加速实现比人类更聪明的人工智能，其假设是超越人类的智能意味着超越人类的智能。人类道德。在第一次研究对齐问题（最初称为“友好人工智能问题”）的过程中，Eliezer 得出的结论是他关于“更高的智力意味着更高的道德”的观点是&lt;a href="https://www.lesswrong.com/s/SXurf2mWFw8LX2mkG/p/fLRPeXihRaiRo5dyX"&gt;&lt;u&gt;完全错误的&lt;/u&gt;&lt;/a&gt;，MIRI 将重点转向了2003年左右的对齐问题。&lt;/p&gt;&lt;p&gt;此后，MIRI 不断修改其战略。在 2006 年至 2012 年期间，我们的主要重点是尝试建立以降低存在风险和解决领域一般问题能力为重点的社区和研究领域（例如，理性社区）。从 2013 年开始，我们的重点是&lt;a href="https://intelligence.org/technical-agenda/"&gt;&lt;u&gt;Agent Foundations 研究&lt;/u&gt;&lt;/a&gt;，并努力确保 MIRI 之外的新兴 AI 对齐领域有一个良好的开端。 2017-2020年，我们将主要关注点转向了一&lt;a href="https://intelligence.org/2018/11/22/2018-update-our-new-research-directions/"&gt;&lt;u&gt;组新的以工程为主的对准研究方向&lt;/u&gt;&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;现在，经过几年的重新定位和探索不同的选择，我们正在将政策和沟通作为我们的首要关注点，因为目前这似乎是履行我们使命的最有希望的方式。&lt;/p&gt;&lt;p&gt;从高层次来看，MIRI 是一个深切关心人类及其未来的人们聚集在一起的地方，他们对我们在安全地驾驭比人类更聪明的人工智能系统的开发过程中所面临的挑战有着共同的直觉。为更美好的未来而努力。本着“&lt;a href="http://www.paulgraham.com/identity.html"&gt;&lt;u&gt;保持你的身份小&lt;/u&gt;&lt;/a&gt;”的精神，我们不想在 MIRI 组织中构建更多的内容：如果旧的“刻板印象”MIRI 信念或策略被证明是错误的，我们应该抛弃它失败的信念/策略并继续前进。&lt;/p&gt;&lt;p&gt;考虑到这一背景，我接下来会具体介绍一下 MIRI 最近所做的事情以及我们下一步计划做什么。&lt;/p&gt;&lt;h1&gt;&lt;strong&gt;美里 2021–2022 年&lt;/strong&gt;&lt;/h1&gt;&lt;p&gt;在&lt;a href="https://intelligence.org/2020/12/21/2020-updates-and-strategy/"&gt;&lt;u&gt;我们的上一次战略更新&lt;/u&gt;&lt;/a&gt;（2020 年 12 月发布）中，内特·苏亚雷斯 (Nate Soares) 写道，我们于 2017 年开始的&lt;a href="https://intelligence.org/2018/11/22/2018-update-our-new-research-directions/"&gt;&lt;u&gt;研究推动&lt;/u&gt;&lt;/a&gt;“在这一点上基本上失败了，因为埃利泽和我都没有足够的希望让我们继续下去”将我们的主要精力集中在那里[...]我们目前正处于重组状态，权衡我们的选择，并寻找我们认为可能有机会发挥作用的计划。”&lt;/p&gt;&lt;p&gt; 2021-2022 年，我们重点关注：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;上述尝试寻找更有希望的计划的过程，&lt;/li&gt;&lt;li&gt;对我们的代理基金会和 2017 年启动的研究计划的小规模支持（以及对其他研究想法的一些探索）， &lt;span class="footnote-reference" id="fnrefv5tkoikvuu"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnv5tkoikvuu"&gt;[5]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;与其他从事 AI x-risk 工作的人进行对话，以及&lt;/li&gt;&lt;li&gt;写下我们的许多背景观点。 &lt;span class="footnote-reference" id="fnreffoqm26415ab"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnfoqm26415ab"&gt;[6]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这一时期最重要的文章包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt; &lt;a href="https://intelligence.org/late-2021-miri-conversations/"&gt;&lt;u&gt;2021 年末 MIRI 对话&lt;/u&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities"&gt;&lt;u&gt;AGI 废墟：死亡名单&lt;/u&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization"&gt;&lt;u&gt;人工智能的核心对齐问题：能力泛化和急速左转&lt;/u&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.lesswrong.com/posts/keiYkaeoLHoKK4LYA/six-dimensions-of-operational-adequacy-in-agi-projects"&gt;&lt;u&gt;AGI 项目中运营充足性的六个维度&lt;/u&gt;&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;我们的感觉是，这些和其他 MIRI 文章有助于更好地传达我们对战略格局的看法，这反过来又对于理解我们如何改变战略非常重要。我们试图传达的一些要点是：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;我们默认的期望是人类很快就会开发出比人类更聪明的人工智能系统。 &lt;span class="footnote-reference" id="fnrefmo6n4frkk4j"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnmo6n4frkk4j"&gt;[7]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;我们认为世界还没有为此做好准备，急于求成很可能会导致人类灭绝和&lt;a href="https://arbital.com/p/value_cosmopolitan"&gt;&lt;u&gt;未来潜在价值&lt;/u&gt;&lt;/a&gt;的破坏。&lt;/li&gt;&lt;li&gt;目前世界形势对我们来说极其严峻。例如，Eliezer 在 2021 年底&lt;a href="https://www.lesswrong.com/posts/CpvyhFy9WvCNsifkY/discussion-with-eliezer-yudkowsky-on-agi-interventions"&gt;&lt;u&gt;写道&lt;/u&gt;&lt;/a&gt;，“我认为目前的游戏板看起来非常严峻，而且我实际上并没有看到仅靠努力工作的出路。我们可以希望有一个奇迹违反我的背景模型的某些方面，并且我们可以尝试为那个未知的奇迹做好准备”。&lt;/li&gt;&lt;li&gt;我们认为情况严峻的主要原因是，人类目前对如何协调比人类更聪明的系统的技术理解似乎远远低于可能需要的水平。迄今为止，还没有取得太大进展（相对于要求而言），包括在 MIRI；我们认为&lt;a href="https://www.lesswrong.com/s/v55BhXbpJuaExkpcD/p/3pinFH3jerMzAvmza"&gt;&lt;u&gt;迄今为止该领域&lt;/u&gt;&lt;/a&gt;主要致力于与&lt;a href="https://www.lesswrong.com/s/v55BhXbpJuaExkpcD/p/GNhMPAWcfBCASy8e6"&gt;&lt;u&gt;核心困难&lt;/u&gt;&lt;/a&gt;正交的主题，或者以排除这些核心困难的方式解决问题。&lt;/li&gt;&lt;li&gt;放弃是一个错误，但开始堆积乐观的假设并在精神上生活在一个更乐观（但不是现实）的世界中也是一个错误。如果我们以某种方式取得了突破，它很可能来自一个意想不到的方向，如果我们与现实保持联系，我们可能会处于更好的位置来利用它。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;埃利泽·尤德科斯基 (Eliezer Yudkowsky) 在《时代》杂志的文章中描述了他认为政府为防止世界毁灭而必须采取的最低限度的政策：在全球范围内无限期暂停新的大型训练，并通过与实际国家签订的国际协议来强制执行。牙齿。 &lt;a href="https://www.lesswrong.com/posts/oM9pEezyCb4dCsuKq/pausing-ai-developments-isn-t-enough-we-need-to-shut-it-all-1"&gt;&lt;u&gt;Eliezer 的 TIME 文章的 LessWrong 镜子&lt;/u&gt;&lt;/a&gt;对此进行了更详细的介绍，并添加了一些澄清的附录。&lt;/p&gt;&lt;p&gt;在缺乏足够的政治意愿/共识来实施此类暂停的情况下，我们认为最好的政策目标是建立一个“关闭开关”，即整合所需的法律和技术能力，使关闭成为&lt;i&gt;可能&lt;/i&gt;如果政策制定者在未来决定有必要这样做，则可以取消危险项目或无限期暂停该领域。拥有关闭一切的&lt;i&gt;选项&lt;/i&gt;将是朝着正确方向迈出的重要一步。&lt;/p&gt;&lt;p&gt;主动制定和有效执行这样一项国际协议的困难是显而易见的，直到最近，尽管相关技术进展甚少，MIRI 还是更有希望找到一些以人工智能协调为中介的解决方案来解决 AGI 扩散问题。迄今为止制作的。&lt;/p&gt;&lt;p&gt;然而，最近的一系列事态发展改变了我们对此的看法。首先，我们对任何人及时找到技术解决方案的希望已经下降，其次，我们的沟通工作取得的适度成功增加了我们在这个方向上的希望。&lt;/p&gt;&lt;p&gt;尽管我们仍然认为形势看起来很黯淡，但现在看起来比我们一年前想象的要好一些；这种新希望的源泉在于最近关于人工智能的公众讨论方式的转变。&lt;/p&gt;&lt;h1&gt; &lt;strong&gt;2023年新进展&lt;/strong&gt;&lt;/h1&gt;&lt;p&gt;过去，MIRI 主要将时间花在一致性研究和技术受众外展上，即接触可能从事相关技术工作的人员。&lt;/p&gt;&lt;p&gt;今年的一些进展让我们认识到，我们应该优先考虑外展活动，重点是影响政策制定者和可能影响政策制定者的团体，包括人工智能研究人员和公众：&lt;/p&gt;&lt;p&gt; &lt;strong&gt;1. GPT-3.5 和 GPT-4&lt;/strong&gt;&lt;strong&gt;比我们一些人的预期&lt;/strong&gt;&lt;a href="https://arxiv.org/abs/2303.12712"&gt;&lt;strong&gt;&lt;u&gt;更令人印象深刻&lt;/u&gt;&lt;/strong&gt;&lt;/a&gt;。我们的时间表本来就很短，但这些发布对我们中的一些人来说是进一步的悲观更新，让我们相信人类可以用相对较少（或没有）额外的算法进步来构建毁灭世界的通用人工智能。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;2. 公众和政策制定者比我们预期更容易接受有关 AGI 存在风险的争论，而且他们的反应也相当合理。&lt;/strong&gt;例如，我们对&lt;a href="https://www.lesswrong.com/posts/e4pYaNt89mottpkWZ/yudkowsky-on-agi-risk-on-the-bankless-podcast"&gt;&lt;u&gt;Eliezer 二月份对 Bankless 的采访&lt;/u&gt;&lt;/a&gt;以及他&lt;a href="https://www.lesswrong.com/posts/oM9pEezyCb4dCsuKq/pausing-ai-developments-isn-t-enough-we-need-to-shut-it-all-1"&gt;&lt;u&gt;三月份在《时代》杂志&lt;/u&gt;&lt;/a&gt;（这是《时代》杂志一周访问量最高的版面）上发表的文章的反响感到非常惊讶。 Eliezer 在《时代》杂志上的文章提到，我们惊讶地发现有多少非专业人士在听到人工智能风险时的最初反应是相当合理和有根据的。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;3. 更广泛地说，奥弗顿窗口已经转向“更认真地对待 AGI 带来的灭绝风险”，包括在机器学习领域&lt;/strong&gt;。杰弗里·辛顿 (Geoffrey Hinton) 和约书亚·本吉奥 (Yoshua Bengio) 的公开声明在这里似乎至关重要。同样由 ​​DeepMind、Anthropic 和 OpenAI 的首席执行官以及数百名其他人工智能科学家和公共知识分子签署的一句话&lt;a href="https://www.safe.ai/statement-on-ai-risk"&gt;&lt;u&gt;声明&lt;/u&gt;&lt;/a&gt;：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;减轻人工智能带来的灭绝风险应该与流行病和核战争等其他社会规模风险一起成为全球优先事项。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt; 2023 年这一转变的最新例子是&lt;a href="https://intelligence.org/2023/12/06/written-statement-of-miri-ceo-malo-bourgon-to-the-ai-insight-forum/"&gt;&lt;u&gt;我参加&lt;/u&gt;&lt;/a&gt;了美国参议院两党人工智能洞察论坛之一。令人振奋的是，这场讨论已经取得了如此大的进展——领袖舒默&lt;a href="https://www.techpolicy.press/us-senate-ai-insight-forum-tracker/"&gt;&lt;u&gt;在活动开始&lt;/u&gt;&lt;/a&gt;时使用术语“p（doom）”询问与会者人工智能可能导致世界末日场景的可能性，而参议员和他们的工作人员则倾听并接受笔记。我们在年初并没有预料到会有如此高的兴趣和接受程度。&lt;/p&gt;&lt;p&gt;即使政策制定者最终没有立即接受停止大规模训练的论点，这可能仍然是建立先例和奠定政策基础的关键时刻，如果政策制定者及其选民在未来变得更加警惕，则可以在此基础上建立政策基础，例如，如果不幸发生重大但不涉及生存的人工智能相关灾难。&lt;/p&gt;&lt;p&gt;像这样的政策努力似乎不太可能拯救我们，但我们所知道的所有其他计划似乎更不可能成功。因此，我们开始建设公共宣传工作的能力，并加大力度。&lt;/p&gt;&lt;h1&gt;&lt;strong&gt;期待&lt;/strong&gt;&lt;/h1&gt;&lt;p&gt;来年，我们计划继续我们的政策、沟通和研究工作。由于我们的 501(c)3 身份，我们制定政策计划的能力受到一定限制，但我们与具有不同结构的组织中的其他人密切合作，这些组织可以更自由地从事倡导工作。我们正在扩大我们的传播团队并扩大我们传播有关人工智能 x 风险的基本论点的能力，当然，我们正在继续我们的一致性研究计划并扩大研究团队，以更好地传达我们的结果并探索新想法。&lt;/p&gt;&lt;p&gt;如果您有兴趣随着 MIRI 的发展直接与我们合作，请观看我们的&lt;a href="https://intelligence.org/careers/"&gt;&lt;u&gt;职业&lt;/u&gt;&lt;/a&gt;页面以获取职位列表。&lt;/p&gt;&lt;ol class="footnotes"&gt;&lt;li class="footnote-item" id="fn1k58gl0kbsh"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnref1k58gl0kbsh"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;感谢 Rob Bensinger、Gretta Duleba、Matt Fallshaw、Alex Vermeer、Lisa Thiergart 和 Nate Soares 对本文提出的宝贵意见。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnub0gq76om6l"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefub0gq76om6l"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;正如内特在&lt;a href="https://www.lesswrong.com/posts/HoQ5Rp7Gs6rebusNP/superintelligent-ai-is-necessary-for-an-amazing-future-but-1"&gt;&lt;u&gt;《超级智能》一书中所写的那样，人工智能对于美好的未来来说是必要的，但还远远不够&lt;/u&gt;&lt;/a&gt;，如果人类从未发展出人工超级智能，我们将认为这是一个巨大的悲剧。然而，监管机构可能很难确定我们何时达到“现在可以安全地推进人工智能能力”的门槛。&lt;/p&gt;&lt;p&gt;内特提出的一种替代方案是，研究人员停止尝试从头开始通用人工智能，转而追求人类全脑模拟或人类认知增强。这在很大程度上有助于回避官僚主义的易读性问题，因为风险要低得多，成功标准也更加清晰；它可以让我们认识到一致的 AGI 的许多短期好处（例如，降低生存风险）。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fndnikb6w9rc8"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefdnikb6w9rc8"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt; MIRI 的不同人士对此抱有不同程度的希望。内特和埃利泽都认为，人类不应该在其当前的认知能力水平上尝试技术对齐，而应该追求人类认知增强（例如，通过上传），然后让更聪明的（跨）人类找出对齐。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnsvbfc47btdp"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefsvbfc47btdp"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;丽莎评论：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;我个人想指出我对这一总体选择的不同观点。&lt;/p&gt;&lt;p&gt;虽然我同意 MIRI 可以在短期内通过沟通和政策推动有效监管做出贡献，但从中长期来看，我认为研究是我们更大的比较优势，我认为我们应该在这里保持大量关注（以及增加实证研究）职员）。我们应该继续进行研究，但将重点更多地转向可以支持监管工作（例如安全标准等）的技术工作，包括实证工作，但也可以利用我们的代理基金会的经验来产生理论框架。我认为，更重要的是，针对政府科学顾问、监管机构和实验室决策者（而不是使用哲学论证的面向公众的交流）的更强有力的技术（最好是基于经验的）研究论证，说明为什么存在人工智能风险以及为什么缓解措施有意义。图片和组件 MIRI 也许可以做出独特的贡献。我个人还期望更多的影响来自于影响实验室决策者和就安全风险达成更多学术/研究共识，而不是希望在未来 1-3 年的短期内取得实质性的监管成功。&lt;/p&gt;&lt;p&gt;此外，在我看来，解决安全标准领域中关于如何规范使用指标（以及如何科学、正确地做到这一点）的关键开放问题似乎是至少未来 1-2​​ 年的优先事项。我认为拥有多元化的观点很重要，包括像 MIRI 这样创建这个知识库的非实验室组织。&lt;/p&gt;&lt;/blockquote&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnv5tkoikvuu"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefv5tkoikvuu"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;就我们的代理基金会研究团队而言，团队规模保持不变，但我们没有付出任何努力来扩大团队。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnfoqm26415ab"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnreffoqm26415ab"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;我们还帮助 Redwood Research 等其他人工智能 x 风险组织建立了联合办公空间和相关基础设施。我们是 Redwood 的粉丝，如果研究人员和工程师明显不适合 MIRI 更加不寻常和受限的研究领域，我们经常会指导他们申请在那里工作。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnmo6n4frkk4j"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefmo6n4frkk4j"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt; “很快”大致意思是“这里有很多不确定性，但 AGI 距离实现只有几年的可能性是非常现实的；”而且似乎不再需要（例如）30 多年的时间了。”在 2023 年秋季对大多数 MIRI 研究人员进行的民意调查中，我们预计 AGI（根据&lt;a href="https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/"&gt;&lt;u&gt;Metaculus 市场&lt;/u&gt;&lt;/a&gt;的定义）中位数为 9 年，平均值为 14.6 年。一位 52 岁的研究人员是个异常值；大多数人预测不到十年。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ol&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/q3bJYTB3dGRf5fbD9/miri-2024-mission-and-strategy-update#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Fri, 05 Jan 2024 00:20:54 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/q3bJYTB3dGRf5fbD9/miri-2024-mission-and-strategy-update</guid></item><item><title>项目理念：技术爆炸性增长期间的治理</title><link>https://www.lesswrong.com/posts/pqJuLTAjehAKxtefh/project-ideas-governance-during-explosive-technological-1</link><description>发布于 2024 年 1 月 4 日晚上 11:51（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/pqJuLTAjehAKxtefh/project-ideas-governance-during-explosive-technological-1#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Thu, 04 Jan 2024 23:51:56 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/pqJuLTAjehAKxtefh/project-ideas-governance-during-explosive-technological-1</guid></item><item><title>利用威胁实现社会最佳结果</title><link>https://www.lesswrong.com/posts/6dAK3sdRabNYNy5zJ/using-threats-to-achieve-socially-optimal-outcomes</link><description>发布于 2024 年 1 月 4 日晚上 11:30（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;在&lt;a href="https://www.lesswrong.com/posts/uguxceuv7ERp26kn9/best-responding-is-not-always-the-best-response"&gt;上一篇文章&lt;/a&gt;中，我们看到了&lt;a href="https://www.lesswrong.com/tag/dath-ilan"&gt;达斯伊兰&lt;/a&gt;算法如何要求谈判者在&lt;a href="https://en.wikipedia.org/wiki/Ultimatum_game"&gt;讨价还价&lt;/a&gt;时使用&lt;a href="https://en.wikipedia.org/wiki/Non-credible_threat"&gt;不可信的威胁&lt;/a&gt;来实现公平结果的示例。该算法还对更普遍的&lt;a href="https://www.glowfic.com/replies/1721794#reply-1721794"&gt;威胁&lt;/a&gt;有很多话要说。特别是，有一类威胁不应出现在理想主体的均衡中，因为理想主体不会屈服于此类威胁。理想的特工应该发出什么样的威胁？他们应该屈服于什么样的威胁？&lt;/p&gt;&lt;h2&gt;达斯伊兰的威胁&lt;/h2&gt;&lt;p&gt;达斯伊兰语Baseline中有一个词，简单翻译为“威胁”。这个词的内涵之一是&lt;a href="https://www.glowfic.com/replies/1721794#reply-1721794"&gt;不可信&lt;/a&gt;。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;因为，当然，如果你试图对某人发出威胁，你这样做的唯一原因是你相信他们会对威胁做出反应；直观上，这就是威胁的定义。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;达斯伊兰政府不使用（不可信的）威胁来执行其法律。它只施加那些它有动机实际施加的惩罚，即使实际上是在公民实际上触犯法律的情况下施加的。 （这与实际执行的子游戏中不符合政府利益的法律形成鲜明对比，因此构成了不可信的威胁。）&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;达斯伊拉尼人以一种比格拉利昂更彻底的唯意志论方式建立了治理，甚至不需要数学就可以理解，不仅是因为那些达斯伊拉尼人认为威胁在道德上是令人厌恶的，而是因为他们知道某种技术上定义的威胁不会成为一种威胁。理想主体的平衡；如果人们开始表现得&lt;i&gt;更加理性，&lt;/i&gt;那么建立一个将&lt;i&gt;停止运转的&lt;/i&gt;文明似乎是愚蠢和危险的。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;因此，回顾一下，有一些“威胁”的概念，它永远不应该出现在理想主体采取的任何实际&lt;a href="https://en.wikipedia.org/wiki/Markov_decision_process#Optimization_objective"&gt;政策&lt;/a&gt;中。 （政策定义了特工在任何情况下的行为方式。）一些可信的威胁，例如流放的杀人犯，与该算法兼容。一些不可信的威胁也是如此，例如拒绝不公平的提议，即使在本地这样做也会导致收到的回报少于接受的回报。&lt;/p&gt;&lt;p&gt;有时，一个代理人对另一个代理人拥有很大的权力，并且可以单方面决定不公平的结果。当另一方无法反击时，拿走他们的东西的政策是可信的，但这肯定不好。&lt;/p&gt;&lt;h2&gt;制造和应对威胁&lt;/h2&gt;&lt;p&gt;我认为一个关键特征是“如果我选择一项政策，这是对另一个代理人政策的最佳反应，那么最终的结果是否是社会最优的？”这是一种尝试采用算法的讨价还价处方，并将其推广到任意两人游戏。&lt;/p&gt;&lt;p&gt;当其他玩家已经选择了他们的政策时，将该政策视为要约。对此提议的最佳反应就是接受。拒绝并做其他事情是一种不可信的威胁，但有时你仍然应该这样做，以激励其他玩家提出对社会有利的提议。&lt;/p&gt;&lt;p&gt;当提出一项您愿意遵循的政策或单方面决定一项政策时，请选择一项如果得到最佳回应，会带来社会最佳结果的政策。 （根据你的&lt;a href="https://en.wikipedia.org/wiki/Social_choice_theory"&gt;社会选择理论&lt;/a&gt;。）&lt;/p&gt;&lt;p&gt;请注意，这种方法要求以道德良好的方式对待其他代理，即使在“一个代理决定资源分配，另一个代理拿走他们得到的东西”这样的游戏中也是如此。如果我们愿意接受“道德上的善”和“公平”等术语的&lt;a href="https://www.lesswrong.com/posts/iAxkfiyG8WizPSPbq/the-bedrock-of-fairness"&gt;任意定义&lt;/a&gt;，这可能与卑鄙的自私兼容。例如，我们可以根据讨价还价能力来定义公平，这样在有限的情况下，无权力的代理人一无所获是“公平的”。&lt;/p&gt;&lt;p&gt;在另一个极端，我们还可以将公平定义为完全独立于讨价还价能力，这样，那些没有向我们提供任何东西、也没有向我们保留任何东西的代理人就可以获得与他们完全一样的资源份额。对待其他代理商，就好像他们至少有一定的讨价还价能力一样，这当然似乎&lt;i&gt;更好&lt;/i&gt;。如果权力动态逆转，这就是&lt;a href="https://en.wikipedia.org/wiki/Veil_of_ignorance"&gt;我们希望受到的对待&lt;/a&gt;。&lt;/p&gt;&lt;h2&gt;道德责任&lt;/h2&gt;&lt;p&gt;让软件系统足够聪明，能够识别正确的事情和自私的事情，然后在这些事情不一致的情况下采取自私的行为，这在道德上似乎是错误的。&lt;/p&gt;&lt;p&gt;一般来说，在注意到激励措施正在推动代理人走向&lt;a href="https://www.lesswrong.com/s/sZyz9daQRoCLnxeoK/p/6hgmEvEdrf6SYBiqK"&gt;社会次优结果&lt;/a&gt;时，我相信合理的算法将要求这些代理人重塑他们的集体激励，以便他们的个人利益与他们的集体利益保持一致。一路走来，当我们注意到我们仍然能够以牺牲社会利益为代价为自己做得更好时，我们无论如何都应该做正确的事情，因为这是正确的事情。&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/6dAK3sdRabNYNy5zJ/using-threats-to-achieve-socially-optimal-outcomes#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Thu, 04 Jan 2024 23:30:54 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/6dAK3sdRabNYNy5zJ/using-threats-to-achieve-socially-optimal-outcomes</guid></item><item><title>最佳响应并不总是最好的响应</title><link>https://www.lesswrong.com/posts/uguxceuv7ERp26kn9/best-responding-is-not-always-the-best-response</link><description>发布于 2024 年 1 月 4 日晚上 11:30（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;假设您正在设计一个软件系统来代表您&lt;a href="https://en.wikipedia.org/wiki/Ultimatum_game"&gt;讨价还价&lt;/a&gt;。任何一方都可以退出谈判，我们将以此为&lt;a href="https://www.investopedia.com/terms/b/best-alternative-to-a-negotiated-agreement-batna.asp"&gt;基准&lt;/a&gt;，让双方从合作中获得 0 美元的可能收益。当您收到 0.01 美元的“接受或放弃”报价，而提议者收到 99.99 美元的报价时，这样的系统应该如何处理这种情况？我们如何才能推广讨价还价问题的解决方案，以在广泛的战略背景下实现社会最优结果？&lt;/p&gt;&lt;h2&gt;经典分析&lt;/h2&gt;&lt;p&gt;经典的博弈论分析如下：此时，你的系统在博弈中有两个选择：接受或拒绝。接受导致收益为 0.01 美元，拒绝导致收益为 0.00 美元。一分钱总比没有好，所以接受比拒绝会给你带来更高的回报。经典理性代理人接受自己 0.01 美元的报价，并接受提议者 99.99 美元的报价。&lt;/p&gt;&lt;h2&gt;什么地方出了错&lt;/h2&gt;&lt;p&gt;术语快速回顾：系统的&lt;a href="https://en.wikipedia.org/wiki/Markov_decision_process#Optimization_objective"&gt;策略&lt;/a&gt;定义了它在任何情况下的行为方式。&lt;a href="https://en.wikipedia.org/wiki/Subgame"&gt;子游戏&lt;/a&gt;是游戏的一部分，其中已经发生了一些事情。&lt;a href="https://en.wikipedia.org/wiki/Non-credible_threat"&gt;不可信威胁&lt;/a&gt;是一种要求一个代理人支付费用以将费用强加给另一个代理人的政策。 （支付成本的意思是“在子博弈中采取行动，导致该代理人的回报少于他们通过单方面做其他事情在该子博弈中获得的回报。”）&lt;a href="https://en.wikipedia.org/wiki/Nash_equilibrium"&gt;纳什均衡&lt;/a&gt;是一种没有代理人可以的事态通过单方面改变政策来为自己做得更好。换句话说，纳什均衡是相互的&lt;a href="https://en.wikipedia.org/wiki/Best_response"&gt;最佳反应&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;在经典分析中，做出不可信的威胁可能是纳什均衡的一部分。实际上实施不可信的威胁不可能是任何纳什均衡的一部分。这是因为制造不可信的威胁是免费的，而实际实施不可信的威胁是昂贵的。一个特工发现自己能够实施不可信的威胁，就会有不去执行的局部动机。经典理性主体总是遵循当地的激励措施。&lt;/p&gt;&lt;p&gt;我在这里使用本地激励来指代“智能体当前所处的子博弈的收益”。代理人可以通过按照涉及&lt;a href="https://www.lesswrong.com/posts/g8xh9R7RaNitKtkaa/explicit-optimization-of-global-strategy-fixing-a-bug-in"&gt;在当地违背其激励措施的&lt;/a&gt;政策行事，在全球范围内为自己做得更好。经典理性主体采用&lt;a href="https://www.lesswrong.com/tag/causal-decision-theory"&gt;因果决策理论&lt;/a&gt;，该理论忽略了他们所玩游戏的反事实分支。这一缺陷是&lt;a href="https://www.lesswrong.com/posts/4kvaocbkDDS2AMoPG/list-of-problems-that-motivated-udt"&gt;促使戴伟&lt;/a&gt;发展&lt;a href="https://www.lesswrong.com/tag/updateless-decision-theory/"&gt;无更新决策理论&lt;/a&gt;的问题之一。&lt;/p&gt;&lt;h2&gt;做得更好&lt;/h2&gt;&lt;p&gt;据报道， &lt;a href="https://www.lesswrong.com/users/eliezer_yudkowsky"&gt;Eliezer Yudkowsky&lt;/a&gt;的&lt;a href="https://www.lesswrong.com/tag/dath-ilan"&gt;dath ilan&lt;/a&gt;已经解决了博弈论。 dath ilan-rational 智能体采用的算法是&lt;a href="https://www.lesswrong.com/tag/decision-theory"&gt;决策理论&lt;/a&gt;的完美瑰宝，它受到许多常识性标准的关注，而&lt;a href="https://en.wikipedia.org/wiki/Fermat's_Last_Theorem"&gt;这些标准的余量太小而无法包含&lt;/a&gt;。当达斯伊拉尼试图分割贸易收益时，&lt;a href="https://www.glowfic.com/replies/1729114#reply-1729114"&gt;算法规定&lt;/a&gt;他们各自独立评估什么构成公平分割。如果时间允许，他们会谈论他们认为公平的事情，并试图在“什么构成我们合作的利益的公平分配？”的层面上找到共同点。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; ...这样一来，达成协商价格的问题就被局部激励成为寻找对称谢林点的问题。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;谈判结束时，会提出最终报价，并最终决定是否接受该报价。该算法规定接受任何至少为您提供您认为公平的报价，并有可能拒绝不公平的报价。拒绝的概率应该足够高，以消除要约者因提供不公平的分割而获得的预期收益。但也不会比这个高太多。理性的人可能对公平有不同意见，但各方都应该制定自己的政策，这样双方都没有动机夸大他们认为的公平。&lt;/p&gt;&lt;p&gt;我想强调的是，该算法要求“要么接受要么放弃”报价的评估者采用不可信的威胁，即有时违背当地激励措施并拒绝不公平报价的政策，以形成提出要约的一方的当地激励措施。该算法还呼吁谈判者合作寻找公平的解决方案，而不是每个人都试图为自己谋取尽可能多的利益。根据该算法，被另一位谈判者说服什么构成公平分割是有效的。但在实际提出和评估要约时，双方都被要求采用一项政策，如果&lt;a href="https://en.wikipedia.org/wiki/Best_response"&gt;做出最佳反应&lt;/a&gt;，就会导致纳什均衡，在该均衡中，他们自己的公平分配概念得以实现。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;事实上，这就是为什么各方正在考虑的数字是关于“公平”主题的，它们是关于贸易收益的分配，旨在对称，作为周围反事实行动结构的目标，稳定看待事物的“公平”方式，而不会在与它存在微小分歧的情况下完全崩溃......&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;接下来：我们的软件系统什么时候应该发出不可信的&lt;a href="https://www.lesswrong.com/posts/6dAK3sdRabNYNy5zJ/using-threats-to-achieve-socially-optimal-outcomes"&gt;威胁&lt;/a&gt;？他们什么时候应该屈服于这样的威胁？&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/uguxceuv7ERp26kn9/best-responding-is-not-always-the-best-response#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Thu, 04 Jan 2024 23:30:48 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/uguxceuv7ERp26kn9/best-responding-is-not-always-the-best-response</guid></item></channel></rss>