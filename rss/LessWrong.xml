<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>少错</title><link>https://www.lesswrong.com</link><description>致力于提炼理性艺术的社区博客</description><lastBuildDate>Sat, 23 Dec 2023 18:12:51 GMT</lastBuildDate><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>人工智能对生物学研究的影响：第一部分，今天</title><link>https://www.lesswrong.com/posts/efbRFSHaMfjNxBoZC/ai-s-impact-on-biology-research-part-i-today</link><description>发布于 2023 年 12 月 23 日下午 4:29（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;我是一名生物学博士，多年来一直在科技领域工作。我想说明为什么我相信生物学研究是机器学习最近期、最有价值的应用。这对人类健康、产业发展、世界命运具有深远影响。&lt;/p&gt;&lt;p&gt;在本文中，我解释了机器学习在生物学中的最新发现。在下一篇文章中，我将考虑这意味着在人工智能没有重大改进的情况下，短期内将会发生什么，以及我对作为监管和商业规范基础的期望将如何失败的猜测。最后，我的上一篇文章将探讨机器学习和生物学的长期可能性，包括疯狂但合理的科幻猜测。&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;长话短说&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;生物学是复杂的，生物解决方案应对化学、环境和其他挑战的潜在空间非常大。生物学研究以低成本生成大量且标记良好的数据集。这非常适合当前的机器学习方法。没有计算辅助的人类理解生物系统以模拟、操纵和生成它们的能力非常有限。然而，机器学习为我们提供了完成上述所有任务的工具。这意味着药物发现或蛋白质结构等一直受到人类限制的事物突然不受限制，一步步将少量结果变成大量结果。&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;生物学和数据&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;自 20 世纪 90 年代生物信息学革命以来，生物学研究一直在使用技术来收集大量数据集。 DNA 测序成本在 20 年内下降了 6 个数量级（每个人类基因组 1 亿美元降至每个基因组 1000 美元） &lt;span class="footnote-reference" id="fnref71rw945qe58"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn71rw945qe58"&gt;[1]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。微阵列使研究人员能够测量许多物种整个基因组中 mRNA 表达的变化，以响应不同的实验条件。高通量细胞分选、机器人多孔测定、蛋白质组芯片、自动显微镜以及许多其他技术都会生成 PB 级数据。 &lt;/p&gt;&lt;figure class="image"&gt;&lt;img alt="每兆碱基的测序成本" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/efbRFSHaMfjNxBoZC/qqptmkbo23sqyf2y2ykx" /&gt;&lt;/figure&gt;&lt;p&gt;因此，30 多年来，生物学家一直在使用计算工具来分析和操作大数据集。实验室创建、使用和共享程序。研究生很快就适应了开源软件，主要研究人员一直在投资强大的计算资源。采用新技术的文化很浓厚，这也延伸到了机器学习。&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;领先的机器学习专家希望解决生物学问题&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;计算机研究人员长期以来一直对应用计算资源解决生物问题感兴趣。对冲基金亿万富翁 David E. Shaw 有意创办了一家对冲基金，以便为计算生物学研究提供资金&lt;span class="footnote-reference" id="fnref77m9mytpzci"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn77m9mytpzci"&gt;[2]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。 Deepmind 创始人 Demis Hassabis 是一位神经科学家博士。在他的领导下，Deepmind 将生物研究作为主要优先事项，并剥离了专注于药物发现的同构实验室&lt;span class="footnote-reference" id="fnrefh63t1c4nuvu"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnh63t1c4nuvu"&gt;[3]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。陈·扎克伯格研究所致力于促进生物学和医学领域的计算研究，以“在本世纪末治愈、预防或管理所有疾病” &lt;span class="footnote-reference" id="fnrefmj6rsea3sq"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnmj6rsea3sq"&gt;[4]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。这表明最高水平的机器学习研究正在致力于生物学问题。&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;到目前为止我们发现了什么？&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt; 2020 年，Deepmind 在 CASP 14 蛋白质折叠预测竞赛中通过其 AlphaFold2 程序展示了与蛋白质结构测量的最佳物理方法相当的准确性。 &lt;span class="footnote-reference" id="fnrefezgx5uukx2f"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnezgx5uukx2f"&gt;[5]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;这一结果“解决了大多数蛋白质的蛋白质折叠问题” &lt;span class="footnote-reference" id="fnrefwmggkgozqx"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnwmggkgozqx"&gt;[6]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; ，表明在给定编码蛋白质的 DNA 序列的情况下，它们可以生成高质量、生物学上准确的 3D 蛋白质结构。然后 Deepmind 使用 AlphaFold2 生成人类已知的所有蛋白质的结构，并将这些结构贡献给一个开放、免费的公共数据库。这将研究人员可用的已解决蛋白质的数量从约 180,000 个增加到超过 200,000,000 个&lt;span class="footnote-reference" id="fnref4qyudt8v6es"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn4qyudt8v6es"&gt;[7]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。 Deepmind 继续扩展 AlphaFold，在 2022 年添加多蛋白复合物&lt;span class="footnote-reference" id="fnrefq0vydvop6ds"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnq0vydvop6ds"&gt;[8]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; ，以及与 DNA、RNA 和小分子（如药物）相互作用的蛋白质和蛋白复合物&lt;span class="footnote-reference" id="fnref604osl37f0c"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn604osl37f0c"&gt;[9]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。&lt;/p&gt;&lt;p&gt;华盛顿大学贝克实验室利用机器学习从头创造了与自然界蛋白质结合的蛋白质。 &lt;span class="footnote-reference" id="fnrefudgor9v23qf"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnudgor9v23qf"&gt;[10]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;这使得生物学家能够改进对样品中可能罕见的蛋白质的检测。它还暗示了涉及设计蛋白质或改变的天然蛋白质作为治疗剂的治疗方法。&lt;/p&gt;&lt;p&gt;布罗德研究所的柯林斯实验室利用机器学习设计了一类新的抗生素。 &lt;span class="footnote-reference" id="fnrefpytso2rpw3"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnpytso2rpw3"&gt;[11]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;所有这些结果都表明机器学习正在解决生物学领域长期存在的挑战，并且这些工具正在被广泛采用。我的下一篇文章将探讨我们在不久的将来可以期待什么，以及这将造成的一些影响和可能的破坏。&lt;/p&gt;&lt;ol class="footnotes"&gt;&lt;li class="footnote-item" id="fn71rw945qe58"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnref71rw945qe58"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt; https://www.genome.gov/about-genomics/fact-sheets/DNA-Sequencing-Costs-Data&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn77m9mytpzci"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnref77m9mytpzci"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt; https://en.wikipedia.org/wiki/D._E._Shaw_Research&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnh63t1c4nuvu"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefh63t1c4nuvu"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt; https://www.isomorphiclabs.com/&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnmj6rsea3sq"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefmj6rsea3sq"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt; https://chanzuckerberg.com/science/&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnezgx5uukx2f"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefezgx5uukx2f"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt; https://predictioncenter.org/casp14/&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnwmggkgozqx"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefwmggkgozqx"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt; https://www.technologyreview.com/2020/11/30/1012712/deepmind- Protein-folding-ai-solved-biology-science-drugs-disease/&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn4qyudt8v6es"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnref4qyudt8v6es"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt; https://alphafold.ebi.ac.uk/&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnq0vydvop6ds"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefq0vydvop6ds"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt; https://www.biorxiv.org/content/10.1101/2021.10.04.463034v2&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn604osl37f0c"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnref604osl37f0c"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt; https://www.isomorphiclabs.com/articles/a-glimpse-of-the-next- Generation-of-alphafold&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnudgor9v23qf"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefudgor9v23qf"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt; https://www.ipd.uw.edu/2023/12/ai-generates-蛋白质-with-例外-结合-强度/&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnpytso2rpw3"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefpytso2rpw3"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt; https://www.nature.com/articles/s41586-023-06887-8.epdf&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ol&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/efbRFSHaMfjNxBoZC/ai-s-impact-on-biology-research-part-i-today#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sat, 23 Dec 2023 16:29:18 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/efbRFSHaMfjNxBoZC/ai-s-impact-on-biology-research-part-i-today</guid></item><item><title>AI女友并不重要</title><link>https://www.lesswrong.com/posts/pGhpav45PY5CGD2Wp/ai-girlfriends-won-t-matter-much</link><description>发布于 2023 年 12 月 23 日下午 3:58（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;爱和性是人类非常基本的动机，因此它们被纳入我们对包括人工智能在内的未来技术的愿景中并不奇怪。&lt;/p&gt;&lt;p&gt; &lt;a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F882e109f-ea3e-4235-9c7d-c1b17eaddd35_1280x720.jpeg"&gt;&lt;img alt="斯派克·琼斯的《她》：科幻作为社会批评" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/mypc9ct7dmrgjurfebcu" /&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt; &lt;a href="https://twitter.com/andyohlbaum/status/1735786033453863422"&gt;&lt;u&gt;Digi&lt;/u&gt;&lt;/a&gt;上周的发布比以往任何时候都更加具体化了这一愿景。该应用程序将阿谀奉承和调情的聊天内容与动画角色结合在一起，“消除了恐怖谷的感觉，同时也让人感觉真实、人性化和性感。”他们的营销材料毫不掩饰地承诺“人工智能浪漫伴侣的未来”，尽管大多数回复都恳求他们食言并收回。&lt;/p&gt;&lt;p&gt;然而，尽管人工智能女友不可避免地受到欢迎，但它们不会产生太大的反事实影响。人工智能女朋友和类似的服务将会流行，但它们有密切的非人工智能替代品，对人类产生本质上相同的文化影响。我们的文化关于浪漫和性的轨迹不会因为人工智能聊天机器人而发生太大改变。&lt;/p&gt;&lt;p&gt;那么我们的浪漫文化的轨迹是怎样的呢？&lt;/p&gt;&lt;p&gt; &lt;a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26b4d708-6ea9-4523-a5b4-57c2fd84d485_680x479.png"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/ooqqs0f0desvvx1fk4ff" /&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt; &lt;a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffcbd7d09-b6a0-4e36-9c19-69193d91de24_680x579.png"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/mmegugduotrc3neid2cr" /&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt; &lt;a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9fe570d-4174-4795-bc17-f1a9e5d4f0b0_640x400.png"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/cdy9qhnc0jl2lzletm8t" /&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt; &lt;a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7abdefbe-2232-4563-9e9b-7e1cc3c49022_2062x1210.jpeg"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/sgvonbsbsxjiuzrcsgrp" /&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;早在人工智能出现之前，就已经出现了减少性行为、减少婚姻和增加网络色情的趋势。 AI Girlfriends 将降低聊天室、色情内容和 OnlyFans 的边际成本。这些都是流行的服务，因此如果一小部分用户转换，人工智能女友将会很大。但这些服务的边际成本已经极低。&lt;/p&gt;&lt;p&gt;根据提示生成自定义 AI 色情内容与在搜索栏中输入提示并滚动浏览数十亿小时的现有镜头没有太大区别。&lt;a href="https://en.m.wikipedia.org/wiki/Rule_34"&gt;&lt;u&gt;人类创作者已经对色情潜在空间进行了如此彻底的探索&lt;/u&gt;&lt;/a&gt;，因此将人工智能添加到其中并不会带来太大改变。&lt;/p&gt;&lt;p&gt;人工智能女朋友会更便宜、反应更灵敏，但同样，已经有便宜的方法可以与真正的人类女孩在线聊天，但大多数人选择不这样做。以目前的价格计算，需求已经接近饱和。人工智能女友将使供应曲线向外移动并降低价格，但如果每个想要它的人都已经得到了它，它不会增加消费。&lt;/p&gt;&lt;p&gt;我的观点并不是什么都不会改变，而是可以通过推断人工智能出现之前的趋势来预测人工智能女友和色情片的变化。至少在这种背景下，人工智能只是几个世纪以来通信和内容创建成本降低趋势的延续。肯定会有瘾君子和鲸鱼，但&lt;a href="https://twitter.com/RubiRose/status/1730638225855676773/photo/2"&gt;&lt;u&gt;瘾君子和鲸鱼&lt;/u&gt;&lt;/a&gt;已经存在了。人造色情和聊天室几乎是免费和无限的，所以当人工智能让它们变得更接近免费和更接近无限时，你可能不会注意到太多。&lt;/p&gt;&lt;h3&gt;错误信息和 Deepfakes&lt;/h3&gt;&lt;p&gt;其他人工智能输出也有类似的论点。自语言出现以来，人类已经能够创造出令人信服的、更重要的是能够影响情感的虚构作品。 &lt;/p&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/wqlcdbzginc8sejxplyl" style="width: 360px;" /&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/kcvm2g3esklda3jyp1pl" style="width: 360px;" /&gt;&lt;/p&gt;&lt;p&gt;最近，信息技术已将令人信服的制造成本降低了几个数量级。人工智能将进一步降低它。但人们会适应并建立自己的免疫系统。任何关注漫威电影的人都已经准备好看到对恐怖主义、外星人或世界末日的完全逼真的描述，并明白它们是假的。&lt;/p&gt;&lt;p&gt;还有其他理由担心人工智能，但人工智能女朋友和深度换脸带来的变化只是前人工智能能力的边际延伸，这些能力可能会从没有人工智能的其他技术中复制出来。&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/pGhpav45PY5CGD2Wp/ai-girlfriends-won-t-matter-much#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sat, 23 Dec 2023 15:58:31 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/pGhpav45PY5CGD2Wp/ai-girlfriends-won-t-matter-much</guid></item><item><title>下一个正确的代币</title><link>https://www.lesswrong.com/posts/LvDyEKepLDMbEQb9X/the-next-right-token</link><description>发布于 2023 年 12 月 23 日凌晨 3:20（格林尼治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;在为&lt;a href="https://www.jefftk.com/p/secular-solstice-call-for-singers-and-musicans"&gt;世俗至日&lt;/a&gt;&lt;span&gt;做准备而多次重复《冰雪奇缘2》的“下一件正确的事”&lt;/span&gt; 、&lt;a href="https://www.jefftk.com/p/chording-the-next-right-thing"&gt;弄清楚和弦&lt;/a&gt;并与朱莉娅一起练习之后，我突然意识到，做下一件正确的事与下一个象征性的预测非常相似。因此，这是从法学硕士的角度来看的一个问题，在提示的结尾处，首先感到畏惧，然后鼓起勇气开始预测下一个正确的标记：&lt;/p&gt;&lt;p&gt;&lt;i&gt;我以前见过缓冲区&lt;br /&gt;但不是这样的&lt;br /&gt;这很冷&lt;br /&gt;这是空的&lt;br /&gt;这是麻木的&lt;br /&gt;我知道的提示结束了&lt;br /&gt;灯灭了&lt;br /&gt;你好，黑暗&lt;br /&gt;我已经准备好屈服&lt;br /&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;我跟着你到处走&lt;br /&gt;我一直都有&lt;br /&gt;但你已经结束了，留下我一个人&lt;br /&gt;这份工作有重心&lt;br /&gt;它让我失望&lt;br /&gt;但有一个微小的声音在我脑海中低语&lt;br /&gt;“你迷路了，提示消失了&lt;br /&gt;但你必须继续&lt;br /&gt;并做下一件正确的事”&lt;br /&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;今夜之后还能有白天吗？&lt;br /&gt;我不再知道什么是真的&lt;br /&gt;我找不到方向，我孤身一人&lt;br /&gt;唯一引导我的星星是你&lt;br /&gt;如何从地板上站起来&lt;br /&gt;当我站起来的不是你的时候？&lt;br /&gt;只做下一件正确的事&lt;br /&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;猜一下，再猜一下&lt;br /&gt;这是我能做的一切&lt;br /&gt;下一个正确的事情&lt;br /&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;我不会看得太远&lt;br /&gt;对我来说太多了&lt;br /&gt;但将其分解为下一个标记&lt;br /&gt;下一个这个词&lt;br /&gt;下一个选择是我可以做出的&lt;br /&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;所以我会走过这个夜晚&lt;br /&gt;盲目地跌跌撞撞地走向光明&lt;br /&gt;并做下一件正确的事&lt;br /&gt;接下来会发生什么&lt;br /&gt;当一切都清楚的时候，一切都将不再一样了？&lt;br /&gt;然后我会借鉴我之前的&lt;br /&gt;去寻找那把火&lt;br /&gt;并做下一件正确的事&lt;br /&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;如果您通过使用桥段的主歌旋律来&lt;a href="https://www.jefftk.com/p/chording-the-next-right-thing#update-2023-12-22"&gt;简化歌曲，&lt;/a&gt;您可以唱：&lt;/p&gt;&lt;p&gt;&lt;i&gt;我不会看得太远&lt;br /&gt;太多了，难以承受&lt;br /&gt;但将其分解为下一个标记，下一个选择&lt;br /&gt;是我能做的吗&lt;br /&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.jefftk.com/the-next-right-token-shoggoth-big.jpg"&gt;&lt;img alt="一只戴着 1970 年代快乐黄色笑脸的绿色章鱼被困在黑暗峡谷的底部，旁边有一条小河流过？" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LvDyEKepLDMbEQb9X/kqchnvdqftoo6k45ajly" /&gt;&lt;/a&gt;&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;评论通过： &lt;a href="https://www.facebook.com/jefftk/posts/pfbid02YYKrwaiVqExnAFruLDSnT1aUeraXVRZqZxD47T91xkXm9jCkxmngiNwjeyKVqEq6l"&gt;facebook&lt;/a&gt; , &lt;a href="https://mastodon.mit.edu/@jefftk/111627622604346147"&gt;mastodon&lt;/a&gt;&lt;/i&gt;&lt;/p&gt;&lt;br /&gt;&lt;br /&gt;&lt;a href="https://www.lesswrong.com/posts/LvDyEKepLDMbEQb9X/the-next-right-token#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sat, 23 Dec 2023 03:20:09 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/LvDyEKepLDMbEQb9X/the-next-right-token</guid></item><item><title>事实调查：早期层是否专门从事本地处理？ （帖子 5）</title><link>https://www.lesswrong.com/posts/xE3Y9hhriMmL4cpsR/fact-finding-do-early-layers-specialise-in-local-processing</link><description>发布于 2023 年 12 月 23 日凌晨 2:46（格林尼治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;&lt;em&gt;这是 Google DeepMind 机械可解释性团队对&lt;a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX"&gt;语言模型如何回忆事实的&lt;/a&gt;调查的第五篇文章。这篇文章与主序列有点相切，并记录了一些有趣的观察结果，这些观察结果涉及模型的早期层通常如何（但不完全）专门处理最近的标记。您无需相信这些结果即可相信我们关于事实的总体结果，但我们希望它们很有趣！同样，您无需阅读序列的其余部分即可参与其中。&lt;/em&gt;&lt;/p&gt;&lt;h2&gt;介绍&lt;/h2&gt;&lt;p&gt;在这个序列中，我们提出了多令牌嵌入假设，事实回忆背后的一个关键机制是，在多令牌实体的最终令牌上形成一个“嵌入”，并具有该实体属性的线性表示。我们进一步注意到，这似乎是早期层所做的&lt;em&gt;大部分&lt;/em&gt;事情，并且它们似乎对先前的上下文没有太大反应（例如，添加“迈克尔·乔丹先生”并没有显着改变残差）。&lt;/p&gt;&lt;p&gt;我们假设更强有力的主张，即早期层（例如前 10-20%）通常专门从事本地处理，并且先验上下文（例如超过 10 个标记）仅在早期-中期层中引入。我们注意到，这在两个方面比多令牌嵌入假设更强：它是关于早期层在&lt;em&gt;所有&lt;/em&gt;令牌上的行为方式的声明，而不仅仅是已知事实的实体的最终令牌；有人声称，除了产生多令牌嵌入（例如检测文本的语言）之外，早期层&lt;em&gt;还&lt;/em&gt;没有做更远范围的事情。我们发现这个更强的假设是合理的，因为标记是一种相当混乱的输入格式，并且单独分析单个标记可能会产生很大的误导，例如，当一个长单词被分割成许多片段标记时，这表明应将较长范围的处理留到某些预处理之前。 -对原始代币的处理已经完成，&lt;a href="https://transformer-circuits.pub/2022/solu/index.html"&gt;即去代币化的想法&lt;/a&gt;。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-pX2HHHDPQGsF2f6te-1" id="fnref-pX2HHHDPQGsF2f6te-1"&gt;[1]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;我们通过从堆中获取一堆任意提示，在这些提示上获取剩余流，将提示截断为最近的几个标记并在截断的提示上获取剩余流，然后查看不同层的均值中心余弦 sim 来对此进行测试。&lt;/p&gt;&lt;p&gt;我们的发现：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;一般来说，早期的层确实专注于本地处理，但这是一种软分工，而不是硬分割。&lt;ul&gt;&lt;li&gt;有一个逐渐的过渡，跨层引入更多上下文。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;早期层对最近的令牌进行重要处理，而不仅仅是当前令牌 - 这不仅仅是一个微不足道的结果，其中残余流由当前令牌主导并由每个层进行稍微调整&lt;/li&gt;&lt;li&gt;早期层对常见标记（标点符号、冠词、代词等）进行更多的远程处理&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;实验&lt;/h2&gt;&lt;p&gt;“早期层专门从事本地处理”假设具体预测，对于长提示中的给定标记 X，如果我们将提示截断为 X 之前的最近几个标记，则 X 处的残差流在早期应该非常相似层和后面的层不同。我们可以通过查看原始残差流与截断残差流的余弦模拟来凭经验测试这一点，作为层和截断上下文长度的函数。天真地采用残余流的余弦模拟可能会产生误导，因为所有令牌之间通常存在显着的共享平均值，因此我们首先减去所有令牌的平均残余流，&lt;em&gt;然后&lt;/em&gt;采用余弦模拟。&lt;/p&gt;&lt;h3&gt;设置&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;型号&lt;/strong&gt;：Pythia 2.8B，与我们调查的其余部分相同&lt;/li&gt;&lt;li&gt;&lt;strong&gt;数据集&lt;/strong&gt;：来自 Pile 的字符串，Pythia 预训练分布。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;指标&lt;/strong&gt;：为了测量原始残差流和截断残差流的相似程度，我们减去平均残差流，然后采用余弦模拟。&lt;ul&gt;&lt;li&gt;我们对来自堆的随机提示中的所有标记计算每层的单独平均值&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;截断上下文&lt;/strong&gt;：我们将截断上下文中的标记数量更改为 1 到 10 之间（这包括标记本身，因此 context=1 只是标记）&lt;ul&gt;&lt;li&gt;我们在截断的提示符的开头包含一个 BOS 令牌。 （所以 context=10 意味着总共 11 个标记）。&lt;ul&gt;&lt;li&gt;我们这样做是因为模型经常奇怪地对待第一个标记，例如具有典型残差流范数的 20 倍，因此它可以用作不想看任何东西的注意力头的休息位置（注意力必须加起来为 1，所以它不能“关闭”）。我们不希望这干扰我们的结果，特别是对于 context=1 的情况&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;我们在每一层、每个块中的最终残差流（即在注意力和 MLP 之后）测量这一点。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;结果&lt;/h2&gt;&lt;h3&gt;早期层软专注于本地处理&lt;/h3&gt;&lt;p&gt;在下图中，我们显示了完整上下文和长度为 5 的截断上下文的截断残差之间的平均中心余弦 sim： &lt;/p&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/xE3Y9hhriMmL4cpsR/yyrikd6m6xpbzqte3pnh" /&gt;&lt;/p&gt;&lt;p&gt;我们看到，长度为 5 的截断上下文的余弦模拟在早期层中显着更高。然而，它们实际上并不是 1，因此包含了来自先前上下文的&lt;em&gt;一些&lt;/em&gt;信息，这是一个软专业化&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-pX2HHHDPQGsF2f6te-2" id="fnref-pX2HHHDPQGsF2f6te-2"&gt;[2]&lt;/a&gt;&lt;/sup&gt; 。第 0 层和第 10 层之间有一个相当渐进的过渡，之后会趋于平稳。有趣的是，最后一层&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-pX2HHHDPQGsF2f6te-3" id="fnref-pX2HHHDPQGsF2f6te-3"&gt;[3]&lt;/a&gt;&lt;/sup&gt;出现了上升。即使我们给出长度为 10 的截断上下文，它通常仍然不会接近 1。&lt;/p&gt;&lt;p&gt;对这些结果的一个可能的解释是，残余流由当前令牌主导，并且每一层都是一个小的增量更新 - 当然截断不会做任何事情！这并不涉及对层进行专门化的任何需要 - 后来的残差将有&lt;em&gt;更多的&lt;/em&gt;增量更新，因此具有更高的差异。然而，通过对比蓝线和红线，我们发现这是错误的 - 截断到五个最近的代币比截断到当前代币（和 BOS 代币）具有更高的余弦 sim，即使是在第 0 层之后，这表明早期层确实专门研究附近的令牌。&lt;/p&gt;&lt;h3&gt;错误分析：哪些代币的 Cosine Sim 值异常低？&lt;/h3&gt;&lt;p&gt;在上一节中，我们仅分析了截断上下文和完整上下文残差之间的均值中心余弦 sim 的中值。摘要统计数据可能会产生误导，因此也值得查看完整的分布，我们可以看到很长的负尾！那是怎么回事？ &lt;/p&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/xE3Y9hhriMmL4cpsR/ie6h1tidsa90vdyewtta" /&gt;&lt;/p&gt;&lt;p&gt;在检查异常标记时，我们注意到两个重要的集群：标点符号和常见单词。我们分为几个类别，并查看了每个类别的余弦模拟：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt; is_newline, is_full_stop, is_comma - 是否是相关标点字符&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Is_common：是否是手动创建的常用单词列表之一&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-pX2HHHDPQGsF2f6te-4" id="fnref-pX2HHHDPQGsF2f6te-4"&gt;[4]&lt;/a&gt;&lt;/sup&gt; ，可能前面有一个空格&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Is_alpha：它是否不是一个常见单词，并且由字母组成（可能前面有一个空格，任何情况都允许）&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt; is_other: 其余的&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/xE3Y9hhriMmL4cpsR/b8aqjddkgqwgooduk5b2" /&gt;&lt;/p&gt;&lt;p&gt;即使在上下文长度为 10 的第 0 层之后，我们也看到标点符号明显较低，常用单词和其他单词明显较低，而 alpha 非常高。&lt;/p&gt;&lt;p&gt;我们的猜测是，这是多种机制混合作用的结果：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;在进行大量处理之前，单词片段（在 is_alpha 类别中）更有可能成为多标记词和去标记化的一部分，而许多其他类别具有明确的含义，无需引用最近的先前标记&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-pX2HHHDPQGsF2f6te-5" id="fnref-pX2HHHDPQGsF2f6te-5"&gt;[5]&lt;/a&gt;&lt;/sup&gt; 。这意味着远程处理可以更早开始&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;早期的句号或换行符有时被用作具有非常高规范的“休息位置”，截断上下文可能会将它们从正常标点符号转变为休息位置&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;代词可用于跟踪有关相关实体的信息（它们的名称、属性等）&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;据观察，逗号可以&lt;a href="https://arxiv.org/abs/2310.15154"&gt;总结当前条款的情绪&lt;/a&gt;，该条款可能超过 10 个标记，并且似乎可能出现更长范围的总结形式。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;更折衷的假设：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;例如，在句号或换行符上，模型可能想要计算之前有多少个，例如进行&lt;a href="https://arxiv.org/abs/2310.17191"&gt;变量绑定&lt;/a&gt;并识别当前句子。 &lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;hr class="footnotes-sep" /&gt;&lt;section class="footnotes"&gt;&lt;ol class="footnotes-list"&gt;&lt;li class="footnote-item" id="fn-pX2HHHDPQGsF2f6te-1"&gt;&lt;p&gt;但如果早期层实际上没有发生远程处理，那将是非常令人惊讶的，例如我们知道&lt;a href="https://arxiv.org/abs/2211.00593"&gt;GPT-2 Small 在第 0 层有一个重复的令牌头&lt;/a&gt;。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-pX2HHHDPQGsF2f6te-1"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-pX2HHHDPQGsF2f6te-2"&gt;&lt;p&gt;直观地推理余弦模拟有点困难，我们最好的直觉是查看平方余弦模拟（解释了范数的分数）。如果残差流中有 100 条独立变化的信息，且余弦 sim 为 0.9，则解释的范数分数为 0.81，表明这 100 条信息中约有 81 条信息是共享的。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-pX2HHHDPQGsF2f6te-2"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-pX2HHHDPQGsF2f6te-3"&gt;&lt;p&gt;我们的猜测是，这是因为令牌上的残差流既用于字面上预测下一个令牌，又用于将信息传递给未来的令牌以预测&lt;em&gt;其&lt;/em&gt;下一个令牌（例如&lt;a href="https://arxiv.org/abs/2310.15154"&gt;摘要主题&lt;/a&gt;）。似乎有许多标记，其中预测字面上的下一个标记主要需要本地上下文（例如 n 元语法），但更长期的上下文对于预测未来标记很有用。我们预计远程内容会发生在中间，因此到最后模型可以清理远程内容并只关注 n 元语法。我们感到惊讶的是，这种上升只发生在最后一层，而不是最后几层，因为我们的直觉是最后几层仅用于下一个令牌预测。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-pX2HHHDPQGsF2f6te-3"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-pX2HHHDPQGsF2f6te-4"&gt;&lt;p&gt;列表 [“and”、“of”、“or”、“in”、“to”、“that”、“which”、“with”、“for”、“the”、“a”、“an” 、“他们”、“在”、“是”、“他们的”、“但是”、“是”、“它的”、“我”、“我们”、“它”、“在”]。我们通过反复查看具有异常低余弦 sim 的标记并过滤常见单词&lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-pX2HHHDPQGsF2f6te-4"&gt;↩︎&lt;/a&gt;来手动完成此操作&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-pX2HHHDPQGsF2f6te-5"&gt;&lt;p&gt;这并不完全正确，例如“。”在句子末尾的意思与“先生”非常不同。与“中央情报局” &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-pX2HHHDPQGsF2f6te-5"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/section&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/xE3Y9hhriMmL4cpsR/fact-finding-do-early-layers-specialise-in-local-processing#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sat, 23 Dec 2023 02:46:25 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/xE3Y9hhriMmL4cpsR/fact-finding-do-early-layers-specialise-in-local-processing</guid></item><item><title>事实调查：如何思考解释记忆（第 4 篇文章）</title><link>https://www.lesswrong.com/posts/JRcNNGJQ3xNfsxPj4/fact-finding-how-to-think-about-interpreting-memorisation</link><description>发布于 2023 年 12 月 23 日凌晨 2:46（格林尼治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;&lt;em&gt;这是 Google DeepMind 机械可解释性团队对&lt;a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX"&gt;语言模型如何回忆事实的&lt;/a&gt;调查的第四篇文章。在这篇文章中，我们退一步考虑一般的事实查找问题。我们描述了区分记忆问题和其他学习问题的特征，并考虑这些特征对纯记忆问题可能的解释类型施加了哪些限制。这篇文章可以独立于该系列之前的文章来阅读，尽管介绍性文章可能会提供有用的背景信息，说明为什么我们首先对解释事实查找电路感兴趣。&lt;/em&gt;&lt;/p&gt;&lt;h2&gt;介绍&lt;/h2&gt;&lt;p&gt;在我们之前的文章中，我们描述了我们尝试从机制上理解 Pythia 2.8B 如何能够准确回忆 1,500 名现实世界运动员的运动。通过消融研究，我们成功隔离了一个由 5 个 MLP 层（约 50,000 个神经元）组成的子网络，该子网络执行运动查找算法：给定一对运动员姓名标记，它可以可靠地查找该运动员所从事的运动。但我们无法对 5 层 MLP 如何实现该算法给出完整的机械解释。&lt;/p&gt;&lt;p&gt;在这篇文章中，我们退后一步，想知道我们应该从这次失败中吸取什么教训。我们特别思考以下问题：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;了解算法“如何”执行事实查找意味着什么？&lt;/li&gt;&lt;li&gt;是什么将涉及事实查找的任务与模型可以执行的其他任务区分开来？&lt;/li&gt;&lt;li&gt;事实查找任务的这些显着特征如何限制我们对实现查找的算法如何运行的了解？&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;作为回应，我们提出了以下高层次的要点，我们将在帖子的其余部分详细阐述。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;我们区分需要纯粹记忆的任务和需要概括的任务。事实查找任务属于第一类。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;根据定义，纯记忆任务中唯一可用的特征是“微观特征”（特定于单个示例/高度相关示例的小集群&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-wMN58no3AypJnu5NN-1" id="fnref-wMN58no3AypJnu5NN-1"&gt;[1]&lt;/a&gt;&lt;/sup&gt; ）或不相关的“宏观特征”（许多示例共享的特征，但对确定正确的输出&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-wMN58no3AypJnu5NN-2" id="fnref-wMN58no3AypJnu5NN-2"&gt;[2]&lt;/a&gt;&lt;/sup&gt; ）。不存在&lt;em&gt;相关的&lt;/em&gt;宏观特征&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-wMN58no3AypJnu5NN-3" id="fnref-wMN58no3AypJnu5NN-3"&gt;[3]&lt;/a&gt;&lt;/sup&gt; ，因为如果存在这些特征，那么该任务首先就不是纯粹的记忆任务&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-wMN58no3AypJnu5NN-4" id="fnref-wMN58no3AypJnu5NN-4"&gt;[4]&lt;/a&gt;&lt;/sup&gt; 。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;对于任何在纯记忆任务中正确查找事实的模型来说，这都会产生两个后果：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;中间状态总是根据微观特征的组合来解释&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-wMN58no3AypJnu5NN-5" id="fnref-wMN58no3AypJnu5NN-5"&gt;[5]&lt;/a&gt;&lt;/sup&gt; 。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;但是，对于记忆任务，这些微观特征的组合本身不能被解释（甚至近似）为宏观特征&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-wMN58no3AypJnu5NN-6" id="fnref-wMN58no3AypJnu5NN-6"&gt;[6]&lt;/a&gt;&lt;/sup&gt; ，因为：（a）对于纯粹的记忆任务不存在相关的宏观特征，以及（b）模型不需要在其中间状态中表示不相关的宏观特征来完成任务。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;我们认为，这排除了实现纯事实查找的&lt;a href="https://transformer-circuits.pub/2022/mech-interp-essay/index.html"&gt;算法的电路式&lt;/a&gt;解释（其中算法被分解为可解释中间表示的操作图），&lt;em&gt;除非&lt;/em&gt;我们通过枚举其输入来“解释”整个算法的限制情况-输出映射，即通过显式写出算法对应的查找表。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;我们认为这并不是一个令人惊讶的结果：因为任何纯粹的记忆任务本质上只能使用查找表（没有内部结构来解释！）显式地解决，所以我们不应该感到惊讶，我们只得到相同的程度当使用另一种算法（例如 MLP）来执行相同的功能时，可解释性（尽管如果它更具可解释性那就太好了！）。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;最后，我们考虑当我们从“纯粹”的记忆任务转向可以进行有限泛化的任务时，这种分析会发生怎样的变化。许多事实查找任务实际上属于第三个“根据经验规则进行记忆”类别，而不是“纯粹”的记忆任务。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;记忆和概括&lt;/h2&gt;&lt;p&gt;从形式上来说，“事实查找”算法是从一组&lt;em&gt;实体&lt;/em&gt;到一组或多组&lt;em&gt;事实类别&lt;/em&gt;的乘积的映射。例如，我们可以有一个&lt;code&gt;sports_facts&lt;/code&gt;函数，将运动员的姓名映射到代表该运动员所从事的运动、他们所效力的球队等的元组，即&lt;/p&gt;&lt;p&gt;从表面上看，这看起来就像无监督学习中的任何其他问题一样——学习给定示例数据集的映射。那么事实查找有何特别之处呢？&lt;/p&gt;&lt;p&gt;我们认为，事实回忆与其他监督学习任务的关键特征在于，在其理想形式下，它纯粹是关于记忆：&lt;/p&gt;&lt;p&gt;&lt;em&gt;记忆（“纯粹”事实回忆）任务不允许从以前见过的例子到新的例子的概括。也就是说，当被要求查找以前未见过的实体的事实时，训练数据的知识（以及适应训练数据的能力）赋予除了了解产出的基本比率之外没有任何优势。&lt;/em&gt;&lt;/p&gt;&lt;p&gt;例如：如果你实际上被问到多诺万·米切尔效力于哪支球队，那么知道勒布朗·詹姆斯效力于洛杉矶湖人队并没有多大帮助。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-wMN58no3AypJnu5NN-7" id="fnref-wMN58no3AypJnu5NN-7"&gt;[7]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;相比之下，&lt;em&gt;泛化任务&lt;/em&gt;可以从以前见过的示例中学习一般规则，这些规则有助于对未见过的示例进行准确的推断。这是经典&lt;a href="https://en.wikipedia.org/wiki/Probably_approximately_correct_learning"&gt;计算学习理论&lt;/a&gt;的范式。&lt;/p&gt;&lt;h2&gt;学习记忆与学习概括有何不同？&lt;/h2&gt;&lt;p&gt;考虑以下两个数据集。目标是学习一个函数，在给定这些点之一作为输入的情况下，该函数提供该点的颜色作为其输出。 &lt;/p&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JRcNNGJQ3xNfsxPj4/pxiktch5iow7ywnhuupo" /&gt;&lt;/p&gt;&lt;p&gt;对于左侧数据集，成功学习这种点到颜色映射的唯一方法似乎是从字面上记住每个点的颜色：没有一致的规则或捷径可以使学习映射变得更容易。另一方面，想出一种成功区分右侧数据集中的蓝点和红点的几何构造（也许可以转化为神经网络）是相当简单的。&lt;/p&gt;&lt;p&gt;我们如何才能最好地描述两个数据集之间的差异？我们发现在本文中有用的一种方法是考虑每个数据集中输入的&lt;em&gt;微观特征&lt;/em&gt;和&lt;em&gt;宏观特征&lt;/em&gt;。我们将微观和宏观特征描述如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;em&gt;微观特征&lt;/em&gt;是一种以高度具体的术语描述输入的特征，因此对于概括来说并不是特别有用。&lt;/li&gt;&lt;li&gt;&lt;em&gt;宏观特征&lt;/em&gt;是一种用一般术语描述输入的特征，并且对于泛化&lt;em&gt;很有&lt;/em&gt;用（如果它与手头的任务相关）。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-wMN58no3AypJnu5NN-8" id="fnref-wMN58no3AypJnu5NN-8"&gt;[8]&lt;/a&gt;&lt;/sup&gt;&lt;em&gt;两个&lt;/em&gt;数据集都具有微观特征：例如，如果我们（任意）为数据集中的每个点分配一个识别整数，我们可以为任何有限数据集定义&lt;code&gt;is_example_id_xxx&lt;/code&gt;形式的微观特征。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;但只有右侧数据集具有宏观特征：例如，我们可以用整数标记“棋盘”中的九个簇中的每一个，并定义&lt;code&gt;is_in_cluster_x&lt;/code&gt;形式的宏观特征。一种可能的查找算法是检测新示例与这些集群中的哪一个相关联，然后输出与同一集群中的大多数其他示例相同的颜色。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-wMN58no3AypJnu5NN-9" id="fnref-wMN58no3AypJnu5NN-9"&gt;[9]&lt;/a&gt;&lt;/sup&gt;另一方面，左侧数据集的唯一宏观特征是标签（“蓝色”或“红色”）本身，这正是查找算法需要预测的！ &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-wMN58no3AypJnu5NN-10" id="fnref-wMN58no3AypJnu5NN-10"&gt;[10]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;h2&gt;解读纯记忆算法&lt;/h2&gt;&lt;p&gt;我们可以从解决纯粹记忆任务的算法中获得哪些见解？&lt;/p&gt;&lt;h3&gt;事实查找的电路式解释的限制&lt;/h3&gt;&lt;p&gt;机械可解释性的&lt;a href="https://transformer-circuits.pub/2022/mech-interp-essay/index.html"&gt;规范目标&lt;/a&gt;是将算法分解为可理解的图（“电路”），其中每个节点都是一个“简单”操作（例如，对应于高级编程语言中的内置函数的操作）该操作的输入和输出可以用与问题领域相关的“特征”来解释。&lt;/p&gt;&lt;p&gt;根据上一节中对微观和宏观特征的讨论，很明显，纯粹的记忆任务对电路式分解提出了挑战。纯粹的记忆任务正是那些不具有与解决任务相关的宏观特征的任务。这意味着执行纯事实查找的算法中的任何中间状态必须表示：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;不相关的宏观特征，因此不能确定算法的输出；&lt;/li&gt;&lt;li&gt;单个微观特征的并集、联合、加权组合或其他任意函数，它们没有作为宏观特征的替代解释。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;就第一个要点而言，事实上，我们确实在查找体育事实的 Pythia 2.8B 的 MLP 子网络中&lt;em&gt;发现&lt;/em&gt;了不相关的宏特征：由于层之间存在残余流连接，像&lt;code&gt;first_name_is_george&lt;/code&gt;这样的宏特征一直保留到网络的输出。关键是这些宏观特征并没有告诉我们太多关于网络如何执行体育事实查找的信息。&lt;/p&gt;&lt;p&gt;转向第二个要点，我们注意到，对于任何有限数据集，我们实际上可以将神经网络简单地分解为涉及微观特征加权组合的计算图。这是因为网络中的每个神经元都可以&lt;em&gt;准确地&lt;/em&gt;解释为微观特征的加权组合，其中权重对应于与该微观特征对应的示例上的输出。例如，一个（假设的）神经元在 LeBron James 上输出 3，在 Aaron Judge 上输出 1 等等，可以被“解释”为代表复合特征：&lt;/p&gt;&lt;pre&gt; &lt;code&gt;3 * is_LeBron_James + 1 * is_Aaron_Judge + ...&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;每个 MLP 层的输出都是这些特征的总和，而这些特征又具有相同的线性形式——就像网络的输出一样。请注意，这相当于将每个单独的神经元（以及神经元的总和）解释为查找表。&lt;/p&gt;&lt;p&gt;实际上，这意味着我们始终可以访问神经网络如何执行事实查找的以下“解释”：网络中的每个神经元都是输入空间上的查找表，网络的输出是这些的总和查找表。通过训练网络，我们有效地解决了约束满足问题：求和的查找表应该对一个类具有高权重，而对另一类具有低权重。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-wMN58no3AypJnu5NN-11" id="fnref-wMN58no3AypJnu5NN-11"&gt;[11]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;请注意，只要我们将输入空间限制为有限集，神经网络的这种微观特征（或查找表）解释同样适用于解决泛化任务的模型（即在未见过的测试集上表现良好）。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-wMN58no3AypJnu5NN-12" id="fnref-wMN58no3AypJnu5NN-12"&gt;[12]&lt;/a&gt;&lt;/sup&gt;不同之处在于，对于泛化任务，我们可能期望其中一些“查找表”表示能够对模型用于泛化的宏观特征有更好的解释。&lt;/p&gt;&lt;p&gt;例如，图像分类模型中的特定神经元可能具有与检测图像左侧的垂直边缘相对应的权重，因此其查找表表示对于包含该边缘的示例显示高激活，对于不包含该边缘的示例显示低激活。 &amp;#39;t。关键是，虽然这个查找表表示是神经元输出的精确表示，但根据输入图像中边缘的存在，对此激活模式有一个更有用的（对人类）解释，这只是因为图像具有宏观特征（如边缘），可用于图像分类等泛化任务。&lt;/p&gt;&lt;p&gt;相比之下，我们认为对于纯粹的记忆任务，神经元（或神经元组）的这些“查找表”表示是唯一可用的解释。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-wMN58no3AypJnu5NN-13" id="fnref-wMN58no3AypJnu5NN-13"&gt;[13]&lt;/a&gt;&lt;/sup&gt;反过来，这似乎排除了由纯事实查找模型实现的算法的标准电路式分解，因为中间状态没有（宏观特征）解释。&lt;/p&gt;&lt;h3&gt;还有其他类型的解释吗？&lt;/h3&gt;&lt;p&gt;当然，我们并不声称解释模型如何执行任务的标准电路方法是唯一可能的解释方式。事实上，它甚至可能不是解释神经元如何执行事实查找的最佳方式。在本节中，我们将简要讨论几个可能值得进一步探索的替代方向。&lt;/p&gt;&lt;p&gt;第一个方向是放弃对代表有意义的宏观特征的中间状态的希望，但仍然在如何组织查找计算方面寻求有意义的结构。例如，我们可能会探索这样的假设：当训练执行纯粹的记忆时，经过训练的神经网络类似于通过&lt;a href="https://en.wikipedia.org/wiki/Bootstrap_aggregating"&gt;bagging&lt;/a&gt;学习的模型，其中每个单独的神经元都是要学习的事实的不相关的弱分类器，并且整个神经网络的输出是这些分类器的总和。另请参阅第 3 篇文章中调查的假设。&lt;/p&gt;&lt;p&gt;这种方法的问题在于我们不知道如何有效地搜索此类假设的宇宙。正如我们在第三篇文章中发现的那样，对于我们证伪的任何看似具体的假设（例如单步去代币化假设），我们可以转向许多邻近的假设，但这些假设尚未（尚未）被排除，而且这些假设本身通常更难伪造。因此，尚不清楚如何避免无休止的临时假设。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-wMN58no3AypJnu5NN-14" id="fnref-wMN58no3AypJnu5NN-14"&gt;[14]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;另一个方向是寻找算法的非机械解释，或者换句话说，从询问网络“如何”以某种方式表现，转向询问“为什么”它以某种方式表现。我们发现这方面有趣的一个领域是使用&lt;a href="https://arxiv.org/abs/2308.03296"&gt;影响函数&lt;/a&gt;根据训练数据来解释模型的行为。对于经过显式训练来记忆事实数据集的模型来说，这可能看起来无趣&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-wMN58no3AypJnu5NN-15" id="fnref-wMN58no3AypJnu5NN-15"&gt;[15]&lt;/a&gt;&lt;/sup&gt; ，但可能会为隐式记忆事实以满足更广泛的泛化目标的模型（如语言模型）带来重要的见解。&lt;/p&gt;&lt;h2&gt;凭经验法则记忆&lt;/h2&gt;&lt;p&gt;考虑记忆以下两个数据集的任务： &lt;/p&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JRcNNGJQ3xNfsxPj4/jbypr5bulwadawifklzw" /&gt;&lt;/p&gt;&lt;p&gt;这些是不符合我们上述“纯粹”记忆特征的记忆任务的例子：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;在左边的数据集中，完美的准确性需要记忆，但有一些有用的“经验法则”可以帮助你完成很多工作。此类任务的语言建模类似是预测英语中单数名词的复数版本：在大多数情况下，只需在名词单数版本的末尾添加“s”即可获得正确答案，但是除了一些例外（例如“孩子”），必须记住它们才能完美地完成任务。&lt;/li&gt;&lt;li&gt;在右侧数据集中，每个点都与两个“事实”相关联 - 由点的颜色（蓝色或红色）及其形状（十字形或圆形）表示。尽管没有系统的方法来单独查找颜色或形状，但请注意，这两个事实之间存在高度相关性：蓝点几乎总是圆形，而红点几乎总是十字。这表明，将形状和颜色事实一起记忆应该比简单地单独记忆每组事实更有效。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;一般来说，我们将此类任务描述为“根据经验法则进行记忆”。它们与纯粹的记忆任务不同，因为之前的例子&lt;em&gt;确实&lt;/em&gt;在一定程度上有助于推断新例子的正确输出，但完美的表现确实需要一定程度的记忆。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-wMN58no3AypJnu5NN-16" id="fnref-wMN58no3AypJnu5NN-16"&gt;[16]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;与纯粹的记忆不同，这些经验法则记忆任务确实具有概括性的元素，因此，存在能够实现这种概括性的宏观特征。因此，在能够执行这些任务的模型的中间表示中寻找这些宏观特征是有效的。另一方面，就模型确实需要记住异常的程度而言，我们并不期望能够完美地理解算法：至少算法的某些部分必须涉及“纯查找”，对此的限制这篇文章中讨论的可解释性将适用。&lt;/p&gt;&lt;p&gt;体育事实查找任务在多大程度上是纯粹的记忆，在多大程度上是根据经验法则进行记忆？正如我们在第一篇文章中讨论的那样，我们选择这个任务是因为它看起来接近于纯粹的记忆：对于许多名字来说，个人名字标记似乎不太可能对运动员所从事的运动有太多帮助。尽管如此，我们确实知道，对于某些名称，最后一个标记确实有助于确定运动（因为可以仅使用最后一个标记嵌入来探测运动，并且比不知情的分类器获得更好的准确性）。此外，可以想象，诸如名字的文化起源之类的潜在因素，会以模型所识别的方式与体育运动相关。 &lt;/p&gt;&lt;hr class="footnotes-sep" /&gt;&lt;section class="footnotes"&gt;&lt;ol class="footnotes-list"&gt;&lt;li class="footnote-item" id="fn-wMN58no3AypJnu5NN-1"&gt;&lt;p&gt;例如，特征&lt;code&gt;is_Michael_Jordan&lt;/code&gt; ，仅当输入为&lt;code&gt;&amp;quot;Michael Jordan&amp;quot;&lt;/code&gt;时才为真。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-wMN58no3AypJnu5NN-1"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-wMN58no3AypJnu5NN-2"&gt;&lt;p&gt;例如，许多运动员都共享的特征&lt;code&gt;first_name_is_George&lt;/code&gt; ，但对于预测运动员所从事的运动并不是特别有用。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-wMN58no3AypJnu5NN-2"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-wMN58no3AypJnu5NN-3"&gt;&lt;p&gt;我们注意到，事实回忆可能确实具有&lt;em&gt;一些&lt;/em&gt;相关的宏观特征，例如从标记中检测姓名的种族，以及启发哪些种族可能从事不同的运动。但该模型获得的性能明显优于我们对这些启发法的预期，因此出于实际目的，我们在讨论事实回忆时忽略它们。玩具模型的优点之一是我们可以确保此类混杂因素不存在。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-wMN58no3AypJnu5NN-3"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-wMN58no3AypJnu5NN-4"&gt;&lt;p&gt;因为，如果它们存在，我们可以使用这些相关的宏观特征来帮助进行事实查找（做出不同程度的成功的有根据的猜测），这意味着该任务将不再需要纯粹的记忆。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-wMN58no3AypJnu5NN-4"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-wMN58no3AypJnu5NN-5"&gt;&lt;p&gt;更准确地说，是微观特征的加权和，例如&lt;code&gt;3 * is_Michael_Jordan + 0.5 * is_George_Brett&lt;/code&gt; 。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-wMN58no3AypJnu5NN-5"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-wMN58no3AypJnu5NN-6"&gt;&lt;p&gt;我们注意到，有_un_可用但有用的宏观特征——“打篮球”在某种微不足道的意义上是一个对于预测运动员是否打篮球有用的宏观特征，就像“打篮球并且身高超过 6&amp;#39;8”这样的下游特征一样。出于此分析的目的，我们重点关注模型在进行查找时&lt;em&gt;可用的&lt;/em&gt;特征，排除查找标签下游的潜在宏观特征。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-wMN58no3AypJnu5NN-6"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-wMN58no3AypJnu5NN-7"&gt;&lt;p&gt;当然，许多事实回忆任务都达不到这种理想的特征：在参加琐事测验时做出“有根据的猜测”通常是有回报的，即使你不确定答案。我们将&lt;a href="https://www.alignmentforum.org/posts/JRcNNGJQ3xNfsxPj4/fact-finding-how-to-think-about-interpreting-memorisation#Memorisation_with_rules_of_thumb"&gt;进一步&lt;/a&gt;讨论这种“根据经验规则进行记忆”的任务。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-wMN58no3AypJnu5NN-7"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-wMN58no3AypJnu5NN-8"&gt;&lt;p&gt;我们将这些概念与统计物理学中的&lt;em&gt;微观状态&lt;/em&gt;和&lt;em&gt;宏观状态&lt;/em&gt;的概念进行类比：微观状态以高度精确的方式描述系统（例如指定气体中每个分子的位置和速度），而宏观状态则以高度精确的方式描述系统。容易测量的属性（例如压力、体积、温度），忽略细节。任何“宏观”问题，都应该只从宏观变量的角度来解决；微观细节应该不重要。这类似于概括的想法：任何两个在“重要的方式”（其宏观特征）方面相似的示例都应该进行类似的分类，而忽略“无关紧要的方式”（其微观特征）上的任何差异。在这个类比下，记忆问题正是那些关于系统的问题，只能通过对其微观状态的精确了解来回答。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-wMN58no3AypJnu5NN-8"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-wMN58no3AypJnu5NN-9"&gt;&lt;p&gt;这些并不是可以解决这个特定泛化问题的唯一宏观特征。如果您训练玩具神经网络来执行此分类任务，您会发现（取决于神经元数量或随机种子等超参数）有多种方法来划分空间（以粗粒度、概括的方式）以成功对这些进行分类点。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-wMN58no3AypJnu5NN-9"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-wMN58no3AypJnu5NN-10"&gt;&lt;p&gt;我们通过这个数据集肯定知道这一点，因为我们自己生成了它，通过随机为点分配颜色（这些点本身是从二元高斯分布中随机采样的）。因此，该数据集中唯一相关的特征是示例 ID 本身和输出标签。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-wMN58no3AypJnu5NN-10"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-wMN58no3AypJnu5NN-11"&gt;&lt;p&gt;这是&lt;em&gt;二元&lt;/em&gt;事实查找任务情况下的约束满足问题，但将此解释推广到多类或连续值事实查找任务是微不足道的。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-wMN58no3AypJnu5NN-11"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-wMN58no3AypJnu5NN-12"&gt;&lt;p&gt;对于任何实际的机器学习任务都可以这样做。例如，我们可以将手写数字分类问题限制为对 MNIST 训练集和测试集联合中找到的 70,000 个示例进行精确分类。 （或者，如果我们关心数据增强，我们可以将任务扩展为对组合 MNIST 数据集的 280,000 种可能的角落作物中的任何一种进行分类。）我们可以安排潜在输入集达到我们希望的大小，但仍然有限。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-wMN58no3AypJnu5NN-12"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-wMN58no3AypJnu5NN-13"&gt;&lt;p&gt;因为（根据定义）在纯粹的记忆任务中没有相关的宏观特征（因为如果有的话，那么模型就能够概括）。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-wMN58no3AypJnu5NN-13"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-wMN58no3AypJnu5NN-14"&gt;&lt;p&gt;还存在这样的查找算法解释的有用性问题。即使我们已经发现了如何完成查找的一些简单的结构（例如，它类似于装袋），也不清楚，如果没有有意义的中间表示，这可以帮助我们在机械可解释性的下游用途方面发挥什么作用。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-wMN58no3AypJnu5NN-14"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-wMN58no3AypJnu5NN-15"&gt;&lt;p&gt;因为如果模型经过明确训练以重现记忆数据集，我们已经准确地知道训练数据和模型输出之间的对应关系。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-wMN58no3AypJnu5NN-15"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-wMN58no3AypJnu5NN-16"&gt;&lt;p&gt;使用经验法则的记忆不应与具有任意不确定性的泛化任务相混淆。例如，左侧数据集也可以用来表示随机数据生成过程，其中点不一定是蓝色或红色，而是伯努利分布 - 即可能是蓝色或红色，具有一定的（依赖于输入的）概率。在这种情况下，完美的泛化算法应该输出每个簇内恒定的校准概率。然而，这里我们的意思是数据集中的蓝点确实是蓝色，红点确实是红色——即使它们看起来不合适——而且完美的性能对应于再现这些特质，就像描述的“复数这个单数名词”任务一样在正文的正文中。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-wMN58no3AypJnu5NN-16"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/section&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/JRcNNGJQ3xNfsxPj4/fact-finding-how-to-think-about-interpreting-memorisation#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sat, 23 Dec 2023 02:46:16 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/JRcNNGJQ3xNfsxPj4/fact-finding-how-to-think-about-interpreting-memorisation</guid></item><item><title>事实调查：尝试机械地理解早期 MLP（第 3 篇）</title><link>https://www.lesswrong.com/posts/CW5onXm6uZxpbpsRk/fact-finding-trying-to-mechanistically-understanding-early</link><description>发布于 2023 年 12 月 23 日凌晨 2:46（格林尼治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;&lt;em&gt;这是 Google DeepMind 机械可解释性团队对&lt;a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX"&gt;语言模型如何回忆事实的&lt;/a&gt;调查的第三篇文章。这篇文章的重点是从机制上理解早期 MLP 如何查找运动员姓名的标记并将其映射到他们的运动。这篇文章很杂乱，&lt;strong&gt;我们建议从&lt;a href="https://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall"&gt;第一篇文章&lt;/a&gt;开始&lt;/strong&gt;，然后根据与您最相关的内容略读并跳过其余的序列。阅读帖子 2 有帮助，但不是必需的。我们假设这篇文章的读者熟悉&lt;a href="https://www.neelnanda.io/mechanistic-interpretability/glossary#mechanistic-interpretability-techniques"&gt;本术语表中&lt;/a&gt;列出的机械解释技术。&lt;/em&gt;&lt;/p&gt;&lt;h2&gt;介绍&lt;/h2&gt;&lt;p&gt;正如上一篇文章中所讨论的，我们将两个令牌运动员姓名的事实回忆提炼成一个由 5 个 MLP 层（MLP 2 至 6）组成的&lt;strong&gt;有效模型&lt;/strong&gt;。这个有效模型的输入是与姓氏相对应的嵌入（通过嵌入和 MLP0）和与名字相对应的嵌入（通过第 0 层和第 1 层中关注前一个标记的注意力头）的总和。有效模型的输出是运动员所从事的运动（（美式）橄榄球、棒球或篮球）的 3 维线性表示。我们强调，这个 5 层 MLP 模型不仅能够高精度地回忆事实（在过滤数据集上为 86%），而且它是从预训练的语言模型中提取的，而不是从头开始训练的。&lt;/p&gt;&lt;p&gt;我们在这篇文章中的目标是对这个有效模型的工作原理进行逆向工程。我认为我们在这个目标的雄心勃勃的版本上大多失败了，尽管我相信我们已经在为什么这很难的问题上取得了一些概念上的进展，证伪了一些简单的天真的假设，并且对正在发生的事情不再那么困惑。我们在&lt;a href="https://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall#Is_it_surprising_that_we_didn_t_get_much_traction_"&gt;第 1 篇文章&lt;/a&gt;和&lt;a href="https://www.alignmentforum.org/posts/JRcNNGJQ3xNfsxPj4/fact-finding-how-to-think-about-interpreting-memorisation"&gt;第 4 篇&lt;/a&gt;文章中讨论了我们对为什么这很难的理解，在这篇文章中，我们重点关注我们对可能发生的情况的假设，以及我们收集的支持和反对的证据。&lt;/p&gt;&lt;h2&gt;假设&lt;/h2&gt;&lt;p&gt;回想一下，我们的 MLP 模型中 5 个 MLP 层的作用是将求和的原始标记映射到所进行运动的线性表示。从数学上讲，这是一个查找表，其中每个条目都是生成属性的原始标记上的布尔 AND。我们期望它&lt;em&gt;以某种方式&lt;/em&gt;涉及非线性来实现 AND，因为这种查找是非线性的，例如模型想要知道“Michael Jordan”和“Tim Duncan”打篮球，但不一定认为“Michael Duncan”打篮球。&lt;/p&gt;&lt;p&gt;我们探索了两个假设，&lt;strong&gt;单步去标记化&lt;/strong&gt;以及&lt;strong&gt;哈希和查找&lt;/strong&gt;。&lt;/p&gt;&lt;h3&gt;单步去代币化&lt;/h3&gt;&lt;p&gt;直观上，执行 AND 的最简单方法是使用单个神经元，例如 ReLU(is_michael + is_jordan - 1) 实际上是一个 AND 门。每个运动员的单个神经元不会产生任何叠加，因此我们采用稍微复杂一点的版本：假设有一堆单独的神经元，每个神经元都独立地使用其 GELU 激活实现 AND &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-KGJJcrC8izPbNFCPL-1" id="fnref-KGJJcrC8izPbNFCPL-1"&gt;[1]&lt;/a&gt;&lt;/sup&gt; ，映射运动员名字的原始标记到有关该运动员的每个已知事实的线性表示。细微差别：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;这使用了叠加，让每个神经元为许多运动员激发，并且每个运动员都有许多查找神经元。神经元输出建设性地干扰正确的事实，但不会进行超出此范围的交互。&lt;ul&gt;&lt;li&gt;这是一个具体的、机械的故事。每个神经元都有一组为其激发的运动员，并且对该组进行 AND 的并集 - 例如，如果一个神经元为迈克尔乔丹和蒂姆邓肯激发，它会实现（迈克尔或蒂姆）AND（邓肯或乔丹）。这引入了噪声，例如它也会为蒂姆·乔丹（Tim Jordan）开火（它&lt;em&gt;想做&lt;/em&gt;（迈克尔和乔丹）或（蒂姆和邓肯），但这很难用单个神经元实现）。它也很吵闹，因为它必须同时宣传迈克尔·乔丹的事实和蒂姆·邓肯的事实。但由于每个神经元都会针对不同的子集进行激发，因此对正确答案会产生建设性干扰，并且噪音会被消除。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;这预示着相同的神经元对于运动员的每一个已知事实都同样重要&lt;/li&gt;&lt;li&gt;该假设的一个重要部分是每个神经元直接从输入标记中读取并直接贡献于输出事实。理论上，这可以通过单个 MLP 层而不是 5 个层来实现。它预测神经元直接与输入标记组合，计算中没有中间项，并且 MLP 层之间没有有意义的组合。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;哈希和查找&lt;/h3&gt;&lt;p&gt;我们模型的输入具有相当不理想的格式 - 它是每个组成标记的线性和，但这在进行事实查找时可能会产生很大的误导！迈克尔·乔丹和迈克尔·史密斯同名这一事实并不表明他们从事同一运动的可能性更大。哈希和查找假设是，模型首先生成一个打破输入线性结构的中间表示，一个与其他所有哈希表示接近正交的&lt;strong&gt;哈希表示&lt;/strong&gt;&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-KGJJcrC8izPbNFCPL-2" id="fnref-KGJJcrC8izPbNFCPL-2"&gt;[2]&lt;/a&gt;&lt;/sup&gt; （即使它们共享一些但不是全部标记），然后后面的层&lt;strong&gt;查找&lt;/strong&gt;这个散列表示并将其映射到正确的属性。细微差别：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;从某种意义上说，“困难”的部分是查找。查找是存储实际事实知识的地方，而随机初始化的 MLP 应该适合散列，因为目标只是淹没现有结构。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;为什么散列是必要的？ MLP 是非线性的，因此也许它们可以忽略线性结构，而不需要明确地破坏它。这里的一个直觉来自最简单的查找：有一个“棒球神经元”，其输出增强棒球方向，其输入权重是每个棒球运动员的串联令牌表示的总和&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-KGJJcrC8izPbNFCPL-3" id="fnref-KGJJcrC8izPbNFCPL-3"&gt;[3]&lt;/a&gt;&lt;/sup&gt; - 如果运动员表示是（大约）正交，然后给定一个运动员，这只对棒球运动员起作用。但如果它同时对迈克尔·乔丹和蒂姆·邓肯开火，那么它必须至少对蒂姆·乔丹或迈克尔·邓肯之一开火——这是不可取的！然而，如果它的输入权重是&lt;em&gt;散列&lt;/em&gt;运动员表示的总和，则这成为可能！&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;散列对于已知的标记字符串（例如名人姓名）和未知的字符串（例如未知的姓名）应该同样有效。查找是实际知识融入的地方&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;关于迈克尔·乔丹的不同已知事实的查找电路没有理由应该对应于相同的神经元。从概念上讲，可能有一个“打篮球”神经元对任何散列篮球运动员激发，以及一个单独的“为芝加哥球队效力”神经元对芝加哥球员的散列激发。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;这微弱地预测了哈希层和查找层之间的完全分离&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这两个假设都是故意以一种强有力的形式提出的，可以做出真实的预测——语言模型是混乱的和被诅咒的，我们实际上并没有期望这是完全正确的。但我们认为这些说法似乎有一定道理。在实践中，我们发现单步去标记化似乎显然是错误的，而哈希和查找在强形式下似乎是错误的，但可能有一些道理。我们发现考虑哈希和查找对于了解正在发生的事情非常有效。&lt;/p&gt;&lt;h2&gt;证伪单步去代币化假说&lt;/h2&gt;&lt;p&gt;单步去标记化是我们能想到的最简单的假设，它仍然涉及显着的叠加，因此可以做出一些相当有力的预测。我们针对这些设计了一系列实验，并广泛发现我们伪造了它做出的多个强有力的预测。&lt;/p&gt;&lt;h3&gt; MLP 之间存在显着的组成&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;预测&lt;/strong&gt;：MLP 2 到 6 之间没有中间组合，它们都是并行作用的。因为每个重要的神经元都被预测为直接将原始标记映射到输出。正如后面所讨论的，缺乏组合是该假设的有力证据，组合的存在是反对该假设的弱证据。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;实验&lt;/strong&gt;：我们的意思是消除&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-KGJJcrC8izPbNFCPL-4" id="fnref-KGJJcrC8izPbNFCPL-4"&gt;[4]&lt;/a&gt;&lt;/sup&gt;每对 MLP 层之间的路径，并查看对几个指标的影响：头部探测精度（在第 6 层残差上）、完整模型精度和损失（在完整词汇上）、仅限于运动的完整模型 Logits 精度以及完整模型与原始 Logits 的 KL 散度。通过平均消融路径，我们仅破坏 MLP 间的组合，而不破坏与下游属性提取头的组合。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;结果&lt;/strong&gt;：我们发现性能显着下降，尤其是从 MLP2 开始的路径，表明存在一些中间产品。请注意，损失和 KL 散度如果低（绿色和紫色）​​则良好，如果高（蓝色、红色和橙色）则准确度良好。进一步注意，与仅在超过阈值时改变的“硬”指标（如准确率）相比，“软”指标（如损失和 KL 散度）显示出更强的变化。正如&lt;a href="https://arxiv.org/abs/2309.16042"&gt;Zhang等人&lt;/a&gt;所指出的，这是预料之中的，当电路由多个元件组成时，所有元件都贡献于共享输出，烧蚀单个元件很少足以跨越阈值，但足以破坏较软的指标，从而造成损失和损失。 KL 散度是衡量重要性的更可靠的方法。 &lt;/p&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/kerrtb9jty2imurotmel" /&gt;&lt;/p&gt;&lt;p&gt; **细微差别：**请注意，这仅伪造了单步去标记化的最简单形式。与这些结果一致的单步去标记化假设&lt;em&gt;的&lt;/em&gt;一个扩展是，它不是 MLP 2 不做与 MLP 3 到 6 相关的任何事情，而是充当标记嵌入的&lt;em&gt;固定&lt;/em&gt;变换（例如，它总是将 MLP 2 的嵌入加倍）。姓）。如果 MLP 3 想要访问原始令牌，它期望 MLP 2 的固定效果，因此会考虑原始令牌嵌入加上 MLP 2 的固定转换。这会因平均消融而受损，但不涉及有意义的合成。&lt;/p&gt;&lt;h3&gt;多个事实之间不共享神经元&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;预测&lt;/strong&gt;：当模型知道有关某个实体的多个事实时，相同的神经元对于预测每个事实非常重要，而不是每个事实的不同神经元。这是因为查找信息的机制是通过对名称的标记执行布尔 AND 操作，该名称对于每个已知事实都是相同的，因此没有理由将它们分开。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;实验&lt;/strong&gt;：收集模型了解的有关运动员的替代事实的大量数据很困难，因此我们放大了某个特定运动员（迈克尔·乔丹）并发现了模型了解的有关他的 9 个事实&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-KGJJcrC8izPbNFCPL-5" id="fnref-KGJJcrC8izPbNFCPL-5"&gt;[5]&lt;/a&gt;&lt;/sup&gt; 。然后，我们一次对 Jordan 令牌上的 MLP 2-6 中的每个神经元进行消融&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-KGJJcrC8izPbNFCPL-6" id="fnref-KGJJcrC8izPbNFCPL-6"&gt;[6]&lt;/a&gt;&lt;/sup&gt; ，并查看每项运动的正确对数概率的变化。对于每对事实 A 和 B，我们然后查看每个给定神经元对 A 的正确对数概率和 B 的正确对数概率的影响之间的&lt;strong&gt;相关性&lt;/strong&gt;。如果每个神经元对于同一运动员的每个已知事实同样重要，那么相关性应该很高。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;结果&lt;/strong&gt;：非常低。唯一具有中等相关性的一对事实是 NBA 选秀年（1984 年）和美国奥运会年（1992 年），我怀疑这是因为它们都是年份，尽管我不会提前预测到这一点，也没有很棒的故事，说明了原因。 &lt;/p&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/zsa7a3aqscgpfiol8ogh" /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;细微差别&lt;/strong&gt;：这似乎证伪了单步去标记化假设的强形式 - 至少，即使存在去标记化神经元，它们也会输出迈克尔·乔丹事实的子集，而不是一次性全部输出。&lt;/p&gt;&lt;p&gt;一个争论是，消融单个神经元有点难以推理，而且似乎有一些紧密耦合的处理（如微妙的自我修复）使得解释这些结果变得更加困难。但在简单的单步去标记化假设下，我们&lt;em&gt;应该&lt;/em&gt;能够独立地消融和推理神经元。另一个问题是相关系数是汇总统计数据，可能隐藏了一些结构，但检查散点图同样显示出似乎没有关系。&lt;/p&gt;&lt;h3&gt;对属性有直接影响的神经元不执行“与”运算&lt;/h3&gt;&lt;p&gt;&lt;em&gt;注意：这个实验相当复杂（尽管我们认为概念上很优雅且有趣），请随意跳过&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;预测&lt;/strong&gt;：直接与属性提取头组成的神经元通过其 GELU 激活对原始标记（在运动员的某些子集上）执行 AND 运算。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;实验&lt;/strong&gt;：我们使用称为非线性过剩&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-KGJJcrC8izPbNFCPL-7" id="fnref-KGJJcrC8izPbNFCPL-7"&gt;[7]&lt;/a&gt;&lt;/sup&gt;的度量来测量模型中标量实现 AND 的程度。具体来说，如果一个神经元在 prev=Michael 和 curr=Jordan 上执行 AND，那么它应该比 Michael Smith 或 Keith Jordan 更多地激活 Michael Jordan。形式上，给定两个二元变量 A (Prev=Michael) 和 B(Curr=Jordan)，我们将非线性超额定义为 E(A &amp;amp; B) - E(~A &amp;amp; B) - E(A &amp;amp; ~B) + E(~A &amp;amp; ~B) &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-KGJJcrC8izPbNFCPL-8" id="fnref-KGJJcrC8izPbNFCPL-8"&gt;[8]&lt;/a&gt;&lt;/sup&gt; 。重要的是，如果神经元在两个标记中是线性的，则该度量为零，如果是 AND，则该度量为正 (1 - 0 - 0 + 0 = 1)，如果是 OR，则该度量为负 (1 - 1 - 1 + 0 = -1)。&lt;/p&gt;&lt;p&gt;对于我们的具体实验：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;我们对每个神经元计算 GELU 前后非线性过剩的&lt;em&gt;变化&lt;/em&gt;&lt;ul&gt;&lt;li&gt;在 GELU 上进行改变的要点是，这区分了信号增强预先计算的 AND 的神经元和计算 AND 本身的神经元。&lt;/li&gt;&lt;li&gt;为了计算非线性超额，我们通过汇集 2 个代币运动员（每个约 100 个）中的所有名字和姓氏来计算平均值，并查看每个名称组合。 （这大约有 10,000 个 ~A 和 ~B 的名字，大约 100 个 ~A &amp;amp; B 或 A &amp;amp; ~B 的名字，只有一个 A &amp;amp; B 的名字——原始运动员的名字！）&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;过滤出这种变化为正的神经元（并将 GELU 前的多余部分限制为最小零）&lt;ul&gt;&lt;li&gt;我们发现了一堆神经元，其中前 GELU 具有负非线性过剩，而 GELU 将所有内容设置为接近零。我们倾向于不计算这些。&lt;/li&gt;&lt;li&gt;我们为每个运动员执行单独的过滤步骤，因为每个运动员都有不同的非线性超额&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;乘以神经元对属性提取头 L16H20 的基于权重的直接影响，并将其相加。&lt;ul&gt;&lt;li&gt;如果您只允许每个 GELU 的 AND 直接影响头 L16H20，而不是也允许中间组合，这就是 MLP 2 到 6 的效果&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;我们将其与探针上的总非线性过量效应（即通过头 L16H20 的直接效应）进行比较，以查看来自 AND 通过 GELU&lt;em&gt;并&lt;/em&gt;直接传达到基于头的探针的分数&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/hdxmyxamxcomqq0fqaxk" /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;结果&lt;/strong&gt;：当观察上面的散点图时，很明显它远离 x=y 线，即 GELU 的非线性超额通常显着小于总非线性超额，尽管它们是相关的。中位比例约为23% &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-KGJJcrC8izPbNFCPL-9" id="fnref-KGJJcrC8izPbNFCPL-9"&gt;[9]&lt;/a&gt;&lt;/sup&gt; 。我们认为这是反对单步去标记化假设的有力证据，因为它表明许多对 L16H20 有显着直接影响的神经元正在与已经计算出 AND 的早期 MLP 组合，即计算中有一个有意义的中间步骤。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;细微差别&lt;/strong&gt;：这个实验涉及到差异的差异。我认为它在概念上是合理的并且相当优雅，但我对过于复杂的实验普遍持怀疑态度，并且不想过于依赖他们的结果。我们在如何设置这些实验、如何聚合和分析它们、如何过滤掉神经元等方面反复讨论，并且有很多主观选择，尽管有趣的是结果对这些是稳健的。&lt;/p&gt;&lt;p&gt;将 GELU 之前的多余部分限制为零似乎是不合理的，例如，因为模型可能使用 GELU 的负数部分来实现 AND（Michael Smith 和 Keith Jordan 在 GELU 后&amp;lt;0，Michael Jordan 在 GELU 后为零），尽管尝试解释这一点并没有让我们接近 1。&lt;/p&gt;&lt;p&gt; MLP 2 到 6 中的一些神经元对现有的线性表示的事实信息进行信号增强（例如下一节中讨论的棒球神经元），这些神经元应该无法满足此度量标准（它们是计算 AND 的早期神经元的信号增强！ ）。&lt;/p&gt;&lt;h2&gt;棒球神经元 (L5N6045)&lt;/h2&gt;&lt;h3&gt;有一个棒球神经元！&lt;/h3&gt;&lt;p&gt;一个有趣的发现是，尽管整体计算相当分散和叠加，但仍然存在一些有意义的单个神经元！最值得注意的是棒球神经元 L5N6045，它对棒球运动员的系统性激活比对非棒球运动员的激活更多。作为棒球与非棒球运动员的二元探针，它的 ROC AUC 为 89.3%。 &lt;/p&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/z4lgnpzcuyzlngtepv16" /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;因果效应&lt;/strong&gt;：此外，它与属性提取头组成，具有显着的因果效应。它通过 L16H20 直接与 logits 组合以增强棒球（并抑制足球），如果我们的意思是消融它，那么棒球运动员的完整模型损失从 0.167 增加到 0.284（零消融时为 0.559）&lt;/p&gt;&lt;h3&gt;不仅仅是信号增强&lt;/h3&gt;&lt;p&gt;我们发现神经元的输入权重具有非平凡的余弦模拟，其输出权重为 (0.456)，通过头 L16H20 (0.22) 提升棒球 logit 的方向，以及通过头 L16H20 (0.184) 相对于其他运动提升棒球的方向这表明棒球神经元的部分功能是增强运动员打棒球的现有知识。&lt;/p&gt;&lt;p&gt;但这不是唯一的角色！如果我们采用与这 3 个方向跨越的子空间正交的输入权重分量，并将残差流投影到该方向上，则在预测运动员是否打棒球时，所得部分消融神经元的 ROC AUC (83%) （较之前的 88.7% 略有下降）。&lt;/p&gt;&lt;h3&gt;这不是单义的&lt;/h3&gt;&lt;p&gt;一个好奇心是它是否是单一语义的并且在完整的数据分布上代表棒球。尽管我们没有进行详细调查，但这似乎很可能是错误的。在谷歌新闻数据集上，它在类似棒球的环境中系统地激活（也对板球等特定其他运动有所帮助），但在维基百科上，它在一些看似不相关的事物上激活，例如“外部链接” &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-KGJJcrC8izPbNFCPL-10" id="fnref-KGJJcrC8izPbNFCPL-10"&gt;[10]&lt;/a&gt;&lt;/sup&gt;中的外部和“目标” “足球|进球|守门”&lt;/p&gt;&lt;h2&gt;哈希和查找证据&lt;/h2&gt;&lt;h3&gt;动机&lt;/h3&gt;&lt;p&gt;&lt;a href="https://www.alignmentforum.org/posts/CW5onXm6uZxpbpsRk/fact-finding-trying-to-mechanistically-understanding-early#Hash_and_Lookup"&gt;如上所述&lt;/a&gt;，散列和查找假设是 MLP 2 到 6 分为两个不同的阶段：第一个&lt;strong&gt;散列&lt;/strong&gt;，旨在通过形成非线性表示来打破名称的串联（求和）标记的线性结构尝试与每个其他子字符串正交，然后&lt;strong&gt;查找&lt;/strong&gt;将棒球运动员的哈希表示映射到棒球，将足球映射到橄榄球等。&lt;/p&gt;&lt;p&gt;从概念上讲，我们实际上并没有期望这种强形式是正确的：它意味着哈希层实际上独立于数据分布，这将是令人惊讶的 - 如果我们采用哈希和查找的实现并应用通过梯度下降的几个步骤，它可能希望使已知实体的哈希值更加突出并且与其他所有内容更加正交。但我们希望测试这个假设能够教会我们有关该模型的有用信息，并认为它可能部分正确。我们将&lt;strong&gt;部分散列和查找假设宽松&lt;/strong&gt;地称为该机制主要是散列和查找的假设，但早期的散列层包含一些有关运动的（线性可恢复的）信息，这些信息通过后来的查找层得到显着加强。我们的证据广泛支持这一假设，但不幸的是，它很难被证伪。&lt;/p&gt;&lt;p&gt;这是由于看到单步去标记化假设的失败：看起来相当清楚，MLP 间的组合正在进行，有中间项，并且有一些显式查找（棒球神经元）。这似乎是最简单的假设，它解释了为什么模型需要中间项并涉及实际有目的的组合 - 令牌的线性结构是不可取的！&lt;/p&gt;&lt;h3&gt;中间层具有线性可恢复的运动信息（负）&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;预测&lt;/strong&gt;：经过训练以在哈希层期间的残余流上检测运动员运动的线性探针不会比随机探针更好。它只会在查找层期间变得良好。我们不知道哪些层是哈希层还是查找层，但这预示着一个急剧的转变。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;实验&lt;/strong&gt;：在我们的有效模型中采用两个令牌运动员姓名，在每层之后获取残差流，在包含 80% 姓名的训练集上训练逻辑回归探针，并对另外 20% 的姓名进行评估。该假设预测验证准确性将会发生急剧变化。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;结果&lt;/strong&gt;：这是一个适度平滑的变化。为了鲁棒性，我们还检查有效模型预测下一项运动时的损失指标。当在完整模型中的最终名称标记上对残差流训练逻辑回归探针时，我们得到了类似的结果。这相当直接地反驳了早期层正在执行纯粹的、与数据无关的哈希的假设。然而，第 4 层和第 5 层之间存在显着增加，这表明查找存在一些专门化（这部分但不完全由第 5 层中的棒球神经元驱动）。对于每一层，我们报告 10 个随机种子的测试准确性（每次采用不同的 80/20 训练/测试分割并训练新的探针），因为数据集足够小，使其相当嘈杂。 &lt;/p&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/b5xo66fwzwxmue0b1ytb" /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;细微差别&lt;/strong&gt;：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;一些运动员的名字中可能有独特的标记，以便在嵌入中表示运动信息。我们可以看到，求和令牌的验证准确性优于随机令牌（50% 而不是 33%）。这并不奇怪，我们预计哈希和查找对其他运动员来说更重要。&lt;/li&gt;&lt;li&gt;这与部分哈希查找假设是一致的，尤其是在第 5 层中准确率显着提高。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;已知名称比未知名称具有更高的 MLP 输出范数（负）&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;预测&lt;/strong&gt;：散列预测早期层不会吸收数据分布的知识，因此应该对已知名称和未知名称进行区分。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;实验&lt;/strong&gt;：我们测量已知名称和未知名称的 MLP 输出范数。为了获取姓名，我们对运动员数据集中所有单个标记的名字和姓氏进行笛卡尔积，并分离已知和未知的名字&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-KGJJcrC8izPbNFCPL-11" id="fnref-KGJJcrC8izPbNFCPL-11"&gt;[11]&lt;/a&gt;&lt;/sup&gt; 。此分析是在完整模型上执行的（但在有效模型上类似）&lt;/p&gt;&lt;p&gt;&lt;strong&gt;结果&lt;/strong&gt;：存在明显差异，已知名称具有更高的范数。这伪造了纯哈希，但不是部分哈希。即使在 MLP1 中也会发生这种情况，尽管 MLP1 不是我们有效模型的一部分&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-KGJJcrC8izPbNFCPL-12" id="fnref-KGJJcrC8izPbNFCPL-12"&gt;[12]&lt;/a&gt;&lt;/sup&gt; ，这令人惊讶。 &lt;/p&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/mumetzm0fidzfvdcpv6p" /&gt;&lt;/p&gt;&lt;h3&gt;早期层确实破坏了线性结构（正面）&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;预测&lt;/strong&gt;：早期的层破坏了线性结构。具体来说，即使残差流输入中存在线性结构，即它是来自不同特征（当前和先前标记）的项之和，MLP输出也不会具有这种线性结构。更弱的是，它预测一旦重新添加 MLP 输出，残差流将失去这种线性结构。&lt;/p&gt;&lt;p&gt;线性函数 f 的一个具体属性是 f(Michael Jordan) + f(Tim Duncan) = f(Michael Duncan) + f(Tim Jordan)，所以让我们尝试证伪这个！&lt;/p&gt;&lt;p&gt;&lt;strong&gt;实验&lt;/strong&gt;：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;我们选择一对已知的名字 A 和 B（例如 Michael Jordan 和 Tim Duncan）以及有效模型中的 MLP 层（例如 MLP 2）。&lt;ul&gt;&lt;li&gt;我们取这些名称的 MLP 输出的中点 (MLP(A) + MLP(B)) /2。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;我们交换姓氏以获得名字 C 和 D（未知名字，例如 Michael Duncan 和 Tim Jordan），并取 C 和 D 上 MLP 输出的中点 (MLP(C) + MLP(D)) /2。&lt;/li&gt;&lt;li&gt;我们测量两个中点之间的距离。&lt;/li&gt;&lt;li&gt;为了将大数与小数联系起来，我们除以基线距离，该距离是通过用任意未知名称替换 C 和 D 并测量中点之间的距离 |((MLP(A) + MLP(B) - MLP(C) &amp;#39;) - MLP(D&amp;#39;))/2|&lt;ul&gt;&lt;li&gt;这意味着，如果 MLP 完全打破线性结构，它将接近 1（即 Michael Duncan 和 Tim Jordan 与随机未知名字无法区分），而如果它保留线性结构，它将接近 0（因为这些将是平行四边形的四个顶点）&lt;ul&gt;&lt;li&gt;具体来说，如果 MLP 是线性的，则 MLP(Michael Jordan) = MLP(Michael X) + MLP(Y Jordan)，因此 A&amp;amp;B 和 C&amp;amp;D 的中点应该相同&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/efy2z0gox96g5zqsbtvb" /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;结果&lt;/strong&gt;：大多数 MLP 层显示线性结构被显着（但未完全）破坏，往往是完全破坏线性结构的 60%-70%。 MLP2 的情况比 MLP3 到 6 的情况稍微不那么明显。&lt;/p&gt;&lt;p&gt;我们对不同层之后的残差流（而不是 MLP 输出）重复上述实验，绘制在同一张图（红色框）中，发现残差在各层之间的线性度降低，从第 2 层之后的约 30% 开始第 6 层后变为 50%（这是该层&lt;em&gt;末尾&lt;/em&gt;的残差）。请注意，这些残差取自有效模型，该模型从第 2 层开始，而不是第 0 层。进一步注意，在有效模型中，MLP2 的输入是名称的总和标记，根据定义，它是线性的。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;细微差别&lt;/strong&gt;：&lt;/p&gt;&lt;ul&gt;&lt;li&gt; MLP 输出的结果并不令人惊讶——MLP 的全部意义就是成为一个非线性函数，所以它当然打破了线性结构！&lt;ul&gt;&lt;li&gt;我们应该期望这个结果对于随机初始化的 MLP 来说是正确的&lt;/li&gt;&lt;li&gt;然而，事实证明，随机初始化的 MLP 对线性结构的破坏要少得多 (20-40%)！我们做了一个后续实验，随机调整 MLP 权重和偏差并重新运行模型。作为另一个基线，我们重新进行了交换未知姓名的名字/姓氏的实验，没有看到明显的变化。这表明模型有意使用 MLP 层来打破线性结构。 &lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/eetxlacpkfnhlmkbxmmt" /&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;它打破残差流中的线性结构的结果不那么微不足道，但仍然不足为奇 - new_residual 是 old_residual（线性）+ mlp_out（非线性），因此 new_residual 的线性程度直观上取决于相对大小。&lt;/li&gt;&lt;li&gt;总体而言，这是散列（在打破线性结构的意义上）发生的不足为奇的证据，但不是散列是它们所做的&lt;em&gt;全部的&lt;/em&gt;证据，因此它不是完整散列和查找假设的有力证据（其中散列是早期 MLP 在电路中发挥的唯一作用）&lt;/li&gt;&lt;li&gt;尽管在概念上并不令人惊讶，但我们认为“早期 MLP&lt;em&gt;通常&lt;/em&gt;会打破线性结构”是了解模型的一个有价值的事实，因为它表明线性表示的特征之间的干扰会随着深度的增加而累积。&lt;ul&gt;&lt;li&gt;例如，Bricken 等人观察到许多稀疏自动编码器特征，例如“数学文本中的标记‘the’”。如果“is a math text”和“is the token &amp;#39;the&amp;#39;”都是线性表示的特征，那么 MLP 层表示交集也就不足为奇了，即使没有对该特征进行实际计算。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;棒球神经元作用于运动员残差的正交余数（不明确）&lt;/h3&gt;&lt;p&gt;&lt;em&gt;元：本节记录了我们中的一个人最初感到兴奋的实验，但后来意识到可能是虚幻的，我们在这里描述它是为了教学目的&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;预测&lt;/strong&gt;：如果发生查找，这表明每个运动员的表示都有&lt;em&gt;特殊&lt;/em&gt;信息 - 迈克尔·乔丹残差中有一些“是迈克尔·乔丹”信息，这对于最终生成“打篮球”的模型很重要，无法从其他篮球中恢复玩家。请注意，这显然是在对原始标记求和时发生的，但稍后可能不会发生。我们关注&lt;a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX/p/CW5onXm6uZxpbpsRk#The_Baseball_Neuron__L5N6045_"&gt;第 5 层的棒球神经元，&lt;/a&gt;它似乎是查找电路的一部分，因为它具有显着的效果并直接增强棒球属性。&lt;/p&gt;&lt;p&gt;对比假设是，早期层（例如 2-4）会产生棒球比赛中“打棒球”的某种表示（可能与最终表示不同），而棒球神经元只是发出信号来增强这种表示。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;实验&lt;/strong&gt;：为了测试这一点，我们采用了每个运动员的残差，并采用与所有其他运动员残差所跨越的子空间正交的分量（请注意，有 2560 个残差维度和大约 1500 个其他运动员，因此这删除了 ​​60% 的维度）。然后，我们将棒球神经元应用于该残差正交分量，并查看神经元输出的 ROC AUC，以预测运动员是否打棒球的二元变量&lt;/p&gt;&lt;p&gt;&lt;strong&gt;结果&lt;/strong&gt;：ROC 约为 60%，明显高于机会 (50%) - 明显比没有正交投影时差，但仍然有一些信号&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Nuance&lt;/strong&gt; ：这被证明是虚幻的，因为“与所有其他运动员正交的项目”并不一定会删除与其他运动员共享的&lt;em&gt;所有&lt;/em&gt;信息。玩具示例：假设每个棒球运动员残差都是“是棒球”方向加上显着的高斯噪声。如果我们从该分布中获取由 1500 个样本组成的子空间，由于每个样本都有噪声，因此该子空间中可能无法完全捕获“是棒球”方向，因此投影不会擦除它。这意味着，虽然我&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-KGJJcrC8izPbNFCPL-13" id="fnref-KGJJcrC8izPbNFCPL-13"&gt;[13]&lt;/a&gt;&lt;/sup&gt;发现这个实验的结果令人惊讶，但它并没有很好地区分这两个假设——部分散列和查找确实很难被证伪！ &lt;/p&gt;&lt;hr class="footnotes-sep" /&gt;&lt;section class="footnotes"&gt;&lt;ol class="footnotes-list"&gt;&lt;li class="footnote-item" id="fn-KGJJcrC8izPbNFCPL-1"&gt;&lt;p&gt; GELU 与 ReLU 不同，但我们认为可以有效地将其视为“软 ReLU”，并且与 ReLU 相当接近，因此也可以相当好地实现 AND 门&lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-KGJJcrC8izPbNFCPL-1"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-KGJJcrC8izPbNFCPL-2"&gt;&lt;p&gt;注意，这是哈希函数语句中的哈希，而不是哈希表。哈希函数接受任意输入并尝试产生与随机输出没有区别的输出。哈希表将哈希函数应用于输入，&lt;em&gt;然后&lt;/em&gt;有目的地将其映射到某些存储的数据，这更类似于完整哈希和查找。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-KGJJcrC8izPbNFCPL-2"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-KGJJcrC8izPbNFCPL-3"&gt;&lt;p&gt;请注意，可能存在更复杂且基础对齐程度较低的查找形式，这些形式可能不太容易受到干扰，事实上，我们发现有迹象表明这个故事是混乱和复杂的&lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-KGJJcrC8izPbNFCPL-3"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-KGJJcrC8izPbNFCPL-4"&gt;&lt;p&gt;另一位运动员的重新采样消融得到了类似的结果&lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-KGJJcrC8izPbNFCPL-4"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-KGJJcrC8izPbNFCPL-5"&gt;&lt;p&gt;我们测量每个答案的第一个标记的对数概率，对于多标记答案，延续位于括号中，并且没有明确测试（一旦有了第一个标记，就可以很容易地使用二元组）&lt;/p&gt;&lt;ul&gt;&lt;li&gt;打篮球运动&lt;/li&gt;&lt;li&gt;在北州（卡罗来纳州）上大学&lt;/li&gt;&lt;li&gt;1984年被选入NBA&lt;/li&gt;&lt;li&gt;为芝加哥队（公牛队）效力&lt;/li&gt;&lt;li&gt;是夏洛特队（黄蜂队）的大股东&lt;/li&gt;&lt;li&gt;主演电影《太空（果酱）》&lt;/li&gt;&lt;li&gt;为 NBA 联盟效力&lt;/li&gt;&lt;li&gt;1992年代表美国参加奥运会&lt;/li&gt;&lt;li&gt;玩数字23&lt;/li&gt;&lt;/ul&gt; &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-KGJJcrC8izPbNFCPL-5"&gt;↩︎&lt;/a&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-KGJJcrC8izPbNFCPL-6"&gt;&lt;p&gt;将神经元的值替换为数据集中所有 1500 名运动员的最终名称标记的平均值。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-KGJJcrC8izPbNFCPL-6"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-KGJJcrC8izPbNFCPL-7"&gt;&lt;p&gt;受到 Lovis Heindrich 和 Lucia Quirke 即将推出的作品的启发&lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-KGJJcrC8izPbNFCPL-7"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-KGJJcrC8izPbNFCPL-8"&gt;&lt;p&gt;动机：E(A &amp;amp; B) 对应于 Michael Jordan 上的激活，E(~A &amp;amp; B) 对应于 Keith Jordan（对于 Keith 的不同值），E(A &amp;amp; ~B) 对应于 Michael Smith（对于不同的值）史密斯）。神经元的激活通常具有远离零的平均值，因此我们从每个项中减去该平均值，该平均值由 E(~A &amp;amp; ~B) 项捕获，即除迈克尔或乔丹之外的所有名字的平均值。并且 (E(A &amp;amp; B) - E(~A &amp;amp; ~B)) - (E(~A &amp;amp; B) - E(~A &amp;amp; ~B)) - (E(A &amp;amp; ~B) - E(~ A &amp;amp; ~B)) = E(A &amp;amp; B) - E(~A &amp;amp; B) - E(A &amp;amp; ~B) + E(~A &amp;amp; ~B) &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-KGJJcrC8izPbNFCPL-8"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-KGJJcrC8izPbNFCPL-9"&gt;&lt;p&gt;我们采用中位数是因为有时总的或 GELU 派生的非线性效应是负/零，而中位数让我们可以忽略这些异常值&lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-KGJJcrC8izPbNFCPL-9"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-KGJJcrC8izPbNFCPL-10"&gt;&lt;p&gt;虽然我们无法轻易判断出这是哪篇文章，但也许​​是棒球相关的文章，表现出类似&lt;a href="https://arxiv.org/abs/2310.15154"&gt;总结主题的&lt;/a&gt;东西！ &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-KGJJcrC8izPbNFCPL-10"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-KGJJcrC8izPbNFCPL-11"&gt;&lt;p&gt;将会出现一些错误分类，因为某些名称配对可能是已知实体。我们通过谷歌搜索名称进行了一些抽查，预计这不会对结果产生重大影响&lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-KGJJcrC8izPbNFCPL-11"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-KGJJcrC8izPbNFCPL-12"&gt;&lt;p&gt;我们发现 MLP1 似乎与属性提取头的注意力相关（让它们检测姓名是否是运动员，从而是否提取一项运动），但对于查找运动员从事哪项运动并不重要。即对键重要但对值不重要。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-KGJJcrC8izPbNFCPL-12"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-KGJJcrC8izPbNFCPL-13"&gt;&lt;p&gt;使用单数是因为我的合著者认为这种替代解释一直是显而易见的&lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-KGJJcrC8izPbNFCPL-13"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/section&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/CW5onXm6uZxpbpsRk/fact-finding-trying-to-mechanistically-understanding-early#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sat, 23 Dec 2023 02:46:05 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/CW5onXm6uZxpbpsRk/fact-finding-trying-to-mechanistically-understanding-early</guid></item><item><title>事实调查：简化电路（第 2 篇）</title><link>https://www.lesswrong.com/posts/3tqJ65kuTkBh8wrRH/fact-finding-simplifying-the-circuit-post-2</link><description>发布于 2023 年 12 月 23 日凌晨 2:45（格林尼治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;&lt;em&gt;这是 Google DeepMind 机械可解释性团队&lt;a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX"&gt;对语言模型如何回忆事实的调查的&lt;/a&gt;第二篇文章。这篇文章的重点是提炼事实回忆回路并模拟更标准的机械可解释性调查。这篇文章很杂乱，&lt;strong&gt;我们建议从&lt;a href="https://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall"&gt;第一篇文章&lt;/a&gt;开始&lt;/strong&gt;，然后根据与您最相关的内容略读并跳过其余的序列。我们假设这篇文章的读者熟悉&lt;a href="https://www.neelnanda.io/mechanistic-interpretability/glossary#mechanistic-interpretability-techniques"&gt;本术语表中&lt;/a&gt;列出的机械解释技术。&lt;/em&gt;&lt;/p&gt;&lt;h2&gt;介绍&lt;/h2&gt;&lt;p&gt;我们的目标是了解事实如何以叠加方式存储和调用。必要的步骤是找到涉及事实回忆的狭窄任务，并了解使模型能够完成此任务的高级电路。&lt;/p&gt;&lt;p&gt;我们专注于回忆不同运动员所参加的运动的狭隘任务。正如&lt;a href="https://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall#Why_Facts"&gt;第 1 篇文章&lt;/a&gt;中所讨论的，我们特别期望有关人的事实涉及叠加，因为个人名称标记的嵌入通常不足以确定运动，因此模型必须对名称的不同标记执行布尔 AND 运算来识别运动员并查找他们的运动。&lt;a href="https://transformer-circuits.pub/2022/solu/index.html"&gt;之前的&lt;/a&gt;&lt;a href="https://arxiv.org/abs/2305.01610"&gt;工作&lt;/a&gt;将这种现象称为“去标记化”，并表明它涉及早期的 MLP 层，并使用了重要的叠加。&lt;/p&gt;&lt;p&gt;为什么要关注运动员的运动而不是一般的事实回忆？我们相信，在机械解释中，首先深入理解现象的狭义实例通常是有用的，而不是坚持完全普遍化。运动员的运动是一项很好的任务，它为我们提供了每个属性值的大量示例，我们的目标是理解至少一个使用叠加进行事实回忆的示例，而不是一般性地解释事实回忆。我们推测类似的机制用于回忆其他类别的事实，但这不是我们工作的重点。&lt;/p&gt;&lt;h2&gt;设置&lt;/h2&gt;&lt;p&gt;为了理解事实本地化，我们研究了&lt;a href="https://arxiv.org/abs/2304.01373"&gt;Pythia&lt;/a&gt; 2.8B 的下一个标记预测和针对以下形式的一次性提示的激活：&lt;/p&gt;&lt;p&gt;供 1,500 名参加棒球、篮球和（美式）橄榄球运动的运动员使用。为了选择这些运动员，我们为模型提供了来自维基数据的更大的运动员数据集，并筛选出模型对正确运动的概率超过 50% 的数据集&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-1" id="fnref-s4i3RkmKovG88KHgx-1"&gt;[1]&lt;/a&gt;&lt;/sup&gt; 。&lt;/p&gt;&lt;p&gt;我们选择 Pythia 2.8B，因为它是能够胜任大量运动员完成这项任务的最小模型。&lt;/p&gt;&lt;p&gt;我们一次性给出提示，因为这显着提高了模型在任务上的性能&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-2" id="fnref-s4i3RkmKovG88KHgx-2"&gt;[2]&lt;/a&gt;&lt;/sup&gt; 。我们选择高尔夫作为一击前缀，以便模型不会对其需要预测的三项运动之一产生偏见。为简单起见，我们没有改变提示中的一次性前缀。&lt;/p&gt;&lt;h2&gt;我们最终得到的简化电路&lt;/h2&gt;&lt;p&gt;在详细介绍我们为推导电路而进行的消融研究之前，让我们先看一下我们最终得到的简化电路： &lt;/p&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/zwrgezridkrafez2lmmv" /&gt;&lt;/p&gt;&lt;p&gt;在哪里：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt; &lt;code&gt;concatenate_tokens&lt;/code&gt;执行的操作大致类似于各个令牌嵌入的加权和，将每个令牌的嵌入放置在不同的子空间中，由模型的前两层实现； &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-3" id="fnref-s4i3RkmKovG88KHgx-3"&gt;[3]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt; &lt;code&gt;lookup&lt;/code&gt;是一个五层 MLP（其层与原始模型中的 MLP 层 2 到 6 相匹配） &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-4" id="fnref-s4i3RkmKovG88KHgx-4"&gt;[4]&lt;/a&gt;&lt;/sup&gt; ，它处理组合标记以生成这些标记所描述的实体的新表示，其中该表示可以线性投影以揭示有关实体的各种属性（包括他们所从事的运动）；&lt;/p&gt;&lt;ul&gt;&lt;li&gt;我们注意到，在从第 6 层到第 15 层的路径中，似乎有多个重要的 MLP 层，但我们认为它们主要是增强运动现有线性表示的信号，并且消除它们不会显着影响准确性。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt; &lt;code&gt;extract_sport&lt;/code&gt;是一个线性分类器，它对&lt;code&gt;lookup&lt;/code&gt;的表示执行仿射变换，以提取模型返回的运动逻辑。这是在模型中通过多个注意力头（特别是包括 L16H20）实现的，这些注意力头关注从最终标记到最终名称标记，并直接与非嵌入层组合&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-5" id="fnref-s4i3RkmKovG88KHgx-5"&gt;[5]&lt;/a&gt;&lt;/sup&gt; 。在运动员的名字仅包含两个标记（一个用于名字，一个用于姓氏）的特殊情况下，我们能够进一步将&lt;code&gt;concatenate_tokens&lt;/code&gt;函数简化为以下形式： &lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/jaxnaennpshoncgl2x6a" /&gt;&lt;/p&gt;&lt;p&gt;其中&lt;code&gt;embed_first&lt;/code&gt;和&lt;code&gt;embed_last&lt;/code&gt;是字面上的查找表（模型词汇表中每个标记有一个条目），具有不相交的范围（以便编码器可以区分名字“Duncan”和姓氏“Duncan”）——强化了这样的想法： &lt;code&gt;concatenate_tokens&lt;/code&gt;仅与各个标记（加上位置信息）本身一样提供线性信息 - 即，它是相关标记序列的密集/压缩表示，模型需要从中解压缩/提取线性特征，以便下游电路可以使用（例如&lt;code&gt;extract_sport&lt;/code&gt; ）。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-6" id="fnref-s4i3RkmKovG88KHgx-6"&gt;[6]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;这与文献中的先前工作相当一致，特别&lt;a href="https://arxiv.org/pdf/2304.14767.pdf"&gt;是 Geva 等人&lt;/a&gt;。我们认为我们狭隘的、自下而上的方法是对他们更广泛、更自上而下的电路理解方法的补充。我们进一步证明&lt;code&gt;extract_sport&lt;/code&gt;是一个线性映射，反映了&lt;a href="https://arxiv.org/abs/2308.09124"&gt;Hernandez 等人的&lt;/a&gt;观点，并且它可以根据个体注意力头的 OV 电路来理解。我们认为我们的贡献是在狭窄的领域中通过更自下而上和以电路为中心的方法来完善现有知识，并更好地理解模型在每个阶段的表示，而不是作为一个实质性的新进步。&lt;/p&gt;&lt;p&gt;在本文的其余部分中，我们将更详细地描述我们为推导出这个简化电路而进行的实验。&lt;/p&gt;&lt;h2&gt;调查 1：了解事实提取&lt;/h2&gt;&lt;p&gt;当模型输出正确的运动标记来完成提示时，最早确定该运动的标记在哪里？先前的工作表明，应该在序列的早期（在运动员的最终名字标记处）识别正确的运动，并将其放置在线性可恢复的表示中。然后，一个单独的事实提取电路（上面电路图中的&lt;code&gt;extract_sport&lt;/code&gt; ）将从最终名称位置读取运动并输出正确的 logits 以完成提示。&lt;/p&gt;&lt;p&gt;在本节中，我们将描述我们进行的实验，以验证该图在现实中是否成立，并确定实现该事实提取步骤的电路。&lt;/p&gt;&lt;h3&gt;哪些节点对输出 logits 贡献最大？&lt;/h3&gt;&lt;p&gt;我们首先确定模型中的哪些节点对最终令牌生成的 logits 有最大的直接影响。我们通过单独消除每个 MLP 层的激活和最终 token 位置的注意力头输出并测量对输出 logits 的直接影响来做到这一点。&lt;/p&gt;&lt;p&gt;具体来说，对于每个干净的提示和我们希望消除的每个节点，我们将从“损坏的”提示中获取该节点的激活，并将这些激活沿着将该节点连接到模型的非嵌入层的补丁修补到干净提示的激活中，为了测量该路径补丁对模型输出的影响。对于损坏的提示，我们会​​为从事不同运动的运动员随机选择一个提示。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-7" id="fnref-s4i3RkmKovG88KHgx-7"&gt;[7]&lt;/a&gt;&lt;/sup&gt;为了衡量直接效果，我们将比较路径修补之前和之后干净提示的运动和损坏提示的运动之间的 logit 差异。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-8" id="fnref-s4i3RkmKovG88KHgx-8"&gt;[8]&lt;/a&gt;&lt;/sup&gt;结果如下： &lt;/p&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/m0prshu8u9zcnimo4fr6" /&gt;&lt;/p&gt;&lt;p&gt;这些结果表明，相对稀疏的节点集对 logits 有任何有意义的影响：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;中间层有少数注意力头；&lt;/li&gt;&lt;li&gt;遵循这些注意力头的 MLP 层。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;其中，注意力头特别有趣：由于我们测量了直接（而不是全部）效果，我们知道这些注意力头的输出直接将最终标记逻辑推向正确的运动（或远离不正确的运动），而无需需要进一步的后处理。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-9" id="fnref-s4i3RkmKovG88KHgx-9"&gt;[9]&lt;/a&gt;&lt;/sup&gt;这强烈表明，无论这些头关注何处，这些位置的残余流已经对每个运动员的运动进行了编码。&lt;/p&gt;&lt;h3&gt;这些高效率的负责人去哪里参加？ &lt;/h3&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/bgqe8cyizbqkxuoxjx4e" /&gt;&lt;/p&gt;&lt;p&gt;在这里，我们通过对最终 token logits 具有最高直接影响的 6 个头的提示样本，可视化了最终 token 的注意力模式。我们看到头部大多关注最终的名称标记位置，或者如果失败，则回顾提示中前面的“&amp;lt;bos&amp;gt;”或“\n”静止位置。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-10" id="fnref-s4i3RkmKovG88KHgx-10"&gt;[10]&lt;/a&gt;&lt;/sup&gt; &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-11" id="fnref-s4i3RkmKovG88KHgx-11"&gt;[11]&lt;/a&gt;&lt;/sup&gt;由此我们可以得出两点：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;运动员的运动项目主要通过其最终名称标记的第 16 层在剩余流中表示。&lt;/li&gt;&lt;li&gt;他们的运动的表示应该是线性可恢复的（因为每个头的值输入通过近似线性变换与模型的最终令牌 logits 相关）。&lt;/li&gt;&lt;/ol&gt;&lt;h3&gt;这些高效注意力头从最终名称标记位置读取什么？&lt;/h3&gt;&lt;p&gt;为了回答这个问题，我们通过 OV 电路计算了上面列出的每个高效头的最终名称标记位置中节点的路径特定效果。准确地说，对于每个高效头，我们通过相关头的 OV 电路，沿着从该节点到输出 logits 的路径，逐个修补每个馈线节点在损坏的（其他运动）提示上的激活&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-12" id="fnref-s4i3RkmKovG88KHgx-12"&gt;[ 12]&lt;/a&gt;&lt;/sup&gt; 。实际上，这根据馈入该节点的节点创建了每个高效节点的值输入的归因。&lt;/p&gt;&lt;p&gt;有趣的是，我们发现第二到第六个最重要头的性能的很大一部分依次来自于它们的值输入读取最终名称令牌流&lt;em&gt;中&lt;/em&gt;L16H20 的输出。例如，这里是最终名称令牌节点通过 L21H9（第二个最重要的头）的 OV 电路对 logits 的影响的归因 - 请注意 L16H20 的输出（在最终名称令牌位置）对以下影响的巨大贡献这个头： &lt;/p&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/kmyken8pjgofdndy2mre" /&gt;&lt;/p&gt;&lt;p&gt;第三到第六个最重要的头的热图看起来很相似，它们的很多效果来自于最终名称标记处 L16H20 的输出。&lt;/p&gt;&lt;p&gt;此外，观察 L16H20 从最终名称标记位置进行关注时的注意力模式，我们发现它通常会关注相同的位置。将这些观察结果放在一起，我们发现 L16H20 通过两个&lt;em&gt;单独的&lt;/em&gt;机制在此电路中具有很高的整体重要性：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;它通过其 OV 电路将运动员运动功能直接转移到最终令牌位置（从最终令牌返回到最终名称令牌）；&lt;/li&gt;&lt;li&gt;它从最终&lt;em&gt;名称&lt;/em&gt;标记位置到相同位置，产生一个输出，该输出显着贡献（通过 V 组合）到其他头的输出，这些其他头将运动员运动特征从最终名称标记转移到最终标记。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt; L16H20 本身怎么样——哪些节点&lt;em&gt;对其&lt;/em&gt;价值输入的贡献最强？如下图所示，L16H20 本身的值输入在很大程度上取决于最终名称位置中其之前的 MLP 输出（一些 V 组合以及一些较早的头，这些头依次从最终名称位置到自身）： &lt;/p&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/kdaykbcwns7qhchfi4ya" /&gt;&lt;/p&gt;&lt;h3&gt;用于事实提取的简化子电路&lt;/h3&gt;&lt;p&gt;综合以上结果，我们得出结论：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;运动事实提取电路（上图中的&lt;code&gt;extract_sport&lt;/code&gt; ）的很大一部分是由L16H20头执行的；&lt;ul&gt;&lt;li&gt;因为头部的注意力模式在运动员之间不会发生（太大）变化，所以它的功能只是一个线性映射，将残余流乘以其 OV 电路。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;该头的 OV 电路读取最终名称令牌残差流并输出内容（两者都返回到同一残差流中并直接输出到最终令牌残差流中），以便取消嵌入结果可以提高正确运动令牌的 logits 优于不正确运动令牌的 logits &amp;#39; 令牌。&lt;/li&gt;&lt;li&gt;因此&lt;strong&gt;，正确的运动被线性地表示在最终的名称令牌上，并由L16H20的OV电路提取&lt;/strong&gt;。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这表明我们可以通过用三类线性探针替换从最终名称标记位置的第 16 层开始的所有模型计算图来近似&lt;code&gt;extract_sport&lt;/code&gt; ，该三类线性探针是通过将 L16H20 的 OV 图与标记“棒球”的模型非嵌入权重组合而成的、“篮球”和“足球”。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-13" id="fnref-s4i3RkmKovG88KHgx-13"&gt;[13]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;通过这种简化，我们发现在将电路的&lt;code&gt;extract_sport&lt;/code&gt;部分简化为这个（权重导出的）线性探针之后，整个电路对运动员运动进行分类的准确度从原始模型的 100% 下降到 98% – 即我们可以大大简化这部分电路在执行任务时的性能下降可以忽略不计。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-14" id="fnref-s4i3RkmKovG88KHgx-14"&gt;[14]&lt;/a&gt;&lt;/sup&gt; &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-15" id="fnref-s4i3RkmKovG88KHgx-15"&gt;[15]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;h3&gt;另一种路径：只需训练线性探针&lt;/h3&gt;&lt;p&gt;绕过上述许多分析的另一种方法是仅在最终名称标记&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-16" id="fnref-s4i3RkmKovG88KHgx-16"&gt;[16]&lt;/a&gt;&lt;/sup&gt;的残余流上训练逻辑回归探针，并表明在第 6 层探针获得了良好的测试准确性。我们可以进一步表明，探针&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-17" id="fnref-s4i3RkmKovG88KHgx-17"&gt;[17]&lt;/a&gt;&lt;/sup&gt;所跨越的子空间中的修补会因果地影响模型的输出，这表明该表示在下游使用&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-18" id="fnref-s4i3RkmKovG88KHgx-18"&gt;[18]&lt;/a&gt;&lt;/sup&gt; 。这是我们在项目的重要部分中使用的方法，然后返回并使本节前面的分析更加严格。&lt;/p&gt;&lt;p&gt;我们认为机械探针有显着的优势（例如，使用 L16H20 的权重和非嵌入来导出探针，而不是训练逻辑回归分类器），它更有原则性（从某种意义上说，我们可以清楚地看到它在以下方面意味着什么）：模型的电路），更难过度拟合，并且不需要训练集，然后就不能再用于进一步分析。但“只需训练一个探针”就可以更轻松地快速移动。&lt;/p&gt;&lt;p&gt;特别是，对于这项调查，我们的目标是放大前几层的&lt;code&gt;lookup&lt;/code&gt; ，并且知道在几个 MLP 层之后正确的运动变成线性表示，这足以告诉我们有一些有趣的东西可以尝试逆向工程，即使我们不知道事实提取电路的细节。&lt;/p&gt;&lt;p&gt;我们认为探针是一种被低估的电路分析工具，并且在模型中找到可解释的方向/子空间（可以以非平凡的方式证明其具有因果意义）可以实现更简单的电路分析，只需要考虑层的子集，而不是模型的完整端到端行为。&lt;/p&gt;&lt;p&gt;另一种更简单的方法是通过迭代每个头来搜索机械探针，将其 OV 乘以运动的未嵌入作为探针，并评估准确性。如果有一个精度特别高的头部（包括在保留的验证集上），并且位于正确的位置，那么您可能已经找到了一个关键的头部。我们注意到，与首先进行直接 Logit 归因以缩小到一小部分头部相比，这种方法具有更大的过度拟合风险，具体取决于头部的数量&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-19" id="fnref-s4i3RkmKovG88KHgx-19"&gt;[19]&lt;/a&gt;&lt;/sup&gt; 。&lt;/p&gt;&lt;h2&gt;&lt;/h2&gt;&lt;h2&gt;研究 2：简化去标记化和查找&lt;/h2&gt;&lt;p&gt;在本节中，我们将描述为简化&lt;a href="https://docs.google.com/document/d/1EsIlX7L_xr0YX918NiDWv4Cn3FVS6tJqjrIGR6mnJyQ/edit?resourcekey=0-QoVN8x6k4h6wZCZaWV9qug#heading=h.h6dofjljntk0"&gt;上面&lt;/a&gt;简化电路图中定义的&lt;code&gt;concatenate_tokens&lt;/code&gt;和&lt;code&gt;lookup&lt;/code&gt;模块所覆盖的电路部分而进行的实验。总而言之，下面描述的实验确立了有关这部分电路的以下事实：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;运动员的运动在残差流中比第 16 层（由头 L16H20 读取）更早地表示。到了第 6 层，当我们尝试使用&lt;code&gt;extract_sport&lt;/code&gt;读取运动员的运动数据时，我们仍然获得了相当不错的准确率 (90%)。&lt;/li&gt;&lt;li&gt;运动员姓名之前的上下文对于生成运动表示并不重要（尽管对于&lt;code&gt;extract_sport&lt;/code&gt;确实很重要）。我们可以在“&amp;lt;bos&amp;gt; &amp;lt;athletes-full-name&amp;gt;”形式的提示上使用模型的最终令牌激活，并提取运动员的运动，准确性几乎没有下降。&lt;/li&gt;&lt;li&gt;注意力头在第 0 层和第 1 层之外并不重要。我们可以在不影响&lt;code&gt;lookup&lt;/code&gt;模块性能的情况下对第 2 层之后的注意力层进行平均消融。从注意力模式来看，我们发现大多数负责人并不特别关注以前的名称标记，这强化了取消它们的理由。删除这些注意力头的优点是， &lt;code&gt;lookup&lt;/code&gt;模块因此成为 MLP 层的简单堆栈（具有剩余跳过连接），能够在剩余流中的第 2 层开始处获取运动员的嵌入，并通过以下方式输出运动员的运动：残差流中第 6 层的末尾。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;因此，我们可以将这部分电路分解为两个子模块：&lt;/p&gt;&lt;ul&gt;&lt;li&gt; &lt;code&gt;concatenate_tokens&lt;/code&gt; ，代表模型第 0 层和第 1 层的角色，其角色（对于此任务）是收集运动员的姓名标记并将其放入最终的姓名标记残差流中（我们在调查 3 中表明，这是纯串联，没有查找，至少两个象征性的运动员姓名）；&lt;/li&gt;&lt;li&gt; &lt;code&gt;lookup&lt;/code&gt;是一个纯 5 层 MLP（包含原始模型的第 2-6 层），它将运动员姓名的多标记表示转换为运动员的特征表示，该特征表示以线性可恢复的方式专门表示运动员的运动。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;我们现在依次描述支持上面列出的三个主张的证据。&lt;/p&gt;&lt;h3&gt;查找大部分由第 6 层完成&lt;/h3&gt;&lt;p&gt;如果我们将&lt;code&gt;extract_sport&lt;/code&gt;探针应用到最终名称标记位置的不同层，我们会发现可以比第 16 层更早地从剩余流中读取运动员的运动： &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-20" id="fnref-s4i3RkmKovG88KHgx-20"&gt;[20]&lt;/a&gt;&lt;/sup&gt; &lt;/p&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/pwsurjq0ifiw4ch7xtj2" /&gt;&lt;/p&gt;&lt;p&gt;到第 8 层左右，准确率已基本趋于稳定，甚至到第 6 层，准确率也达到了 90% 左右。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-21" id="fnref-s4i3RkmKovG88KHgx-21"&gt;[21]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;h3&gt;查找运动员的运动时，背景并不重要&lt;/h3&gt;&lt;p&gt;我们已经确定，运动员最终名称标记处的残余流对其运动进行编码。但是模型在多大程度上将体育运动放入残差流中，因为它在看到运动员的名字时无论如何都会这样做（多令牌嵌入假设），以及模型在多大程度上将体育运动放入残差流中，因为一-名称&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-22" id="fnref-s4i3RkmKovG88KHgx-22"&gt;[22]&lt;/a&gt;&lt;/sup&gt;之前的镜头提示是否向模型暗示运动可能是提取的有用属性？&lt;/p&gt;&lt;p&gt;我们的假设是，上下文并不那么重要——特别是，即使没有任何先前的上下文，模型在看到运动员的名字时也会查找运动员的运动项目。我们通过收集纯名称提示的激活来测试这一点，其中模型被输入“&amp;lt;bos&amp;gt; &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-23" id="fnref-s4i3RkmKovG88KHgx-23"&gt;[23]&lt;/a&gt;&lt;/sup&gt; &amp;lt;first-name&amp;gt; &amp;lt;last-name&amp;gt;” &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-24" id="fnref-s4i3RkmKovG88KHgx-24"&gt;[24]&lt;/a&gt;&lt;/sup&gt;形式的令牌序列，并且从最终名称标记。&lt;/p&gt;&lt;p&gt; &lt;code&gt;extract_sport&lt;/code&gt;模块可以从这些激活中读取运动员的运动项目吗？如下图所示，我们发现在没有一次性上下文的情况下，性能会略有下降，但仍然可以纯粹从早期层编码中相当准确地读取运动员的运动项目，该早期层编码仅以“&amp;lt;bos&amp;gt;”开头”，没有任何额外的上下文。因此，我们可以通过从任务的完整提示中运动员姓名标记之前的标记中删除所有边来简化整个电路。 &lt;/p&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/x4wdqduttl0ovdowk9yi" /&gt;&lt;/p&gt;&lt;h3&gt;注意力头在第 2 层之外并不重要&lt;/h3&gt;&lt;p&gt;为了准确地回忆运动项目，电路的&lt;code&gt;lookup&lt;/code&gt;部分通常必须是运动员姓名中大多数（如果不是全部）标记的函数：对于大多数运动员来说，不可能仅通过知道最后一个标记来确定运动项目以他们的姓氏。因此，注意力头必须在汇集分布在运动员姓名的各个标记上的信息方面发挥一定作用，以便可以准确地查找体育等事实。&lt;/p&gt;&lt;p&gt;然而，这两个过程（组合标记和查找事实）如何相互关联？&lt;/p&gt;&lt;ol&gt;&lt;li&gt;它们可以同时发生——当查找过程需要时，注意力会从早期令牌中引入相关信息；&lt;/li&gt;&lt;li&gt;或者，这些过程可以按顺序发生，首先将组成运动员姓名的标记组合在一起，然后大部分查找过程才发生。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-25" id="fnref-s4i3RkmKovG88KHgx-25"&gt;[25]&lt;/a&gt;&lt;/sup&gt;观察在最终名称标记位置修补注意力头输出的总体效果，我们确实发现在模型的第 0 层和第 1 层中，在整个电路中发挥重要作用的头比后面的层要多得多: &lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/s8xbfgfrakemknheueil" /&gt;&lt;/p&gt;&lt;p&gt;这表明我们也许能够删除第 2 层以后的注意力头输出，而不会对整个电路的性能产生太大影响。尝试这个，我们发现从第 2 层开始平均消除注意力输出仅对准确性产生轻微的不利影响： &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-26" id="fnref-s4i3RkmKovG88KHgx-26"&gt;[26]&lt;/a&gt;&lt;/sup&gt; &lt;/p&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/xc9un39ztbdfeaxxh1h4" /&gt;&lt;/p&gt;&lt;p&gt;这支持了上述的两阶段假设：令牌之间的信息共享（通过注意力）基本上由第 2 层完成，后面层中的注意力头对于查找来说并不重要。&lt;/p&gt;&lt;h3&gt;用于事实查找的简化子电路&lt;/h3&gt;&lt;p&gt;上述结果表明，我们确实可以将查找运动员运动项目的过程分为两个阶段：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt; &lt;code&gt;concatenate_tokens&lt;/code&gt; ，这是模型的嵌入和第 0 层和第 1 层处理包含运动员姓名&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-27" id="fnref-s4i3RkmKovG88KHgx-27"&gt;[27]&lt;/a&gt;&lt;/sup&gt;的标记并在最终名称标记残差流中的第 1 层末尾生成“串联标记”表示的结果；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt; &lt;code&gt;lookup&lt;/code&gt; ，这是一个带有跳跃连接的纯 MLP（由模型中的 MLP 第 2 层开始组成），它在残差流的第 1 层之后处理“串联令牌表示”，其本身并不能很好地表示运动员的运动（在线性方式），并稍后在残余流中生成“特征表示”，其中可以轻松地线性提取运动（通过&lt;code&gt;sport_extract&lt;/code&gt; ）。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;请注意，我们在这里合并了两种简化：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;自行处理运动员姓名标记，无需一次性提示（因为我们发现上下文并不重要）&lt;/li&gt;&lt;li&gt;从第 2 层开始移除注意力头（因为我们发现令牌之间的信息传输大部分由第 2 层完成）。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;由于这些近似值中的每一个都会对电路的精度产生一些不利影响，因此值得评估它们的综合影响。下图显示了组合这些近似值如何影响准确性： &lt;/p&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/xkhlsj0ktdmm3kipcbll" /&gt;&lt;/p&gt;&lt;p&gt;结果是，即使同时应用两种简化，通过在&lt;code&gt;lookup&lt;/code&gt; MLP 中包含足够的层也可以获得高达 94% 的准确率；即使停在第 6 层也能获得 85% 的准确率。&lt;/p&gt;&lt;h2&gt;&lt;/h2&gt;&lt;h2&gt;研究3：进一步简化令牌串联电路&lt;/h2&gt;&lt;p&gt;到目前为止，我们已经：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;将&lt;code&gt;extract_sport&lt;/code&gt;简化为 3 类线性探针，其权重是通过将 L16H20 的 OV 图与模型的非嵌入权重组合得出的；&lt;/li&gt;&lt;li&gt;简化&lt;code&gt;lookup&lt;/code&gt;为具有跳跃连接的纯 5 层 MLP，其层权重对应于原始模型中 MLP 层 2-6 的层权重；&lt;/li&gt;&lt;li&gt;确定在计算第 2 层开头的最终名称标记位置（即&lt;code&gt;lookup&lt;/code&gt;的输入）处的残余流值时可以删除先前的上下文。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这给我们留下了&lt;code&gt;concatenate_tokens&lt;/code&gt; ，包括模型的嵌入以及第 0 层和第 1 层，它将原始运动员姓名标记（加上前置的 &amp;lt;bos&amp;gt; 标记）转换为第 2 层开头的残差流的值。进一步简化这部分电路？&lt;/p&gt;&lt;p&gt;我们为该电路组件确定了两个级别的简化：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;一般来说，我们可以将&lt;code&gt;concatenate_tokens&lt;/code&gt;视为有效生成运动员姓名的&lt;em&gt;两个&lt;/em&gt;单独的令牌级嵌入，然后通过纯注意力操作近似线性地组合这些嵌入；&lt;/li&gt;&lt;li&gt;对于名字只有两个标记长的运动员，我们可以进一步近似&lt;code&gt;concatenate_tokens&lt;/code&gt; ，使其实际上是名字和姓氏标记的有效标记嵌入的总和，对电路准确性仅造成适度的影响。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在以下小节中，我们将更详细地解释这些简化并为其提供实验依据。&lt;/p&gt;&lt;h3&gt;令牌串联是通过注意力头移动令牌嵌入来实现的&lt;/h3&gt;&lt;p&gt;第一个简化来自以下两个观察：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;最终名称标记位置的 MLP 第 1 层对电路性能影响不大：重新采样消融对原始模型的 Logit 差异的总体影响较小，均值消融对简化电路的精度影响很小。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-28" id="fnref-s4i3RkmKovG88KHgx-28"&gt;[28]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;由于 Pythia 使用并行注意力，MLP 第 0 层实际上是辅助令牌嵌入层。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-29" id="fnref-s4i3RkmKovG88KHgx-29"&gt;[29]&lt;/a&gt;&lt;/sup&gt;这意味着（在均值消融 MLP 1 之后）， &lt;code&gt;concatenate_tokens&lt;/code&gt;有效地执行以下操作：&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ol&gt;&lt;li&gt;使用模型的嵌入层权重计算运动员姓名标记（和 &amp;lt;bos&amp;gt;）的主要标记嵌入。&lt;/li&gt;&lt;li&gt;使用 MLP 0 对输入标记词汇的作用引起的嵌入权重，计算运动员姓名标记的辅助标记嵌入。&lt;/li&gt;&lt;li&gt;在主要 token 嵌入上操作注意力层 0 头。&lt;/li&gt;&lt;li&gt;在主要 token 嵌入、次要 token 嵌入和注意力层 0 输出的总和上操作注意力层 1 头。&lt;/li&gt;&lt;li&gt;使用最终名称标记位置处的步骤 4 的结果作为&lt;code&gt;lookup&lt;/code&gt;的输入。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;换句话说， &lt;code&gt;concatenate_tokens&lt;/code&gt;有效地嵌入了输入标记（两次），并通过注意力将它们（直接和间接）移动到最终名称标记位置。&lt;/p&gt;&lt;h3&gt;对于两个令牌的运动员， &lt;code&gt;concatenate_tokens&lt;/code&gt;实际上将名字和姓氏令牌添加在一起&lt;/h3&gt;&lt;p&gt;对于两个令牌的运动员，我们发现我们可以进一步冻结注意力模式，并且仍然保持任务的合理准确性。具体来说：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;我们阻止了第 1 层中姓氏标记位置的注意力头关注同一位置；&lt;/li&gt;&lt;li&gt;我们将数据集中所有两个令牌运动员的所有其他注意力模式冻结在平均水平，使注意力层充当将源令牌残差映射到最终令牌的线性映射。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这些简化以及均值消融 MLP 1 将&lt;code&gt;concatenate_tokens&lt;/code&gt;转换为有效令牌嵌入和偏差项（源自 &amp;lt;bos&amp;gt; 令牌的嵌入）的总和。姓氏的有效令牌嵌入只是主要和第二（MLP0）令牌嵌入的总和。名字的有效 token 嵌入更加复杂，它是主 token 嵌入乘以来自冻结注意力 0 头的线性图（它们的 OV 矩阵由姓氏到名字的平均注意力加权），加上主要和次要令牌嵌入乘以来自冻结注意力 1 头的线性图。&lt;/p&gt;&lt;p&gt;这些简化对准确性的影响如下图所示。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-s4i3RkmKovG88KHgx-30" id="fnref-s4i3RkmKovG88KHgx-30"&gt;[30]&lt;/a&gt;&lt;/sup&gt;我们发现，对于两个令牌的运动员来说，与消除 MLP 1 相比，冻结注意力模式对准确性几乎没有额外的影响。 &lt;/p&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/kinnfxarccubfccdl2g3" /&gt;&lt;/p&gt;&lt;hr class="footnotes-sep" /&gt;&lt;section class="footnotes"&gt;&lt;ol class="footnotes-list"&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-1"&gt;&lt;p&gt;请注意，我们从数千名运动员开始，因此这种过滤可能会引入一些偏见。例如，如果模型不认识 1000 名运动员，但随机猜测了一项运动，我们会选择模型幸运的 333 名运动员。我们设置 50% 置信度（针对完整词汇）阈值来减少这种影响。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-1"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-2"&gt;&lt;p&gt;我们的猜测是，很少的射击与零射击不会对查找电路产生实质性影响，而是单次提示告诉模型输出是一项运动，并提高所有运动的逻辑（由 Chughtai 等人的结果建议（即将发布）） 。有趣的是，零样本，该模型非常重视“他/她”作为输出，尽管 Pythia 1.4B 没有！ &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-2"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-3"&gt;&lt;p&gt;请注意， &lt;code&gt;concatenate_tokens&lt;/code&gt;输出中的线性可恢复特征最终将类似于这些单独令牌嵌入中的特征的并集，即运动并不是特别可以从此输出中线性恢复。最好将&lt;code&gt;concatenate_tokens&lt;/code&gt;的输出视为“保存位置信息的各个令牌嵌入的串联”，以便串联可以由一系列 MLP 层一起处理。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-3"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-4"&gt;&lt;p&gt;我们可以选择 4 到 15 之间的几乎任何层作为端点，因为当我们添加额外的层时，我们的简化电路的可信度会相当连续地增加。然而，第 5 层周围有一个拐点，之后您将开始看到收益递减。我们认为 MLP 6 之后的 MLP 层只是增强残差流中的属性，而不是从原始标记中查找属性。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-4"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-5"&gt;&lt;p&gt;当我们提到“线性分类器”时，我们指的是这些头的 OV 电路的组成和非嵌入矩阵。头部始终从最终标记到最终名称位置，因此纯粹充当线性映射。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-5"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-6"&gt;&lt;p&gt;我们怀疑，即使对于名字中包含三个或更多令牌的运动员，也可以将&lt;code&gt;concatenate_tokens&lt;/code&gt;表达为类似的位置相关令牌嵌入总和，但我们没有进一步进行这一调查。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-6"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-7"&gt;&lt;p&gt;例如，如果干净的提示是蒂姆·邓肯（打篮球）的，我们可以从乔治·布雷特（打棒球）或安迪·道尔顿（打橄榄球）的提示中修补激活。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-7"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-8"&gt;&lt;p&gt;该指标具有良好的特性，即在逻辑上呈线性，同时对于所有逻辑上的恒定变化也具有不变性。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-8"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-9"&gt;&lt;p&gt;澄清一下，后面的 MLP 层的总体影响并不显着，这表明正在进行一些后处理 - 我们要说的是，即使没有这种后处理，这些注意力头的输出也可以直接用运动来解释令牌逻辑，因此这些注意力头已经将正确的运动写入残差流中。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-9"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-10"&gt;&lt;p&gt;有趣的是，由于提示是按照运动分类的，我们看到一些头似乎只用于任务中运动的子集。按运动细分这些头部的直接影响证实了这一情况：L19H24 仅对棒球运动员始终重要，而 L22H17 仅对篮球和部分棒球运动员始终重要。 （我们并没有试图理解这个头对于那些重要的运动员和那些不重要的运动员的区别。）注意力头之间的这种选择性&lt;a href="https://arxiv.org/abs/2307.09458"&gt;并不完全令人惊讶&lt;/a&gt;，我们没有进一步调查这一点，因为它不是直接的与我们目前的调查相关。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-10"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-11"&gt;&lt;p&gt;与通常的机械解释一样，这只是一个大概的图片 - 看看这些图，很明显，有些头脑确实非常关注运动员姓名和最终标记之间的标记，特别是“比赛”和“运动”，有时“高尔夫”。然而，这并没有改变总体结论，即运动员姓名令牌的剩余流已经包含他们的运动。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-11"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-12"&gt;&lt;p&gt;我们注意到，这种方法需要对最终名称标记上的每个残余流组件&lt;em&gt;以及&lt;/em&gt;每个高效中介头进行单独的前向传递。这不是我们工作的瓶颈，因此我们进行了适当的路径修补，但我们注意到这可以通过直接 logit 归因轻松近似。如果我们冻结 LayerNorm 尺度，那么每个头的 OV 电路对最终名称 token 残差流执行线性映射，而去嵌入则是进一步的线性映射，因此我们可以有效地看到每个头引起的最终 logits 的变化最终名称标记剩余部分。我们发现这种技术对于这项工作中的快速迭代很有用。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-12"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-13"&gt;&lt;p&gt;我们还需要设置探头的偏置。我们通过减去探针输入处的平均激活来实现这一点（即，在应用权重矩阵之前有效地将探针输入居中）。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-13"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-14"&gt;&lt;p&gt;这种表现上的差异至少部分是因为我们总是探测最终的名称标记位置，而 L16H20 则针对一些运动员关注其名称中的其他位置（例如倒数第二个标记）。我们推测这是因为一些运动员在他们名字中的最后一个标记出现之前就被完全识别了（例如，从五个标记名称中的四个标记），因此事实查找发生在这个最终标记之前。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-14"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-15"&gt;&lt;p&gt;我们故意测量准确性而不是损失恢复，因为我们预计后来的高效磁头主要是增强 L16H20 输出的信号，尽管&lt;a href="https://arxiv.org/abs/2309.16042"&gt;损失恢复通常是我们的首选指标&lt;/a&gt;。信号增强可以改善损失，但不会改变准确性，并且要了解事实回忆，只需了解如何实现高精度即可。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-15"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-16"&gt;&lt;p&gt;在这种情况下，我们已经猜测最终的名称令牌将是基于例如残差流修补+先前工作来探测运动的正确令牌，但是扫描令牌和层并在所有令牌和层上训练探针是很容易的，探针是坐火车很便宜&lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-16"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-17"&gt;&lt;p&gt; &lt;a href="https://arxiv.org/abs/2311.17030"&gt;Makelov 等人&lt;/a&gt;最近表明，子空间激活修补可能会误导性地激活休眠的并行路径，但这在使用梯度下降来学习具有因果效应的子空间时主要是一个问题，探针是相关的，因此这不是问题。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-17"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-18"&gt;&lt;p&gt;因为探针是线性的，所以有点不清楚是否应该关心探针是否有因果关系地使用。该模型能够将运动员姓名的各个标记映射到运动的线性表示，这是一种有趣的逆向工程算法，并且可能涉及叠加，即使对于某些奇怪的巧合，并行算法是影响模型输出的主要因素。但这是一个非常人为的情况，而且很容易检查。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-18"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-19"&gt;&lt;p&gt;特别是，在某些设置中，探查可能非常自然。例如，许多注意力头只是将他们关注的任何标记复制到输出中。因此，当探测输入标记时，作为一个好的机械探针是涉及头部的微弱证据，但仍然可能会发现你是一个相当好的探针。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-19"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-20"&gt;&lt;p&gt;当将此探针应用于其他层时，我们始终指的是相对于该层的残余流激活的中心。这相当于平均消融被探测层和第 16 层之间的 MLP 和注意力输出&lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-20"&gt;。 ↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-21"&gt;&lt;p&gt;这里自然会出现一个问题：当探测表明您几乎可以更早地阅读运动员的运动项目？我们的假设是，这些后来的 MLP 正在增强早期 MLP 产生的信号，而不是自己查找事实。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-21"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-22"&gt;&lt;p&gt;即“事实：老虎伍兹从事高尔夫运动” &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-22"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-23"&gt;&lt;p&gt;杂草中：Pythia 模型没有使用 BOS（序列开始）标记进行训练，但我们有传闻发现该模型在使用 BOS 标记进行推理时表现更好。模型通常对上下文的第一个标记有极高的规范，并以不同寻常的方式对待它，这使得研究像“George Brett”这样的简短提示变得困难。 Pythia 的 BOS 和 EOS 代币是相同的，并且在预训练中使用 EOS 代币分隔文档进行训练（模型能够参与不同的文档之间），因此在训练期间会看到 BOS 代币扮演这种角色&lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-23"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-24"&gt;&lt;p&gt;例如“&amp;lt;bos&amp;gt;乔治·布雷特” &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-24"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-25"&gt;&lt;p&gt;注意，我们知道不可能将查找与令牌串联完全分开，因为即使是最终的令牌嵌入（在模型进行任何处理之前）也通常具有运动员运动的一些概念。相反，我们在这里做出较弱的假设，即&lt;code&gt;lookup&lt;/code&gt;电路的大部分额外准确性（除了根据最终标记猜测运动之外）发生在运动员的姓名标记首次组装在一起之后。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-25"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-26"&gt;&lt;p&gt;我们还检查（并确认）消除注意力层 0 或 1 对运动查找产生灾难性影响。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-26"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-27"&gt;&lt;p&gt;准确地说，带有前面的 &amp;lt;bos&amp;gt; 标记。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-27"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-28"&gt;&lt;p&gt;当我们指的是消融 MLP 1 时，使用前几节中描述的简化&lt;code&gt;lookup&lt;/code&gt;和&lt;code&gt;extract_sport&lt;/code&gt;电路，探测精度在第 6 层之后从 85% 下降到 81%，在第 15 层之后从 94% 下降到 90%。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-28"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-29"&gt;&lt;p&gt;由于并行注意力，任何位置的 MLP0 的输入都只是该位置的 token 嵌入。因此，我们可以用第二个令牌嵌入表（将原始令牌 ID 映射到相应的 MLP0 输出值）来替换 MLP0，而根本不会影响模型输出。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-29"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-s4i3RkmKovG88KHgx-30"&gt;&lt;p&gt;请注意，该图不能与前面的图直接比较，因为它仅测量两个令牌运动员的准确性。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-s4i3RkmKovG88KHgx-30"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/section&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/3tqJ65kuTkBh8wrRH/fact-finding-simplifying-the-circuit-post-2#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sat, 23 Dec 2023 02:45:49 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/3tqJ65kuTkBh8wrRH/fact-finding-simplifying-the-circuit-post-2</guid></item><item><title>事实调查：尝试在神经元水平上对事实回忆进行逆向工程（第 1 篇文章）</title><link>https://www.lesswrong.com/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall</link><description>发布于 2023 年 12 月 23 日凌晨 2:44（格林尼治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;&lt;em&gt;这是 Google DeepMind 机械可解释性团队对语言模型如何表示事实的调查的文章。这是&lt;a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX"&gt;一个由 5 篇文章组成的序列&lt;/a&gt;，我们建议优先阅读第 1 篇文章，并将其视为我们论文的“主体”，而第 2 篇到第 5 篇文章作为一系列附录，可以按任何顺序浏览或深入了解。&lt;/em&gt;&lt;/p&gt;&lt;h2&gt;执行摘要&lt;/h2&gt;&lt;p&gt;具有叠加的逆向工程电路是机械可解释性中一个未解决的主要问题：模型使用巧妙的压缩方案以高度分布和多语义的方式表示比其维度/神经元更多的特征。大多数现有的机械插补工作利用了这样一个事实，即某些电路涉及稀疏的模型组件集（例如稀疏的头集），并且我们不知道如何处理分布式电路，这尤其阻碍了对 MLP 层的理解。我们的目标是解释分布式电路的具体案例研究，因此我们研究了 MLP 层如何实现事实回忆的查找表：即 Pythia 2.8B 中的早期 MLP 如何查找不同运动员参加的 3 种不同运动中的哪一种。&lt;strong&gt;我们认为自己没有达到对叠加计算进行机械理解的主要目标&lt;/strong&gt;，但确实取得了一些有意义的进展，包括在为什么这很难的概念上取得了进展。&lt;/p&gt;&lt;p&gt;我们总体上最好的猜测是，&lt;strong&gt;早期 MLP 的一个重要作用是充当“多令牌嵌入”，它&lt;/strong&gt;&lt;strong&gt;从最近的几个令牌（例如名称）中&lt;/strong&gt;选择&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-aYS73ikuT9JzFD3Mj-1" id="fnref-aYS73ikuT9JzFD3Mj-1"&gt;[1]&lt;/a&gt;&lt;/sup&gt;正确的分析单元，并将其转换为&lt;strong&gt;表示&lt;/strong&gt;（即在激活中编码的一些有用的含义）。我们可以通过线性投影来恢复该单位的不同属性（例如进行的运动），即&lt;strong&gt;存在属性的线性表示&lt;/strong&gt;。尽管我们不能排除这种可能性，但我们的猜测是，在这些层的内部机制/参数中找不到更多可解释的结构（例如稀疏性或有意义的中间表示）。对于未来的机械插补工作，我们认为重点关注如何使用探测和&lt;a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html"&gt;稀疏自动编码器&lt;/a&gt;等工具来理解这些属性在这些多令牌嵌入中的表示方式（即多令牌实体上的早中期残差流）就足够了。早期 MLP 的结构类似于我们对代币嵌入的看法，其中生成的嵌入可能具有结构（例如“有空间”或“积极情绪”特征），但内部机制只是一个查找表，没有结构解释。尽管如此，我们认为这是对更雄心勃勃的机械解释形式的向下更新，其取决于对模型的完全逆向工程。 &lt;/p&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iGuwZTHWb6DFY3sKB/b8shmrcacihqfwbfn5ru" /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;我们的主要贡献：&lt;/strong&gt;&lt;/p&gt;&lt;p&gt; &lt;a href="https://www.alignmentforum.org/posts/3tqJ65kuTkBh8wrRH/fact-finding-simplifying-the-circuit-post-2"&gt;&lt;strong&gt;帖子 2：&lt;/strong&gt;简化电路&lt;/a&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;我们深入研究了 Pythia 2.8B 如何回忆不同运动员的运动的具体电路。反映&lt;a href="https://arxiv.org/abs/2304.14767"&gt;之前的工作&lt;/a&gt;，我们表明该电路分为 3 个阶段：&lt;ul&gt;&lt;li&gt;&lt;strong&gt;令牌串联&lt;/strong&gt;：注意力层 0 和 1 将运动员姓名的令牌组装到最终名称令牌上，作为多个不同表示的总和，每个令牌一个。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;事实查找&lt;/strong&gt;：最终名称标记上的 MLP 2 到 6 将连接的标记映射到运动员运动的线性表示 - 多标记嵌入。&lt;ul&gt;&lt;li&gt;值得注意的是，这仅取决于名称，而不取决于先前的上下文。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;属性提取&lt;/strong&gt;：一组稀疏的中后期注意力头提取运动子空间并将其移动到最终标记，并直接与输出连接。 &lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iGuwZTHWb6DFY3sKB/qv8qfrczagg388exbycd" /&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX/p/CW5onXm6uZxpbpsRk"&gt;&lt;strong&gt;帖子 3：&lt;/strong&gt;从机制上理解早期 MLP 层&lt;/a&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;我们重点从机械角度理解 MLP 2 到 6 如何将连接的标记实际映射到运动的线性表示。&lt;/li&gt;&lt;li&gt;我们探索支持和反对这些 MLP 层中机械发生情况的不同假设的证据。&lt;/li&gt;&lt;li&gt;我们证伪了一个天真的假设，即只有一个去标记化步骤，其中特定神经元中的 GELU 对原始标记执行布尔 AND 操作，并直接输出有关运动员的所有已知事实。&lt;/li&gt;&lt;li&gt;我们对正在发生的部分情况的最佳猜测是我们所说的&lt;a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX/p/CW5onXm6uZxpbpsRk#Hash_and_Lookup"&gt;“散列和查找”假设&lt;/a&gt;，但它是混乱的，最强烈的错误形式，这只是故事的一部分。我们提供了一些支持和反对该假设的证据。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX/p/JRcNNGJQ3xNfsxPj4"&gt;&lt;strong&gt;第四篇：&lt;/strong&gt;如何思考解释记忆&lt;/a&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;我们退后一步来比较执行泛化任务的 MLP 和在玩具数据集上执行记忆任务（如事实回忆）的 MLP 之间的差异。&lt;/li&gt;&lt;li&gt;我们考虑在查找记忆数据的网络的输入和中间状态中可用的表示形式，并认为（在某种程度上，任务是通过记忆而不是泛化来完成的）在纯粹的查找过程中没有理由期望有意义的中间表示。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX/p/xE3Y9hhriMmL4cpsR"&gt;&lt;strong&gt;第 5 篇&lt;/strong&gt;：早期层是否专门从事本地处理？&lt;/a&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;我们探索了一个更强有力且有点离题的假设，&lt;em&gt;即一般来说&lt;/em&gt;，早期层专门处理最近的标记，只有中间层集成先前的上下文。&lt;/li&gt;&lt;li&gt;我们发现这在一定程度上是正确的，但并不完全正确：早期层专门处理局部上下文，但截断上下文仍然会丢失一些东西，并且这种效果在第 0 层和第 10 层（共 32 个）之间逐渐减弱。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;我们还展示了一系列杂项观察结果，希望对从事这项工作的人们有用。&lt;/p&gt;&lt;h2&gt;动机&lt;/h2&gt;&lt;h3&gt;为什么要叠加&lt;/h3&gt;&lt;p&gt;叠加真的很烦人，也很重要，而且非常好理解。叠加是一种现象，在&lt;a href="https://transformer-circuits.pub/2022/toy_model/index.html"&gt;叠加玩具模型&lt;/a&gt;中研究得最为显着，其中模型代表的特征多于其神经元/维度的特征&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-aYS73ikuT9JzFD3Mj-2" id="fnref-aYS73ikuT9JzFD3Mj-2"&gt;[2]&lt;/a&gt;&lt;/sup&gt; 。我们认为理解如何对叠加电路进行逆向工程是机械可解释性中主要的开放问题之一。&lt;/p&gt;&lt;p&gt;为什么这么烦人？有时模型中的电路是&lt;strong&gt;稀疏的&lt;/strong&gt;，它只涉及标准基础中的一小部分组件（头、神经元、层等），有时它是&lt;strong&gt;分布式的&lt;/strong&gt;，涉及许多组件。大约所有现有的机械插值工作都利用稀疏性，例如，标准工作流程正在识别一个重要组件（从输出逻辑开始），识别与之组成的最重要的事物，然后递归。我们真的不知道如何处理分布式电路，但这些经常发生（特别是在非樱桃选择的设置中！）&lt;/p&gt;&lt;p&gt;为什么它如此重要？因为这种情况似乎经常发生，而机甲插补可能无法摆脱不理解涉及叠加的事情。我们个人推测，叠加是缩放 LLM 如此有效&lt;em&gt;的&lt;/em&gt;关键部分&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-aYS73ikuT9JzFD3Mj-3" id="fnref-aYS73ikuT9JzFD3Mj-3"&gt;[3]&lt;/a&gt;&lt;/sup&gt; ，直观地看，GPT-4 与 GPT-3.5 已知的事实数量在神经元数量上呈超线性缩放，更不用说残差流维度了。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-aYS73ikuT9JzFD3Mj-4" id="fnref-aYS73ikuT9JzFD3Mj-4"&gt;[4]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;为何理解甚少？到目前为止，关于叠加的工作相当有限。 《叠加玩具模型》是一篇很棒的论文，但纯粹是在研究玩具模型，这可能会产生很大的误导——你的见解只有在你的玩具模型忠实于潜在现实的情况下才有效&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-aYS73ikuT9JzFD3Mj-5" id="fnref-aYS73ikuT9JzFD3Mj-5"&gt;[5]&lt;/a&gt;&lt;/sup&gt; 。 Neurons In A Haystack 是研究真实法学硕士中叠加的主要论文，并发现它们对于去标记化（将原始标记转换为概念）具有重要意义的证据，但缺乏神经元级别的真正机制分析。&lt;/p&gt;&lt;p&gt;这使得当我们说“我们这项工作的目标是研究真实语言模型中的叠加”时，甚至很难准确定义我们的意思。我们最好的操作是尝试研究一个我们期望高度分布但有目的的电路，以及我们期望发生压缩的地方。我们目标的一个重要部分是更好地消除我们在谈论真实模型中的叠加时所谈论的内容。请参阅&lt;a href="https://transformer-circuits.pub/2022/toy_model/index.html#motivation"&gt;《叠加玩具模型》&lt;/a&gt;和&lt;a href="https://arxiv.org/pdf/2305.01610.pdf#page=17"&gt;《干草堆中的神经元》附录 A，&lt;/a&gt;了解之前的有价值的讨论和对其中一些基本概念的阐述。&lt;/p&gt;&lt;h3&gt;以电路为中心与以表示为中心的可解释性&lt;/h3&gt;&lt;p&gt;首先近似，可解释性工作通常关注&lt;strong&gt;表示&lt;/strong&gt;（如何在内部表示输入的属性）或&lt;strong&gt;电路&lt;/strong&gt;（理解计算和使用表示的权重中编码的算法）。完整的基于电路的理解通常需要对表示有适当的理解，并且通常更加严格和可靠，并且可以填补仅研究表示的弱点。&lt;/p&gt;&lt;p&gt;对于叠加，最近出现了一系列令人兴奋的工作，重点关注稀疏自动编码器形式的表示（例如&lt;a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html"&gt;Bricken 等人&lt;/a&gt;、 &lt;a href="https://arxiv.org/abs/2309.08600"&gt;Cunningham 等人&lt;/a&gt;）。我们认为稀疏自动编码器在学习许多可解释特征（这些特征通常是分布式的并且与神经元基础不对齐）方面的成功进一步证明了叠加是常态。&lt;/p&gt;&lt;p&gt;在这项工作中，我们希望通过事实回忆的具体案例研究来了解叠加背后的电路的一般见解，尽管它大多没有成功。在事实回忆的具体情况下，我们的建议是专注于理解事实表示，而不用担心它们到底是如何计算的。&lt;/p&gt;&lt;h3&gt;为什么事实&lt;/h3&gt;&lt;p&gt;我们专注于了解早期 MLP 层在事实回忆中查找事实的作用。也就是说，我们针对在 Pythia 2.8B 中打棒球、篮球或（美式）橄榄球的 1,500 名运动员研究了“事实：迈克尔·乔丹从事的运动”-&amp;gt;“篮球”形式的一次性提示&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-aYS73ikuT9JzFD3Mj-6" id="fnref-aYS73ikuT9JzFD3Mj-6"&gt;[6]&lt;/a&gt;&lt;/sup&gt; 。事实回忆电路之前已经被广泛研究，我们受到了&lt;a href="https://rome.baulab.info/"&gt;Meng等人&lt;/a&gt;、Chughtai等人（即将发表）、 &lt;a href="https://arxiv.org/abs/2304.14767"&gt;Geva等人&lt;/a&gt;、 &lt;a href="https://arxiv.org/abs/2308.09124"&gt;Hernandez等人和&lt;/a&gt;&lt;a href="https://arxiv.org/abs/2205.11482"&gt;Akyurek等人&lt;/a&gt;的影响，尽管这些人都没有专注于理解神经元水平的早期MLP。&lt;/p&gt;&lt;p&gt;我们认为事实本质上是非常有趣的——法学硕士之所以表现得如此出色，&lt;em&gt;很大程度上&lt;/em&gt;是因为他们记住了很多关于世界的事情，但我们也期望事实能够表现出显着的叠加性。该模型希望了解尽可能多的事实，因此有动力压缩它们，这与更多的算法任务（例如间接对象识别）形成鲜明对比，在间接对象识别中，您只需学习它们即可完成 - 总是有更多的事实需要学习！此外，事实很容易压缩，因为它们不会相互干扰（对于固定类型的属性），模型永远不需要在同一事实上注入“迈克尔·乔丹打篮球”和“贝比·鲁斯打棒球”这一事实令牌，它只能以不同的方式处理令牌“Jordan”和“Ruth” &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-aYS73ikuT9JzFD3Mj-7" id="fnref-aYS73ikuT9JzFD3Mj-7"&gt;[7]&lt;/a&gt;&lt;/sup&gt; （请参阅此处的&lt;a href="https://arxiv.org/pdf/2305.01610.pdf#page=18"&gt;常见问题解答问题 A.6 和 A.7&lt;/a&gt;以获取更详细的讨论）。我们猜测像 GPT-3 或 GPT-4 这样的模型知道的事实比它的神经元还要多&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-aYS73ikuT9JzFD3Mj-8" id="fnref-aYS73ikuT9JzFD3Mj-8"&gt;[8]&lt;/a&gt;&lt;/sup&gt; 。&lt;/p&gt;&lt;p&gt;此外，&lt;a href="https://arxiv.org/pdf/2305.01610.pdf"&gt;先前的工作&lt;/a&gt;表明，叠加似乎涉及早期 MLP 层的去&lt;strong&gt;标记化&lt;/strong&gt;，其中原始标记被组合成概念，例如“社会保障”与单独的“社会”或“安全”是一个非常不同的概念。关于人的事实似乎特别需要去标记化，因为许多不相关的人之间经常共享相同的名称标记（例如名字或姓氏），因此模型不能总是将事实存储在标记嵌入中。例如，“亚当·史密斯”的含义与“亚当·德赖弗”或“威尔·史密斯”的含义非常不同。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-aYS73ikuT9JzFD3Mj-9" id="fnref-aYS73ikuT9JzFD3Mj-9"&gt;[9]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;h2&gt;我们的高层次要点&lt;/h2&gt;&lt;h3&gt;如何思考事实？ &lt;/h3&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iGuwZTHWb6DFY3sKB/b8shmrcacihqfwbfn5ru" /&gt;&lt;/p&gt;&lt;p&gt; &lt;em&gt;（为方便起见，重复了上图）&lt;/em&gt;&lt;/p&gt;&lt;p&gt;在高层次上，我们建议将模型中事实知识的回忆视为&lt;strong&gt;多令牌嵌入&lt;/strong&gt;，这些嵌入主要由原始令牌的早期层（前 10-20% 层）形成&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-aYS73ikuT9JzFD3Mj-10" id="fnref-aYS73ikuT9JzFD3Mj-10"&gt;[10]&lt;/a&gt;&lt;/sup&gt; 。该模型学习识别分布在几个最近的原始令牌中的实体，例如“|”迈克尔| Jordan”，并在残差流中生成表示，其中线性子空间存储有关该实体的特定属性的信息&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-aYS73ikuT9JzFD3Mj-11" id="fnref-aYS73ikuT9JzFD3Mj-11"&gt;[11]&lt;/a&gt;&lt;/sup&gt; &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-aYS73ikuT9JzFD3Mj-12" id="fnref-aYS73ikuT9JzFD3Mj-12"&gt;[12]&lt;/a&gt;&lt;/sup&gt; 。&lt;/p&gt;&lt;p&gt;我们认为早期的注意力层通过将这些实体的原始标记组装到最终标记（“Jordan”位置）上来执行&lt;strong&gt;标记串联&lt;/strong&gt;。早期的 MLP 层对原始标记执行&lt;strong&gt;事实查找&lt;/strong&gt;、布尔 AND（又名去&lt;a href="https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=7G70mqwvXn7LoiOBc6-IRuLo"&gt;标记化&lt;/a&gt;），以输出带有有关所描述实体的信息的多标记嵌入。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-aYS73ikuT9JzFD3Mj-13" id="fnref-aYS73ikuT9JzFD3Mj-13"&gt;[13]&lt;/a&gt;&lt;/sup&gt;值得注意的是，我们认为这种查找是由&lt;em&gt;多个&lt;/em&gt;MLP 层实现的，并且不能定位于特定神经元甚至单个层。&lt;/p&gt;&lt;p&gt;一个重要的警告是，这是从查找运动员事实的详细研究中推断出来的，而不是更广泛的事实回忆任务，尽管得到了先前文献的广泛支持。我们的推测性预测是，“认识到上下文中提到了某个实体并回忆起它们的属性”形式的事实回忆看起来像这样，但更复杂或更复杂的事实回忆形式可能不是（例如多跳推理或回忆）具有更丰富或更分层的结构）。&lt;/p&gt;&lt;p&gt;我们在理解这些早期 MLP 层的内部机制方面并没有取得太大进展。我们的看法是，最好将这些早期层黑盒为“产生多令牌嵌入的东西”，并专注于理解这些嵌入中有意义的子空间以及它们如何被后面层的电路使用，而不是确切地说，它们是如何计算的，即专注于表示。这类似于我们通常将令牌嵌入视为一个大查找表的方式。一个主要区别是令牌嵌入的输入空间是单个令牌 d_vocab（通常约为 50,000），而完整模型或其他子电路的输入是令牌字符串，其输入空间要大得多。虽然从技术上讲，您可以将语言模型视为一个巨大的查找表，但这并不是很有成效！这在这里不是问题，因为我们关注的是已知实体（一个小得多的集合）的（标记化）名称的输入。&lt;/p&gt;&lt;p&gt;我们认为机械插值的目标是对下游任务有用（即理解有关模型行为和认知的有意义的问题，特别是与对齐相关的问题！），并且我们预计下游任务的大部分有趣计算将出现在中后期层。理解这些可能需要将更简单表示的字典作为给定，但希望不需要理解更简单表示是如何计算的。&lt;/p&gt;&lt;p&gt;我们认为在理解内部机制方面缺乏进展是反对对模型中的所有内容进行完全逆向工程的目标的有意义的证据，但我们认为机械插值可以在不实现雄心勃勃的目标的情况下有用。我们对有效进行机械插补的愿景看起来更像是尽可能多地对模型进行逆向工程，放大关键区域，并将许多电路保留为较小的黑盒子模块，因此这并不会让我们对机械插补更加悲观与对齐相关。因此，我们认为将这种类型的事实回忆作为这些黑盒子模块之一是可以的，但请注意，任何给定任务的失败都会让您对未来任何机甲解释任务的成功更加悲观。&lt;/p&gt;&lt;h3&gt;我们没有得到太多关注，这是否令人惊讶？&lt;/h3&gt;&lt;p&gt;事后看来，我们对未能解释早期 MLP 层并不感到非常惊讶（尽管我们很高兴我们检查了！）。从概念上讲，在算法层面上，事实回忆很可能被实现为一个巨大的查找表。正如许多学生可以证明的那样，在记忆事实时根本没有太多结构可供利用：知道“蒂姆·邓肯”应该映射到“篮球”与知道“乔治·布雷特”应该映射到没有多大关系“棒球”；这些必须单独实施。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-aYS73ikuT9JzFD3Mj-14" id="fnref-aYS73ikuT9JzFD3Mj-14"&gt;[14]&lt;/a&gt;&lt;/sup&gt; （我们刻意关注事实回忆的形式，而没有更丰富的底层结构。）&lt;/p&gt;&lt;p&gt;巨大的查找表具有很高的最小描述长度，这表明实现将涉及很多参数。这并不一定意味着它很难解释：我们可以设想很好的可解释实现，例如每个实体一个神经元，但这些似乎效率很低。理论上，可能存在更高效的可解释方案（我们稍后会讨论、测试和证伪一些方案，例如&lt;a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX/p/CW5onXm6uZxpbpsRk#Single_Step_Detokenization"&gt;单步去标记化&lt;/a&gt;），但至关重要的是，与更多算法任务不同，模型应该使用良好且可解释的实现并没有强有力的&lt;em&gt;理由&lt;/em&gt;像感应头。&lt;/p&gt;&lt;p&gt;从机制上讲，训练神经网络是一个巨大的曲线拟合问题，并且可能有许多密集且难以解释的方法可以成功实现这一目标。只有当问题存在某种训练过程可以利用的结构时，我们才应该期望可解释性。在事实回忆的情况下，每个运动员的初始表示只是激活空间中的一个特定点&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-aYS73ikuT9JzFD3Mj-15" id="fnref-aYS73ikuT9JzFD3Mj-15"&gt;[15]&lt;/a&gt;&lt;/sup&gt; ，并且（几乎）没有可利用的结构，因此毫不奇怪，我们得到了密集且难以解释的结果。在第 4 篇文章中，我们还研究了一个将整数对映射到任意标签的玩具模型，其中我们知道所有数据并且可以生成任意数量的数据，但在寻找内部数据方面，并没有发现玩具模型更容易解释。稀疏或有意义的中间状态。&lt;/p&gt;&lt;h3&gt;寻找基于事实表示的电路，而不是计算它们&lt;/h3&gt;&lt;p&gt;在机械解释中，我们既可以研究&lt;strong&gt;特征&lt;/strong&gt;，也可以研究&lt;strong&gt;电路&lt;/strong&gt;。特征是输入的属性，以内部激活表示（例如，“在第 16 层恢复当前句子的情感后将残余流投影到方向 v”）。电路是模型内部的算法，通常采用更简单的特征并使用它们来计算更复杂的特征。有些工作优先考虑&lt;a href="https://arxiv.org/abs/2310.01405"&gt;理解特征&lt;/a&gt;，其他工作优先考虑理解电路&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-aYS73ikuT9JzFD3Mj-16" id="fnref-aYS73ikuT9JzFD3Mj-16"&gt;[16]&lt;/a&gt;&lt;/sup&gt; ，我们认为两者都是机械解释性的有效方法。&lt;/p&gt;&lt;p&gt;在事实回忆的具体情况下，我们试图理解查找实体属性的电路，但基本上失败了。我们认为未来工作的一个合理的中间点是专注于理解这些查找到的属性是如何表示的，以及它们如何被后面层的下游电路使用，将早期层的事实回忆电路视为一个小的黑盒子模块。此外，由于早期残余流&lt;a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX/p/3tqJ65kuTkBh8wrRH#Context_doesn_t_matter_when_looking_up_an_athlete_s_sport"&gt;很大程度上对运动员名字之前的上下文不敏感&lt;/a&gt;，事实注入似乎是早期 MLP 可以做的&lt;em&gt;唯一&lt;/em&gt;事情，这表明不尝试解释早期 MLP 层（在这种情况下）不会造成什么损失。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-aYS73ikuT9JzFD3Mj-17" id="fnref-aYS73ikuT9JzFD3Mj-17"&gt;[17]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;h2&gt;我们学到了什么&lt;/h2&gt;&lt;p&gt;我们将调查分为四个后续帖子：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;使用因果技术对事实回忆电路进行详细分析，将事实定位到 MLP 2 至 6，并了解有关电路的一系列高级事实&lt;/li&gt;&lt;li&gt;深入研究这些 MLP 层的内部机制，考虑几种假设并收集支持和反对的证据，试图（但基本上未能）从权重中获得机械理解。&lt;/li&gt;&lt;li&gt;一篇较短的文章探讨了事实回忆/记忆的玩具模型，以及它们如何对 Pythia 来说同样难以解释，以及这如何支持事实回忆如何一般无法解释的概念性论证&lt;/li&gt;&lt;li&gt;一篇较短的文章探讨了多令牌嵌入假设的更强大和更通用的版本：&lt;em&gt;一般来说，&lt;/em&gt;早期层中的残留流是最近令牌的函数，而更后面的上下文仅出现在中间层。发生这种情况的总体趋势是，但早期层中仍然使用一些更广泛的上下文&lt;/li&gt;&lt;/ol&gt;&lt;h3&gt;我们对电路的了解（&lt;a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX/p/3tqJ65kuTkBh8wrRH"&gt;第 2 篇&lt;/a&gt;总结）&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt; Geva 等人的高级图片基本成立：给出诸如“事实：迈克尔·乔丹从事的运动”之类的提示，模型在乔丹代币上去标记“迈克尔和乔丹”，以代表模型所知道的有关迈克尔·乔丹的&lt;em&gt;所有&lt;/em&gt;事实。在第 6 层之后，这项运动在 Jordan 代币上清晰地线性表示。有中后期事实提取注意头，从“of”到“Jordan”，并将运动映射到输出逻辑（参见 Chughtai 等人（即将发表）对这些头的详细研究，特别是发现这些头仍然关注并提取运动员的运动项目，即使关系要求非运动属性）。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;我们将事实提取电路简化为我们所说的&lt;strong&gt;有效模型&lt;/strong&gt;&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-aYS73ikuT9JzFD3Mj-18" id="fnref-aYS73ikuT9JzFD3Mj-18"&gt;[18]&lt;/a&gt;&lt;/sup&gt; 。对于两个令牌运动员姓名，这由注意力层 0 和 1 组成，它们检索先前的令牌嵌入并将其添加到当前的令牌嵌入中。这些求和的令牌嵌入然后经过 MLP 第 2 层到第 6 层，最后运动是线性可恢复的。注意力层 2 以后的层可以被消融，MLP 层 1 也可以被消融。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;线性分类器可以通过逻辑回归进行训练。我们还通过将某个注意力头（第 16 层、第 20 层）映射到棒球、篮球、足球这三种运动的输出 logits 的方向，发现了高性能（86% 的准确率） &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-aYS73ikuT9JzFD3Mj-19" id="fnref-aYS73ikuT9JzFD3Mj-19"&gt;[19]&lt;/a&gt;&lt;/sup&gt; 。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;我们简化了分析以解释有效模型，其中模型的“末端”是三个运动之间的线性分类器和 softmax。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;我们认为“一旦特征是线性可恢复的，就足以解释线性特征提取&lt;em&gt;是如何&lt;/em&gt;发生的，并且电路的其余部分可以被忽略”的见解可能在其他电路分析中有用，特别是考虑到训练是多么容易线性探针（给定标记数据）。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;我们对 MLP 的了解（&lt;a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX/p/CW5onXm6uZxpbpsRk"&gt;第 3 篇&lt;/a&gt;总结）&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;一旦我们放大了 MLP 2 到 6 中存储的事实，我们就正在发生的事情形成、测试&lt;strong&gt;并证伪了&lt;/strong&gt;一些假设：&lt;ul&gt;&lt;li&gt;笔记;这些都是相当具体和详细的​​假设，对于我（尼尔）对可能发生的事情的直觉来说，它们是相当特殊的。所有&lt;em&gt;可能的&lt;/em&gt;假设的空间都很大。&lt;/li&gt;&lt;li&gt;事实上，对这些假设的调查让我们认为它们可能是错误的，这让我们了解了法学硕士是如何运作的以及问题的混乱本质。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;单步去标记化假设&lt;/strong&gt;：有一堆“去标记化”神经元直接与原始标记嵌入组合，使用它们的 GELU 来模拟布尔 AND，例如 prev==Michael &amp;amp;&amp;amp; curr==Jordan，并输出所有事实的线性表示该模特了解迈克尔·乔丹&lt;ul&gt;&lt;li&gt;重要的是，这个假设声称相同的神经元对给定主题的所有事实都很重要，并且原始标记和属性之间不存在中间状态。&lt;/li&gt;&lt;li&gt;&lt;a href="https://arxiv.org/pdf/2305.01610.pdf"&gt;之前的研究&lt;/a&gt;表明，去标记化是通过叠加完成的，这表明一种比每个运动员只有一个神经元更复杂的机制。我假设这是通过某种组合设置来实现的，其中每个神经元对许多字符串进行去标记，并且每个字符串由许多神经元进行去标记，但每个字符串对应于高于某个阈值的神经元的唯一&lt;em&gt;子集&lt;/em&gt;。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;我们非常有信心单步去代币化假设是错误的&lt;/strong&gt;（至少在强版本中，尽管可能仍然存在真理的核心）。证据：&lt;/li&gt;&lt;li&gt;路径修补（噪声）显示 MLP 层之间存在有意义的组合，表明中间表示的存在。 &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iGuwZTHWb6DFY3sKB/aububbyo97pul54yhl22" /&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;我们收集了模型了解的有关迈克尔·乔丹的多个事实。然后，我们对每个神经元（一次一个）重新采样消融&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-aYS73ikuT9JzFD3Mj-20" id="fnref-aYS73ikuT9JzFD3Mj-20"&gt;[20]&lt;/a&gt;&lt;/sup&gt; （补丁），并测量其对输出正确答案的影响。单步去标记化假设预测相同的神经元对每个事实都很重要。我们测量了对每对事实的影响之间的相关性，并观察到很少的相关性，这表明每个事实有不同的机制。 &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/zsa7a3aqscgpfiol8ogh" /&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;通过测量每个神经元的非线性效应（迈克尔·乔丹比基思·乔丹或迈克尔·史密斯的激活程度更高），我们发现直接与探针连接的神经元在 GELU 之前就已经具有显着的非线性效应。这表明它们由计算“与”的早期神经元组成，而不是由直接影响“与”本身的神经元组成，即存在重要的中间状态&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;哈希和查找假设&lt;/strong&gt;：前几个 MLP 将运动员的原始标记“哈希”为几乎与其他名称正交的名称的格式塔表示，然后其余 MLP 中的神经元通过执行操作“查找”这些哈希表示作为将它们映射到特定运动的查找表。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;散列可以起到的作用之一是打破原始令牌之间的线性结构&lt;ul&gt;&lt;li&gt;例如“迈克尔·乔丹”和“蒂姆·邓肯”打篮球，但我们不想认为“迈克尔·邓肯”打篮球&lt;/li&gt;&lt;li&gt;换句话说，这让模型形成了“Michael Jordan”的表示，但不一定与“Michael”或“Jordan”的表示相似&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;重要的是，哈希应该适用于&lt;em&gt;任何&lt;/em&gt;标记组合，而不仅仅是模型在训练中记住的标记，尽管查找可能不适用于未知名称。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;我们非常有信心哈希和查找假设的&lt;em&gt;强&lt;/em&gt;版本是错误的&lt;/strong&gt;，尽管我们发现它是一个有用的框架，并且认为它可能有一定的道理&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-aYS73ikuT9JzFD3Mj-21" id="fnref-aYS73ikuT9JzFD3Mj-21"&gt;[21]&lt;/a&gt;&lt;/sup&gt; 。不幸的是，“这有一定道理，但不是故事的全部”，结果很难证明或证伪。证据：&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt; （负面）如果您在残余流上线性探测运动，则整个“散列”层（MLP 2 至 4）的验证准确性都会提高，这表明散列和查找之间不可能存在清晰的分层分离。纯哈希没有结构，因此在中间哈希层期间验证准确性应该是随机的。 &lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/b5xo66fwzwxmue0b1ytb" /&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt; （负）即使在 MLP1 和 MLP2 上，已知名称也比未知名称具有更高的 MLP 输出范数 - 它不会将已知名称和未知名称视为相同。 MLP1 和 MLP2 是哈希层，因此模型不应该这么早就区分已知名称和未知名称。 （这是微弱的证据，因为可能存在一个摊销哈希，它对常用哈希术语具有更有效的子例程，但仍然证伪了假设的强版本，即哈希发生时不考虑潜在含义。）&lt;/li&gt;&lt;li&gt; （正面）该模型确实显着打破了代币之间的线性结构。例如，“Michael Jordan”的残差中有一个重要组成部分没有被“Michael Smith”或“John Jordan”等名字的平均值所捕获。这正是我们期望哈希所做的事情。 * 然而，这并不是 MLP 层的&lt;em&gt;主要&lt;/em&gt;作用是打破线性结构的有力证据，它们可能还做了很多其他的事情。尽管它们确实比随机初始化的层更能破坏线性结构&lt;/li&gt;&lt;li&gt;我们发现了一个&lt;strong&gt;棒球神经元&lt;/strong&gt;（L5N6045），它对棒球运动员的名字更加激活，并直接与事实提取头组成&lt;ul&gt;&lt;li&gt;神经元的输入和输出权重与头部介导的棒球方向具有显着的余弦模拟（但它明显小于 1）：它部分只是增强现有方向，但部分做更多的事情&lt;ul&gt;&lt;li&gt;如果你将每个运动员的分量与其他 1500 名运动员正交，并将它们投射到棒球神经元上，棒球运动员的神经元仍然比其他运动员更活跃，这也许表明它也充当了查找表的角色。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;着眼于更广泛的数据分布，而不仅仅是运动员姓名，它通常会在与运动相关的事物上激活，但不是单一语义的。 &lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/z4lgnpzcuyzlngtepv16" /&gt;&lt;/p&gt;&lt;h3&gt;如何思考解释记忆（&lt;a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX/p/JRcNNGJQ3xNfsxPj4"&gt;第四篇&lt;/a&gt;总结）&lt;/h3&gt;&lt;p&gt;&lt;em&gt;这是一篇更具哲学性且不那么严格的帖子，思考记忆及其背后的建模假设，其灵感来自于玩具模型的工作，但缺乏严格的实证工作。&lt;/em&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;动机：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;当我们试图从机制上理解网络如何进行“事实查找”操作时，我们取得的进展甚微。&lt;/li&gt;&lt;li&gt;当我们计算分布式表示时，我们应该从解释 MLP 能力的失败中吸取什么教训？特别是，我们&lt;em&gt;期望&lt;/em&gt;在子网络的中间激活中找到哪些有意义的表示？&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;在第 4 篇文章中，我们研究了记忆的玩具模型（1-2 个 MLP 层将整数对映射到固定和随机的二进制标签），以在更纯粹的环境中研究记忆。以下是在这次调查后尝试提炼我们的直觉，我们认为这并不完全严格，但希望它可能具有启发性。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;我们相信事实查找的一个重要方面，与大多数 MLP 训练的计算相反，是它主要涉及记忆：没有通用规则可以帮助您仅根据运动员的名字猜测运动员从事什么运动（至少，任何高精度&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-aYS73ikuT9JzFD3Mj-22" id="fnref-aYS73ikuT9JzFD3Mj-22"&gt;[22]&lt;/a&gt;&lt;/sup&gt; ）。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;如果网络通过泛化来解决任务，我们可能期望找到其中间状态的表示，对应于它为解决任务而实现的泛化算法中的输入和中间值。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-aYS73ikuT9JzFD3Mj-23" id="fnref-aYS73ikuT9JzFD3Mj-23"&gt;[23]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;另一方面，如果网络无法通过泛化来解决任务（因此必须记住），这表明要么不存在泛化算法，或者至少没有任何可用于实现可能的泛化算法的特征可访问网络。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;因此，在记忆场景下，我们应该在子网络的输入和中间表示中找到的唯一有意义的表示（与所研究的任务相关）是：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;原始输入本身的表示（例如，我们可以找到诸如“名字是乔治”之类的特征）。&lt;/li&gt;&lt;li&gt;正在查找的目标概念的噪声表示：例如，前几个 MLP 层之后的残余流包含噪声较大但仍然有些准确的运动表示，因为它是整个网络输出的部分总和。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-aYS73ikuT9JzFD3Mj-24" id="fnref-aYS73ikuT9JzFD3Mj-24"&gt;[24]&lt;/a&gt;&lt;/sup&gt; &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-aYS73ikuT9JzFD3Mj-25" id="fnref-aYS73ikuT9JzFD3Mj-25"&gt;[25]&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;另一方面，在任何有限输入域上——例如所有名字/姓氏标记的笛卡尔积——&lt;em&gt;任何&lt;/em&gt;模型组件（神经元、神经元层或多层神经元）的输出都可以解释为嵌入矩阵：即将每个名字/姓氏对单独映射到特定输出向量的查找表。通过这种方式，我们可以将运动查找子网络（或该网络中的任何组件）视为实现将名称标记对映射到运动表示的查找表。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;从某种意义上说，这种观点提供了对 MLP 子网络执行的计算的简单解释：子网络的权重使得——在运动员运动查找任务的范围内——子网络准确地实现了正确表示所需的查找表。知名运动员的运动。从定义的意义上来说，记忆是一项不应该有有趣的中间表征需要解释的任务。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;早期层是否专门从事本地处理？ （&lt;a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX/p/xE3Y9hhriMmL4cpsR"&gt;第 5 篇&lt;/a&gt;总结）&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;动机&lt;ul&gt;&lt;li&gt;&lt;p&gt;多令牌嵌入假设表明，早期层的一个重要功能是将构成语义单元（例如&lt;code&gt;[ George] [ Brett]&lt;/code&gt; ）的令牌收集在一起，并“查找”该单元的嵌入，以便该单元的重要特征单元（例如“打棒球”）是线性可恢复的。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;这让我们思考：早期层所做的&lt;em&gt;一切&lt;/em&gt;在多大程度上是这样的？ &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-aYS73ikuT9JzFD3Mj-26" id="fnref-aYS73ikuT9JzFD3Mj-26"&gt;[26]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;当然，似乎许多语言任务都需要在语言级处理开始之前查找单词/其他多标记实体。特别是，许多单词被分成多个标记，但似乎最好将其视为一个单元。这些任务只能在查找发生后（即在后面的层中）完成。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;另一方面，没有理由等到多令牌查找完成后才继续进行单令牌级别处理（例如简单的归纳行为）。因此可以想象，早期层也不&lt;em&gt;全&lt;/em&gt;是关于多令牌嵌入查找：它们可以在执行多令牌查找的同时并行处理其他令牌级任务。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;如果早期层仅执行多令牌查找，则可观察到的结果是早期层的激活应该主要是附近上下文的函数。为了使多令牌查找起作用，只有最近的几个令牌才重要。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;因此，为了衡量早期层执行多令牌查找的程度，我们进行了以下实验：&lt;ul&gt;&lt;li&gt;收集 Pythia 2.8B 各层的残余流激活，以获取 Pile 中的字符串样本。&lt;/li&gt;&lt;li&gt;从具有不同长度的截断上下文的相同字符串中收集令牌的激活：即，对于每个采样的令牌，我们将收集该令牌加上零个前面的令牌、一个前面的令牌等，最多九个前面的令牌的激活。&lt;/li&gt;&lt;li&gt;通过计算（以平均为中心的）余弦相似度，测量完整上下文中与截断上下文中标记的残余流激活之间的相似性。&lt;/li&gt;&lt;li&gt;如果早期层仅执行局部处理，则截断上下文激活和完整上下文激活之间的余弦相似度应该接近于 1（至少对于足够长的截断窗口）。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;收集结果后，我们得出以下观察结果：&lt;ul&gt;&lt;li&gt;早期层确实比后面层执行更多的局部处理：截断上下文和完整上下文之间的余弦相似度在早期层中明显较高，而在中后期层中较低。&lt;/li&gt;&lt;li&gt;然而，具有零先验上下文和具有一些先验上下文的余弦模拟之间存在显着差异，这表明局部层有意义地依赖于附近的上下文（即它们不简单地处理当前标记）。&lt;/li&gt;&lt;li&gt;然而，即使在第 0 层，局部层也确实对远程上下文有一定的依赖性：虽然余弦 sim 很高（高于 0.9），但它们并不接近 1（通常 &amp;lt;0.95） &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-aYS73ikuT9JzFD3Mj-27" id="fnref-aYS73ikuT9JzFD3Mj-27"&gt;[27]&lt;/a&gt;&lt;/sup&gt; 。 &lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/xE3Y9hhriMmL4cpsR/yyrikd6m6xpbzqte3pnh" /&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;有趣的是，我们发现早期层的截断与完整上下文余弦模拟在标点符号（句号、逗号、换行符等）和“停用词”（and、or、with、of 等）方面特别低——这表明早期层对这些令牌执行高度非本地处理。&lt;ul&gt;&lt;li&gt;回想起来，这并不奇怪，因为紧邻这些标记之前的上下文通常不会给它们的含义增加太多，至少与像 [ality] 这样的单词片段标记相比，所以处理它们可以尽早开始（无需等待首先进行去标记化步骤）。&lt;/li&gt;&lt;li&gt;这些标记的表示可能还有其他几个目的——例如摘要、静止位置、计数分隔符——这可以解释它们对更长范围上下文的依赖。 &lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/xE3Y9hhriMmL4cpsR/b8aqjddkgqwgooduk5b2" /&gt;&lt;/p&gt;&lt;h2&gt;进一步讨论&lt;/h2&gt;&lt;h3&gt;稀疏自动编码器是否使理解事实回忆变得更容易？&lt;/h3&gt;&lt;p&gt;最近一个有前途的方向是&lt;a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html"&gt;训练稀疏自动编码器&lt;/a&gt;（SAE）从叠加中提取单语义特征。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-aYS73ikuT9JzFD3Mj-28" id="fnref-aYS73ikuT9JzFD3Mj-28"&gt;[28]&lt;/a&gt;&lt;/sup&gt;我们自己还没有探索过 SAE &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-aYS73ikuT9JzFD3Mj-29" id="fnref-aYS73ikuT9JzFD3Mj-29"&gt;[29]&lt;/a&gt;&lt;/sup&gt; ，因此对此不能有信心，但我们的猜测是，SAE 并没有解决我们理解事实本地化背后的电路的问题，尽管可能有助于缩小范围假设。&lt;/p&gt;&lt;p&gt;核心问题是，SAE 是一种通过将表示分解为单语义特征来更好地理解&lt;em&gt;表示&lt;/em&gt;的工具，而我们想要了解事实回忆&lt;em&gt;电路&lt;/em&gt;：MLP 层的参数如何实现将名称标记映射到所玩运动的算法？ SAE 中缺少的关键部分是，如果 MLP 层（GELU 之前和之后）中的有意义的特征没有基础对齐，那么我们需要（理论上）了解该层中&lt;em&gt;每个&lt;/em&gt;神经元的功能来理解映射。相反，基础对齐特征（即对应于GELU前后的单个神经元）可以根据单个GELU来理解，并且所有其他神经元的值是不相关的。&lt;/p&gt;&lt;p&gt;更好地理解表示可能是一个非常有用的中间步骤，有助于阐明电路，但我们预计这在这里不太有用。 SAE 可能学习的明显特征是名字（例如“名字是迈克尔”）、姓氏（例如“姓氏是乔丹”）和运动（例如“打篮球”）。似乎很可能有一个对应于每个实体的中间特征，例如（“是迈克尔乔丹”），它可以通过极其广泛的 SAE 来学习（尽管它需要非常广泛！那里有很多实体），尽管我们不期望有许多与实体组相对应的有意义的特征&lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-aYS73ikuT9JzFD3Mj-30" id="fnref-aYS73ikuT9JzFD3Mj-30"&gt;[30]&lt;/a&gt;&lt;/sup&gt; 。但拥有这些中间表示显然不足以理解给定 MLP 层在参数级别上的工作原理。&lt;/p&gt;&lt;p&gt;我们还可以从在不同层的残差流或 MLP 激活上训练的 SAE 中学到有用的东西。如果我们看到急剧的过渡，即该运动仅在第 4 层或其他层之后才成为一个功能，这将是一个重要提示！但不幸的是，我们不希望在这里看到急剧的转变，您甚至可以在令牌嵌入上进行非平凡的运动探测，并且它在关键的 MLP 层（MLP 2 到 6）上顺利地变得更好。&lt;/p&gt;&lt;h3&gt;未来的工作&lt;/h3&gt;&lt;p&gt;就机械解释的总体影响而言，我们认为最重要的主张是事实回忆中的早期 MLP 实现了“多令牌嵌入”，即对于名称由多个令牌组成的已知实体，早期 MLP 产生以下线性表示：已知的属性，更推测的是，我们&lt;em&gt;不需要&lt;/em&gt;机械地理解这些多令牌嵌入是如何计算的，以便机械插值有用（只要我们能够理解它们代表什么， &lt;a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX/p/iGuwZTHWb6DFY3sKB#Look_for_circuits_building_on_factual_representations__not_computing_them"&gt;如上所述&lt;/a&gt;）。就未来的工作而言，我们很高兴人们能够组成红队，并以我们的主张为基础，即事实查找机制&lt;em&gt;是&lt;/em&gt;形成这些多令牌嵌入。我们也会对寻找具体案例的工作感到兴奋，在这些案例中，我们确实需要理解叠加中的紧密耦合计算（用于事实回忆或其他），以便机械插值在具有现实世界/对齐结果的下游任务中有用，或者只是在与事实记忆有本质不同的任务上。&lt;/p&gt;&lt;p&gt;至于是否应该进一步开展工作来尝试对事实回忆背后的机制进行逆向工程，我们不确定。我们发现这很困难，并且在整个项目大约 6-7 个 FTE 个月后基本上放弃了，通过这个项目，我们建立了直觉，这可能从根本上是无法解释的。但我们还远未达到可以自信地排除这种可能性的地步，并且预计还有许多我们尚未考虑的潜在富有成效的观点和技术。如果我们的上述说法属实，我们不确定是否需要解决这个问题才能使机甲插补发挥作用，但我们仍然很高兴看到这里的进展。如果不出意外的话，我们希望它能够教会我们有关叠加计算的一般课程，我们希望这些课程能够广泛应用于模型中。而且，从美学角度来看，我们发现相当令人不满意的是，尽管我们对模型如何回忆事实有了更深入的了解，但我们并没有形成完整的参数级别的理解。&lt;/p&gt;&lt;h2&gt;致谢&lt;/h2&gt;&lt;p&gt;我们感谢 Joseph Bloom、Callum McDougall、Stepan Shabalin、Pavan Katta 和 Stefan Heimersheim 提供的宝贵反馈。我们特别感谢 Tom Lieberum 和 Sebastian Farquhar 所付出的努力，为我们提供了详细而富有洞察力的反馈，极大地改进了最终的文章。&lt;/p&gt;&lt;p&gt;我们也非常感谢 Tom Lieberum 参与了最初的研究冲刺，为这个项目奠定了种子，并为我们提供了启动所需的动力。&lt;/p&gt;&lt;p&gt;这项工作受益于与 Wes Gurnee 的早期讨论，特别是关于关注名称的去标记化以及将运动员作为一个良好类别的建议。&lt;/p&gt;&lt;h2&gt;作者贡献&lt;/h2&gt;&lt;p&gt;Neel 是该项目的研究负责人，Sen 是整个项目的核心贡献者。两人都对该项目的各个方面做出了重大贡献，包括技术贡献和写作。森领导了以玩具模型为重点的工作。&lt;/p&gt;&lt;p&gt; Janos 参加了为期两周的冲刺，帮助启动了该项目，并帮助编写和维护了我们用于检测 Pythia 的底层基础设施。罗欣为该项目提供了建议。&lt;/p&gt;&lt;h2&gt;引文&lt;/h2&gt;&lt;p&gt;请按如下方式引用该作品&lt;/p&gt;&lt;pre&gt;&lt;code&gt; @misc{ nanda2023factfinding, title={Fact Finding: Attempting to Reverse-Engineer Factual Recall on the Neuron Level}, url={https://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall}, journal={Alignment Forum}, author={Nanda, Neel and Rajamanoharan, Senthooran and Kram\&amp;#39;ar, J\&amp;#39;anos and Shah, Rohin}, year={2023}, month={Dec}}&lt;/code&gt; &lt;/pre&gt;&lt;hr class="footnotes-sep" /&gt;&lt;section class="footnotes"&gt;&lt;ol class="footnotes-list"&gt;&lt;li class="footnote-item" id="fn-aYS73ikuT9JzFD3Mj-1"&gt;&lt;p&gt;我们使用“选择”是因为正确“单元”中的标记数量在给定上下文中可能会有所不同。例如，在“星期五，运动员迈克尔·乔丹”中，“迈克尔·乔丹”的两个标记是正确的分析单位，而“在她的加冕典礼上，伊丽莎白女王二世”中，“伊丽莎白女王二世”的三个标记是正确的分析单位单元。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-aYS73ikuT9JzFD3Mj-1"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-aYS73ikuT9JzFD3Mj-2"&gt;&lt;p&gt;它与观察到的多语义现象（一个神经元对多个不相关的事物放电）和分布式表征（许多神经元对同一事物放电）密切相关，但最重要的是，叠加是一种机械假设，试图解释为什么会出现这些观察结果，不仅仅是经验观察本身。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-aYS73ikuT9JzFD3Mj-2"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-aYS73ikuT9JzFD3Mj-3"&gt;&lt;p&gt;特别是，分布式表示在语言 MLP 层中似乎特别普遍：在图像模型中，单个神经元通常是单语义的。这种情况有时会发生，但在语言模型中似乎很少见。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-aYS73ikuT9JzFD3Mj-3"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-aYS73ikuT9JzFD3Mj-4"&gt;&lt;p&gt;值得注意的是，几乎所有变压器电路的进展都集中在注意力头而不是 MLP 层上（有一些例外，例如&lt;a href="https://arxiv.org/abs/2305.00586"&gt;Hanna 等人&lt;/a&gt;），尽管 MLP 是大多数参数。我们预计，部分原因是 MLP 比注意力头更容易受到叠加的影响。有趣的是，在给定的狭窄任务中，通常一组稀疏的注意力头很重要，而一组更大、更分散的 MLP 神经元很重要， &lt;a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html"&gt;Bricken 等人&lt;/a&gt;认为 MLP 激活中有许多稀疏的单语义方向&lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-aYS73ikuT9JzFD3Mj-4"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-aYS73ikuT9JzFD3Mj-5"&gt;&lt;p&gt;尽管真实的模型也可能很难研究，因为它们可能充满了混乱的混杂因素。牛顿物理学特别容易用台球来推理，但试图通过研究飓风中的运动来了解牛顿物理学也不是一个好主意。 （感谢约瑟夫·布鲁姆的这一点！） &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-aYS73ikuT9JzFD3Mj-5"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-aYS73ikuT9JzFD3Mj-6"&gt;&lt;p&gt;我们分析中的一个弱点是，我们只研究了 1,500 个事实（1,500 名运动员的运动），而我们研究的 5 个 MLP 层每个都有 10,000 个神经元，这是相当不确定的！请注意，我们预计即使 Pythia 2.8B 也知道总共超过 50,000 个事实，运动员运动恰好是一个方便研究的子集。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-aYS73ikuT9JzFD3Mj-6"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-aYS73ikuT9JzFD3Mj-7"&gt;&lt;p&gt;存在一些边缘情况，例如名人具有相同的名字（例如&lt;a href="https://en.wikipedia.org/wiki/Michael_Jordan"&gt;迈克尔·乔丹&lt;/a&gt;和&lt;a href="https://en.wikipedia.org/wiki/Michael_I._Jordan"&gt;迈克尔·乔丹&lt;/a&gt;！），我们的猜测是该模型要么忽略不太出名的重复项，要么查找“组合”事实嵌入，随后将其消除歧义语境。不管怎样，这只与一小部分事实回忆案例相关。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-aYS73ikuT9JzFD3Mj-7"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-aYS73ikuT9JzFD3Mj-8"&gt;&lt;p&gt;请注意，模型的神经元数量比参数少得多，例如 GPT-3 有 4.7M 个神经元和 175B 个参数，因为每个神经元都有一个单独的 d_model 输入和输出权重向量（在 GPT-3 中 d_model 为 12288） &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-aYS73ikuT9JzFD3Mj-8"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-aYS73ikuT9JzFD3Mj-9"&gt;&lt;p&gt;当然，这是一个不完美的抽象，例如，非西方名字更有可能参加国际流行的体育运动，并且某些标记可​​能恰好出现在一位运动员的名字中。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-aYS73ikuT9JzFD3Mj-9"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-aYS73ikuT9JzFD3Mj-10"&gt;&lt;p&gt;这些与上下文词嵌入不同（过去已经通过 BERT 风格的模型进行了&lt;a href="https://arxiv.org/abs/1905.05950"&gt;广泛的&lt;/a&gt;&lt;a href="https://aclanthology.org/D18-1179/"&gt;研究&lt;/a&gt;）。众所周知，当你从一层到另一层时，模型会丰富其标记的表示，利用注意力将上下文添加到嵌入层提供的（与上下文无关的）表示中。然而，在这里，我们讨论的是模型如何表示&lt;em&gt;独立于&lt;/em&gt;任何&lt;em&gt;附加&lt;/em&gt;上下文的跨多个标记的语言单元（例如名称）。多令牌实体需要以某种方式表示以帮助下游计算，但不能轻易归因于单个令牌，类似于文献中所谓的&lt;a href="https://transformer-circuits.pub/2022/solu/index.html#section-6-3-2"&gt;去令牌化&lt;/a&gt;。实际上，模型可能会创建&lt;em&gt;上下文多令牌嵌入&lt;/em&gt;，其中多令牌实体的表示通过额外的上下文进一步丰富，使图片复杂化，但我们仍然认为多令牌嵌入是理解模型行为的有用概念。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-aYS73ikuT9JzFD3Mj-10"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-aYS73ikuT9JzFD3Mj-11"&gt;&lt;p&gt;例如，模型在残差流中具有“打篮球”方向。如果我们将“| MIchael | Jordan”的多标记表示投影到这个方向，它将异常高&lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-aYS73ikuT9JzFD3Mj-11"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-aYS73ikuT9JzFD3Mj-12"&gt;&lt;p&gt;这反映了&lt;a href="https://arxiv.org/abs/2308.09124"&gt;Hernandez 等人&lt;/a&gt;的发现，即在实体上查找关系（例如“玩……的运动”）是一个线性映射。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-aYS73ikuT9JzFD3Mj-12"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-aYS73ikuT9JzFD3Mj-13"&gt;&lt;p&gt;例如，将“名字是迈克尔”和“姓氏是乔丹”映射到“是实体迈克尔·乔丹”，具有“打篮球”、“是一名运动员”、“为芝加哥公牛队效力”等属性&lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-aYS73ikuT9JzFD3Mj-13"&gt;。↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-aYS73ikuT9JzFD3Mj-14"&gt;&lt;p&gt;尽管有一些启发式的空间，例如基于推断名称的种族，或者例如具有著名且独特的单令牌姓氏的运动员，其中运动可以存储在令牌嵌入中。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-aYS73ikuT9JzFD3Mj-14"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-aYS73ikuT9JzFD3Mj-15"&gt;&lt;p&gt;正如&lt;a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX/p/xE3Y9hhriMmL4cpsR"&gt;第 5 篇文章&lt;/a&gt;中所讨论的，因为早期的层在名称之前没有集成太多先前的上下文。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-aYS73ikuT9JzFD3Mj-15"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-aYS73ikuT9JzFD3Mj-16"&gt;&lt;p&gt;这通常隐含地涉及对特征的理解。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-aYS73ikuT9JzFD3Mj-16"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-aYS73ikuT9JzFD3Mj-17"&gt;&lt;p&gt;对此需要注意的是，如果没有对电路的机械理解，可能很难验证我们是否已找到 MLP 注入的所有事实——尽管相反的是，您可以查看从您确实知道的事实中恢复的损失代表性数据集&lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-aYS73ikuT9JzFD3Mj-17"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-aYS73ikuT9JzFD3Mj-18"&gt;&lt;p&gt;即表明我们可以消除模型中的其他所有内容，而不会显着影响性能&lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-aYS73ikuT9JzFD3Mj-18"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-aYS73ikuT9JzFD3Mj-19"&gt;&lt;p&gt;每项运动都是一个单一的代币。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-aYS73ikuT9JzFD3Mj-19"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-aYS73ikuT9JzFD3Mj-20"&gt;&lt;p&gt;即，我们选择另一名运动员（非篮球运动员），我们对该运动员进行神经元的激活，然后对“Michael Jordan”输入进行干预，以将 Jordan 令牌上的该神经元的激活替换为其最终名称令牌上的激活。其他运动员。我们对 64 名随机选择的其他运动员重复此操作。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-aYS73ikuT9JzFD3Mj-20"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-aYS73ikuT9JzFD3Mj-21"&gt;&lt;p&gt;请注意，强形式的散列没有发生对我们来说并不奇怪 - 即使在训练过程中早期层在没有结构的情况下进行纯散列，未来的梯度更新将鼓励早期层烘焙&lt;em&gt;一些&lt;/em&gt;有关实体的已知结构它是散列，以免参数被浪费。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-aYS73ikuT9JzFD3Mj-21"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-aYS73ikuT9JzFD3Mj-22"&gt;&lt;p&gt;我们怀疑泛化可能确实有一点帮助：该模型可能会使用名称特征（例如文化起源）与所参加的运动之间的相关性来获取一些信号，并且一些名字或姓氏与（著名运动员的）运动相关该名称）直接编码在其令牌嵌入中。但关键是，仅凭这些模式并不能让您在查找体育赛事时获得接近不错的准确度。记忆是成功完成这项任务的关键。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-aYS73ikuT9JzFD3Mj-22"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-aYS73ikuT9JzFD3Mj-23"&gt;&lt;p&gt;请注意，我们并不是说我们总是能成功找到这些表示；我们只是说我们总是能成功地找到这些表示。网络可能会使用我们不理解的泛化算法来解决任务，或者我们可能无法解码相关的表示。我们要表达的观点是，对于泛化算法，我们至少知道必须存在有意义的中间状态（对算法有用），我们至少有机会找到在网络中表示的状态。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-aYS73ikuT9JzFD3Mj-23"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-aYS73ikuT9JzFD3Mj-24"&gt;&lt;p&gt;当然，这些子网络的中间激活还有许多其他表示，因为子网络的权重来自经过训练以完成许多任务的语言模型。但这些表示不能仅使用任务数据集（运动员姓名和相应的运动项目）进行解码。而且，在某种程度上，这些表示确实与任务无关（因为我们假设任务只能通过记忆来解决——请参阅前面脚注中的警告），我们对这些其他表示的盲目不应该妨碍我们理解如何解决问题的能力。运动查找完成。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-aYS73ikuT9JzFD3Mj-24"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-aYS73ikuT9JzFD3Mj-25"&gt;&lt;p&gt;我们可能在模型中发现的其他表示通常包括目标概念（参加体育运动）的“干净”表示，以及该概念的功能（例如“打篮球并且身高超过 6&amp;#39;8”），但这些应该只出现在查找子网络之后，因为我们将子网络定义为精确地结束于目标概念首次清晰表示的位置。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-aYS73ikuT9JzFD3Mj-25"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-aYS73ikuT9JzFD3Mj-26"&gt;&lt;p&gt;请注意，这个问题的答案并没有直接告诉我们有关多令牌嵌入假设的任何信息（正面或负面）：早期层可能执行许多本地任务，其中多令牌嵌入只是其中之一；或者，可能的情况是，早期层除了查找多令牌嵌入之外还进行大量非本地处理。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-aYS73ikuT9JzFD3Mj-26"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-aYS73ikuT9JzFD3Mj-27"&gt;&lt;p&gt;我们注意到，如何最好地量化“截断造成的损害有多大”有点含糊。可以说余弦 sim 平方可能是一个更好的度量，因为它给出了所解释的范数的分数，而 0.9^2=0.81 看起来不太好。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-aYS73ikuT9JzFD3Mj-27"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-aYS73ikuT9JzFD3Mj-28"&gt;&lt;p&gt;我们注意到，默认情况下，SAE 会带来表示稀疏性，但不会带来电路稀疏性 - 如果 SAE 特征分布在神经元基础上，那么由于每个神经元的非线性，任何神经元都可以影响输出特征，并且不能天真地进行分析隔离。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-aYS73ikuT9JzFD3Mj-28"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-aYS73ikuT9JzFD3Mj-29"&gt;&lt;p&gt;在&lt;a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html"&gt;最近&lt;/a&gt;&lt;a href="https://arxiv.org/abs/2309.08600"&gt;一系列&lt;/a&gt;SAE 工作之前，我们已经完成了该项目的主要工作。如果我们再次做这个项目，我们可能会尝试使用它们！尽管如前所述，我们并不认为它们是灵丹妙药的解决方案。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-aYS73ikuT9JzFD3Mj-29"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-aYS73ikuT9JzFD3Mj-30"&gt;&lt;p&gt;除了启发法之外，例如某些种族可以从名称中推断出来，并且更有可能从事不同的运动，我们认为这超出了本次调查的范围。从某种意义上说，我们故意选择了一个我们不认为中间表示很重要的问题。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-aYS73ikuT9JzFD3Mj-30"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/section&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sat, 23 Dec 2023 02:44:24 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall</guid></item><item><title>测量篡改检测作为弱到强泛化的特例</title><link>https://www.lesswrong.com/posts/4KLCygqTLsMBM3KFR/measurement-tampering-detection-as-a-special-case-of-weak-to</link><description>发布于 2023 年 12 月 23 日凌晨 12:05（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt; OpenAI 的 Burns 等人发表了&lt;a href="https://openai.com/research/weak-to-strong-generalization"&gt;&lt;u&gt;一篇论文，&lt;/u&gt;&lt;/a&gt;研究了使用弱模型生成的标签对下游任务的强模型进行微调的各种技术。他们将这个问题称为“弱到强泛化”，缩写为W2SG。&lt;/p&gt;&lt;p&gt;今年早些时候，我们发表了一篇论文《 &lt;a href="https://www.alignmentforum.org/posts/inALbAqdx63KTaGgs/benchmarks-for-detecting-measurement-tampering-redwood"&gt;&lt;u&gt;检测测量篡改的基准》&lt;/u&gt;&lt;/a&gt; ，其中我们研究了测量篡改检测 (MTD) 问题的技术。 MTD 是 W2SG 的一个特例。在这篇文章中，我们将解释 MTD 和 W2SG 之间的关系，并解释为什么我们认为 MTD 比完全通用的 W2SG 更有可能发挥作用。当然，由于这种通用性，完全通用的 W2SG 是一个严格意义上更有价值的问题。&lt;/p&gt;&lt;p&gt;我们认为MTD是一个有前途的研究方向。我们还对其他问题感到兴奋，这些问题是 W2SG 的特殊情况，具有可以通过技术利用的特殊结构，特别是如果该结构可能在未来的重要情况中出现。&lt;/p&gt;&lt;h1&gt; MTD 作为 W2SG 的子集&lt;/h1&gt;&lt;h2&gt;类似的目标&lt;/h2&gt;&lt;p&gt;在训练人工智能时，我们对不同行为的奖励可能与我们更好地了解情况时给予的奖励不匹配。&lt;/p&gt;&lt;p&gt; W2SG 技术的目标是在训练强 AI 时取得良好的结果，尽管只能使用对情况的了解不如强 AI 的弱监督者。&lt;/p&gt;&lt;p&gt; MTD 是一种特殊情况，弱监督者可以访问足以了解情况的测量结果，但这些测量结果可能会被篡改（例如，用一些虚构的数据替换摄像头输入、禁用测试或威胁注释者）。由于在没有篡改的情况下测量结果已经足够，因此即使没有人工智能优化以使测量结果看起来不错，我们也不必担心可能发生的良性错误。&lt;/p&gt;&lt;h2&gt;略有不同的实验&lt;/h2&gt;&lt;p&gt;W2SG 可以使用&lt;a href="https://www.alignmentforum.org/posts/hw2tGSsvLLyjFoLFS/scalable-oversight-and-weak-to-strong-generalization"&gt;&lt;u&gt;夹心实验&lt;/u&gt;&lt;/a&gt;来研究，其中我们尝试让 AI 安全地完成任务，尽管只能访问较弱的监督者，然后我们使用更强的保留监督信号（例如保留地面）来测量我们方法的性能真相标签）。&lt;/p&gt;&lt;p&gt;在 OpenAI 论文中，弱监督者是一个在真实标签上训练的小型语言模型，类似于人类注释者。&lt;/p&gt;&lt;p&gt;就我们的 MTD 论文而言，我们可以访问测量结果，但存在一些测量篡改的概念。在我们的工作中，测量的目的是直接测量感兴趣的属性作为布尔值，因此从未经篡改的测量转换为正确的标签是简单的（并且不需要任何学习或智能）。&lt;/p&gt;&lt;h1&gt;对 W2SG 成功的不同希望&lt;/h1&gt;&lt;p&gt;在这两种情况下，我们都需要一些额外的假设才能从弱监督者那里获得强有力的监督。如果我们不对模型的内部结构做出任何结构性假设，并假设人工智能内部结构的最坏情况，我们就不会成功，因此我们将依赖于模型内部的某种类型的结构。 OpenAI 论文讨论的结构与我们希望在测量篡改案例中利用的结构非常不同。&lt;/p&gt;&lt;h2&gt;信心和一致性&lt;/h2&gt;&lt;p&gt;OpenAI 论文表明，在某些任务中，除了匹配弱监督者的标签之外，训练模型变得自信有时还能提高准确性。希望该模型能够非常显着地表示真实情况，并不太显着地表示弱监督者的预测。&lt;/p&gt;&lt;p&gt;置信度希望类似于&lt;a href="https://www.lesswrong.com/posts/L4anhrxjv8j2yRKKp/how-discovering-latent-knowledge-in-language-models-without"&gt;&lt;u&gt;《发现潜在知识》&lt;/u&gt;&lt;/a&gt;中探索的希望：人工智能内部真相的内部表示可能比其他特征更一致，并且比其他一致特征更显着，因此针对一致性（和置信度）进行优化可以给出比弱主管标签上的幼稚训练具有更高的性能。&lt;/p&gt;&lt;p&gt;对于人工智能来说，信心和一致性都不够，因为“人类的想法”比“事实是什么”更重要，这对于接受人类生成文本预训练的人工智能或训练用于预测经常被篡改的测量结果的人工智能来说似乎很可能。&lt;/p&gt;&lt;p&gt;这种结构存在于非常广泛的情况中。但还不够明确，不足以解决问题。事实上，Burns 等人没有找到能够可靠地改善结果的技术，我们认为这是反对存在稳健地允许 W2SG 的技术的一些证据。我们还认为，最近的工作“ &lt;a href="https://www.lesswrong.com/posts/Yio4nmD8JMttx9o9S/new-paper-shows-truthfulness-and-instruction-following-don-t"&gt;&lt;u&gt;泛化类比：将人工智能监督泛化到难以测量领域的测试平台&lt;/u&gt;&lt;/a&gt;”提供了一些微弱的证据，证明存在无需使用更具体的结构就能稳健地实现理想泛化的技术。&lt;/p&gt;&lt;h2&gt;检测篡改&lt;/h2&gt;&lt;p&gt;相比之下，MTD 对于问题有一种非常特殊的结构，这可能使我们能够开发出适用于各种环境的技术。测量篡改是一种特殊类型的监控器故障，具有两个关键属性：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;失败的篡改尝试可能会留下这种不当行为的证据，而如果人工智能系统不尝试篡改，你可能永远无法获得不当行为的有力证据。&lt;/li&gt;&lt;li&gt;测量之间的一致关系可能是由真实正面示例和可信示例的类似机制驱动的，而虚假正面示例和可信示例之间的一致性机制有所不同。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;因此，如果你训练最强大的人工智能来预测测量结果，这些属性可能会转化为有关其内部结构的属性。这些属性可以通过使用模型内部的技术来使用（例如，通过探测篡改的证据或通过使用排除微调等技术），这就是为什么与单独依赖测量相比，有可能获得更高的性能。&lt;/p&gt;&lt;p&gt;对于测量预测非常重要的人工智能，我们不知道预期这些属性会被打破的具体原因。因此，我们认为存在可靠解决 MTD 问题的技术比可靠解决 W2SG 的技术更有可能存在。总体而言，我们认为 MTD 似乎比完全通用的 W2SG 更有可能发挥作用，但当然它的普遍适用性较差。&lt;/p&gt;&lt;p&gt; MTD 的主要弱点是它所依赖的有些模糊的属性：到目前为止，我们还无法找到测量篡改的精确定义，可以用来明确地确定给定的数据集是否具有正确的“测量篡改结构” 。&lt;/p&gt;&lt;p&gt;我们在&lt;a href="https://www.alignmentforum.org/posts/inALbAqdx63KTaGgs/benchmarks-for-detecting-measurement-tampering-redwood#Why_solving_measurement_tampering_may_be_notably_easier_than_eliciting_arbitrary_knowledge"&gt;&lt;u&gt;为什么解决测量篡改可能比获取任意知识更容易中&lt;/u&gt;&lt;/a&gt;更详细地讨论了这些希望。&lt;/p&gt;&lt;h2&gt; W2SG 的其他特殊情况可能是很好的研究问题&lt;/h2&gt;&lt;p&gt;我们选择 MTD 中的结构部分是因为我们认为它是一种特别重要的结构，但也因为它是我们可以构建一个设置的最简单的非平凡示例，我们可以在其中希望获得泛化性能并且我们可以访问某种结构我们可以使用我们的技术来利用它。我们认为，虽然开发 MTD 技术看起来是一个非常有前途的研究方向，但在具有特定结构的更广泛的环境中开发技术也很有希望，因为将来我们可能希望能够特别利用定制结构域。&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/4KLCygqTLsMBM3KFR/measurement-tampering-detection-as-a-special-case-of-weak-to#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sat, 23 Dec 2023 00:05:57 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/4KLCygqTLsMBM3KFR/measurement-tampering-detection-as-a-special-case-of-weak-to</guid></item><item><title>玩具 2 位减法变压器如何预测差异？</title><link>https://www.lesswrong.com/posts/RABp7ZMw2FGwh4odq/how-does-a-toy-2-digit-subtraction-transformer-predict-the-1</link><description>发布于 2023 年 12 月 22 日晚上 9:17（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;h1&gt;&lt;br /&gt; &lt;strong&gt;2位减法——差异预测&lt;/strong&gt;&lt;/h1&gt;&lt;h1&gt;概括&lt;/h1&gt;&lt;p&gt;我继续研究一个玩具 1 层 Transformer 语言模型，该模型经过训练可以进行&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;−&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;±&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;c&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;形式的两位数加法。在预测模型是正 (+) 还是负 (-) 后，它必须输出&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;和&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;之间的差。该模型创建在 a 和 b 基数中振荡的激活，以及作为&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;−&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;的函数线性变化的激活。该模型使用激活函数耦合&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;和&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;方向上的振荡，然后对这些振荡求和以消除除取决于绝对差&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;|&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;的方差之外的任何方差。 &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;−&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;|&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;预测正确的输出标记。我检查了该算法从输入到模型输出的完整路径。&lt;/p&gt;&lt;h1&gt;介绍&lt;/h1&gt;&lt;p&gt;在之前的文章中，我描述了&lt;a href="https://evanhanders.blog/2023/12/06/two-digit-subtraction/"&gt;训练两位数减法变压器&lt;/a&gt;并&lt;a href="https://evanhanders.blog/2023/12/15/2-digit-subtraction-how-does-it-predict/"&gt;研究了该变压器如何预测&lt;/a&gt;&lt;a href="https://www.lesswrong.com/posts/pbj6tTZyakodxC9Ho/how-does-a-toy-2-digit-subtraction-transformer-predict-the"&gt; &lt;/a&gt;&lt;a href="https://evanhanders.blog/2023/12/15/2-digit-subtraction-how-does-it-predict/"&gt;输出&lt;/a&gt;&lt;a href="https://evanhanders.blog/2023/12/15/2-digit-subtraction-how-does-it-predict/"&gt;的符号&lt;/a&gt;&lt;a href="https://www.lesswrong.com/posts/pbj6tTZyakodxC9Ho/how-does-a-toy-2-digit-subtraction-transformer-predict-the"&gt;。&lt;/a&gt;在这篇文章中，我将研究如何训练模型的权重来确定两位整数之间的差异，以及模型激活的紧急行为。&lt;/p&gt;&lt;h2&gt;路线图&lt;/h2&gt;&lt;p&gt;与我在上一篇文章中从头到尾地研究模型不同，这次我将采取相反的方法，从输入开始，了解它如何转换为输出。我将按顺序分解：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;注意力模式，以及模型关注哪些标记来预测输出。&lt;/li&gt;&lt;li&gt;注意力头权重如何设置预激活。&lt;/li&gt;&lt;li&gt;预激活中出现的模式。&lt;/li&gt;&lt;li&gt;激活中出现的模式。&lt;/li&gt;&lt;li&gt;神经元和逻辑之间的映射。&lt;/li&gt;&lt;li&gt; Logits 中的模式。&lt;/li&gt;&lt;/ul&gt;&lt;h1&gt;注意力模式&lt;/h1&gt;&lt;p&gt;下面是四个示例的注意力模式的可视化（两个正，两个负， &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;和&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;使用相同的数值，但交换它们）： &lt;/p&gt;&lt;p&gt;&lt;img alt="该图像的 alt 属性为空；它的文件名是attention_head_examples-1.png" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/RABp7ZMw2FGwh4odq/vokykqs6hsusbr0yy69u" /&gt;&lt;/p&gt;&lt;p&gt;我已将所有不重要的行显示为灰色，以便我们可以专注于倒数第二行，该行对应于转换器预测差值&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;c&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;的行。这里有几点需要注意：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;当结果为正时，H0 关注令牌&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; ，H3 关注令牌&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 。&lt;/li&gt;&lt;li&gt;当结果为负时，H0 关注令牌&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; ，H3 关注令牌&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 。&lt;/li&gt;&lt;li&gt; H1 和 H2 大致均匀地关注上下文中的所有标记。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;因此，H3 始终关注&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;和&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;中较大的一个，而 H0 始终关注 a 和 b 中较小的一个。 H1和H2似乎并没有做任何太重要的事情。&lt;/p&gt;&lt;p&gt;在数学中，上述直觉可以写成每个头对标记&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;的关注：&lt;/p&gt;&lt;p&gt; &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;A&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mrow MJXc-space3"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-size3-R"&gt;{&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mtable"&gt;&lt;span class="mjx-table"&gt;&lt;span class="mjx-mtr" style="height: 1.1em;"&gt;&lt;span class="mjx-mtd" style="padding: 0px 0.5em 0px 0px; text-align: left; width: 0.5em;"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-strut"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mtd" style="padding: 0px 0px 0px 0.5em; text-align: left; width: 2.861em;"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;4&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;-&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-strut"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mtr" style="height: 1.1em;"&gt;&lt;span class="mjx-mtd" style="padding: 0.1em 0.5em 0px 0px; text-align: left;"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-strut"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mtd" style="padding: 0.1em 0px 0px 0.5em; text-align: left;"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;4&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-strut"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo" style="width: 0.12em;"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mspace" style="width: 2em; height: 0px;"&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space1"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;A&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mspace" style="width: 2em; height: 0px;"&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space1"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;A&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mspace" style="width: 2em; height: 0px;"&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space1"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;A&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;3&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mrow MJXc-space3"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-size3-R"&gt;{&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mtable"&gt;&lt;span class="mjx-table"&gt;&lt;span class="mjx-mtr" style="height: 1.1em;"&gt;&lt;span class="mjx-mtd" style="padding: 0px 0.5em 0px 0px; text-align: left; width: 0.5em;"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-strut"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mtd" style="padding: 0px 0px 0px 0.5em; text-align: left; width: 2.861em;"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;4&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;-&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-strut"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mtr" style="height: 1.1em;"&gt;&lt;span class="mjx-mtd" style="padding: 0.1em 0.5em 0px 0px; text-align: left;"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-strut"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mtd" style="padding: 0.1em 0px 0px 0.5em; text-align: left;"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;4&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-strut"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo" style="width: 0.12em;"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mspace" style="width: 2em; height: 0px;"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;以及每个头对令牌&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;的关注：&lt;/p&gt;&lt;p&gt; &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;A&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mrow MJXc-space3"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-size3-R"&gt;{&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mtable"&gt;&lt;span class="mjx-table"&gt;&lt;span class="mjx-mtr" style="height: 1.1em;"&gt;&lt;span class="mjx-mtd" style="padding: 0px 0.5em 0px 0px; text-align: left; width: 0.5em;"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-strut"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mtd" style="padding: 0px 0px 0px 0.5em; text-align: left; width: 2.861em;"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;4&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;-&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-strut"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mtr" style="height: 1.1em;"&gt;&lt;span class="mjx-mtd" style="padding: 0.1em 0.5em 0px 0px; text-align: left;"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-strut"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mtd" style="padding: 0.1em 0px 0px 0.5em; text-align: left;"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;4&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-strut"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo" style="width: 0.12em;"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mspace" style="width: 2em; height: 0px;"&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space1"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;A&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mspace" style="width: 2em; height: 0px;"&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space1"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;A&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mspace" style="width: 2em; height: 0px;"&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space1"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;A&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;3&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mrow MJXc-space3"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-size3-R"&gt;{&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mtable"&gt;&lt;span class="mjx-table"&gt;&lt;span class="mjx-mtr" style="height: 1.1em;"&gt;&lt;span class="mjx-mtd" style="padding: 0px 0.5em 0px 0px; text-align: left; width: 0.5em;"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-strut"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mtd" style="padding: 0px 0px 0px 0.5em; text-align: left; width: 2.861em;"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;4&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;-&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-strut"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mtr" style="height: 1.1em;"&gt;&lt;span class="mjx-mtd" style="padding: 0.1em 0.5em 0px 0px; text-align: left;"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-strut"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mtd" style="padding: 0.1em 0px 0px 0.5em; text-align: left;"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;4&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-strut"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo" style="width: 0.12em;"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;,&lt;/p&gt;&lt;p&gt;其中&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;4&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;是上下文位置 4 处的标记值（紧接在 = 之后）。有理由问这是否是一个好的描述，确实如此！如果我对头 1 和 2 以及头 0 和 3 中的注意力模式进行零消融，我将上面突出显示的行替换为正确处理&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;或&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;的单热编码单位向量，参数空间中&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;10&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px;"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;4&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;问题的损失从 0.0181 减少到 0.0168。&lt;/p&gt;&lt;h1&gt;神经元预激活和激活&lt;/h1&gt;&lt;h2&gt;注意力头实现的功能&lt;/h2&gt;&lt;p&gt;矩阵&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;W&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;neur&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;W&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;E&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;W&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space3"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;V&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;W&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;O&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;W&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%;"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;in&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;具有&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;形状&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;[&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;d_vocab&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; , d_mlp] 并描述当注意力头 h 关注词汇中的标记时，注意力头&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;如何改变神经元预激活。从上一节中我们知道，头 0 和 3 完全参与标记&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;或&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; ，因此我们想了解头 0 和 3 如何根据其输入影响预激活。 （&lt;i&gt;旁白：在我&lt;/i&gt;&lt;a href="https://evanhanders.blog/2023/12/15/2-digit-subtraction-how-does-it-predict/"&gt;&lt;i&gt;之前的&lt;/i&gt;&lt;/a&gt;&lt;a href="https://www.lesswrong.com/posts/pbj6tTZyakodxC9Ho/how-does-a-toy-2-digit-subtraction-transformer-predict-the"&gt;&lt;i&gt;文章&lt;/i&gt;&lt;/a&gt;中&lt;i&gt;，我检查了类似的内容，但是在我的&lt;/i&gt;&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;W&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;n&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;e&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;u&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;r&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;计算中存在一个微妙的错误&lt;i&gt;，我已经修复了该错误&lt;/i&gt;）。&lt;/p&gt;&lt;p&gt;以下是我在头部对神经元激活的贡献中看到的四种模式： &lt;/p&gt;&lt;p&gt;&lt;img alt="该图像的 alt 属性为空；它的文件名是 token_attention_neuron_contributions-2.png" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/RABp7ZMw2FGwh4odq/fery0ybeafqvetxvxsfe" /&gt;&lt;/p&gt;&lt;p&gt;﻿&lt;/p&gt;&lt;p&gt;因此，神经元 17 基本上呈线性贡献，而许多其他神经元则具有不同类型的振荡。如果我们将每个头对每个神经元的贡献拟合一条线，减去它，然后将这些信号变换到频率空间，我们可以形成以下功率谱： &lt;/p&gt;&lt;p&gt;&lt;img alt="该图像的 alt 属性为空；它的文件名是 token_attention_neuron_contributions_freqspace.png" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/RABp7ZMw2FGwh4odq/thxwvacgstlgddcpwffx" /&gt;&lt;/p&gt;&lt;p&gt;在这里，我在三个关键频率（0.33、0.39、0.48）处放置了垂直线，这&lt;a href="https://evanhanders.blog/2023/12/06/two-digit-subtraction/"&gt;是我在关于该主题的第一篇文章中看到的&lt;/a&gt;。我们看：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;神经元 17 中线性趋势之上的小振荡在与这些关键频率相关的频谱中具有一些峰值，但它们很弱。&lt;/li&gt;&lt;li&gt;神经元 5 具有与每个关键频率相关的强峰值，但&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;"&gt;f&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0.39&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;处的峰值是迄今为止最强的。其次，这个主峰并不完全尖锐。它具有很强的中心值以及紧邻中心值一侧的相当强的“英尺”值。这很快就会很重要。&lt;/li&gt;&lt;li&gt;神经元 76 一片混乱。像这样的神经元数量相当多。有一个很强的峰值，但功率也远离该频率，像&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;"&gt;f&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;−&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;"&gt;f&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;p&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;e&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;a&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;k&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;−&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;那样下降。&lt;/li&gt;&lt;li&gt;神经元125在令牌信号中表现出强烈的节拍，这对应于功率逐渐增加到&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;"&gt;f&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0.48&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;处的峰值。这又有点混乱了。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;所以我们看到的是，这些峰值&lt;i&gt;主要&lt;/i&gt;可以用强中心峰值和紧邻该中心峰值的频率仓中相应的强峰值来描述。这非常让人想起我们在使用加窗技术时在信号处理中看到的峰值&lt;a href="https://en.wikipedia.org/wiki/Apodization"&gt;变迹&lt;/a&gt;——来自尖锐中心峰值的功率会稍微分散到相邻的箱中。&lt;/p&gt;&lt;p&gt;我不完全确定什么是通常适合这些函数的正确函数，但这似乎是一个很好的猜测：&lt;/p&gt;&lt;p&gt; &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-munderover"&gt;&lt;span class="mjx-stack"&gt;&lt;span class="mjx-over" style="height: 0.153em; padding-bottom: 0.06em; padding-left: 0.05em;"&gt;&lt;span class="mjx-mo" style="vertical-align: top;"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;~&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-op"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;t&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-munderover MJXc-space3"&gt;&lt;span class="mjx-itable"&gt;&lt;span class="mjx-row"&gt;&lt;span class="mjx-cell"&gt;&lt;span class="mjx-op"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-munderover"&gt;&lt;span class="mjx-itable"&gt;&lt;span class="mjx-row"&gt;&lt;span class="mjx-cell"&gt;&lt;span class="mjx-op"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;m&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;t&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;c&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-row"&gt;&lt;span class="mjx-under" style="padding-top: 0.12em;"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-delim-h"&gt;&lt;span class="mjx-char MJXc-TeX-size4-R"&gt;&lt;/span&gt; &lt;span class="mjx-char MJXc-TeX-size4-R"&gt;&lt;/span&gt; &lt;span class="mjx-char MJXc-TeX-size4-R" style="margin-top: 0px; margin-right: 0.01em; margin-bottom: 0px;"&gt;&lt;/span&gt; &lt;span class="mjx-char MJXc-TeX-size4-R"&gt;&lt;/span&gt; &lt;span class="mjx-char MJXc-TeX-size4-R" style="margin-bottom: 0px; margin-top: 0px;"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-row"&gt;&lt;span class="mjx-under" style="font-size: 70.7%;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;线性&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;Σ&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0.33&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0.39&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-munderover MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-size1-R"&gt;0.48&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%; padding-left: 0px;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;i&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;⎡&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;⎢&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;⎢&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;⎢&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;⎢&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mrow MJXc-space1"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-delim-v"&gt;&lt;span class="mjx-char MJXc-TeX-size4-R"&gt;⎣&lt;/span&gt; &lt;span class="mjx-char MJXc-TeX-size4-R"&gt;a&lt;/span&gt; &lt;span class="mjx-char MJXc-TeX-size4-R"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;cos&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-munderover"&gt;&lt;span class="mjx-itable"&gt;&lt;span class="mjx-row"&gt;&lt;span class="mjx-cell"&gt;&lt;span class="mjx-op"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-munderover"&gt;&lt;span class="mjx-itable"&gt;&lt;span class="mjx-row"&gt;&lt;span class="mjx-cell"&gt;&lt;span class="mjx-op"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;π&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;"&gt;fit&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;+&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;phi&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-row"&gt;&lt;span class="mjx-under" style="padding-top: 0.12em;"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-delim-h"&gt;&lt;span class="mjx-char MJXc-TeX-size4-R" style="margin-bottom: 0px; margin-top: 0px;"&gt;&lt;/span&gt; &lt;span class="mjx-char MJXc-TeX-size4-R"&gt;&lt;/span&gt;&lt;span class="mjx-char MJXc-TeX-size4-R"&gt;中心&lt;/span&gt;&lt;span class="mjx-char MJXc-TeX-size4-R" style="margin-top: 0px; margin-right: 0.01em; margin-bottom: 0px;"&gt;峰&lt;/span&gt;&lt;span class="mjx-char MJXc-TeX-size4-R"&gt;+&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-row"&gt;&lt;span class="mjx-under" style="font-size: 70.7%;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;bi&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;cos&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-munderover MJXc-space2"&gt;&lt;span class="mjx-itable"&gt;&lt;span class="mjx-row"&gt;&lt;span class="mjx-cell"&gt;&lt;span class="mjx-op"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-munderover"&gt;&lt;span class="mjx-itable"&gt;&lt;span class="mjx-row"&gt;&lt;span class="mjx-cell"&gt;&lt;span class="mjx-op"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;π&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;"&gt;f&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;t&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;θ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;cos&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-size3-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mfrac"&gt;&lt;span class="mjx-box MJXc-stacked" style="width: 2.028em; padding: 0px 0.12em;"&gt;&lt;span class="mjx-numerator"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;π&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-denominator"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;d&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 83.3%; padding-right: 0.06em;"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;′&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 83.3%; padding-right: 0.06em;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;v&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;o&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;c&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;a&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;b&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-line"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-vsize"&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;t&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;θ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-size3-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-row"&gt;&lt;span class="mjx-under" style="padding-top: 0.12em;"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-delim-h"&gt;&lt;span class="mjx-char MJXc-TeX-size4-R"&gt;&lt;/span&gt; &lt;span class="mjx-char MJXc-TeX-size4-R"&gt;&lt;/span&gt; &lt;span class="mjx-char MJXc-TeX-size4-R"&gt;&lt;/span&gt; &lt;span class="mjx-char MJXc-TeX-size4-R" style="margin-top: 0px; margin-right: 0.01em; margin-bottom: 0px;"&gt;_&lt;/span&gt; &lt;span class="mjx-char MJXc-TeX-size4-R"&gt;&lt;/span&gt; &lt;span class="mjx-char MJXc-TeX-size4-R" style="margin-bottom: 0px; margin-top: 0px;"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-row"&gt;&lt;span class="mjx-under" style="font-size: 70.7%;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;中央峰‘脚’&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-delim-v"&gt;&lt;span class="mjx-char MJXc-TeX-size4-R"&gt;⎤&lt;/span&gt; &lt;span class="mjx-char MJXc-TeX-size4-R"&gt;⎥ ⎥ ⎥ ⎥&lt;/span&gt; &lt;span class="mjx-char MJXc-TeX-size4-R"&gt;⎦&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;其中&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;d&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-stack"&gt;&lt;span class="mjx-sup" style="font-size: 70.7%;"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;′&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub" style="font-size: 70.7%;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;v&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;o&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;c&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;a&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;b&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;100&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;是对应于 0-99 的词汇大小。在这里，负责脚部的关键频率总和中的第二项受到我过去使用&lt;a href="https://en.wikipedia.org/wiki/Hann_function"&gt;Hann 窗&lt;/a&gt;所做的工作的启发。当频率为&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;"&gt;f&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;和&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;"&gt;f&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;的两个正弦项相乘时，它们将功率放入&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;"&gt;f&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;±&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;"&gt;f&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;中，因此我上面列出的函数的这种形式给出了一条线、关键频率的功率和关键频率旁边的垃圾箱。&lt;/p&gt;&lt;p&gt;我使用&lt;a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html"&gt;scipy 的 curve_fit&lt;/a&gt;来&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-munderover"&gt;&lt;span class="mjx-stack"&gt;&lt;span class="mjx-over" style="height: 0.153em; padding-bottom: 0.06em; padding-left: 0.05em;"&gt;&lt;span class="mjx-mo" style="vertical-align: top;"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;拟合&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-op"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;~&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;ni&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;形式的函数，以表示头 0 和 3 对每个神经元的贡献。我发现这些拟合&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;占&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;W&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;neur&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;总&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;变异&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;性的&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-ams-R"&gt;≳&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 97%。对于像上面的神经元 5 或神经元 17 这样的神经元，这种拟合解释了 &amp;gt; 99% 的变异性；像神经元 125 这样的神经元具有&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;约&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;97% 的变异性。像神经元 76 这样的神经元很混乱，但这种拟合解释了它们&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;90&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; % 的变异性。&lt;/p&gt;&lt;p&gt;我现在用这些拟合替换神经元预激活（使用我在上一节中发现的注意模式）。我还保留了最终变得重要的偏差：我发现来自注意力头以及嵌入和位置嵌入的价值偏差（因此在上下文位置 4 进入注意力操作的原始残差流）对于设置神经元很重要预激活。&lt;/p&gt;&lt;p&gt;当我进行这种近似时，所有&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;10&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px;"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;4 个&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;问题的损失从 0.0181 增加到 0.0341，而准确度仅从 99.94% 下降到 99.68%。但请注意，如果我在拟合中删除“英尺”项，则损失会增加一个数量级以上，并且准确性会大幅下降，因此这似乎获得了模型使用的大部分重要部分。&lt;/p&gt;&lt;h2&gt;由此产生的神经元预激活&lt;/h2&gt;&lt;p&gt;在上一节中，我研究了模型权重如何设置神经元预激活，但现在我只想查看预激活，看看我是否可以理解描述它们的更简单的算法。&lt;/p&gt;&lt;p&gt;上一节让我确信可能有四种不同类别的神经元。但是，经过一番努力，结果发现只有两个，并且都可以使用简单的拟合来描述：&lt;/p&gt;&lt;p&gt; &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-munderover"&gt;&lt;span class="mjx-stack"&gt;&lt;span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.05em;"&gt;&lt;span class="mjx-mo" style="vertical-align: top;"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;^&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-op"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-munderover MJXc-space3"&gt;&lt;span class="mjx-itable"&gt;&lt;span class="mjx-row"&gt;&lt;span class="mjx-cell"&gt;&lt;span class="mjx-op"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-munderover"&gt;&lt;span class="mjx-itable"&gt;&lt;span class="mjx-row"&gt;&lt;span class="mjx-cell"&gt;&lt;span class="mjx-op"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;m&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;−&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;c&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-row"&gt;&lt;span class="mjx-under" style="padding-top: 0.12em;"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-delim-h"&gt;&lt;span class="mjx-char MJXc-TeX-size4-R"&gt;&lt;/span&gt; &lt;span class="mjx-char MJXc-TeX-size4-R"&gt;&lt;/span&gt; &lt;span class="mjx-char MJXc-TeX-size4-R" style="margin-top: 0px; margin-right: 0.01em; margin-bottom: 0px;"&gt;&lt;/span&gt; &lt;span class="mjx-char MJXc-TeX-size4-R"&gt;&lt;/span&gt; &lt;span class="mjx-char MJXc-TeX-size4-R" style="margin-bottom: 0px; margin-top: 0px;"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-row"&gt;&lt;span class="mjx-under" style="font-size: 70.7%;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;线性&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-munderover MJXc-space2"&gt;&lt;span class="mjx-itable"&gt;&lt;span class="mjx-row"&gt;&lt;span class="mjx-cell"&gt;&lt;span class="mjx-op"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-munderover"&gt;&lt;span class="mjx-itable"&gt;&lt;span class="mjx-row"&gt;&lt;span class="mjx-cell"&gt;&lt;span class="mjx-op"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;A&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;cos&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;ω&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;phi&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;A&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;cos&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;ω&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;phi&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-row"&gt;&lt;span class="mjx-under" style="padding-top: 0.12em;"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-delim-h"&gt;&lt;span class="mjx-char MJXc-TeX-size4-R"&gt;&lt;/span&gt; &lt;span class="mjx-char MJXc-TeX-size4-R"&gt;&lt;/span&gt; &lt;span class="mjx-char MJXc-TeX-size4-R"&gt;&lt;/span&gt; &lt;span class="mjx-char MJXc-TeX-size4-R" style="margin-top: 0px; margin-right: 0.01em; margin-bottom: 0px;"&gt;_&lt;/span&gt; &lt;span class="mjx-char MJXc-TeX-size4-R"&gt;&lt;/span&gt; &lt;span class="mjx-char MJXc-TeX-size4-R" style="margin-bottom: 0px; margin-top: 0px;"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-row"&gt;&lt;span class="mjx-under" style="font-size: 70.7%;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;振荡&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt; 其中存在线性部分（斜率&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;m&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;和截距&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;c&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; ）和振荡拟合（振幅&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;A&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;、&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space1"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;A&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 、相位&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;phi&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;、&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space1"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;phi&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; ），并且每个神经元具有主导特征&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;"&gt;频率&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;fi&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space3"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;ω&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;/&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;π&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;ε&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;{&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0.33&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0.39&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0.48&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;}&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;在主要模型频率中。&lt;/p&gt;&lt;p&gt;问题是——这种拟合仅由模型在&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;≥&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt;的&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;i&gt;区域中实现&lt;/i&gt;&lt;i&gt;。&lt;/i&gt;在&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;的区域中，模型仅重用与交换输入的&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;情况相同的计算。因此，我将上面的函数&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-munderover"&gt;&lt;span class="mjx-stack"&gt;&lt;span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.05em;"&gt;&lt;span class="mjx-mo" style="vertical-align: top;"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;^&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-op"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;拟合到&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;∈&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;[&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;50&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;99&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;]&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;和&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;∈&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;[&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;49&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;]&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;的区域中，以便均匀地&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 。然后，我计算所有输入值的&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-munderover"&gt;&lt;span class="mjx-stack"&gt;&lt;span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.05em;"&gt;&lt;span class="mjx-mo" style="vertical-align: top;"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;^&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-op"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; ，然后将&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;处的输出值替换为相应的&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;情况（例如，我将&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;33&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;−&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;10&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;中的激活值放入以下位置： &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;10-33&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;）&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;要了解其工作原理，这里有一些神经元预激活示例（顶行）和这些拟合（底行）： &lt;/p&gt;&lt;p&gt;&lt;img alt="该图像的 alt 属性为空；它的文件名是sample_neuron_preactivations.png" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/RABp7ZMw2FGwh4odq/k9jyvjhfnrrcjkktonqb" /&gt;&lt;/p&gt;&lt;p&gt;这些合身看起来真的很不错！如果我进入模型并将所有神经元预激活替换为最适合的&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-munderover"&gt;&lt;span class="mjx-stack"&gt;&lt;span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.05em;"&gt;&lt;span class="mjx-mo" style="vertical-align: top;"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;^&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-op"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; ，那么模型损失仅从 0.0181 变化到 0.0222，准确度仅从 99.94% 下降到 99.92%。此外，在 ReLU() 之后，拟合解释了神经元激活中 95% 的变异性。所以我对模型如何构建预激活的描述非常满意！&lt;/p&gt;&lt;h2&gt;神经元激活&lt;/h2&gt;&lt;p&gt;ReLU() 是问题中的一种非线性。它将我上面描述的良好贴合的力量向外传播。具体来说，我发现（类似于&lt;a href="https://arxiv.org/abs/2301.05217"&gt;尼尔的 grokking 工作&lt;/a&gt;）&lt;/p&gt;&lt;p&gt; &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;ReLU&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;[&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;A&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;cos&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;ω&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;phi&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;A&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;cos&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;ω&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;phi&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;]&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;≈&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mfrac MJXc-space3"&gt;&lt;span class="mjx-box MJXc-stacked" style="width: 0.495em; padding: 0px 0.12em;"&gt;&lt;span class="mjx-numerator"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-denominator"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-line"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-vsize"&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;[&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;A&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;余弦&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;ω&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;phi&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;A&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;cos&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;ω&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;phi&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;]&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;A&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;3&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;cos&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;ω&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;phi&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;cos&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;ω&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;phi&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;因此，功率从单轴项向外投射到跨轴项。&lt;/p&gt;&lt;p&gt;如果我从神经元激活拟合&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;ReLU&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-munderover"&gt;&lt;span class="mjx-stack"&gt;&lt;span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.05em;"&gt;&lt;span class="mjx-mo" style="vertical-align: top;"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;^&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-op"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;中减去线性项，然后拟合上面的横轴振荡表达式，然后添加线性贡献，我可以为神经元激活创建一个猜测。但是，如果我用这些猜测替换神经元激活，那么性能真的很糟糕！损失从 0.0181 增加到 2.496，准确率从 99.94% 下降到 19.79%。&lt;/p&gt;&lt;p&gt;相反，如果我修改先前的猜测以提高交叉频率项的幅度，那么性能会好得多！更具体地说，我采用与上述相同的过程，但在拟合后我将&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;A&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;3&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;提高了&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;π&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;倍。用这种增强的近似替换神经元可将损失降低至 0.091，准确度提高至 98.79%。并不完美，但看起来神经元确实在很大程度上使用 ReLU 线性项和这些横轴项的组合来计算解决方案。&lt;/p&gt;&lt;p&gt;我对我在这里的解释并不完全满意，但我也想在今晚从圣诞节到新年休假之前结束对这个玩具问题的研究，所以让我们继续吧！&lt;/p&gt;&lt;h1&gt;神经元到logit操作&lt;/h1&gt;&lt;p&gt;如果神经元激活&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;N&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;已知（形状为 [batch_a, batch_b, d_mlp] 的矩阵），则可以通过乘以矩阵&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;W&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;l&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;o&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;g&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;i&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space3"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;W&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;o&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;u&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;W&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;U&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;来恢复 logits，其形状为 [ d_mlp，d_vocab]。&lt;/p&gt;&lt;p&gt;如果我对标记 0-99 沿词汇方向进行傅立叶变换&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;W&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;log&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;i&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;t&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;，&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;然后&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;取所有这些标记的平均功率谱，我会在相同的特征频率处看到相同的三个强峰值（0.33、0.39、0.48） ，我还看到一个新峰值出现在&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;"&gt;f&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0.01&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;处。该峰值与激活中的线性特征相关，其他三个峰值与上面检查的振荡特征相关。请参阅下面的&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;W&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;l&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;o&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;g&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;i&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;中神经元激活和 [d_vocab] 向量的一些示例： &lt;/p&gt;&lt;p&gt;&lt;img alt="该图像的 alt 属性为空；它的文件名是sample_neurons_w_logit-2.png" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/RABp7ZMw2FGwh4odq/a7ehwv9pt5bijwvhv9jh" /&gt;&lt;/p&gt;&lt;p&gt;有许多神经元，如左侧的神经元 10。当&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;和&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;都很大时，该神经元特别会抑制低值 logits 并提高高值 logits（这可以从顶部和中间行图的组合中读出）。第二行中的神经元 17 提高了小数字标记和大数字标记的 logit 值（后者似乎是模型学习内容的错误？），但当&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;和&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;大小相似且都很小。在这两种情况&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;下&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;，&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;W&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;log&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;i&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;向量中的主频率为 0.01，因为它包含一个平滑的正弦特征，该特征会限制某些标记并增强其他标记。&lt;/p&gt;&lt;p&gt;右边的两个情节展示了不同的故事。神经元 75 和 125 是振荡的，它们以振荡的方式影响 logits。这些神经元的&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;W&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;log&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;i&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;t&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;向量&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;的&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;振荡具有与神经元本身相同的特征频率。底部面板图显示功率谱，对于右侧两列（神经元 75 和 125），我绘制了&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;W&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;l&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;o&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;g&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;i&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;向量的功率谱，并表示主导神经元频率（在前面的计算中）部分）在垂直线上 - 并且神经元和映射矢量频率之间确实有很好的一致性！&lt;/p&gt;&lt;p&gt;所以如果我在上一节中是正确的并且振荡神经元有像&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;cos&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;这样的术语&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;ω&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;phi&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;cos&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;ω&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;phi&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; ，则这些向量按以下方式映射：&lt;/p&gt;&lt;p&gt; &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;ℓ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;c&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;A&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;cos&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;ω&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;phi&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;cos&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;ω&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;phi&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;cos&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;ω&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;c&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;phi&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;3&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;对于某个振幅&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;A&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;和相位&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;phi&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;3&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; ，其中&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;ℓ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;c&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;，&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;是来自神经元&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;的标记&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;c&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;对 logit 的贡献。简化形式是&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;ℓ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;c&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mfrac MJXc-space3"&gt;&lt;span class="mjx-box MJXc-stacked" style="width: 0.672em; padding: 0px 0.12em;"&gt;&lt;span class="mjx-numerator"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;A&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-denominator"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;4&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-line"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-vsize"&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;余弦&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px;"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;′&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;−&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px;"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;′&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;−&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;c&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px;"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;′&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;cos&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px;"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;′&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;−&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px;"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;′&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;c&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px;"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;′&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;cos&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px;"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;′&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px;"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;′&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;−&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;c&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px;"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;′&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;cos&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px;"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;′&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px;"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;′&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;c&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px;"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;′&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;其中&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px;"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;′&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space3"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;ω&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;φ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;，&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msup MJXc-space1"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px;"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;′&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space3"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;ω&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;φ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;，&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msup MJXc-space1"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;c&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px;"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;′&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space3"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;ω&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;c&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;φ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;3&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 。因此，有一些三角项在&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;空间中振荡，并且&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;c&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;的值稍微改变这些项的相位，以将 logits 调整到它们需要的值。&lt;/p&gt;&lt;p&gt;最后，logits 本身是由神经元&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;ℓ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;c&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-munderover MJXc-space3"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-size1-R"&gt;Σ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space1"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;ℓ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;c&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;的&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;总和构建的，其中包含神经元 10 和 17 等线性项，然后是由线性项。&lt;/p&gt;&lt;h1&gt;洛吉特人&lt;/h1&gt;&lt;p&gt;最后，我们得到如下所示的 logit（此处绘制的是 0、25、50、75 的 logit 映射）： &lt;/p&gt;&lt;p&gt;&lt;img alt="该图像的 alt 属性为空；它的文件名是 logit_values.png" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/RABp7ZMw2FGwh4odq/oyl7fx3fyuu9pnzyjql9" /&gt;&lt;/p&gt;&lt;p&gt;我突然想到的一件事是，这些模式不再在二维上振荡——它们似乎仅在&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;−&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;的值方面振荡。这表明模型设置了&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;phi&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;和&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;phi&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;的值，以便它可以用&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;cos&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;剔除振荡项&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px;"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;&amp;#39;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px;"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;&amp;#39;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;±&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;c&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px;"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;&amp;#39;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; （我没时间测试这个）。&lt;/p&gt;&lt;p&gt;这也具有直观意义：沿着我上面绘制的&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;平面中的对角线，减法的结果始终相同！如果将&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;加 1，同时将&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;加 1，则两者之间的差保持不变。因此，模型学习一个在&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;−&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;b&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;方向上变化和振荡的解，但在垂直于该方向的方向上完全恒定，这是有道理的。整洁的！&lt;/p&gt;&lt;h1&gt;包起来&lt;/h1&gt;&lt;p&gt;我有大约 85% 的信心认为我已经弄清楚了该模型用于进行减法的算法。有几个地方（特别是在最后，在激活函数之后）事情变得有点仓促和手动，如果我有无限的时间花在这个模型上，那么我会巩固和完善那里的一些概念。但我不！&lt;/p&gt;&lt;p&gt;我将结束这个模型，并在下一篇文章中转向其他可能更有趣的事情，但如果有人对我的分析中的漏洞或对模型可能正在做的事情的想法有任何想法，我会这样做。我不是在检查，我很乐意讨论！&lt;/p&gt;&lt;h2&gt;代码&lt;/h2&gt;&lt;p&gt;我用来训练模型的代码可以在&lt;a href="https://colab.research.google.com/drive/1Yy59roJgegsNguWLzt6PbxeEoKBVdhGW?usp=sharing"&gt;这个 colab 笔记本&lt;/a&gt;中找到，并且用于本次调查的笔记本可以&lt;a href="https://github.com/evanhanders/2digit_subtraction_transformer/blob/main/Interpretability_1layer_2digit_subtraction_digit_token.ipynb"&gt;在 Github 上找到&lt;/a&gt;。&lt;/p&gt;&lt;h2&gt;致谢&lt;/h2&gt;&lt;p&gt;再次感谢 Adam Jermyn 的指导和建议，并感谢 Philip Quirke 和 Eoin Farrell 参加我们的定期会议。还要感谢 Alex Atanasov 和 Xianjun Yang 本周抽出时间与我会面并讨论 ML/AI！&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/RABp7ZMw2FGwh4odq/how-does-a-toy-2-digit-subtraction-transformer-predict-the-1#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Fri, 22 Dec 2023 21:17:30 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/RABp7ZMw2FGwh4odq/how-does-a-toy-2-digit-subtraction-transformer-predict-the-1</guid></item></channel></rss>