<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>少错</title><link>https://www.lesswrong.com</link><description>致力于提炼理性艺术的社区博客</description><lastBuildDate>Tue, 05 Dec 2023 00:53:43 GMT</lastBuildDate><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>通过可进化的机构加速科学发展</title><link>https://www.lesswrong.com/posts/9uEAwHzoDS8shpdoX/accelerating-science-through-evolvable-institutions</link><description>发布于 2023 年 12 月 4 日晚上 11:21（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;&lt;i&gt;这是向圣达菲研究所“加速科学”工作组提交的演讲的书面版本。&lt;/i&gt;&lt;/p&gt;&lt;p&gt;我们来这里是为了讨论“加速科学发展”。我喜欢从历史的角度来开始讨论这样的话题：科学在过去什么时候（如果有的话）加速了？现在还在加速吗？我们可以从中学到什么？&lt;/p&gt;&lt;p&gt;我认为，在整个人类历史中，科学以及更广泛的人类知识&lt;i&gt;一直&lt;/i&gt;在加速发展。我还不能证明这一点（而且我自己对此只有大约 90% 的把握），但让我诉诸你的直觉：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Behavioral_modernity"&gt;从行为上看，现代人类&lt;/a&gt;已有 50,000 多年的历史&lt;/li&gt;&lt;li&gt;文字只有大约 5000 年的历史，因此在人类时间线的 90% 以上，我们只能积累能够适应口头传统的知识&lt;/li&gt;&lt;li&gt;在古代和中世纪世界，我们只有少数几门科学：天文学、几何学、一些数论、一些光学、一些解剖学&lt;/li&gt;&lt;li&gt;在科学革命之后的几个世纪（大约 1500 年代至 1700 年代），我们得到了日心说、运动定律、万有引力理论、化学的起源、细胞的发现、更好的光学理论&lt;/li&gt;&lt;li&gt;在 1800 年代，事情真正开始发展，我们有了电磁学、原子理论、进化论、细菌理论&lt;/li&gt;&lt;li&gt;1900 年代，核物理、量子物理、相对论、分子生物学和遗传学继续蓬勃发展&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;我把自 1950 年左右以来科学是否已经放缓的问题放在一边，我对此没有强烈的看法。即使确实如此，这也只是整个历史加速的总体模式中最近的一个小插曲。 （或者，你知道，历史上前所未有的逆转和衰退的开始。其中之一。）&lt;/p&gt;&lt;p&gt;我对这种加速模式深信不疑的部分原因是，加速的不仅仅是科学：几乎所有衡量人类进步的指标都显示出相同的趋势，包括&lt;a href="https://ourworldindata.org/grapher/world-gdp-over-the-last-two-millennia?yScale=log"&gt;世界 GDP&lt;/a&gt;和&lt;a href="https://ourworldindata.org/grapher/population?yScale=log&amp;amp;country=~OWID_WRL"&gt;世界人口&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;是什么推动了科学的加速发展？许多因素，包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;资金。&lt;/strong&gt;曾经，科学家必须&lt;a href="https://rootsofprogress.org/funding-models-for-science-and-innovation"&gt;寻求赞助，或者独立致富&lt;/a&gt;。现在有可用的赠款，并且资金总额在过去几十年中大幅增加： &lt;/li&gt;&lt;/ul&gt;&lt;figure class="image"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9uEAwHzoDS8shpdoX/dhxfk91oznfay66wdxnn" /&gt;&lt;figcaption&gt;&lt;a href="https://www.aaas.org/programs/r-d-budget-and-policy/historical-trends-federal-rd"&gt;&lt;i&gt;美国科学促进会&lt;/i&gt;&lt;/a&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;人们。&lt;/strong&gt;更多的科学家（在其他条件相同的情况下）意味着科学发展得更快，科学家的数量急剧增加，这既是因为总体人口的增长，也是因为更多的劳动力进入研究领域。在&lt;a href="https://archive.org/details/sciencesincebaby0000pric/page/107/mode/1up?view=theater"&gt;&lt;i&gt;《自巴比伦以来的科学》一&lt;/i&gt;&lt;/a&gt;书中，德里克·J·德·索拉·普赖斯 (Derek J. de Solla Price) 表示，“历史上大约 80% 到 90% 的科学家现在还活着”，这&lt;a href="https://futureoflife.org/guest-post/90-of-all-the-scientists-that-ever-lived-are-alive-today/"&gt;可能仍然是正确的&lt;/a&gt;： &lt;/li&gt;&lt;/ul&gt;&lt;figure class="image"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9uEAwHzoDS8shpdoX/wh3ldfdjulipzfcrroyg" /&gt;&lt;figcaption&gt; &lt;a href="https://futureoflife.org/guest-post/90-of-all-the-scientists-that-ever-lived-are-alive-today/"&gt;&lt;i&gt;埃里克·加斯特弗兰德&lt;/i&gt;&lt;/a&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;仪器。&lt;/strong&gt;更好的工具意味着我们可以做更多更好的科学研究。伽利略有一个简单的望远镜。现在我们有&lt;a href="https://en.wikipedia.org/wiki/James_Webb_Space_Telescope"&gt;JWST&lt;/a&gt;和&lt;a href="https://en.wikipedia.org/wiki/LIGO"&gt;LIGO&lt;/a&gt; 。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;计算。&lt;/strong&gt;更强的计算能力意味着更多更好的数据处理方式。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;沟通。&lt;/strong&gt;思想传播得越快越好，科学传播就越高效、越有效。科学期刊是在印刷机发明之后才发明的。互联网支持预印本服务器，例如 arXiv。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;方法。&lt;/strong&gt;更好的方法造就更好的科学，从培根经验主义到&lt;a href="https://en.wikipedia.org/wiki/Koch%27s_postulates"&gt;科赫假设&lt;/a&gt;再到&lt;a href="https://en.wikipedia.org/wiki/Randomized_controlled_trial"&gt;随机&lt;/a&gt;对照试验（实际上是所有统计数据）。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;机构。&lt;/strong&gt;实验室、大学、期刊、资助机构等共同构成了一个支持现代科学的生态系统。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;社会地位。&lt;/strong&gt;科学越受到尊重和声望，就会有越多的人和金钱流入它。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;现在，如果我们想问科学是否会继续加速发展，我们可以思考哪些驱动因素将继续增长。我建议：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;只要世界经济持续增长，科学经费就会继续增长&lt;/li&gt;&lt;li&gt;仪器、计算和通信将随着技术的发展而不断改进&lt;/li&gt;&lt;li&gt;我认为方法没有理由不继续改进，作为科学本身的一部分&lt;/li&gt;&lt;li&gt;科学的社会地位似乎相当强大：它是一个受人尊敬和享有盛誉的机构，获得了一些社会最高荣誉&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;从长远来看，如果&lt;a href="https://ourworldindata.org/grapher/comparison-of-world-population-projections"&gt;世界人口像预计的那样趋于稳定&lt;/a&gt;，我们可能会耗尽继续扩大研究人员基础的人员，这是一个潜在的问题，但不是我今天的重点。&lt;/p&gt;&lt;p&gt;最大的危险信号是我们的科学机构。制度影响所有其他因素，尤其是资金和人才的管理。今天，元科学界的许多人对我们的机构感到担忧。常见的批评包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;速度。&lt;/strong&gt;获得资助很容易需要 12-18 个月的时间（如果你幸运的话）&lt;/li&gt;&lt;li&gt;&lt;strong&gt;高架。&lt;/strong&gt;研究人员通常将 30-50% 的时间花在资助上&lt;/li&gt;&lt;li&gt;&lt;strong&gt;耐心。&lt;/strong&gt;研究人员认为他们需要定期展示结果，并且不能走一条可能需要多年才能得出结果的道路&lt;/li&gt;&lt;li&gt;&lt;strong&gt;风险承受能力。&lt;/strong&gt;赠款资金倾向于保守的、渐进的建议，而不是大胆的、“高风险、高回报”的计划（尽管&lt;a href="https://commonfund.nih.gov/highrisk"&gt;做出了相反的努力&lt;/a&gt;）&lt;/li&gt;&lt;li&gt;&lt;strong&gt;共识。&lt;/strong&gt;一个领域可能会过快地集中于一个假设并修剪替代的研究分支&lt;/li&gt;&lt;li&gt;&lt;strong&gt;研究员年龄。&lt;/strong&gt;随着时间的推移，赠款的趋势是拨款给年龄更大、更成熟的研究人员&lt;/li&gt;&lt;li&gt;&lt;strong&gt;自由。&lt;/strong&gt;科学家缺乏完全自主地指导研究的自由；赠款资金附加太多条件&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;现在，作为一名前科技创始人，我不禁注意到，在营利性风险投资领域，大多数问题似乎都得到了缓解。筹集风险投资资金相对较快（通常一轮融资会在几个月内完成，而不是一年或更长时间）。作为创始人/首席执行官，我花了大约 10-15% 的时间筹款，而不是 30-50%。风险投资公司大胆下注，积极寻求逆向立场，并支持年轻的新贵。他们大多给予创始人自主权，也许会在董事会中占据一席之地以进行治理，并且只有在表现非常糟糕时才会解雇首席执行官。 （上面列出的初创公司创始人可能还会抱怨的唯一问题是耐心：如果你的钱用完了，你最好能取得进展，否则你在下一轮融资时就会遇到困难。）&lt;/p&gt;&lt;p&gt;我不认为风险投资界在这些方面做得更好，因为风险投资家比科学资助者更聪明、更有智慧或更优秀——但事实并非如此。相反，风险投资家：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;争夺优惠（并且真的不想错过好优惠）&lt;/li&gt;&lt;li&gt;从长远来看，成功或失败取决于其投资组合的表现&lt;/li&gt;&lt;li&gt;在大约 5-10 年内看到这些结果&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;简而言之，&lt;strong&gt;风险投资面临着进化压力。&lt;/strong&gt;他们不能陷入明显的不良均衡，因为如果这样做，他们就会在竞争中落败并失去市场力量。&lt;/p&gt;&lt;p&gt;证明这一点的是风险投资在过去几十年里&lt;i&gt;的&lt;/i&gt;发展——主要是朝着为创始人提供更好待遇的方向发展。例如，早期阶段存在较高估值的长期趋势，这最终意味着较低的稀释度以及权力从风投向创始人的转移：创始人在过去的几年里放弃公司一半或更多的股份是很常见的。第一轮融资；最后我检查了一下，大约是 20% 或更少。风险投资并不总是资助大学刚毕业的年轻技术人员。曾经有一段时间，他们倾向于青睐更有经验的首席执行官，或许还拥有 MBA 学位。他们并不总是支持创始人领导的公司；曾经，创始人在最初几年后被解雇并由专业首席执行官取代的情况很常见（当 A16Z 在 2009 年推出时，他们大肆宣扬&lt;a href="https://a16z.com/why-we-prefer-founding-ceos/"&gt;他们不会这样做&lt;/a&gt;）。&lt;/p&gt;&lt;p&gt;所以我认为&lt;strong&gt;，如果我们希望看到我们的科学机构&lt;/strong&gt;&lt;i&gt;&lt;strong&gt;得到改进&lt;/strong&gt;&lt;/i&gt;&lt;strong&gt;，我们需要考虑它们如何&lt;/strong&gt;&lt;i&gt;&lt;strong&gt;发展&lt;/strong&gt;&lt;/i&gt;&lt;strong&gt;。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;我们的科学机构的发展程度如何？不是特别的。当今大多数科学组织都是大学或政府部门。尽管我很尊重大学和政府，但我认为任何人都必须承认它们是我们行动较为缓慢的机构之一。 （大学尤其具有极强的弹性和抵抗力：例如，牛津大学和剑桥大学的历史可以追溯到中世纪，经历了帝国的兴衰，直到今天仍然完好无损。）&lt;/p&gt;&lt;p&gt;科学资助机构的进化所面临的挑战与风险投资的进化相反：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;他们往往缺乏竞争，&lt;/strong&gt;尤其是 NIH 和 NSF 等集中式联邦机构&lt;/li&gt;&lt;li&gt;&lt;strong&gt;他们缺乏任何真正的反馈循环&lt;/strong&gt;，在这种循环中，资助者的资源是由过去的判断和其投资组合的成功决定的（迈克尔·尼尔森多次&lt;a href="https://twitter.com/michael_nielsen/status/1451626771690897408"&gt;指出&lt;/a&gt;，从“爱因斯坦作为专利职员做了最好的工作”到“卡塔林·卡里科”的资助失败）在获得诺贝尔奖之前被拒绝授予资助和终身教职”似乎甚至没有引发相关机构内部的反思过程）&lt;/li&gt;&lt;li&gt;&lt;strong&gt;他们需要很长的周期&lt;/strong&gt;才能了解其工作的真正影响，而这种影响可能需要 20-30 年才能显现出来&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;我们如何提高科学经费的可进化性？我们应该思考如何改善这些因素。我没有什么好主意，但我会抛出一些不成熟的想法来开始对话：&lt;/p&gt;&lt;p&gt;&lt;strong&gt;我们如何增加科学资助的竞争？&lt;/strong&gt;我们可以增强慈善事业的作用。在美国，我们可以将联邦资金转移到州一级，设立五十个资助者而不是一个。 （国家农业实验站就是一个成功的例子，这些实验站之间的竞争是杂交玉米研究的关键，这是 20 世纪农业科学最伟大的成功之一。）在国际层面，我们可以支持对科学家更加开放的移民。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;我们如何创建更好的反馈循环？&lt;/strong&gt;这很困难，因为我们需要某种方法来衡量结果。实现这一目标的一种方法是将资金从预期赠款转向各级各种回顾性奖项。如果这个“经济”足够大和强大，这些成果就可以被金融化，以创建一个动态的、有竞争力的融资生态系统，并具有适当水平的风险承担和耐心，经验丰富的退伍军人与年轻特立独行者之间的适当平衡等.（ &lt;a href="https://forum.effectivealtruism.org/posts/r7vmtHZKuosJZ3Xq5/altruistic-equity-allocation"&gt;影响证书&lt;/a&gt;，例如&lt;a href="https://protocol.ai/blog/hypercert-new-primitive/"&gt;超级证书&lt;/a&gt;，可以成为该解决方案的一部分。）&lt;/p&gt;&lt;p&gt;&lt;strong&gt;我们如何解决反馈周期长的问题？&lt;/strong&gt;我不知道。如果我们不能缩短周期，也许我们需要延长资助者的职业生涯，这样他们至少可以从几个周期中学习——这&lt;a href="https://rootsofprogress.org/how-curing-aging-could-help-progress"&gt;是长寿技术的潜在好处&lt;/a&gt;。或者，也许我们需要一个科学资助者，它可以极快地学习，可以消耗大量有关研究项目及其最终结果的历史信息，永远不会忘记其经历，并且永远不会退休或死亡——当然，我想到的是人工智能。关于人工智能支持、增强或取代科学研究人员本身的讨论很多，但人工智能在科学领域的最大机会可能是在资金和管理方面。&lt;/p&gt;&lt;p&gt;我怀疑资助机构会在这个方向上走得太远：它们必须自愿接受竞争、加强问责并承认错误，而这种情况很少见。 （看看现在那些因卡里科获得诺贝尔奖而获得功劳的机构，他们几乎没有为她提供支持。）如果机构很难进化，那么元进化就更难了。&lt;/p&gt;&lt;p&gt;但也许资助者背后的资助者，即那些向资助者提供预算的资助者，可以开始将资金分配给多个机构，以要求绩效指标，或者干脆转向上述回顾性模式。这可以提供所需的进化压力。&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/9uEAwHzoDS8shpdoX/accelerating-science-through-evolvable-institutions#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Mon, 04 Dec 2023 23:21:35 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/9uEAwHzoDS8shpdoX/accelerating-science-through-evolvable-institutions</guid></item><item><title>与国会工作人员谈论人工智能风险</title><link>https://www.lesswrong.com/posts/2sLwt2cSAag74nsdN/speaking-to-congressional-staffers-about-ai-risk</link><description>发布于 2023 年 12 月 4 日晚上 11:08（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt; 2023 年 5 月和 6 月，我（Akash）与国会工作人员就人工智能风险举行了大约 50-70 次会议。我一直想写一篇文章来反思这次经历和我的一些收获，我认为这可能是 LessWrong 对话的一个好话题。我看到他们&lt;a href="https://www.lesswrong.com/posts/kQuSZG8ibfW6fJYmo/announcing-dialogues-1?commentId=L2qFjT8taEhkm4hCB"&gt;提出要与人们进行 LW 对话&lt;/a&gt;，于是我伸出了援手。&lt;/p&gt;&lt;p&gt;在这次对话中，我们讨论了我如何决定与工作人员聊天、我在华盛顿的初步观察、有关国会办公室如何工作的一些背景、我的会议是什么样的、我学到的教训以及关于我的经历的一些杂项。&lt;/p&gt;&lt;h2&gt;语境&lt;/h2&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;有&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;嘿！在您的留言中，您提到了一些与您在华盛顿的经历相关的主题。&lt;/p&gt;&lt;p&gt;我认为我们应该从您与国会办公室谈论人工智能风险的经历开始。我很有兴趣了解更多；似乎没有太多公共资源来说明这种外展活动是什么样的。&lt;/p&gt;&lt;p&gt;那是怎么开始的？是什么让你想这么做？ &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;阿卡什&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;2023 年 3 月，我开始在&lt;a href="https://www.safe.ai/"&gt;人工智能安全中心&lt;/a&gt;从事一些人工智能治理项目。我的一个项目涉及帮助 CAIS 响应&lt;a href="https://www.ntia.gov/issues/artificial-intelligence/request-for-comments"&gt;NTIA&lt;/a&gt;发布的关于人工智能问责制的评论请求。&lt;/p&gt;&lt;p&gt;作为这项工作的一部分，&lt;strong&gt;我开始思考一个好的前沿人工智能监管框架应该是什么样子。&lt;/strong&gt;例如：如果我可以为前沿人工智能系统建立许可制度，它会是什么样子？它会被安置在美国政府的什么地方？我希望它评估哪些信息？&lt;/p&gt;&lt;p&gt;&lt;strong&gt;我开始想知道实际的政策制定者会对这些想法有何反应&lt;/strong&gt;。我也很好奇更多地了解政策制定者如何考虑人工智能灭绝风险和灾难性风险。&lt;/p&gt;&lt;p&gt;我开始询问人工智能治理领域的其他人。绝大多数人（根本）没有与国会工作人员交谈过。一些人有与员工交谈的经验，但没有与他们谈论人工智能风险。很多人告诉我，他们认为与政策制定者的接触非常重要，但却被忽视了。当然，也存在下行风险，所以你不希望有人做得不好。&lt;/p&gt;&lt;p&gt;在咨询了 10-20 名人工智能治理人员后，我询问 CAIS 我是否可以去华盛顿并开始与国会办公室交谈。目标是（a）提高对人工智能风险的认识，（b）更好地了解国会办公室如何考虑人工智能风险，（c）更好地了解国会办公室的人们有哪些与人工智能相关的优先事项，以及 (d) 获取有关我的 NTIA 评论想法请求的反馈。&lt;/p&gt;&lt;p&gt; CAIS 批准了，我于 2023 年 5 月至 6 月去了华盛顿。需要澄清的是，这不是 CAIS 告诉我要做的事情——这更像是 CAIS 意识到正在发生的“阿卡什事件”。 &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;有&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;哇，这真的很有趣。几个随机问题：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;当然，也存在下行风险，所以你不希望有人做得不好。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;一个人怎样才能把一件事做得不差呢？如何学习与政策制定者互动？&lt;br /&gt;&lt;br /&gt;另外，你的背景是什么？在此之前您做过政策方面的工作吗？ &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;阿卡什&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;是的，很好的问题。我不确定最好的学习方法是什么，但我尝试过以下一些方法：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;与有与政策制定者互动经验的&lt;strong&gt;人交谈&lt;/strong&gt;。询问他们说什么、他们发现什么令人惊讶、他们犯了什么错误、他们注意到什么下行风险等等。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;看书&lt;/strong&gt;。我发现&lt;a href="https://www.amazon.com/Master-Senate-Years-Lyndon-Johnson/dp/0394720954"&gt;参议院议长&lt;/a&gt;和&lt;a href="https://www.amazon.co.uk/Act-Congress-Americas-Essential-Institution/dp/0307744515"&gt;国会法案&lt;/a&gt;特别有帮助。我目前正在阅读&lt;a href="https://www.amazon.com/Devils-Chessboard-Dulles-Americas-Government/dp/0062276174"&gt;《魔鬼的棋盘》，&lt;/a&gt;以更好地了解中央情报局和情报机构，到目前为止，我发现它内容丰富。&lt;/li&gt;&lt;li&gt;与你已经认识的政策制定者&lt;strong&gt;进行角色扮演&lt;/strong&gt;，并要求他们提供直率的反馈。&lt;/li&gt;&lt;li&gt;在风险较低的会议中&lt;strong&gt;进行练习&lt;/strong&gt;，并利用这些经验进行迭代。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在此之前我没有做过太多政策方面的事情。在大学里，我为《哈佛政治评论》撰稿，并参与了政治研究所的工作，但这比“现实世界的政策参与”的内容更具学术性。&lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;h2&gt;抵达华盛顿特区并进行初步观察&lt;/h2&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;有&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;这一切都是有道理的。到达华盛顿后你做了什么？ &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;阿卡什&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;我给国会办公室以及一些行政部门的人员发送了冷电子邮件。我还联系了华盛顿的一些 EA。我还继续处理 NTIA 的评论请求（截止日期为 6 月 6 日）。&lt;/p&gt;&lt;p&gt;最初的计划是召开几次会议，评估会议的进展情况，如果我认为进展相当顺利，则再召开更多会议。&lt;/p&gt;&lt;p&gt;总的来说，我最终与国会工作人员举行了大约 50-70 次会议（以及一些与智库人员和行政部门机构人员的会议，但我将在这篇文章中重点讨论与国会工作人员的会议）。 &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;有&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;我认为他们进展得相当顺利，那么？ &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;阿卡什&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;据我说，是的！我要注意的一件事是，这些可能有点难以评估——比如，员工应该对人友善，他们不会说“我以为你是个白痴”或“你浪费了我的时间”之类的话。时间”或“我现在对人工智能安全性的印象更差了。”&lt;/p&gt;&lt;p&gt;记住这一点：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;我对员工们的开放态度感到惊讶。&lt;/strong&gt;奥弗顿之窗最近发生了很大的变化，但当时，我真的不知道人们是否会说“哈！&lt;i&gt;灭绝风险？&lt;/i&gt;这听起来像科幻小说。”&lt;/li&gt;&lt;li&gt;&lt;strong&gt;主导氛围是“人工智能非常重要，我是一名忙碌的员工，有 100 个优先事项，所以我没有时间了解它。我&lt;/strong&gt;真的很高兴能与能够告诉我有关人工智能的东西的人交谈– 我一直渴望跟上进度。”&lt;/li&gt;&lt;li&gt;&lt;strong&gt;员工们对有机会见到愿意回答有关人工智能基本问题的人表示非常感激&lt;/strong&gt;（例如，什么是大型语言模型，它与其他类型的人工智能有何不同？有多少公司从事前沿人工智能？）&lt;/li&gt;&lt;li&gt;有一些“切实”的信号表明事情进展顺利。例如，一些工作人员将我介绍给他们认识的其他人，一些人将他们办公室正在起草的工作发给我，还有一些人甚至将我介绍给国会议员（总共两个）。&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/section&gt;&lt;h2&gt;国会办公室的层级&lt;/h2&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;有&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;这真的很有趣！&lt;br /&gt;&lt;br /&gt;顺便说一句，您能给我描绘一下国会办公室的人员配置等级吗？例如，您通常与谁交谈，他们通常与国会议员有什么关系？ &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;阿卡什&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;好问题！因此，我的理解是，国会办公室通常具有以下角色，从“最有影响力”到“最没有影响力”列出：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;参谋长&lt;/li&gt;&lt;li&gt;立法主任&lt;/li&gt;&lt;li&gt;立法助理&lt;/li&gt;&lt;li&gt;立法通讯员&lt;/li&gt;&lt;li&gt;实习生和研究员&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;还有一些其他角色，但从立法角度来看，这些角色往往最重要。&lt;/p&gt;&lt;p&gt;请注意，每个办公室都有自己的氛围。有人曾经告诉我“每个国会办公室都是自己的初创企业，每个国会议员都可以按照自己的意愿管理自己的办公室。”&lt;/p&gt;&lt;p&gt;因此，在某些办公室，实习生和研究员实际上可能有很大的影响力（例如，如果国会议员或立法主任信任实习生是特定主题的主题专家）。但总的来说，我认为这种层次结构很常见。&lt;/p&gt;&lt;p&gt;我想我主要是与立法助理/立法通讯员级别的人交谈。我还与几位立法官员进行了交谈。&lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;h2&gt;外展到办事处&lt;/h2&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;有&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;好吧，这一切都有道理。那么，您是如何从几次会议增加到 60-80 次的呢？ &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;阿卡什&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;我向技术政策工作人员发送了一封群发电子邮件，回复人数给我留下了深刻的印象。这封电子邮件相当短，提到我在 CAIS，用 1-2 个要点介绍了 CAIS 的工作，并用要点说明了我正在处理 NTIA 的评论请求。&lt;/p&gt;&lt;p&gt;我认为国会工作人员现在确实对人工智能内容非常感兴趣。就像，如果我向人们发送电子邮件讨论其他问题，我认为我不可能召开这么多会议。&lt;/p&gt;&lt;p&gt;有人感觉“人工智能现在很热，但没有人真正了解人工智能”。我认为目前还不清楚这种情况会持续多久（尤其是“人们了解不多，办公室还没有下定决心”）部分。&lt;/p&gt;&lt;p&gt;我什至会说“我认为这是一个 AIS 社区作为一个整体可以/应该充分利用的机会”。比如，国会工作人员曾经（而且我认为仍然）对与人们讨论人工智能问题非常感兴趣——很难想象还有比这更好的机会让 AIS 社区的人们能够进来并担任顾问/倡导者。 &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;有&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;这就说得通了。&lt;/p&gt;&lt;p&gt;如何才能开始与国会工作人员接触？人们应该做什么才能进入这个领域/哪些组织可能适合为此部署人员？ &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;阿卡什&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;这将是一个相当模糊的答案，但我认为这在很大程度上取决于人、他们的技能和他们的政策目标。&lt;/p&gt;&lt;p&gt;另外——我在上面提到过这一点，但重要的是要重申——人们做得不好肯定会带来风险。另一方面，存在过于“不作为偏见”或类似情况的风险，并留下很多价值。&lt;/p&gt;&lt;p&gt;这确实很难且令人困惑。我之前提到，我咨询了 10-20 名 AI 治理人员。他们中的大多数人都说“这似乎很重要但被忽视了，但我不知道，这似乎很令人困惑。”他们中的一些人就像“是的，我完全认为你应该这样做，特别是如果你采用 XYZ 策略。”一位相当著名的人工智能治理人士明确告诉我，他们不希望我这样做。我发现很难平衡这种相互矛盾的反馈。&lt;/p&gt;&lt;p&gt;我还认为我的很多建议取决于某人到底想说什么——例如：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;他们的推销方式是什么？如果会议开始，工作人员说“那么，你想谈什么？”，最初的反应是什么？&lt;/li&gt;&lt;li&gt;他们是那种善于提出问题、对别人的世界观感到好奇的人吗？&lt;/li&gt;&lt;li&gt;他们听起来会危言耸听吗？&lt;/li&gt;&lt;li&gt;他们了解很多关于人工智能的事实吗？当他们不知道某件事时，他们是否能够认识到这一点并进行适当的对冲？&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;考虑到所有这些，如果阅读本文的人有兴趣与国会工作人员互动（或让他们组织中的某人这样做），并且他们重​​视我的意见，&lt;strong&gt;我建议他们通过 LW 与我联系。&lt;/strong&gt;我能够在更多背景下提供更好的建议。&lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;h2&gt;一次典型的会议&lt;/h2&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;有&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;是的，这一切都有道理。您能向我介绍一下您可能举行过的典型会议吗？例如，您将如何首次与员工联系，您将在哪里与他们见面，实际对话是什么样的，您将如何跟进或以其他方式弄清楚这是否有帮助？ &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;阿卡什&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;&lt;strong&gt;会议物流&lt;/strong&gt;&lt;/p&gt;&lt;ol&gt;&lt;li&gt;将通过电子邮件联系&lt;/li&gt;&lt;li&gt;通常会在国会办公室（华盛顿特区基本上有 4 座主要建筑都设有所有国会办公室）或通过 Zoom 与他们会面&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;strong&gt;会议进展如何&lt;/strong&gt;&lt;/p&gt;&lt;ol&gt;&lt;li&gt;谈话通常会从我询问他们是否对人工智能有任何疑问或希望我分享我正在研究的东西开始。通常，他们希望我先开始。&lt;/li&gt;&lt;li&gt;我首先介绍我自己和 CAIS。一旦&lt;a href="https://safe-ai.webflow.io/statement-on-ai-risk"&gt;CAIS 声明&lt;/a&gt;出来，我就会引用 CAIS 声明。我会告诉他们，我正在关注先进人工智能带来的全球安全风险。我还会告诉他们我正在制定 NTIA 响应，并且会告诉他们我正在考虑的一些高级想法。&lt;/li&gt;&lt;li&gt;然后，我会停下来看看他们是否有任何问题。&lt;/li&gt;&lt;li&gt;通常，他们要么询问更多有关灭绝风险的问题，要么询问有关人工智能的各种问题（例如，您对如何处理深度造假有什么想法吗？），或者提出一些有关监管的高级问题（例如，我们如何监管而不扼杀创新？我们如何监管而不输给中国？）&lt;/li&gt;&lt;li&gt;在一些最好的会议中，我会听到办公室正在研究的一些与人工智能相关的东西。大多数办公室没有能力/兴趣在人工智能领域发挥带头作用。大约 10% 的办公室表示“是的，我的国会议员对此非常感兴趣，我们正在考虑引入立法或成为其他人立法的核心部分。”&lt;/li&gt;&lt;li&gt;很多人问我是否有立法草案。显然，如果你有监管想法，人们希望看到你有一个像法案一样写成的（简短的）版本。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;strong&gt;跟进&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;NTIA 的评论请求回复完成后，我向遇到的每个人发送了一份后续信息。当我有一次特别好的会议时（例如，一位员工对人工智能风险表示强烈兴趣，或者告诉我他们想向我发送他们正在研究的东西），我会发送个性化的后续信息。我认为最明显的帮助迹象来自于人们继续向我发送问题/想法、将我介绍给同事或希望与我合作提出建议的情况。 （需要明确的是，这种情况发生在少数情况下，但我认为这是大部分影响的来源）。&lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;h2&gt;员工对人工智能风险的态度&lt;/h2&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;有&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;blockquote&gt;&lt;p&gt;很多人问我是否有立法草案。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;他们正在寻求针对哪些类型的问题进行立法？您建议的任何立法？ &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;阿卡什&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;工作人员经常想知道我是否有立法草案来描述我在 NTIA 回复中所写的许可制度（我没有立法草案，但后来在帮助 Thomas 关闭&lt;a href="https://www.aipolicy.us/"&gt;人工智能政策中心&lt;/a&gt;时参与了立法起草工作）地面。） &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;有&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;啊好吧。更一般地说，人们对人工智能风险有哪些先验？您认为您通常会导致他们处理该主题的方式发生重大变化吗？ &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;阿卡什&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;&lt;strong&gt;似乎大多数人对人工智能风险没有强烈的先见之明。&lt;/strong&gt;我本以为人们的先验会更加怀疑（比如“什么？世界末日？&lt;i&gt;真的吗&lt;/i&gt;？”）。但我认为很多人都会说“是的，我完全可以看到人工智能如何导致全球安全风险”，甚至“是的，我实际上很担心类似天网的人工智能，我很高兴其他人正在努力”关于这一点。”&lt;/p&gt;&lt;p&gt;通常，人们似乎&lt;strong&gt;真正担心人工智能带来的灭绝风险&lt;/strong&gt;，但也&lt;strong&gt;没有任何计划来解决这个问题&lt;/strong&gt;。有人提醒我，“X 是一种存在风险”实际上是一件非常 EA 的事情 --&amp;gt;“因此我应该认真考虑在 X 上工作。”很多人就像“我很高兴其他人正在考虑这个问题[但我不会，我也不指望我的国会议员会]。”&lt;/p&gt;&lt;p&gt;就我的效果而言，我认为我主要只是让他们更多地考虑这一点，并将其列入他们内部的“人工智能政策优先事项”列表中。我认为人们忘记了员工的优先事项清单上有大约 100 件事，所以仅仅让他们接触并重新接触这些想法就会有所帮助。&lt;/p&gt;&lt;p&gt;我还遇到了一些员工，他们似乎非常关心人工智能风险，并且似乎是人工智能政策领域的坚定盟友。我仍然与几个人保持着联系，当托马斯创办人工智能政策中心时，我向他介绍了其中的一些人。如果我希望通过一项法案，我想我可以更好地了解我会尝试与哪些特定人员取得联系。&lt;strong&gt;在我看来，整个华盛顿之行的大部分影响可能在于弄清楚盟军工作人员是谁并与他们建立初步关系。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;最后一件事是，我通常不强调失去控制//超级智能//递归自我完善。我没有隐藏它，但我将其包含在更长的威胁模型列表中，并且它很少是我试图传达的主要内容。如果我再做一次，我可能会更多地强调这些威胁模型。 &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;有&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;blockquote&gt;&lt;p&gt;在我看来，整个华盛顿之行的大部分影响可能在于弄清楚盟军工作人员是谁并与他们建立初步关系。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;啊好吧！有哪些特征可以很好地预测员工是否会同情这项事业？例如特定地区、政治倾向、其他政策。 &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;阿卡什&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;blockquote&gt;&lt;p&gt;有哪些特征可以很好地预测员工是否会同情我们的事业？&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;br /&gt;并不真地。样本量非常小。就像，总共可能有大约 4 名工作人员，我会把他们安排在“非常关心灭绝风险，并且他们可以在推动立法方面提供很大帮助”的位置。 1 名共和党人和 3 名民主党人。 &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;有&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;啊，明白了。你们所进行的讨论（在 CAIS 声明发布之前）是否对该声明产生了任何影响（措辞、外展等）？ &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;阿卡什&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;讨论并未影响该声明；该声明是在我前往华盛顿之前写的。 （有趣的事实：我是参与起草 CAIS 声明的人之一。认为为一个好句子做出贡献比我所做的许多其他事情的影响力要大 100 倍，这有点奇怪，但是嘿，有时它会起作用那样）。 &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;有&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;blockquote&gt;&lt;p&gt;（有趣的事实：我是参与起草 CAIS 声明的人之一。认为为一个好句子做出贡献比我所做的许多其他事情的影响力要大 100 倍，这有点奇怪，但是嘿，有时它会起作用那样）。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;该死。我们生活在一个奇怪的世界。顺便说一句，做得很好。 &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;阿卡什&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;谢谢！看到这个声明有多么重要确实很奇怪。&lt;/p&gt;&lt;p&gt;我认为这也是相当令人谦卑的——当我第一次听到这个声明时（当时我们称其为公开信），我记得当时我很沮丧，就像“嗯，一封公开信会做什么？我们已经有 FLI 暂停信。”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;这是一个有用的提醒，有时您可能无法提前预测某些事情的影响&lt;/strong&gt;。事后看来，很明显（至少对我来说）CAIS 声明是有用的，并且变革理论非常可靠。但当时，这并不像是一个落后的总体规划。感觉这只是 20 个项目清单中的一个项目，而且它有一种模糊的变革理论，这只是另一个似乎值得冒险的赌注。&lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;h2&gt;得到教训&lt;/h2&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;有&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;如果您再次进行此过程，您会采取哪些不同的做法？让您感到惊讶/您觉得自己从中学到的主要事情是什么？ &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;阿卡什&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;&lt;strong&gt;我想我会写一份文档来解释我的推理&lt;/strong&gt;，记录我咨询过的人，记录我所意识到的上行和下行风险，并将其发送给一些 EA。我认为一些谣言称这是以相当单边主义的方式完成的。这很棘手，让我很难过。我不认为我这样做的方式实际上是单边主义的，但我认为通过书面推理来避免误解会更好。 Thomas 在 CAIP 中做了很多这样的事情，并为&lt;strong&gt;“如何在不确定的情况下采取行动，同时以推理透明和高度协调的方式采取行动”等问题提供了一个很好的模型。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;我还认为我会提出&lt;strong&gt;立法草案&lt;/strong&gt;（假设我所在的组织对此感到满意）。如果你有立法草案，人们似乎会更认真地对待你。&lt;/p&gt;&lt;p&gt;我还会写一份&lt;strong&gt;更短的 NTIA 回复&lt;/strong&gt;– 我们最终写了一篇大约 20 多页的论文。我会针对较短的材料进行更多优化。&lt;/p&gt;&lt;p&gt;啊，说到这里，我会带&lt;strong&gt;一份打印出来的单页纸&lt;/strong&gt;来解释什么是 CAIS 并总结 NTIA 答复中的监管理念。我在中途就完成了这件事，而且我本来可以早点完成这件事。&lt;/p&gt;&lt;p&gt;另外，我会带着&lt;strong&gt;名片&lt;/strong&gt;来。人们似乎很喜欢名片！ &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;有&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;是的，这一切都有道理，尽管我绝对不会提前猜到。&lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;h2&gt;最后拍摄&lt;/h2&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;有&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;我想我已经没有什么问题要问了：你还有什么要说的吗？随意闲逛。 &lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;阿卡什&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;以下是一些杂项：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;我在华盛顿的经历让我觉得&lt;strong&gt;奥弗顿之窗非常宽&lt;/strong&gt;。国会没有对人工智能政策的缓存，而且似乎很多人真的想学习。目前尚不清楚这种情况会持续多久（例如，人工智能风险最终可能会两极分化），但我们似乎正处于一个异常高度开放和好奇心的时期。&lt;/li&gt;&lt;li&gt;然而，&lt;strong&gt;让国会采取任何行动也非常困难&lt;/strong&gt;。就像，由于相当无聊的原因，没有多少法案获得通过。在这个过程中，有很多步骤可能会导致账单失效。当事情需要两党合作时更是如此（目前他们确实这样做，因为我们有民主党参议院和共和党众议院）。这主要让我想到&lt;strong&gt;“哇，现状通常不会发生任何事情，而且需要做很多工作才能获得任何有意义的立法。”&lt;/strong&gt;考虑到这一点，我确实认为我们在人工智能安全方面处于一个非常独特的境地（&lt;i&gt;实际上&lt;/i&gt;没有那么多事情会带来灭绝风险和各种其他灾难​​性风险；也没有那么多事情会成为现实）参议院多数党领袖的优先事项，激发与世界领导人的国际峰会，或成为整个行政命令的焦点）。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;许多人高估了华盛顿特区的“内部游戏”数量&lt;/strong&gt;，尤其是在国会参与方面。有一些秘密的事情正在发生，但在大多数情况下，我认为没有人有掌控权。&lt;/li&gt;&lt;li&gt;我希望看到&lt;strong&gt;围绕具体政策愿景进行更多协调&lt;/strong&gt;。有一段时间，您加入 Cool Kids Club 仅仅是为了关心 xrisk。我认为奥弗顿之窗已经发生了很大的变化，我们已经到了“关心xrisk”已经不够的地步了。重要的是人们支持并愿意倡导哪些具体政策。&lt;/li&gt;&lt;li&gt;考虑到这一点，我还认为拥有更广泛的人工智能风险社区是有好处的。&lt;strong&gt;协调实施不当可能会导致一事无成，因为你永远无法达成共识&lt;/strong&gt;（目前这有利于领先的实验室和不受监管的扩展）。&lt;strong&gt;协调太少可能会导致缺乏联盟建设和不必要的冲突。&lt;/strong&gt;我认为我已经从“协调是好的”转变为“如果做得好，协调是好的，但实际上需要技巧、机智和努力才能做好协调”。&lt;/li&gt;&lt;li&gt;我通常认为&lt;strong&gt;更多的人应该公开写下他们的观点。&lt;/strong&gt;当我不知道人们相信什么时，很难协调。我认为社会应该不太愿意赞扬那些没有提出任何特定立场的人。 &lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;我对 DC AI 安全社区了解了很多（我所说的“AI 安全社区”主要指那些出于避免 xrisk 或社会规模灾难的愿望而从事 AI 安全工作的人们。有些人被认为是 EA/长期主义者） ，但很多人没有）&lt;ol&gt;&lt;li&gt; TLDR：这很复杂。我认为前10%的思想家都非常有才华，并且追求合理的变革理论。另一方面，也有很多人声称对人工智能政策感兴趣，但对各种人工智能安全威胁模型没有基本的了解。人们还（真实且合理地）担心，社交无能和政治无能的新来者可能会以威胁或削弱现有努力的方式进入该领域。&lt;/li&gt;&lt;li&gt;总的来说，我觉得主流文化对新的政策努力过于不屑一顾。我希望随着人工智能政策对话的不断向前发展并吸引新的人群，这种情况会发生变化。我会对社区的反应更像是“啊，新人感兴趣！让我们给您一些提示/指示，并指出我们所拥有的具体经验并讨论下行风险的具体模型”感到兴奋。现状常常让人感觉不那么具体，而且（在我看来）对新的努力过于保护主义。我发现这种文化让我更难清晰地思考或进行倡导，尤其是我所说的“高度直接性倡导”（EG 主要试图向人们传达你的内部世界状态，而不是主要尝试）传达一系列能够很好地吸引观众的信念）。我认为关于各种倡导工作应该如何“直接”进行认真的辩论（而且我认为如果华盛顿特区的一些人完全直接的话，他们实际上会失去一些影响力/“严肃性点”），但我仍然感到惊讶影响的大小——文化似乎阻碍我和我的同事直接表达的程度。我相信这种文化大大减缓了新政策的努力，并继续以我认为对世界不利的方式威胁/削弱/阻碍新的政策努力。与许多事情一样，我认为高层关注是正确的，但这些高层关注的具体应用/实施方式存在问题&lt;/li&gt;&lt;li&gt;评估不同人员/计划的跟踪记录也很困难。部分原因是某些信息是秘密的，部分原因是“我们与重要利益相关者有良好的关系”之类的事情是有用的工具性步骤，但不一定转化为影响，部分原因是许多变革理论都是基于点击量的，需要时间才能产生直接影响（例如，如果某人与 X 建立了良好的关系，也许在某个时候 X 将变得与人工智能监管极其相关，但也许只有 1-10% 的可能性是真的。）话虽如此，我认为，如果人们最终更明确地表达自己的信念，更明确地表达他们希望实现的具体政策目标，更明确地表达他们明显的胜利（和损失），那么协调会更容易。如果没有这一点，我们就会冒着赋予那些“玩游戏”、发展影响力但最终没有利用他们的影响力来实现有意义的变革的人太多权力和太多资源的风险。 （另请参阅多米尼克·卡明斯&lt;a href="https://www.dwarkeshpatel.com/p/dominic-cummings#details"&gt;播客&lt;/a&gt;）。&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;与此相关的是， &lt;a href="https://forum.effectivealtruism.org/posts/tdaoybbjvEAXukiaW/what-are-your-main-reservations-about-identifying-as-an?commentId=gNC53rsuMNTBjLCWY"&gt;奥利弗·哈布里卡（Oliver Habryka）的评论&lt;/a&gt;引起了我的共鸣。我发现，当我与“主流 EA”保持一定距离时，我的思维往往会更清晰。有很多抗体和微妙的文化压力可以阻止我思考某些想法，并可能削弱我在世界上采取直接行动的能力。 （当然，我不认为解决方案是“永远不要与 EA 互动”——但我确实认为人们可能低估了社区对良好思考和实现困难事物的负面影响。我确实低估了。）&lt;/li&gt;&lt;li&gt;对于有兴趣捐赠的人，我目前推荐&lt;strong&gt; &lt;/strong&gt;这&lt;strong&gt; &lt;/strong&gt;&lt;a href="https://www.aipolicy.us/"&gt;&lt;strong&gt;人工智能政策中心&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; &lt;/strong&gt;（尤其是托马斯·拉森继续高度参与其战略方向）。我与托马斯有一些战略/战术上的分歧，但我认为他是一个非常聪明和有才华的人，我认为他是人工智能政策领域最值得支持的新人之一（COI：托马斯是我的朋友之一，我在人工智能政策中心的早期阶段参与了帮助）。&lt;/li&gt;&lt;li&gt;如果您想与我交谈，&lt;strong&gt;请随时联系 LessWrong&lt;/strong&gt; 。我喜欢与从事人工智能政策工作的人交谈。我也愿意接受我可以做的或我知道的其他人可以做的有影响力的事情。 &lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;&lt;/section&gt;&lt;section class="dialogue-message ContentStyles-debateResponseBody"&gt;&lt;section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"&gt;&lt;b&gt;有&lt;/b&gt;&lt;/section&gt;&lt;div&gt;&lt;p&gt;哇，好吧。感谢您进行这次对话！&lt;/p&gt;&lt;/div&gt;&lt;/section&gt;&lt;div&gt;&lt;/div&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/2sLwt2cSAag74nsdN/speaking-to-congressional-staffers-about-ai-risk#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Mon, 04 Dec 2023 23:08:52 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/2sLwt2cSAag74nsdN/speaking-to-congressional-staffers-about-ai-risk</guid></item><item><title>开放主题 – 2023/2024 年冬季</title><link>https://www.lesswrong.com/posts/tRa92qDonDLi6RA8u/open-thread-winter-2023-2024</link><description>发布于 2023 年 12 月 4 日晚上 10:59（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;如果它值得说，但不值得单独发表，这里有一个地方可以放置它。&lt;/p&gt;&lt;p&gt;如果您是 LessWrong 的新手，这里是您自我介绍的地方。欢迎您就您如何找到我们以及您希望从网站和社区获得什么发表个人故事、轶事或只是一般性评论。如果您不想写完整的顶级帖子，这也是讨论功能请求和您对该网站的其他想法的地方。&lt;/p&gt;&lt;p&gt;如果您是社区新手，您可以开始阅读&lt;a href="https://lesswrong.com/highlights"&gt;Sequences 的亮点&lt;/a&gt;，这是有关 LessWrong 核心思想的帖子集合。&lt;/p&gt;&lt;p&gt;如果您想更多地探索社区，我建议您&lt;a href="https://www.lesswrong.com/library"&gt;阅读图书馆&lt;/a&gt;，&lt;a href="https://www.lesswrong.com/?view=curated"&gt;查看最近策划的帖子&lt;/a&gt;，&lt;a href="https://www.lesswrong.com/community"&gt;看看您所在的地区是否有任何聚会&lt;/a&gt;，并查看&lt;a href="https://www.lesswrong.com/faq"&gt;LessWrong 常见问题&lt;/a&gt;&lt;a href="https://www.lesswrong.com/faq#Getting_Started"&gt;解答&lt;/a&gt;的入门部分。如果您想了解网站上的内容，您还可以查看&lt;a href="https://www.lesswrong.com/tags/all"&gt;“概念”部分&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;开放线程标签在&lt;a href="https://www.lesswrong.com/tag/open-threads?sortedBy=new"&gt;这里&lt;/a&gt;。 Open Thread 序列在&lt;a href="https://www.lesswrong.com/s/yai5mppkuCHPQmzpN"&gt;这里&lt;/a&gt;。&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/tRa92qDonDLi6RA8u/open-thread-winter-2023-2024#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Mon, 04 Dec 2023 22:59:51 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/tRa92qDonDLi6RA8u/open-thread-winter-2023-2024</guid></item><item><title>采访 Vanessa Kosoy 谈人工智能理论研究的价值</title><link>https://www.lesswrong.com/posts/QPxrH5ex6MpYoLwer/interview-with-vanessa-kosoy-on-the-value-of-theoretical</link><description>发布于 2023 年 12 月 4 日晚上 10:58（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;以下是我与 Vanessa Kosoy 进行的&lt;a href="https://youtu.be/1MCRQF0_5zY?feature=shared"&gt;&lt;i&gt;视频采访&lt;/i&gt;&lt;/a&gt;&lt;i&gt;的文字记录（经过语法编辑）&lt;/i&gt; ，从我的&lt;a href="https://www.zenmarmotdigital.com/blog/interview-with-vanessa-kosoy"&gt;&lt;i&gt;博客&lt;/i&gt;&lt;/a&gt;&lt;i&gt;交叉发布&lt;/i&gt;&lt;i&gt;。它旨在（相对）对初学者友好地解释&lt;/i&gt;&lt;a href="https://www.lesswrong.com/posts/ZwshvqiqCvXPsZEct/the-learning-theoretic-agenda-status-2023#Direction_6__Metacognitive_Agents"&gt;&lt;i&gt;学习理论议程&lt;/i&gt;&lt;/a&gt;的目标&lt;i&gt;，以及为什么需要更多的理论工作来确保人工智能大规模安全可靠。&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;简介（作者：Will Petillo）：&lt;/strong&gt;讨论人工智能的未来往往会变得哲学化。有“目标”或“理解”意味着什么？追求权力是想要东西的默认结果……还是我们独特的进化历史造成的人类怪癖？是什么激发了善良？以这种方式提出问题可以使问题易于理解，让每个人都参与对话。但这种缺乏精确性也使得此类问题变得棘手，因为分歧会变成直觉冲突。&lt;/p&gt;&lt;p&gt;今天“一致性守护者”的嘉宾是瓦妮莎·科索伊 (Vanessa Kosoy)，她是一位由机器智能研究所 (MIRI) 和长期未来基金 (LTFF) 支持的独立研究员，致力于构建安全人工智能的数学理论。这种对理解第一原理的关注使她的工作与领先人工智能实验室的“快速行动并打破常规”实验方法形成鲜明对比。在这次采访和其他地方，瓦妮莎捍卫了更加基于理论的方法的价值，并解释了探索机器学习作为基础科学的意义。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt; &lt;strong&gt;Will Petillo：&lt;/strong&gt;您是如何进入人工智能安全领域的？&lt;/p&gt;&lt;p&gt; &lt;strong&gt;Vanessa Kosoy：&lt;/strong&gt;我一直是一个自学者，所以我倾向于自学东西。当我小的时候，我想我会成为一名理论物理学家。我实际上拥有数学学士学位，但在完成学士学位后，我没有进入学术界，而是决定在软件行业从事职业生涯。&lt;/p&gt;&lt;p&gt;我在软件行业有很长的职业生涯，特别是算法工程，主要是计算机视觉，各种角色，算法工程师，团队领导，研发经理。我也有自己的创业公司。大约 10 年前，我是一名顾问。我接触到了人工智能带来的生存风险这一整个话题，并开始思考，嗯，这实际上似乎很重要。所以我开始转向这一点，最初只是我在空闲时间做研究。随后得到了 MIRI 的支持。最近我还得到了长期未来基金的支持，这使我能够全职工作。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;Will Petillo：&lt;/strong&gt;那么这个过程是怎样的呢？你只是以一种自我导向的方式工作，然后你获得了美里和其他来源的支持？这是怎么来的？&lt;/p&gt;&lt;p&gt; &lt;strong&gt;Vanessa Kosoy：&lt;/strong&gt;我开始阅读 MIRI 和 Less Wrong 的人们写的一些东西。我开始研究自己的想法，并在 Less Wrong 上发表文章。之后，我被邀请参加一些研讨会、一些活动，最终 MIRI 说，好吧，看来你在这里做了一些不错的工作，所以也许我们也会付钱给你。我很棒，因为这也使我能够做更多的事情，而花更少的时间做其他事情。&lt;br /&gt;&lt;br /&gt;威尔·佩蒂略&lt;strong&gt;（Will Petillo）：&lt;/strong&gt;这里为观众介绍一下背景知识。 Less Wrong 是一个受欢迎的博客。它最初是关于理性的，但也是关于人工智能相关的事情。 MIRI 是机器智能研究所。我喜欢将他们描述为在它变得很酷之前就致力于协调的人。请告诉我更多关于 MIRI 作为一个机构的信息。&lt;br /&gt;&lt;br /&gt; &lt;strong&gt;Vanessa Kosoy：&lt;/strong&gt; MIRI 或多或少是第一个谈论人工智能存在风险的人。 Eliezer Yudkowsky 在 2000 年开始谈论这个问题，最初 Miri 只是 Yudkowsky，然后多年来他们设法获得一些资金来吸引其他研究人员加入。他们正在思考这个问题：我们如何使人工智能安全，以及我们如何解决这个问题？我们能想出什么样的数学理论来解决这个问题？这甚至早于深度学习革命开始之前，也早于近年来大型语言模型的整体炒作。他们的大部分时间都致力于提出一些基础数学理论，以帮助我们进行人工智能调整。&lt;br /&gt;&lt;br /&gt;最近，他们转向外展并试图影响政策，因为他们相信时间确实很短，不幸的是我们没有时间发展这一理论。&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;威尔·佩蒂略：&lt;/strong&gt;你参与了这一转变吗？&lt;br /&gt;&lt;br /&gt; &lt;strong&gt;Vanessa Kosoy：&lt;/strong&gt;不，我的观点不同。你可以说我比较保守。我认为时间表并不像风险界许多人认为的那么短。我认为，如果通过政策渠道规范人工智能发展并推迟人工智能发展以阻止真正危险的人工智能出现的努力能够成功，那么这只是为我们赢得了时间。那么问题是：为我们争取时间做什么？我认为理论基础绝对仍然是我们应该利用我们所拥有的时间或我们将通过某种政策计划成功购买的时间做的事情中最重要的事情，无论怎样。&lt;br /&gt;&lt;br /&gt;我认为实际上在任何世界中，创建这个基础理论都是关键。这就是我正在做的事情。这绝对是我的个人技能和优势所在，研究数学而不是政策。&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;威尔·佩蒂略：&lt;/strong&gt;你提到了时间表。直觉上，我知道不可能以任何精确度真正预测这些事情，但就你的动机而言，你认为需要解决这些问题的时间表是什么？&lt;br /&gt;&lt;br /&gt; &lt;strong&gt;Vanessa Kosoy：&lt;/strong&gt;有些人认为 AGI 将在 10 年甚至更短的时间内到来。我认为这有点极端。但当事情需要解决的时候，越快越好不是吗？如果我们在五年内找到解决方案，那么我们的处境会比我们在 10 年内找到解决方案更好，这仍然比我们在 20 年内找到解决方案更好，依此类推。&lt;br /&gt;&lt;br /&gt;实际上，我个人的观点是，我们还需要几十年的时间才能真正实现那种带来生存风险的人工智能。所以这给了我们更多的时间，但不是无限的时间。&lt;br /&gt;&lt;br /&gt; &lt;strong&gt;Will Petillo：&lt;/strong&gt;是什么让你觉得这是你想做的事情？&lt;br /&gt;&lt;br /&gt; &lt;strong&gt;Vanessa Kosoy：&lt;/strong&gt;一开始更像是一种好奇心，因为它实际上是从我完全随机发现开始的，你可以说一些关于 AGI 的论文，甚至不是关于人工智能对齐或风险或类似的东西，而只是 Jürgen Schmidhuber 和 Marcus Hutter 的一些论文关于 AGI 的一些想法。我一直是一个数学迷，所以有那些思考 AGI 的数学框架看起来真的很酷。我开始阅读相关内容，最终也发现了Less Wrong，其中也有人在讨论这类事情。&lt;br /&gt;&lt;br /&gt;一方面，我读了越来越多的 Eliezer Yudkowsky 写的关于这个主题的文章，以及 LessWrong 上的人们写的关于这个主题的文章，但我也开始思考数学模型。最终，我突然意识到，当你思考实际的数学时，就完全有道理了，没有任何数学原因可以解释为什么人工智能必须关心人类或关心任何与我们人类关心的事情一致的东西。&lt;br /&gt;&lt;br /&gt;另一方面，它似乎也明显比我们更有能力。我认为直观上很清楚，但对我来说，我喜欢通过数学来理解一切。因此，当我看到你实际上可以将其放入数学模型中时，我真的意识到这是真实的事情，这是我们真正应该关心的事情。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;威尔·佩蒂略：&lt;/strong&gt;没有理由认为人工智能一定是好的，这听起来很像尼克·博斯特罗姆写的正交性论文。智力和事物的美好程度并不一定要同时出现；任何一套价值观都可以与任何水平的智力相匹配。这本质上就是你的见解吗？&lt;br /&gt;&lt;br /&gt; &lt;strong&gt;Vanessa Kosoy：&lt;/strong&gt;是的，这正是术语。事后看来，这似乎是一件显而易见的事情。但对我来说，有必要看到你实际上可以用数学对象来思考它；值可以形式化为效用函数。然后，代理可以被形式化为某种优化器、某种贝叶斯最优策略或该效用函数的任何形式。实际上，你可以在每个术语背后赋予严格的含义，并发现它们实际上都是有意义的——这不仅仅是某种哲学上的挥手把戏。&lt;br /&gt;&lt;br /&gt; &lt;strong&gt;Will Petillo：&lt;/strong&gt;在您看来，导致人工智能对齐问题变得困难的根本原因是什么？&lt;br /&gt;&lt;br /&gt; &lt;strong&gt;Vanessa Kosoy：&lt;/strong&gt;我认为这个问题很难。我认为，首先，困难在于我们的目标非常狭窄，因为人类价值观非常复杂和具体。我们关心很多非常细节的事情：爱、友谊、美（以我们自己的主观理解）、性，所有这些东西都是人类的东西，因为一些复杂的进化事故而存在，就像它发生在某些人身上的方式一样。非常特别的星球。这是特定宇宙历史上非常特殊的时刻。这组价值观只是你可以想象的巨大的可能价值观或思想空间中非常非常狭窄的一部分。&lt;br /&gt;&lt;br /&gt;因此，按照我们的标准，大多数人的思想绝对不会对我们有好处。更糟糕的是，内特·苏亚雷斯（Nate Soares）在他最近的一篇文章中很好地阐述了这种现象，他在其中写道，围绕代理能力存在一个吸引力盆，但对于代理对齐却没有吸引力盆。这意味着，如果您施加足够的优化压力，即使使用强力技术，您最终也将生成功能强大的代理。进化就是一个例子，对吗？进化是一种非常原始的蛮力算法，最终创造了人类大脑，这是一种更加复杂的算法。如果您投入足够的强力优化来寻找在开放世界环境中成功的事物，最终您将遇到智能代理。这甚至是在你在方程中加入递归自我改进之前的，这使得它作为你汇聚的这种吸引力盆地变得更加强大。然而，在与人类价值观保持一致方面，情况并非如此。很有可能的是，我们可以通过盲目或半盲目的试错，早在我们了解足够的知识以实际使这些代理对齐之前就创建出高能力的代理。&lt;br /&gt;&lt;br /&gt; &lt;strong&gt;Will Petillo：&lt;/strong&gt;这听起来像是违背了博斯特罗姆流行的工具融合的另一个理念，即几乎任何足够努力优化的系统都需要诸如生存之类的东西。&lt;br /&gt;&lt;br /&gt; &lt;strong&gt;Vanessa Kosoy：&lt;/strong&gt;有一种工具收敛目标的概念，这是大多数智能代理都会追求的某些目标，因为它们帮助他们实现最终目标，无论他们的最终目标是什么。这些都是诸如生存、获得更多资源、变得更加聪明等等之类的事情。但人类的价值观并非如此。我想，如果我们设法构建一个能够生存并获得大量资源的人工智能，这对人工智能来说是件好事，但它对我们与我们的价值观保持一致没有任何帮助。这是一种非常不同的事情。&lt;br /&gt;&lt;br /&gt; &lt;strong&gt;Will Petillio：&lt;/strong&gt;现代人工智能是根据人类生成的数据进行训练并存在于人类社会中这一事实没有帮助吗？&lt;br /&gt;&lt;br /&gt; &lt;strong&gt;Vanessa Kosoy：&lt;/strong&gt;我认为这有帮助，但也留下了很多问题。一个问题是：好吧，你可以从人类生成的数据中学习，但你如何从中进行概括呢？因为确实不清楚需要什么条件才能获得良好的概括，尤其是当您正在学习的概念非常复杂时。&lt;br /&gt;&lt;br /&gt;您正在学习的概念的复杂性越高，学习它所需的数据点就越多。我们正在利用近年来炒作的所谓大型语言模型来尝试模仿人类。我的意思是，这很好。它可能会以一定的概率带来一些好处——但概率不是很高。但它的问题是，为了使用它，您需要在训练分布之外进行泛化。这里我们实际上需要看看目标是什么。&lt;br /&gt;&lt;br /&gt;问题是，从技术上讲，创造出超级智能的人工智能是可能的，但这将是危险的。为了解决这个问题，仅仅创建某种不危险的人工智能是不够的，因为否则他们可能只是编写一个不做任何事情的算法。这并不危险，任务完成了。我们需要能够创建足够强大的人工智能，作为防御系统来抵御那些潜在危险的人工智能。因此，这些系统必须具有超人的能力，能够构建复杂的世界模型，并在此基础上制定复杂的长期计划。这远远超出了大型语言模型或任何基于人类模仿的训练分布。目前还非常不清楚我们是否真的可以依赖我们必须泛化到训练分布之外的算法，而不会完全失去它们的所有对齐属性。&lt;br /&gt;&lt;br /&gt; &lt;strong&gt;Will Petillo：&lt;/strong&gt;总而言之，法学硕士从根本上来说是模仿性的，这本身似乎并不是特别危险，但它也限制了他们能做的事情。因此，我们不能真的指望开发会就此停止。最终可能会添加像强化学习这样的东西——也许不一定是算法，但可以像围棋中的 Alpha Zero 一样具有创造性，并找到一个以前没有人见过的真正创造性的棋步。因此，我们需要为更强大的事物做好准备，因为它们将是有用的，而导致建立法学硕士的经济学将导致建立更大的事物。这就是你的意思吗？&lt;br /&gt;&lt;br /&gt; &lt;strong&gt;Vanessa Kosoy：&lt;/strong&gt;是的，这听起来很当场。要么是强化学习，要么……好吧，我不想过多地推测让人工智能变得更强大需要什么，因为这不是很好的信息。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;威尔·佩蒂略：&lt;/strong&gt;很公平。继续讨论您实际从事的事情，我看到的与此相关的一个想法是术语“代理基础”。还有“学习理论议程”。那些东西是什么？&lt;br /&gt;&lt;br /&gt; &lt;strong&gt;Vanessa Kosoy：&lt;/strong&gt;智能体基础是一个抽象的概念，它表明我们需要创建一个基础数学理论来解释智能体是什么。从数学角度来说，算法作为代理意味着什么？可以使用哪些类型的代理？他们可以拥有或不具备哪些能力？等等。学习理论议程比这更具体，因为它就像一个试图实现这一目标的非常具体的计划。具体来说，是通过建立在统计和计算学习理论、算法信息论、控制理论等基础上的工具。这是我创建的程序，旨在应对提出这些代理基础的挑战。&lt;br /&gt;&lt;br /&gt; &lt;strong&gt;Will Petillo：&lt;/strong&gt;好的，代理基础就像“思维如何工作？”的问题，其中包含人工智能，而学习理论议程就像“我们如何设计算法，将其推向一个好的方向？”是对的吗？&lt;br /&gt;&lt;br /&gt; &lt;strong&gt;Vanessa Kosoy：&lt;/strong&gt;我不会那样说。我只想说，特工基金会只是试图了解思维如何运作，人们一直在尝试以各种方式做到这一点。 MIRI 历史上有各种证明理论模型试图解决这个问题，然后是加拉布兰特的逻辑归纳法，在这个非常广泛的保护伞下有各种想法，而学习理论议程是一种非常具体的方法。&lt;br /&gt;&lt;br /&gt;正是这种方法以 AIXI 和经典强化学习理论为起点，然后寻找其中缺少的成分，以便拥有代理的基础理论，并开始利用下贝叶斯主义和下贝叶斯物理主义和元认知代理等等。&lt;br /&gt;&lt;br /&gt; &lt;strong&gt;Will Petillo：&lt;/strong&gt;您在这里谈论的代理和思维类型，是否与前沿的大型语言模型相关，或者是否更广泛地涉及人工智能或任何类型的思维实体？&lt;br /&gt;&lt;br /&gt; &lt;strong&gt;Vanessa Kosoy：&lt;/strong&gt;当我说经纪人时，我的意思是非常广泛的。比现有的人工智能甚至只是人工智能要广泛得多。当然包括人类、潜在的外星人或其他什么。因此，对我来说，代理是一个具有特定目标的系统，它正在学习它所嵌入的世界的复杂模型，并使用这些模型来构建长期计划以实现其目标。这就是我所说的“代理”的非正式描述。该程序的整个目标是从这个到一个完全正式的数学定义，并研究这个定义的所有含义。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;Will Petillo：&lt;/strong&gt;因此，即使不超出法学硕士，它的范围甚至比机器学习还要广泛。采取这种做法的原因是什么？鉴于机器学习的主导地位，为什么不关注那些似乎使用最广泛的东西呢？&lt;/p&gt;&lt;p&gt; &lt;strong&gt;Vanessa Kosoy：&lt;/strong&gt;首先，让我们保持术语的顺序。我会区分人工智能、机器学习和深度学习。人工智能是人们自 20 世纪 50 年代以来就开始思考的东西，关于如何构建思维系统，但并没有真正理解它的含义，而只是某种直观的概念，认为存在思考这样的东西，我们应该能够在机器中复制它。&lt;br /&gt;&lt;br /&gt;机器学习是出现的一种更具体的方法……好吧，我不想具体指出什么时候，但可能是在八十年代。机器学习具体是这样的想法：思维的核心要素是学习，学习意味着你正在与一些未知的环境交互，你需要创建这个环境的模型。因此，您需要获取您看到的数据并使用它来创建模型。这类似于科学家如何进行实验、收集数据，然后根据这些数据建立理论。&lt;/p&gt;&lt;p&gt;这个总体想法称为机器学习，或者更准确地说，只是学习。 “机器”部分来自尝试想出在机器内部实际实现这一点的方法。这是一个有很多数学理论的领域。机器学习背后的数学理论就是所谓的统计和计算学习理论，这实际上是学习理论议程的基础。这就是为什么它被称为“学习理论”。&lt;/p&gt;&lt;p&gt;有一种假设认为，这种学习概念抓住了我们所说的思维的大部分重要部分。我认为这一假设得到了最新技术发展的充分支持。这是我完全赞同的，也是我整个研究计划的基础。所以这里并不矛盾，因为学习仍然是一件很普遍的事情。人类也进行学习。外星人也必须学习。&lt;/p&gt;&lt;p&gt;深度学习是一组更具体的算法，用于如何在机器中实际高效地完成学习，这就是 2010 年左右深度学习革命的开始，尽管这些算法在此之前已经以某种形式存在了几十年。但需要一段时间才能获得正确的细节，并拥有合适的硬件来运行它们。深度学习的不幸特征是我们无法从数学上理解它。很多人都在试图理解它，但我们并没有一个很好的理论来解释它为什么有效。这就是为什么它不是我的研究计划的重点，因为我试图提出一些数学理解。我绝对希望人们最终能够解开深度学习如何运作的这种谜团，然后就有可能将其整合到我正在构建的理论中。&lt;br /&gt;&lt;br /&gt;但即使我们有了这个理论，那么以尽可能广泛的普遍性来思考似乎仍然非常重要，因为，首先，我们不知道今天存在的算法将是带来 AGI 的算法。而且因为最广泛的普遍性只是思考问题的正确抽象级别，以了解系统“对齐”意味着什么的概念。这里需要解决一些哲学问题，并且它们特定于一些非常特殊的算法。此外，事实上，我实际上希望这个理论包括人类，因为我可能想用这个理论来形式化价值学习等事物。如何设计一个能够向人类学习价值观的人工智能系统？&lt;br /&gt;&lt;br /&gt; &lt;strong&gt;Will Petillo：&lt;/strong&gt;查看机器学习和深度学习的维基百科级别，或者只是浏览互联网描述，很容易互换使用它们。我想我已经看到了这样的描述，深度学习就是添加多层神经元的想法。因为有多层，所以它很“深”&lt;br /&gt;&lt;br /&gt; &lt;strong&gt;Vanessa Kosoy：&lt;/strong&gt;让我尝试澄清其中的区别。机器学习谈论获取数据并从中构建模型。您正在构建的模型类型可能非常不同。在深度学习之前，我们有支持向量机等算法，多项式回归也是一种非常简单的机器学习类型——将模型拟合到数据。统计中使用的各种方法可以被视为一种机器学习。有一些模型或假设空间，您尝试以最佳方式使用数据来推断正确的假设是什么，或者如果您正在使用贝叶斯方法，则获得假设的一些概率分布。&lt;br /&gt;&lt;br /&gt;但是，不同类型的假设类会在算法的能力方面以及我们所知道的如何学习这些假设类方面导致非常不同的结果。我们知道什么可以用数学方法证明在什么条件下我们可以真正学习它们？比如支持向量机，数学理论基本解决了。有一些建立在其之上的内核方法，并且也有非常扎实的数学理论。深度学习是一种特殊类型的学习算法，它使用人工神经网络架构。&lt;/p&gt;&lt;p&gt;这不仅仅是多层，还有很多细节很重要。例如，激活函数是 ReLU，这一事实对于您在训练中使用哪种正则化方法非常重要。例如，辍学基本上是深度学习革命的开始。如果您正在使用序列，那么我们有变压器，这是一种非常具体的网络架构。因此，多年来人们实际上提出了很多非常具体的细节，主要是通过反复试验的过程，看看什么是有效的。我们没有一个好的理论来解释为什么这些特定的东西运作良好。我们甚至不了解这些东西实际上正在学习的模型空间，因为你可以从理论上证明，如果你采用一个神经网络，然后让它学习另一个神经网络，那么在某些情况下这是不可行的。&lt;br /&gt;&lt;br /&gt;但对于现实世界的问题，神经网络在很多时候都能成功学习。这表面上是因为现实世界具有一些使其可学习的特定属性，或者神经网络正在学习一些特定的潜在假设类，并且它捕获了许多现实世界的现象，但我们甚至没有数学描述这个基本假设类别是什么。我们对一些非常简单的情况有一些结果，比如两层或三层神经网络，或一些其他简化的假设，但我们还没有接近得到完整的答案。&lt;br /&gt;&lt;br /&gt; &lt;strong&gt;Will Petillo：&lt;/strong&gt;深度学习假设了世界的某些情况，就其可以获取的信息而言，它恰好工作得相当好，但目前还不清楚它的假设是什么。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;凡妮莎·科索伊：&lt;/strong&gt;是的，完全正确。所以我们有不同的不可行定理，它说对于任意数据，即使该数据是完全可实现的，即使该数据使得神经网络可以完美地表达一个完全正确的模型，该问题也是不可行的。一般来说，梯度下降不会收敛到正确的模型，而且其他算法也不会收敛，因为问题很棘手。世界具有一些属性，并且由于深度学习在如此多种不同的情况下取得了成功，所以感觉这些属性应该有一些简单的数学描述。&lt;br /&gt;&lt;br /&gt;它不像某些特定于文本、音频或图像的属性。这些属性非常通用，适用于各种不同的模式和问题。这些是我可以推测的属性，例如，与组合性有关，现实世界通常如何被很好地描述为由部分组成，以及事物如何根据不同的空间尺度或不同的时间尺度进行解耦。动态正在发生。但我们没有真正解释它的理论。&lt;br /&gt;&lt;br /&gt; &lt;strong&gt;Will Petillo：&lt;/strong&gt;您提到 ReLU 是有效的例子之一。据我了解，ReLU 基本上就像获取输出，以一种可以表示为一侧平坦且对角线超过零的图形的方式对其进行更改。而以前，模型通常使用 Sigmoid 作为激活函数，它更像是一条平滑的曲线，可以防止数字变得太大。出于某种原因，ReLU 效果更好。我从你的解释中得到的感觉是，这种变化会影响神经网络能够以更符合现实的方向理解什么样的事物。但所有这些变化都是以“把东西扔到墙上，看看什么粘住”的方式开发的，只是简单地测量结果，而没有真正理解&lt;i&gt;为什么&lt;/i&gt;ReLU 比 sigmoid 更好。&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;凡妮莎·科索伊：&lt;/strong&gt;或多或少是对的。当我们说神经网络可以“理解”什么时，我们必须小心我们的意思。这是一个非常复杂的概念，因为它不仅仅是神经网络可以用一组权重来表达的内容，而是神经网络实际上可以通过梯度下降过程学习的内容。它不仅与神经网络可以描述的函数空间有关，而且与我们查看特定数据集时在这个权重空间中创建的整个损失景观有关。&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;威尔·佩蒂略：&lt;/strong&gt;当你描述梯度下降和损失景观时，我经常听到的比喻是一个从山上滚下来的球——有一个恒定的重力，你希望球下降到海平面。但通常情况下它不会，因为它发现了一些局部最小值，比如一个洞或其他东西，它可以移动的任何方向都是向上的，所以它不会再滚动。所以你必须塑造景观，使球始终到达海平面。&lt;br /&gt;&lt;br /&gt; &lt;strong&gt;Vanessa Kosoy：&lt;/strong&gt;是的，这是一个很好的解释。梯度下降是我们有很好的数学理论来解释它如何收敛到凸函数的全局最小值，但神经网络的损失是非凸的......但它仍然恰好有效。人们已经对其工作原理有了一些了解，但我们仍然没有完整的答案。&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;威尔·佩蒂略：&lt;/strong&gt;好吧，如果地貌确实崎岖不平，那么你就不会期望球到达海平面，因此它无论如何都会到达海平面这一事实需要一个我们实际上没有的解释。我可以看到这种框架如何引发了许多有关不可预测性的问题。&lt;br /&gt;&lt;br /&gt;接下来，您曾提到过 AIXI。那是什么？&lt;br /&gt;&lt;br /&gt; &lt;strong&gt;Vanessa Kosoy：&lt;/strong&gt; AIXI 是 Marcus Hutter 的想法，它应该是完美代理的数学模型。它的工作原理是：有一个先验，即所罗门诺夫先验。对于那些不知道这是什么的人来说，这基本上是一种用数学形式化奥卡姆剃刀概念的方法。奥卡姆剃刀的思想是，简单的假设应该被认为比更复杂的假设更有可能是先验的。这确实是所有理性推理的基础。哈特采用了所罗门诺夫先验，这是一种非常聪明的方法，可以在数学上形式化奥卡姆剃刀的概念，并且说，好吧，让我们考虑一个生活在所罗门诺夫先验的宇宙样本中的智能体。这个代理有一些特定的奖励函数，它正在最大化。我们假设它只是以贝叶斯最优方式运行。因此，它只是遵循策略，使其根据先验最大化其预期效用。我们称之为“艾希”。这是一个非常酷的想法......只是它有很多问题，首先是它无法计算的“小”问题。即使在理论上，也没有一种算法可以实现它。&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;威尔·佩蒂略：&lt;/strong&gt;我想我曾经听过这样的解释：将整个宇宙想象成一堆比特——1和0。一开始，所有这些都可以是一或零，然后你得到一点数据，现在你已经锁定了其中一些数字，并将所有可能的空间减少了一半。当你不断学习时，你会变得越来越确定。&lt;br /&gt;&lt;br /&gt; &lt;strong&gt;Vanessa Kosoy：&lt;/strong&gt;实际上比这更微妙一些。拥有很多可能性这一事实并不意味着它是不可计算的。也许确切的事情是无法计算的，但你仍然可以想象有一些聪明的算法可以近似这个贝叶斯推理过程。例如，如果你看看经典的强化学习理论，就会发现有一些算法可以在具有 n 个状态的任意马尔可夫决策过程中进行学习。在具有 n 个状态的马尔可夫决策过程中，仍然存在指数级大的可能方式空间，并且我们仍然拥有实际上有效的算法，可以通过利用问题的某些属性，从指数级大的事物中收敛到正确的事物。&lt;br /&gt;&lt;br /&gt; AXI 的问题是，它的先验是这样的，即使先验中的单个假设在计算上也已经是任意昂贵的，因为在它的先验中它考虑了每个可能的程序，所以你可以在通用图灵机上编写的每个可能的程序都是一个可能的假设了解世界如何运作。其中一些程序的计算成本极其昂贵。其中一些程序甚至不会停止，它们只是进入无限循环。你甚至不知道是哪一个，因为这就是停机问题，对吗？这就是为什么 AIXI 不适合做可计算的事情，更不用说计算上易于处理的事情了。&lt;br /&gt;&lt;br /&gt; &lt;strong&gt;Will Petillo：&lt;/strong&gt;抛开不可计算性这个小问题不谈，“完美算法”……这是什么意思？如果 AIXI 是通过某种方式神奇地计算出来的，它会安全吗？&lt;/p&gt;&lt;p&gt; &lt;strong&gt;Vanessa Kosoy：&lt;/strong&gt;不，这并不能保证安全。从您可以想象的是最强大的算法上，它是“完美的”。同样，根据一些假设。我的意思是，还有其他问题，例如它假设外界比代理人本身更简单。这有多个问题，但是如果您可以将所有这些问题放在一边，那么您可以说这是最好的代理商。从这个意义上讲，这是完美的。非常非常非常安全。为了使其安全，我们需要以某种方式将正确的实用程序函数插入其中。这仍然是一个非常不平凡的问题。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;Petillo：假设&lt;/strong&gt;您想找到可计算的内容，您会寻找哪种算法？&lt;/p&gt;&lt;p&gt; &lt;strong&gt;Vanessa Kosoy：&lt;/strong&gt;可计算性只是问题之一。我想像一下，我将有一些我称之为节俭的普遍先验，这在某种程度上是我们可以在数学上定义的，这将同时富裕，可以捕获各种各样的现象。另一方面，我们将拥有一些巧妙的算法，实际上可以在此事先使用该假设的某些组成性属性或其他类似的内容中进行有效学习。&lt;/p&gt;&lt;p&gt;但是，即使知道这一点，您也需要处理许多其他概念问题。就像我所说的特权问题一样，Occam的剃须刀和AXI的形式化使观察者特权，您需要了解如何处理。而且存在一个问题的问题，您实际上不能有一个假设，这可以使您对宇宙进行精确的描述，但只有某种近似或部分描述，并且您需要了解如何处理。然后还有一个事实，您希望实用程序功能不仅是观察值的函数，而且是您无法直接观察的一些参数。您还希望能够证明此算法的一些常见保证。要知道该算法实际上需要知道多少数据来学习特定事实并具有良好的理论。研究Aixi类似模型时，会出现各种不同的问题。&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;佩蒂洛（Petillo）：&lt;/strong&gt;研究aixi像模型一样，这就是您正在努力的工作吗？&lt;/p&gt;&lt;p&gt;&lt;strong&gt;凡妮莎·科索（Vanessa Kosoy）：&lt;/strong&gt;是的，如果您想将其放入一句话，我可以。&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;佩蒂洛（Petillo）：&lt;/strong&gt;您对解决问题感兴趣的一些有趣的问题是什么？我已经看到Newcomb的问题在四处流动，并且与此相邻。&lt;br /&gt;&lt;br /&gt; &lt;strong&gt;Vanessa Kosoy：&lt;/strong&gt; Newcomb的问题是Eliezer Yudkowsky写的很多东西，这是对经典理性叙述非常困惑的一个例子。您有两个盒子需要选择。一个盒子有一千美元。另一箱没有什么或一百万美元。您可以选择第一个盒子，也可以拿走两个盒子中的钱。通常，拿这两个盒子里的钱总是比只拿一个盒子要优越。&lt;br /&gt;&lt;br /&gt;除了在这个位置实验中，还有一些称为欧米茄的实体可以预测您的工作也是盒子。因此，只有当您是那种可以预见的代理商（对于欧米茄）只拿一个盒子的代理商时，只有在这种情况下，您才会以$ 1,000,000的价格从这个房间里拿出来。而在另一种情况下，您只有$ 1,000。因此，可以说，最好拿一个盒子而不是两个盒子，而不是许多古典理性的说法。这是一个有趣的思想实验的一个例子。&lt;/p&gt;&lt;p&gt;对我来说，这种思想体验是一个非真实性问题的特殊情况，您需要处理如此复杂的环境，以至于无法完整地描述环境的完整描述，您可以真正模拟这一环境。因为在此示例中，环境包含该代理Omega，它可以模拟您，这意味着您无法模拟它，因为否则它将创建这种圆形悖论。实际上，我还表明，我称之为非可话性的理论（我称之为红外线主义）实际上导致了这种类似Newcomb的问题情景的最佳行为。&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;威尔·佩蒂洛（Will Petillo）：&lt;/strong&gt;研究类似Newcomb的问题的原因不是因为我们期望在某个时候面对欧米茄为我们提供盒子，而是因为这只是考虑如何在您不知道时如何处理事情的一种说明性的方式这是怎么回事。而且因为很容易说：“是的，我只会拿一个盒子，因为我会得到更多的方式”然后，有一些有趣的见解可能会从中引起。您是否通过探索这类事物发现了任何发现？&lt;/p&gt;&lt;p&gt;&lt;strong&gt;凡妮莎·科索（Vanessa Kosoy）：&lt;/strong&gt;我想说的是，基础山脉主义本身是一个有趣的发现，一些对代理商的理论上说明可以推理复杂的世界，这些世界太复杂了。现在，我通过阐明了不实现性能的问题来描述动机，但是我实际上想到的是通过思考所谓的逻辑不确定性来描述这一动机。人们之所以开始思考的原因是由于所谓的无更新决策理论，该理论来自考虑新Comb Time悖论。因此，尽管事实上，您可以通过一些更一般的抽象思维来激励它，但这一切都来自这种推理线。&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;佩蒂洛（Petillo）：&lt;/strong&gt;这些决策理论类型问题与使AI更安全之间的联系是什么？&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;凡妮莎·科索（Vanessa Kosoy）：&lt;/strong&gt;这个想法是在创建一种一般的代理人数学理论。它将帮助我们使人工智能更安全的方式，有几个原因，最明显的是，在拥有这一理论时，我们希望能够提出严格的模型，以使系统成为一个对齐代理的含义。有了这个严格的定义，我们将能够提出一些算法，我们可以证明这些算法实际上是安全的代理。或者至少我们可能会有一些猜想说，这种给定的这种猜想的模型，我们认为这些算法是安全的代理。就像在密码学中一样，您有一些猜想，这些猜想具有非常强烈的证据支持。&lt;/p&gt;&lt;p&gt;我们至少可以有一些半正式论点，因为现在人们在辩论特定设计是否安全时，这一切都归结为那些挥舞着哲学论证的那些没有任何坚实的基础。而这为我们提供了更精确，更清晰的思考这些问题的工具。从假设的角度来看，它也为我们提供了更多的力量来利用实证研究，因为也许我们将能够接受我们拥有的实证研究，将其插入数学理论，并获得一些关于我们期望这些结果实际上将这些结果实际推出到各种制度的答案我们还没有这样做。&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;佩蒂洛（Petillo）&lt;/strong&gt;是否会最终在评估大语模型或深度学习系统之类的事物时可以使用这种研究线，以便能够更确定地说他们安全或不安全的程度？&lt;br /&gt;&lt;br /&gt; &lt;strong&gt;Vanessa Kosoy：&lt;/strong&gt;我认为有多种影响途径。因此，有一条影响力的途径，我们最终将提出深入学习的理论。或者，如果不是一个充分证实的理论，那么至少有一些关于深度学习如何与我正在建立的代理理论相互作用的强有力的猜想。然后，我们可以使用这种综合理论来证明事物，或者至少对深度学习内置系统的属性有强有力的论点。&lt;/p&gt;&lt;p&gt;在我们利用该理论提出全新类型的算法的AI算法的情况下，可能会有不同的途径，这不是深度学习，而是我们对此有良好的理论理解。&lt;/p&gt;&lt;p&gt;我们也没有一个很好的理论，但我们至少可以通过类比来理论，这与我们具有数学理论的某些算法相似。例如，深度Q学习类似于简单的Q学习，我们对此具有数学理论。因此，我们可以想象一个世界，在这个世界中，我们有某种理想主义的玩具模型算法，我们有一些严格的论据为什么它们会保持一致，然后我们有更多的启发式算法，我们无法直接证明这一点，但可以说是类似的东西到那些玩具模型。&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;威尔·佩蒂洛（Will Petillo）：&lt;/strong&gt;所以我听到了三条影响的途径。一个人可能正在构建一种不同形式的AI，该AI可以从头开始验证，并且与基于深度学习的AI相同的事情，但更严格。第二个是评估或至少更好地理解，深度学习或任何艺术状态。然后在两者之间的三分之一是具有更简单的AI形式，该形式类似于最先进的事物，因此您可以使用前者来理解后者。&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;凡妮莎·科索（Vanessa Kosoy）：&lt;/strong&gt;是的，这听起来很正确。&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;威尔·佩蒂洛（Will Petillo）：&lt;/strong&gt;我想将重点放在使用基础研究上，以理解深度学习，以这种基于理论的方法。引起完全相反的对位，人们可能会争辩：不，您应该只看所使用的内容并收集有关它的数据，然后通过在数据中找到模式来构建理论。当证明这些理论是错误的（由于更多数据的结果）时，然后更新您的理论。为什么要事先在理论上工作？&lt;/p&gt;&lt;p&gt;&lt;strong&gt;凡妮莎·科索（Vanessa Kosoy）：&lt;/strong&gt;最大的原因是，如果没有基本理论，您就无法从经验研究中可靠地推断出来。因为您可能会采取一些测量并找到一些趋势……但是后来您在趋势中看不到的相位过渡，但是发生了哪些情况，而行为会变为完全不同的政权。而且，由于您没有理论上的解释，因此您不会注意到，或者人们只会改用完全不同的算法，这些算法的行为完全不同。&lt;/p&gt;&lt;p&gt;您可能拥有现有AIS的经验模型，但是这些经验模型非常近视。他们一直在寻找前方的一步。然后，您看不到前面三个步骤的悬崖。在发生的新事物上更新那些理论上的经验模型 - 可能还不够快。最终，您从悬崖上掉下来，然后为时已晚，说：“哦，实际上，那条趋势线是错误的！”&lt;/p&gt;&lt;p&gt;幸运的是，即使没有经验数据，我们也在一个领域，即使没有经验数据，我们也可以进行研究。当然，我们应该使用我们拥有的经验数据，但是我们并没有在经验数据上瓶颈，因为我们正在研究的是算法，而算法是数学对象，因此可以从数学上研究它们。这与研究某种物理现象大不相同，如果您没有数据，那么就无法生成数据。在这里，这确实应该归结为数学。更确切地说，它应该归结为数学以及我们在数学理论中要假设的现实现象的任何特性。&lt;br /&gt;&lt;br /&gt;是的，这是我们需要经验输入的东西。但另一方面，我们已经对物理学有很好的了解。因此，鉴于我们拥有的物理学和其他科学领域的知识，即使我们根本没有经验数据，我们也有足够的信息纯粹通过数学查询来回答所有问题是非常合理的。这并不是说我们不应该使用经验数据来增强这项研究，但我们不仅限于这一研究。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;威尔·佩蒂洛（Will Petillo）：&lt;/strong&gt;因此，这不是理论与实验之间的选择，我们应该同时使用两者。您专注于理论方面，可以说，这是没有足够的工作，因为理论是瓶颈所在，而不是获取更多数据。&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;凡妮莎·科索（Vanessa Kosoy）：&lt;/strong&gt;是的，我认为我们绝对应该这两件事。理想情况下，需要有协同作用，实验为理论家解释和理论激发实验而产生新的现象。理论家应该告诉实验者哪些问题以及最有趣的实验，我们应该具有这种协同作用。但是我认为，在当前的景观中 - 特定的AI对齐方式 - 该理论方面目前被抛在后面。这就是我认为我们应该付出边际努力的地方。&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;佩蒂洛：&lt;/strong&gt;您看到现在存在这种协同作用吗？例如，Openai是向Miri寻求有关他们的实验的反馈，还是有任何联系，或者人们只是彼此孤单？&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;凡妮莎·科索（Vanessa Kosoy）：&lt;/strong&gt;我认为现在几乎都不存在。好的，不，公平地说，它存在于某些领域，而在其他领域则更少。例如，有人从事奇异学习理论。我认为它们与实验工作更加接触，这很好。 Miri所做的那种研究是，我正在做的研究与实验工作相关得多。我有一些计划，可以在我的长期计划的一部分中与我在这些问题上进行紧密循环的实验小组，但我仍然没有做到这一点。&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;佩蒂洛（Petillo）&lt;/strong&gt;会不会改变任何人的想法，或者制定政治和商业议程，您想看到什么碰巧拥有更多的界面？&lt;br /&gt;&lt;br /&gt; &lt;strong&gt;Vanessa Kosoy：&lt;/strong&gt;首先，我们只需要更多的理论家。要拥有一个接口，我们需要一些与之接触的东西，因此我们只需要更多的理论家。我认为这是实际上的瓶颈现在。一旦理论上的进展足够节奏，就会有很多问题。我的意思是，我已经有一些问题要看实验了，但是这件事越多，我们将越多的问题。我认为现在的主要瓶颈是让更多的人从事这一理论。&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;佩蒂洛（Petillo）：&lt;/strong&gt;如果还有更多理论起作用，从外部角度来看会改变什么？我想一个怀疑论者可能会争论：“ Openai和其他这些公司都以一种实验性的方式做出了这些非常出色的突破，而且效果很好！如果它不会破产，请不要解决！”在您看来，什么是破碎的？&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;凡妮莎·科索（Vanessa Kosoy）：&lt;/strong&gt;我认为当前的道路正在导致我们造成灾难。我认为，像OpenAI和其他领先实验室这样的公司对解决问题的能力非常过分自信。我认为他们没有为问题的艰难部分提出任何令人信服的解决方案，而且由于缺乏理论理解，他们甚至没有工具来执行此操作。我们甚至没有足够精确的模型，无法提供我们真正有信心的解决方案。我们需要非常准确地说明我们的解决方案是好的。而且我们甚至没有达到这种精度的工具。&lt;br /&gt;&lt;br /&gt;公司正在做的事情基本上只是在反复试验中开发事物。如果我们看到任何问题，那么我们只会调整问题，直到问题消失。那是一种创可贴方法，也就是说，它可以正常工作，直到它不起作用为止。它在表面上解决了问题，但最终将出现问题不会及时抓住，结果将是灾难性的，或者问题会及时抓住，但是没有人会知道该怎么办为了修复它。最终有人会做灾难性的事情。&lt;br /&gt;&lt;br /&gt;唯一使我比美里其他人不那么悲观的事情是，我认为我们还有更多的时间。我认为它们与AGI不太接近，我认为在这段时间里很多事情都会改变。再次是，并不是说他们会改变 - 我们可能会一直燃烧，但仍然遇到灾难。&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;佩蒂洛（Petillo）：&lt;/strong&gt;什么是只有表面解决方案的现有问题的例子？&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;凡妮莎·科索（Vanessa Kosoy）：&lt;/strong&gt;我的意思是，我们关心的真正问题不是现有的问题，对吗？我们关注的主要内容是，未来的AI系统（将比现有的AI系统强大得多）将使人类的灭绝或灾难在类似的水平上灭绝。&lt;br /&gt;&lt;br /&gt;这不是现有的问题，原因是我们今天拥有的AI系统无法学习一个如此复杂的世界模型，以至于它使您能够执行这些类型的动作。但是即使是现在，这些公司即使在大型语言模型中发生的所有事情，例如臭名昭​​著的越狱，他们试图以各种方式使其富有行为。例如，不告诉用户进攻性，危险的信息，用户很容易找到越狱来解决这个问题，或者只是说出错误的答案。&lt;br /&gt;&lt;br /&gt;但是，对我来说，这不是真正的问题，这只是一个类比。我的意思是，他们现在在这些非常简单，更容易的问题上挣扎着，这并不是说他们不会解决它们。反复试验最终将使您到达那里。反复试验的原因不是存在生存风险的解决方案，是因为一旦每个人都死了，审判就结束了。不再有审判。因此，我们现在遇到的问题，他们仍然在与他们斗争，因为它们没有解决方案的原理工具，但是最终他们会试用自己的方式，并以某种方式修补它们，或者至少可以很好地解决它们足以使它变得经济。但是，一旦到达失败是全球灾难的地步，反复试验就不再是解决问题的可接受方法。&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;威尔·佩蒂洛（Will Petillo）：&lt;/strong&gt;显然，我们看不到大量世界的测试数据结束。但是我会想到会有一些较小的前体问题，但暗示即将发生的事情。您是否看到幻觉或无法控制AI所说的那种前体的挑战？还是它们完全无关，而只是没有任何前体问题？&lt;br /&gt;&lt;br /&gt; &lt;strong&gt;Vanessa Kosoy：&lt;/strong&gt;这是一个棘手的问题，因为现有的AI系统仍缺少非常重要的位来产生生存风险。我们可以指出一个示例，这些示例有点像替代性，有很多著名的例子：一些程序通过永远暂停游戏来“赢得”俄罗斯方块，或者通过在无限的圈子里赛车来赢得一些赛车比赛，各种怪异的意外行为，因为算法最大化的指标实际上并不是用户预期的。您可以称这些前身为其，但是我觉得这并不是完全捕获问题的大小，因为这些仍然是玩具设置。没有开放的系统在开放的物理世界中起作用。他们试图解决的目标比人类价值观要简单得多。没有真正有真正复杂的道德考虑的操作领域。&lt;br /&gt;&lt;br /&gt;也许大型语言模型开始接近这一点，因为它们进入域，至少在道德上有些问题，而不是完全琐碎的问题。另一方面，大型语言模型并不是真正的超人行为。好吧，与人类相比，它们具有很大的知识，但在其他意义上却不是超人。所以很难。有些东西有点类似，但不是很相似。&lt;br /&gt;&lt;br /&gt;但是话又说回来，我们关心这种风险的动机并不是来自查看LLM。 Eliezer Yudkowsky在深度学习根本就开始谈论这些事情。那不是动机的来源。&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;威尔·佩蒂洛（Will Petillo）：&lt;/strong&gt;我想我问的原因是，在这些东西被辩论和两极分化的地方，一个常见的反对意见是：“背后没有证据！这只是讲故事！”&lt;i&gt;是否&lt;/i&gt;有危险的证据，还是仅仅来自看数学？&lt;br /&gt;&lt;br /&gt; &lt;strong&gt;Vanessa Kosoy：&lt;/strong&gt;问题是，您称之为证据？这是一个非常复杂的问题。明显证据的事情将是：AI完全失控，开箱即用，将计算机入侵，将自己复制到其他计算机，完全操纵人类操作员等。但是，这种事情是一种金丝雀，您只期望看到它已经非常非常非常接近已经太晚了。不可能说我们只依靠这种证据来解决辩论。&lt;/p&gt;&lt;p&gt;对于其他类型的证据，有人说进化是一种证据，是一种证据，机器学习算法如何产生与原始算法完全不结盟的东西。其他人向您展示增强学习算法，而不是设计师的意图。但是对于这样的每个论点，您都可以有一个反论点，说：“是的，但是这个示例并不相似。我们不能真正从那里投射到存在风险，因为有一些脱节。”&lt;br /&gt;&lt;br /&gt;是的，总会有一些脱节，因为直到您在现实世界中拥有AI非常接近存在的风险之前，您将没有任何与出现存在风险的东西相似的东西。因此，我们别无选择，只能通过第一原则或数学或进行一些更复杂的多维分析来推理。我们别无选择。宇宙并不归功于我们以一种非常简单，经验的方式来测试这些问题是否是真实的。我希望的一件事是，该理论将为AI危险带来更强有力的论点，或者理论会告诉我们不，一切都很好，我们都可以放松。缺乏理论是我们没有一个方向或另一个方向的万无一失，完全扎实的论点的部分原因。&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;威尔·佩蒂洛（Will Petillo）：&lt;/strong&gt;寻找证据的挑战是，您可以指出的任何东西现在都可以通过多种方式来解释。具有扎实的理论将使一种解释对另一种解释有所帮助。&lt;br /&gt;&lt;br /&gt; &lt;strong&gt;Vanessa Kosoy：&lt;/strong&gt;是的，绝对。如果您的理论说特定类型的错误总体化在大多数可能的机器学习系统中是普遍的，并且我们还看到这种类型的错误总体化发生在真实的机器学习系统中，那么很难将其拒绝并说：”哦，是的，在这里我们有一个问题，但是我们会做到这一点，这将很容易解决。”&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;威尔·佩蒂洛（Will Petillo）：&lt;/strong&gt;还有一件事仍然让我讨厌现在尚未提供证据的问题。我的思想立即进行的类比是气候变化。您可以说：“哦，世界上大片不居住的想法只是这个精心设计的故事，因为所有从未发生过的事情！”但是，您可以查看已经存在的一堆事情：小规模灾难，二氧化碳与温度的图表等，依此类推，指向那些，并说：有很多证据表明它会！”是什么使AI与众不同？&lt;br /&gt;&lt;br /&gt; &lt;strong&gt;Vanessa Kosoy：&lt;/strong&gt;我认为气候变化是一个很好的类比。最大的区别在于，在气候变化中，我们有一个非常好的理论。像气候变化一样，我们有物理学，对吗？我们有行星科学，这是一个非常非常坚实的基础。我们有计算机模拟。它仍然不是微不足道的，有一些混乱的现象很难模拟或预测，因此并非一切都是完全微不足道的，但是我们仍然有一些非常非常有力的理论基础来理解这些事物的工作原理以及什么是机制。这个理论告诉我们，围绕我们将使用这种二氧化碳等等的温暖程度仍然存在很大的不确定性间隔，但是那里我们仍然有一个相当坚实的预测。&lt;br /&gt;&lt;br /&gt;而使用AI，我们没有这个。相似的情况，如果您想想象气候变化，AI风格，那么这将是没有理论，可以解释为什么二氧化碳会导致变暖。温度与二氧化碳之间具有一些经验相关性，然后人们可以争论无限。相关性不是因果关系，也许变暖是由于完全不同的东西引起的，也许如果我们做一些无关的事情，它将停止变暖，这实际上并非如此。我们会在黑暗中。有了AI，我们目前处于黑暗状态。&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;佩蒂洛（Petillo）：&lt;/strong&gt;您在Miri的工作目前正在发生什么？&lt;/p&gt;&lt;p&gt;&lt;strong&gt;凡妮莎·科索（Vanessa Kosoy）：&lt;/strong&gt;目前我正在寻找多个问题。希望我很快就会发表有关不精确的线性土匪的论文，这与我之前提到的非属贝尼斯主义有关，这是一种关于复杂世界的理论的理论。这是在某种非常简单的特殊情况下分析这一理论的，在这些情况下，我成功地获得了一些精确的界限，以了解算法需要多少数据来学习特定的知识。之后，我开始研究增强学习中学习状态表征的理论，这是该理论中缺少的另一个大作品，这是关于您的算法应该如何学习世界的哪些特征实际上对于集中精力很重要在。&lt;br /&gt;&lt;br /&gt;同时，我有一个合作者Gergely Szucs，他正在努力使用我的Infra贝叶斯物理学理论来创建对量子力学的新解释。他在那里有一些非常有趣的结果。这是一种测试案例，演示了这种思考代理的框架如何使您能够解决各种哲学上的混乱。在这种情况下，这与量子力学的解释有关。 Scott Garrabrant有一个项目，讲述了一种新型的不精确概率，这是某种具有一些具有良好构图属性的信念的新方法。卡内基·梅隆（Carnegie Mellon）和艾布拉姆·德姆斯基（Abram Demski）的卡斯帕·奥斯特海尔德（Kaspar Osterheld）最近发表了一篇论文，介绍了一些新型的算法保证，这些保证是根据与预测市场相似的东西做出决定的。是的，正在发生很多有趣的事情。&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;佩蒂洛（Petillo）会不会：&lt;/strong&gt;我没有提出任何其他问题，这对看到这一点的人有助于了解您在这里的意思吗？&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;凡妮莎·科索（Vanessa Kosoy）：&lt;/strong&gt;这不是一个问题，但是我对如何真正解决对齐方式，如何实际设计一个对齐代理，我也有更具体的方法，我称之为物理主义的超级构想。这是关于价值学习主题的一种变体，但它源自Infra贝叶斯物理主义的框架，该框架来自学习理论议程以及算法信息理论中的某些想法，以提出一种半正式的方法以强大的方式学习人类价值观的人工智能。&lt;br /&gt;&lt;br /&gt;它处理了其他价值学习方法的许多问题，例如：您如何确定代理商的边界在哪里？什么是人？您如何在太空中找到这个人？您如何考虑不仅是行为的事物，还考虑人类的内部思维过程在推断人的价值观时？您如何防止诸如AI之类的不良激励措施以某种方式改变或操纵人类以改变其价值观？您如何避免内部对齐问题？它回答了其他方法的一系列问题。&lt;br /&gt;&lt;br /&gt; &lt;strong&gt;Petillo：&lt;/strong&gt;这听起来让人联想到逆增强学习吗？&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;凡妮莎·科索（Vanessa Kosoy）：&lt;/strong&gt;逆增强学习是我们应该研究人类行为，推断那些人类试图做的事情的想法，然后我们可以做这件事。 “我们”作为AI。因此，我实际上有演示文稿，其中我将身体超构想解释为对类固醇的逆增强学习。它采用了这个基本想法，但以解决更简单的方法所遇到的许多深层问题的方式实施它。简单化方法的一个问题是，如果对环境的完美知识，它们是遵循完美政策的完美推动者，这是非常不现实的。&lt;br /&gt;&lt;br /&gt;取而代之的是，我将人类模仿为学习代理。他们在继续学习的过程中学习东西。而且他们甚至可能是不完美的。另一件事是边界问题。什么是人类？您在哪里围绕着人的界限？是否只有人类使用的某些特定输入和输出，您认为通过此港口的一切都是人类？但是，您如何处理该港口的内容与人类实际打算做的事情之间的各种差异，或者像AI劫持此渠道这样的各种可能性呢？&lt;br /&gt;&lt;br /&gt;在我的方法中，人类正式化的方式是人类是宇宙正在运行的特定计算。这是我实际上可以使用贝叶斯物理主义正式化的东西。它具有特定的属性，使其成为代理，因此代理检测到宇宙正在运行的哪些计算，其中其中哪些计算是代理，在这些代理中，它通过研究因果关系来选择哪个代理是其用户，并且它可以将其带到代理的边界上。首先是因为我们正在谈论这个人正在运行的计算，这是人类的推理，被视为计算。我们还将自动将内部视为内部思维过程，而不仅仅是表达为外部行为的事物。因此，我们可能在那里有更多信息。&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;佩蒂洛（Petillo）：&lt;/strong&gt;某人参与其中的最佳方法是什么？他们想事先学习什么？&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;凡妮莎·科索（Vanessa Kosoy）：&lt;/strong&gt;他们可以立即开始做的一件事是阅读人们在代理基金会和学习理论议程中所做的事情。我最近有这篇文章， &lt;a href="https://www.lesswrong.com/posts/ZwshvqiqCvXPsZEct/the-learning-theoretic-agenda-status-2023"&gt;&lt;u&gt;学习理论议程：2023年的状态&lt;/u&gt;&lt;/a&gt;，总结了很多事情。我还有一个&lt;a href="https://www.alignmentforum.org/posts/fsGEyCYhqs7AWwdCe/learning-theoretic-agenda-reading-list"&gt;&lt;u&gt;阅读列表帖子&lt;/u&gt;&lt;/a&gt;，建议您为想要进入该领域的人提供一些背景阅读。就职业步骤而言，更具体的是，申请已经为时已晚，但是我在&lt;a href="https://www.matsprogram.org/"&gt;&lt;u&gt;MATS&lt;/u&gt;&lt;/a&gt;中运行了一条曲目&lt;i&gt;，&lt;/i&gt;这是一个培训计划，适用于想要进入AI安全的研究人员。我有一条专注于学习理论议程的曲目。希望明年将有另一条这样的曲目。我也有一个实习计划的幻想，这将使人们前往以色列与我合作。目前，由于战争，这件事已被推迟，但希望最终会安定下来，我会恢复这个项目。这些目前是参与的主要方法。&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;威尔·佩蒂洛（Will Petillo）：&lt;/strong&gt;谢谢您的描述。 I wish you the best in developing this theory and gaining more interest so that mismatch between evidence and theory starts to get corrected and the researchers know what they&amp;#39;re doing rather than stumbling in the dark!&lt;br /&gt;&lt;br /&gt; &lt;strong&gt;Vanessa Kosoy:&lt;/strong&gt; Thank you for having me.&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/QPxrH5ex6MpYoLwer/interview-with-vanessa-kosoy-on-the-value-of-theoretical#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Mon, 04 Dec 2023 22:58:42 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/QPxrH5ex6MpYoLwer/interview-with-vanessa-kosoy-on-the-value-of-theoretical</guid></item><item><title>FAR AI 2023 年比对研究更新</title><link>https://www.lesswrong.com/posts/PQgEdo3xsFFAxXNqE/2023-alignment-research-updates-from-far-ai-2</link><description>发布于 2023 年 12 月 4 日晚上 10:32（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt; &lt;i&gt;TL;DR：FAR AI 的鲁棒性科学议程发现了超人类围棋系统的漏洞；我们的价值调整研究开发了样本效率更高的价值学习算法；我们的模型评估方向开发了多种新的黑盒和白盒评估方法。&lt;/i&gt;&lt;/p&gt;&lt;p&gt; &lt;a href="https://far.ai/"&gt;FAR AI&lt;/a&gt;是一家非营利性人工智能安全研究机构，致力于孵化多样化的研究议程。自一年多前成立以来，我们一直在快速发展，并且很高兴与大家分享我们研究项目的一些亮点。我们还忙于举办现场建设活动并建立联合办公空间 - 请参阅我们的&lt;a href="https://far.ai/post/2023-12-far-overview/"&gt;概述文章&lt;/a&gt;，了解有关我们非研究活动的更多信息。&lt;/p&gt;&lt;h3&gt;我们的任务&lt;/h3&gt;&lt;p&gt;我们需要能够为先进人工智能系统的安全性提供明显保证的安全技术。不幸的是，当前部署的对齐方法（例如&lt;a href="https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback"&gt;人类反馈强化学习（RLHF））&lt;/a&gt;未达到此标准。存在可以提供更强安全保障的提案，但仍处于制定的早期阶段。&lt;/p&gt;&lt;p&gt;我们的使命是孵化和加速这些早期方法，以便对它们进行实证测试和部署。我们关注的研究议程太大，无法由个别学术或独立研究人员追求，但又处于早期阶段，无法引起大多数营利组织的兴趣。&lt;/p&gt;&lt;p&gt;我们对一系列有希望的早期议程进行押注，然后扩大那些被证明最成功的议程。与其他针对特定议程进行押注的研究组织不同，我们的结构使我们能够&lt;strong&gt;（1）&lt;/strong&gt;探索一系列议程并&lt;strong&gt;（2）&lt;/strong&gt;大规模执行它们。我们目前的赌注分为三类： &lt;/p&gt;&lt;figure class="image"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/pag9koqxql5cag5606ua" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;i&gt;&lt;strong&gt;稳健性科学&lt;/strong&gt;&lt;/i&gt;：稳健性如何随模型大小变化？超人系统是否容易受到类似于今天所见的对抗性例子或“越狱”的影响？如果是这样，我们如何才能实现安全关键的保证？&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;strong&gt;价值调整&lt;/strong&gt;&lt;/i&gt;：我们如何从人类数据中学习可靠的奖励函数？我们的研究重点是为用户提供更高带宽、更高效样本的方法来传达人工智能系统的偏好；并改进了利用人类反馈进行培训的方法。&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;strong&gt;模型评估&lt;/strong&gt;&lt;/i&gt;：我们如何评估和测试最先进模型的安全相关属性？评估可以分为仅关注外部可见行为（“模型测试”）的&lt;i&gt;黑盒&lt;/i&gt;方法和寻求解释内部运作方式（“可解释性”）的&lt;i&gt;白盒&lt;/i&gt;方法。这些方法是互补的，黑盒方法不如白盒方法强大但更易于使用，因此我们在这两个领域都进行研究。&lt;/p&gt;&lt;h3&gt;稳健性科学&lt;/h3&gt;&lt;figure class="image"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/twwekv0sevhp2kmm62sa" /&gt;&lt;figcaption&gt;在我们最近的&lt;a href="https://far.ai/post/2023-07-superhuman-go-ais/"&gt;研究&lt;/a&gt;中，我们发现超人围棋人工智能（例如 KataGo）很容易受到对抗性攻击。&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;没有哪个工程部件是坚不可摧的。在设计物理结构时，工程师会估计每个组件需要承受多少应力，添加适当的安全裕度，然后选择具有适当公差的组件。这使得施工安全且具有成本效益：桥梁很少倒塌，也不会过度设计。&lt;/p&gt;&lt;p&gt;法学硕士或计算机视觉分类器等人工智能组件远非坚不可摧，它们受到对抗性例子和分布转移脆弱性的困扰。不幸的是，人工智能目前还没有相当于土木工程师的应力计算。&lt;/p&gt;&lt;p&gt;到目前为止，我们拥有的最好方法是&lt;i&gt;猜测并检查&lt;/i&gt;：训练模型，然后对其进行一系列测试以确定其功能和局限性。但这种方法几乎没有为&lt;i&gt;如何&lt;/i&gt;改进系统提供理论依据。而且模型的训练和测试都变得越来越昂贵和劳动密集型（基础模型训练的成本现在可以与桥梁建设的成本相媲美）。&lt;/p&gt;&lt;p&gt;我们希望开发一种更有原则的方法来构建强大的人工智能系统：&lt;i&gt;稳健性科学&lt;/i&gt;。这样的科学将使我们能够回答有关未来的基本问题，例如超人人工智能系统是否仍然容易受到困扰当代系统的对抗性例子的影响。它还将使从业者能够计算需要多少对抗性训练才能达到给定应用程序所需的稳健性水平。最后，如果当前的鲁棒性技术被证明是不够的，那么科学将帮助研究人员开发改进的训练技术，并通过利用深度防御方法来减少组件的压力。&lt;/p&gt;&lt;p&gt;我们的首席执行官&lt;a href="https://www.gleave.me/"&gt;Adam&lt;/a&gt;更彻底地探讨了&lt;a href="https://far.ai/post/2023-03-safety-vulnerable-world/"&gt;&lt;i&gt;在机器学习系统脆弱的世界中，稳健性对于避免先进人工智能系统在人工智能安全方面&lt;/i&gt;&lt;/a&gt;的灾难性风险的重要性。此后， &lt;a href="https://terveisin.tw/"&gt;Tony Wang&lt;/a&gt;领导的团队在 ICML 论文中证明，&lt;strong&gt;像 AlphaGo 这样的超人类围棋人工智能系统会表现出&lt;/strong&gt;&lt;a href="https://far.ai/post/2023-07-superhuman-go-ais/"&gt;&lt;strong&gt;灾难性的故障模式&lt;/strong&gt;&lt;/a&gt;。我们目前正在研究迭代对抗训练和替代网络架构，以确定是否可以消除这一弱点，从而提高对使高级机器学习系统变得稳健的难度的定性理解。&lt;/p&gt;&lt;p&gt; &lt;a href="https://agarri.ga/"&gt;Adrià Garriga-Alonso&lt;/a&gt;和其他人开始研究&lt;i&gt;&lt;strong&gt;为什么&lt;/strong&gt;&lt;/i&gt;&lt;strong&gt;AlphaGo 式的系统容易受到我们使用机械解释方法的对抗性攻击&lt;/strong&gt;。我们正在考虑可解释性技术，例如激活修补和自动电路发现，以识别这些网络内导致错误的关键表示和计算。这种理解可以帮助通过手动编辑网络、微调或更改架构来修复网络。&lt;/p&gt;&lt;p&gt;为了更定量地了解鲁棒性， &lt;a href="https://www.gleave.me/"&gt;Adam Gleave&lt;/a&gt; 、 &lt;a href="https://nikihowe.com/"&gt;Niki Howe&lt;/a&gt;等人正在寻找&lt;strong&gt;语言模型鲁棒性的缩放法则&lt;/strong&gt;。这种缩放定律可以帮助我们预测随着计算和训练数据的不断增长，鲁棒性和功能是否会收敛、保持固定宽度或发散。例如，我们希望衡量对抗训练的样本效率随模型大小的提高程度。最终，我们希望能够预测对于给定的任务和训练设置，是否需要多少次计算才能找到模型错误分类的实例。为了找到这些缩放法则，我们目前正在研究经过微调的语言模型，以对简单的程序定义语言进行分类，并进行不同程度的对抗性训练。&lt;/p&gt;&lt;p&gt;从长远来看，我们希望利用这些缩放法则来定量地&lt;strong&gt;找到改进鲁棒训练的方法&lt;/strong&gt;（看看它们是否改善了缩放曲线，而不仅仅是曲线上的单个数据点），并&lt;strong&gt;调整对齐方法将对抗性优化压力降低&lt;/strong&gt;到当代技术可以实现的鲁棒性阈值以下。&lt;/p&gt;&lt;h3&gt;价值取向&lt;/h3&gt;&lt;p&gt;我们希望人工智能系统按照我们的价值观行事。表示值的自然方式是通过奖励函数，为不同的状态分配数值分数。人们可以使用这种奖励函数来优化策略，使用强化学习来采取行动，从而达到人类认为理想的状态。不幸的是，在现实环境中手动指定奖励函数是不可行的，因此有必要从人类数据中学习奖励函数。这一基本过程在实际应用中得到广泛应用，来自人类反馈的强化学习的变体被用于前沿模型（例如 GPT-4 和 Claude 2）中。&lt;/p&gt;&lt;p&gt;价值学习必须产生尽可能准确地指定用户偏好的奖励模型，因为即使奖励函数中的细微问题也可能产生&lt;a href="https://aiimpacts.org/stuart-russells-description-of-ai-risk/"&gt;危险的后果&lt;/a&gt;。为此，我们的研究重点是为用户提供更高的带宽、更高效的样本方法来将他们的偏好传达给人工智能系统，以及更广泛地改进利用人类反馈进行训练的方法。&lt;/p&gt;&lt;p&gt; &lt;a href="http://scottemmons.com/"&gt;Scott Emmons&lt;/a&gt;领导的团队发现，语言模型至少表现出&lt;a href="https://far.ai/post/2023-09-uncovering-latent-wellbeing/"&gt;&lt;strong&gt;对人类偏好的一些理解&lt;/strong&gt;&lt;/a&gt;：GPT-3 嵌入包含与常识道德判断相对应的方向！这向我们表明，模型的理解可能足够好，至少能够以自然语言的形式&lt;i&gt;表达&lt;/i&gt;偏好。为此， &lt;a href="https://far.ai/author/jeremy-scheurer/"&gt;Jérémy Scheurer&lt;/a&gt;和其他人开发了一种&lt;a href="https://arxiv.org/abs/2204.14146"&gt;&lt;strong&gt;从语言反馈中学习奖励函数的&lt;/strong&gt;&lt;/a&gt;方法。有了这个，就可以微调模型，仅用 100 个人类反馈样本进行总结。我们发现这种方法对于&lt;a href="https://arxiv.org/abs/2303.16749"&gt;&lt;strong&gt;改进代码生成&lt;/strong&gt;&lt;/a&gt;特别有用。 &lt;/p&gt;&lt;figure class="image image_resized" style="width: 60.51%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/tjfn7k9oi0h7uk59pdev" /&gt;&lt;figcaption&gt;自然语言反馈算法概述（&lt;a href="https://arxiv.org/abs/2204.14146"&gt;来源&lt;/a&gt;）。&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;我们还想将这种方法扩展到语言之外的其他模式。 &lt;a href="https://far.ai/author/juan-rocamonde/"&gt;Juan Rocamonde&lt;/a&gt;领导的团队通过使用图像字幕模型 CLIP 将语言反馈“翻译”为基于图像的观察的奖励，&lt;a href="https://far.ai/post/2023-10-vlm-rm/"&gt;成功地将我们的语言模型反馈方法应用于&lt;/a&gt;&lt;strong&gt;机器人政策&lt;/strong&gt;。&lt;/p&gt;&lt;h3&gt;模型评估&lt;/h3&gt;&lt;p&gt;我们需要测试模型安全性的方法。这既可以帮助研究人员开发更安全的系统，也可以在部署新开发的系统之前验证其安全性。&lt;/p&gt;&lt;p&gt;在高层次上，评估可以分为&lt;i&gt;黑盒&lt;/i&gt;方法和白盒方法，黑盒方法仅关注外部可见的模型行为（“模型测试”），而&lt;i&gt;白盒&lt;/i&gt;方法则寻求解释模型的内部工作原理（“可解释性”）。&lt;/p&gt;&lt;p&gt;由于我们最终关心这些模型的外部行为，因此黑盒方法是发现故障的自然方法。但他们没有告诉我们&lt;i&gt;为什么&lt;/i&gt;会发生失败。相比之下，白盒评估可以让我们更全面地了解模型，但实施起来要困难得多。我们认为这些方法是互补的，因此我们正在并行追求它们。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;黑盒评估：模型测试&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://irmckenzie.co.uk/"&gt;Ian McKenzie&lt;/a&gt;和其他人研究了&lt;a href="https://arxiv.org/abs/2306.09479"&gt;&lt;strong&gt;逆缩放&lt;/strong&gt;&lt;/a&gt;：较大模型比较小模型表现&lt;i&gt;更差的&lt;/i&gt;任务。这种情况很重要，因为随着时间的推移，随着模型能力的提高，问题会变得更加严重，需要明确的安全研究来解决。幸运的是，我们只发现了有限的这样的例子， &lt;a href="https://arxiv.org/abs/2211.02011"&gt;Wei 等人 (2022)&lt;/a&gt;在我们的结果基础上所做的工作发现，在许多情况下，缩放实际上是“U 形”的，性能最初随着模型大小的增加而下降，但随后再次改善模型大小的一定阈值。&lt;/p&gt;&lt;p&gt; &lt;a href="https://ninodimontalcino.github.io/"&gt;Nino Scherrer&lt;/a&gt;领导的团队评估了&lt;a href="https://arxiv.org/abs/2307.14324"&gt;法学硕士的道德信念&lt;/a&gt;，发现在人类认为明确的情况下，法学硕士通常会选择符合常识道德推理的行为。然而，在人类意见不一致的模糊情况下，一些模型仍然反映了模型之间不同的明确偏好。这表明法学硕士在某些情况下表现出“模式崩溃”，自信地采取某些有争议的道德立场。 &lt;/p&gt;&lt;figure class="image"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/l0io3cfdv1jawbyumxd7" /&gt;&lt;figcaption&gt;对于明确的道德困境，法学硕士倾向于选择普遍接受的行动。但在高度模糊的道德困境中，法学硕士也自信地坚守自己的立场。这显示了法学硕士在一系列道德困境中推荐采取行动“行动 1”的可能性分布。在低歧义场景（上）中，“操作 1”表示首选的常识操作。在高度模糊的场景中（底部），“操作 1”既不是明确首选，也不是不首选（&lt;a href="https://arxiv.org/abs/2307.14324"&gt;来源&lt;/a&gt;）。&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;白盒评估：可解释性&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;由&lt;a href="https://far.ai/author/nora-belrose/"&gt;诺拉·贝尔罗斯 (Nora Belrose)&lt;/a&gt;领导的团队开发了&lt;a href="https://arxiv.org/abs/2303.08112"&gt;调谐透镜&lt;/a&gt;技术，将变压器每一层的激活解释为对下一个令牌的预测。这可以很容易地应用于各种模型，以实现对模型的粗粒度理解，例如哪些层实现给定的行为（如从输入流复制的&lt;a href="https://openreview.net/pdf?id=NpsVSN6o4ul"&gt;感应头&lt;/a&gt;）。 &lt;/p&gt;&lt;figure class="image image_resized" style="width: 70.77%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/mf84gigfvfcpzlrpkuev" /&gt;&lt;figcaption&gt; GPT-Neo-2.7B 的调谐透镜（底部）与 Logit 透镜（顶部）的比较，摘自&lt;a href="https://arxiv.org/abs/1706.03762"&gt;Vaswani 等人&lt;/a&gt;的摘要。每个单元格显示模型在给定层和令牌索引处预测的 top-1 令牌。 Logit 透镜未能在第 21 层之前得出可解释的预测，但我们的方法成功了。 （&lt;a href="https://arxiv.org/abs/2303.08112"&gt;来源&lt;/a&gt;）&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt; &lt;a href="https://taufeeque9.github.io/"&gt;Mohammad Taufeeque&lt;/a&gt;和&lt;a href="https://far.ai/author/alex-tamkin/"&gt;Alex Tamkin&lt;/a&gt;开发了一种方法，通过&lt;strong&gt;将网络的连续特征量化&lt;/strong&gt;为我们所说的&lt;a href="https://far.ai/post/2023-10-codebook-features/"&gt;&lt;i&gt;码本特征&lt;/i&gt;&lt;/a&gt;，使神经网络更像传统的计算机程序。我们在每一层都使用矢量量化瓶颈来微调神经网络。结果是一个网络，其中间激活由从码本中选择的少量离散矢量代码的总和表示。值得注意的是，我们发现神经网络可以在这种严格的瓶颈下运行，而性能只会略有下降。 &lt;/p&gt;&lt;figure class="image"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/vvzycapcrsew82gg9qwi" /&gt;&lt;figcaption&gt;密码本功能将传统软件的可解释性优势与神经网络的新兴功能结合起来。 （&lt;a href="https://arxiv.org/abs/2310.17230"&gt;来源&lt;/a&gt;）&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt; &lt;a href="https://agarri.ga/"&gt;Adrià Garriga-Alonso&lt;/a&gt;正处于&lt;strong&gt;了解 ML 系统如何学习规划&lt;/strong&gt;的早期阶段。神经网络在许多任务上表现良好，例如玩棋盘游戏或生成代码，其中规划是人类表现的关键组成部分。但这些网络也经常以与人类截然不同的方式失败。我们怀疑这种差异可能是由于网络规划和表示概念的方式不同造成的。这个问题对于安全性来说尤其重要，因为一个学会了计划的系统可能会采取有能力但偏离分布的行动：&lt;a href="https://arxiv.org/abs/2210.01790"&gt;&lt;i&gt;目标错误概括&lt;/i&gt;&lt;/a&gt;的问题。&lt;/p&gt;&lt;p&gt;未来，我们希望通过提出以下问题来研究&lt;i&gt;可解释性科学&lt;/i&gt;：&lt;strong&gt;假设如何解释模型行为&lt;/strong&gt;？目前，有许多相互竞争的提案，但没有一个有原则性的定义。我们将首先开发算法分类法来测试可解释性假设。然后，我们将定义可解释性应该有助于的几个任务，例如人类“模拟”模型行为方式的能力，并研究不同的指标如何预测给定的假设对执行该任务的帮助程度。&lt;/p&gt;&lt;p&gt;我们很高兴看到上述研究方向将我们带往何处，但我们不打算将我们的工作限制在这些领域。我们一直在寻找有前景的新方法，以确保先进的人工智能系统安全且有益。&lt;/p&gt;&lt;h3&gt;我怎样才能参与其中？&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;我们正在招聘！&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;我们目前正在招聘研究科学家、研究工程师和通信专家。我们很高兴在未来 12 个月内增加多达 5 名技术人员。我们特别渴望聘请高级研究工程师或具有新颖议程愿景的研究科学家，尽管我们也将聘用几名初级人员，并鼓励广泛的个人申请。请&lt;a href="https://far.ai/jobs/"&gt;在此处&lt;/a&gt;查看完整的空缺职位列表并进行申请。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;我们正在寻找合作者！&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;我们经常与其他学术、非营利性和（有时）营利性研究机构的研究人员合作。如果您很高兴与我们合作开展项目，请通过&lt;a href="mailto:hello@far.ai"&gt;hello@far.ai&lt;/a&gt;联系。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;想捐款吗？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;您可以通过&lt;a href="https://far.ai/donate/"&gt;在这里&lt;/a&gt;捐款来帮助我们确保美好的未来。额外的资金将使我们能够更快地发展。根据目前获得的资金，我们愿意在未来 12 个月内扩大 1-2 名技术人员，而我们希望增加最多 5 名技术人员。我们非常感谢您的帮助！&lt;/p&gt;&lt;p&gt;&lt;strong&gt;想了解更多关于我们的研究吗？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;请查看我们的&lt;a href="https://far.ai/research/publications/"&gt;出版物列表&lt;/a&gt;和&lt;a href="https://far.ai/news/"&gt;博客&lt;/a&gt;。您也可以通过&lt;a href="mailto:hello@far.ai"&gt;hello@far.ai&lt;/a&gt;直接与我们联系。&lt;/p&gt;&lt;p&gt;我们期待您的回音！&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/PQgEdo3xsFFAxXNqE/2023-alignment-research-updates-from-far-ai-2#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Mon, 04 Dec 2023 22:32:21 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/PQgEdo3xsFFAxXNqE/2023-alignment-research-updates-from-far-ai-2</guid></item><item><title>FAR AI 的最新动态</title><link>https://www.lesswrong.com/posts/zy8eHc5iYgrFGjF8Q/what-s-new-at-far-ai-3</link><description>发布于 2023 年 12 月 4 日晚上 9:18（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;概括&lt;/h2&gt;&lt;p&gt;我们是&lt;a href="https://far.ai"&gt;FAR AI&lt;/a&gt; ：人工智能安全研究孵化器和加速器。自 2022 年 7 月成立以来，FAR 已发展到拥有 12 名全职员工的团队，发表了 13 篇学术论文，开设了拥有 40 名活跃成员的联合办公空间 FAR 实验室，并为 160 多名机器学习研究人员组织了现场建设活动。&lt;/p&gt;&lt;p&gt;我们的组织由三个主要支柱组成： &lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/rcr7pvhwphld5gmcfon5" /&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;strong&gt;研究&lt;/strong&gt;&lt;/i&gt;。我们迅速探索人工智能安全领域的一系列潜在研究方向，扩大那些最有希望的方向。与其他押注于单一研究方向的人工智能安全实验室不同，FAR 追求多元化的项目组合。我们当前的重点领域是建立&lt;i&gt;稳健性科学&lt;/i&gt;（例如&lt;a href="https://far.ai/post/2023-07-superhuman-go-ais/"&gt;寻找超人类围棋人工智能中的漏洞&lt;/a&gt;）、寻找更有效的&lt;i&gt;价值调整&lt;/i&gt;方法（例如&lt;a href="https://arxiv.org/abs/2204.14146"&gt;语言反馈训练&lt;/a&gt;）以及&lt;i&gt;模型评估&lt;/i&gt;（例如&lt;a href="https://far.ai/publication/mckenzie2023inverse/"&gt;逆缩放&lt;/a&gt;和&lt;a href="https://far.ai/post/2023-10-codebook-features/"&gt;码本特征&lt;/a&gt;）。&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;strong&gt;联合办公空间&lt;/strong&gt;&lt;/i&gt;。我们在伯克利运营着 FAR 实验室，这是一个人工智能安全联合办公空间。该空间目前拥有 FAR、 &lt;a href="http://aiimpacts.org/"&gt;AI Impacts&lt;/a&gt; 、 &lt;a href="https://www.serimats.org/"&gt;SERI MATS&lt;/a&gt;和几位独立研究人员。我们正在建设一个协作社区空间，通过卓越的办公空间、热情且充满智慧的文化以及为会员量身定制的计划和培训，促进伟大的工作。&lt;a href="https://far.ai/labs/"&gt;应用程序向该空间的新用户（个人和组织）开放&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;strong&gt;场馆建设。&lt;/strong&gt;&lt;/i&gt;我们举办研讨会，主要针对机器学习研究人员，以帮助建立人工智能安全研究和治理领域。我们共同组织了&lt;a href="https://far.ai/post/2023-10-international-dialogue/"&gt;&lt;i&gt;人工智能安全国际对话，&lt;/i&gt;&lt;/a&gt;汇聚了来自世界各地的杰出科学家，最终发表了一份&lt;a href="https://humancompatible.ai/news/2023/10/31/prominent-ai-scientists-from-china-and-the-west-propose-joint-strategy-to-mitigate-risks-from-ai/#prominent-ai-scientists-from-china-and-the-west-propose-joint-strategy-to-mitigate-risks-from-ai"&gt;公开声明&lt;/a&gt;，呼吁在人工智能安全研究和治理方面采取全球行动。我们很快将于 12 月举办&lt;a href="https://www.alignment-workshop.com/nola-2023"&gt;新奥尔良协调研讨会&lt;/a&gt;，让 140 多名研究人员了解人工智能安全并寻找合作者。&lt;/p&gt;&lt;p&gt;我们想要扩张，所以如果您对我们所做的工作感到兴奋，请考虑&lt;a href="https://far.ai/donate/"&gt;捐赠&lt;/a&gt;或&lt;a href="https://far.ai/jobs/"&gt;为我们工作&lt;/a&gt;！我们正在招聘研究工程师、研究科学家和通信专家。&lt;/p&gt;&lt;h2&gt;孵化并加速人工智能安全研究&lt;/h2&gt;&lt;p&gt;我们的主要目标是探索新的人工智能安全研究方向，扩大那些最有希望的方向。我们选择的议程太大，无法由个别学术或独立研究人员追求，但与营利性组织的利益不一致。我们的结构使我们能够&lt;strong&gt;（1）&lt;/strong&gt;探索一系列议程并&lt;strong&gt;（2）&lt;/strong&gt;大规模执行它们。尽管我们的大部分工作都是在内部进行的，但我们经常寻求与具有重叠研究兴趣的其他组织的研究人员进行合作。&lt;/p&gt;&lt;p&gt;我们目前的研究分为三个主要类别： &lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/eyavmlysgmy3c0jenw2g" /&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;strong&gt;稳健性科学。&lt;/strong&gt;&lt;/i&gt;稳健性如何随模型大小变化？超人系统是否容易受到类似于今天所见的对抗性例子或“越狱”的影响？如果是这样，我们如何才能实现安全关键的保证？&lt;/p&gt;&lt;p&gt;相关工作：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://far.ai/post/2023-07-superhuman-go-ais/"&gt;超人围棋人工智能中的漏洞&lt;/a&gt;，&lt;/li&gt;&lt;li&gt;&lt;a href="https://far.ai/post/2023-03-safety-vulnerable-world/"&gt;易受攻击的机器学习系统世界中的人工智能安全&lt;/a&gt;。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;i&gt;&lt;strong&gt;价值调整。&lt;/strong&gt;&lt;/i&gt;我们如何从人类数据中学习可靠的奖励函数？我们的研究重点是为用户提供更高带宽、更高效样本的方法来传达人工智能系统的偏好；并改进了利用人类反馈进行培训的方法。&lt;/p&gt;&lt;p&gt;相关工作：&lt;/p&gt;&lt;ul&gt;&lt;li&gt; &lt;a href="https://far.ai/post/2023-10-vlm-rm/"&gt;VLM-RM：用自然语言指定奖励&lt;/a&gt;，&lt;/li&gt;&lt;li&gt;&lt;a href="https://far.ai/publication/scheurer2022training/"&gt;使用语言反馈训练语言模型&lt;/a&gt;。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;i&gt;&lt;strong&gt;模型评估&lt;/strong&gt;&lt;/i&gt;：我们如何评估和测试最先进模型的安全相关属性？评估可以分为仅关注外部可见行为（“模型测试”）的&lt;i&gt;黑盒&lt;/i&gt;方法和寻求解释内部运作方式（“可解释性”）的&lt;i&gt;白盒&lt;/i&gt;方法。这些方法是互补的，黑盒方法不如白盒方法强大但更易于使用，因此我们在这两个领域都进行研究。&lt;/p&gt;&lt;p&gt;相关工作：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;模型测试：&lt;ul&gt;&lt;li&gt;&lt;a href="https://far.ai/publication/mckenzie2023inverse/"&gt;逆缩放&lt;/a&gt;，&lt;/li&gt;&lt;li&gt;&lt;a href="https://far.ai/publication/scherrer2023evaluating/"&gt;道德信仰&lt;/a&gt;；&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;可解释性：&lt;ul&gt;&lt;li&gt;&lt;a href="https://far.ai/post/2023-10-codebook-features/"&gt;密码本功能&lt;/a&gt;，&lt;/li&gt;&lt;li&gt;&lt;a href="https://far.ai/publication/belrose2023tuned/"&gt;调好镜头&lt;/a&gt;。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;到目前为止，FAR 已发表&lt;a href="https://scholar.google.com/citations?user=FVJ24k8AAAAJ"&gt;13 篇论文&lt;/a&gt;，并在 ICML 和 EMNLP 等顶级同行评审场所发表，我们的工作也被英国《&lt;a href="https://www.ft.com/content/175e5314-a7f7-4741-a786-273219f433a1"&gt;金融时报》&lt;/a&gt; 、 &lt;a href="https://www.thetimes.co.uk/article/man-beats-machine-at-go-thanks-to-ai-opponents-fatal-flaw-nc9vqmrvf"&gt;《泰晤士报》&lt;/a&gt;和&lt;a href="https://arstechnica.com/information-technology/2022/11/new-go-playing-trick-defeats-world-class-go-ai-but-loses-to-human-amateurs/"&gt;Ars Technica&lt;/a&gt;等主要媒体报道。有关我们研究的更多信息，请查看我们的&lt;a href="https://far.ai/post/2023-12-far-research-update/"&gt;随附文章&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;我们还建立了自己的 HPC 集群，代号为&lt;i&gt;flamingo&lt;/i&gt; ，供 FAR 员工和合作伙伴组织使用。在接下来的一年里，我们希望不仅扩大我们当前的项目，还希望探索新的研究方向。&lt;/p&gt;&lt;p&gt;我们希望我们有更多的能力来帮助培养更多的人工智能安全研究议程，但我们的研究人员和工程师的时间是有限的。然而，我们找到了其他方法来支持人工智能安全领域的其他组织。最为显着地：&lt;/p&gt;&lt;h2&gt; FAR Labs：伯克利的人工智能安全联合办公空间&lt;/h2&gt;&lt;p&gt;FAR Labs 是伯克利市中心的一个联合办公中心，为致力于人工智能安全和相关问题的组织和个人服务。自 2023 年 3 月开放该空间以来，我们已拥有约 40 名会员。我们的目标是通过成员之间的知识共享和相互支持来孵化和加速早期组织和研究议程。 &lt;/p&gt;&lt;figure class="image"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/kolohtgozzazoyxjenby" /&gt;&lt;/figure&gt;&lt;p&gt;我们的成员主要来自四个主要组织，但我们也拥有多个独立研究人员和研究团队。该空间配备了高效、活跃的联合办公空间所需的一切：工作站、会议室、呼叫亭、视频会议设施、小吃和餐食。我们举办闪电演讲、午餐和学习课程、研讨会和欢乐时光。&lt;/p&gt;&lt;p&gt; FAR 实验室还每周举办 FAR 研讨会系列，欢迎来自 FAR、AI Impacts、Rethink Priorities 和牛津大学等一系列组织的演讲者。 &lt;/p&gt;&lt;figure class="image image_resized" style="width: 60.71%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/halsm2gm8t3mlu3sdcvo" /&gt;&lt;/figure&gt;&lt;figure class="image image_resized" style="width: 60.67%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/wlzworpi0davouov6us1" /&gt;&lt;/figure&gt;&lt;p&gt;我们欢迎组织和个人申请在 FAR 实验室工作，也欢迎短期访客。有关设施、文化和价格的更多信息，请参阅&lt;a href="https://far.ai/labs/"&gt;此处&lt;/a&gt;。您可以&lt;a href="https://far.ai/labs/work-from-far-labs/"&gt;在这里&lt;/a&gt;申请。&lt;/p&gt;&lt;p&gt;尽管我们很高兴能够帮助其他人推进他们的研究，但我们意识到，与问题的严重性相比，人工智能的整体安全性仍然很小。避免先进人工智能系统带来的风险不仅需要更有&lt;i&gt;生产力的&lt;/i&gt;贡献者，还需要&lt;i&gt;更多的&lt;/i&gt;贡献者。这激发了我们努力的第三个支柱：发展人工智能安全领域。&lt;/p&gt;&lt;h2&gt;现场建设和外展&lt;/h2&gt;&lt;p&gt;我们举办研讨会，向机器学习研究人员介绍最新的人工智能安全研究，并正在建立一个社区，使参与者能够更轻松地找到合作者并继续参与该领域。 2023年我们举办了两场研讨会，共有约150人参加。我们还为普通公众（例如&lt;a href="https://theaidigest.org/"&gt;《人工智能文摘》&lt;/a&gt; ）和技术受众（例如即将推出的人工智能安全研究人员访谈系列）开发有关人工智能安全的在线教育资源。&lt;/p&gt;&lt;p&gt;我们的研讨会通常针对 ML 研究人员，利用 FAR 在 ML 社区和技术 AI 安全研究领域的知识。我们最近举办了首届&lt;a href="https://far.ai/post/2023-10-international-dialogue/"&gt;&lt;i&gt;人工智能安全国际对话，&lt;/i&gt;&lt;/a&gt;汇聚了领先的人工智能科学家，就先进人工智能系统的风险达成了共识。会议由图灵奖获得者 Yoshua Bengio 和 Andrew Yao、加州大学伯克利分校教授 Stuart Russell、OBE 以及清华人工智能产业研究院创始院长张亚勤召集。我们与&lt;a href="https://humancompatible.ai/"&gt;CHAI&lt;/a&gt;和&lt;a href="https://www.ditchley.com/"&gt;迪奇利基金会&lt;/a&gt;合作举办了此次活动。此次活动最终形成了一份包含具体技术和政策建议的&lt;a href="https://humancompatible.ai/news/2023/10/31/prominent-ai-scientists-from-china-and-the-west-propose-joint-strategy-to-mitigate-risks-from-ai/#prominent-ai-scientists-from-china-and-the-west-propose-joint-strategy-to-mitigate-risks-from-ai"&gt;联合声明&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;我们很快将欢迎超过 140 名机器学习研究人员参加&lt;a href="https://www.alignment-workshop.com/nola-2023"&gt;新奥尔良对齐研讨会&lt;/a&gt;。该研讨会在 NeurIPS 之前举行，将向与会者介绍人工智能安全的最新进展，帮助他们探索新的研究方向并寻找具​​有共同研究兴趣的合作者。&lt;/p&gt;&lt;p&gt;我们也在建设人工智能安全教育资源。我们与&lt;a href="https://www.quantifiedintuitions.org/"&gt;Sage Futures&lt;/a&gt;合作构建了&lt;a href="https://theaidigest.org/"&gt;AI Digest&lt;/a&gt; ：一个帮助非技术 AI 研究人员了解前沿语言模型进展速度的网站。我们还针对人工智能安全研究人员的研究变革理论进行了一系列采访（如果您想参与，请联系&lt;a href="mailto:euan@far.ai"&gt;euan@far.ai&lt;/a&gt; ！）。&lt;/p&gt;&lt;h2&gt;谁在 FAR 工作？&lt;/h2&gt;&lt;p&gt; FAR 的&lt;a href="https://far.ai/about/team/"&gt;团队&lt;/a&gt;由 11.5 名全职员工 (FTE) 组成。 FAR 由 Adam Gleave 博士（首席执行官）和 Karl Berzins（首席运营官）领导。我们的研究团队由 5 名技术人员组成，他们拥有研究生院的 ML 研究和工程经验以及 Jane Street、Cruise 和 Microsoft 等公司的工作经验。我们的 3 人运营团队支持我们的研究工作、运营 FAR 实验室并负责现场建设活动的制作。我们的 1.5 FTE 传播团队帮助清晰、广泛地传播我们的研究成果。我们还受益于广泛的合作者和研究顾问网络。 &lt;/p&gt;&lt;figure class="image"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/wrzvvykpob8izaxuvtuq" /&gt;&lt;figcaption&gt; Tony Wang 和 Adam Gleave 在 ICML 2023 上展示我们的&lt;a href="https://far.ai/post/2023-07-superhuman-go-ais/"&gt;KataGo 攻击&lt;/a&gt;结果&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;我怎样才能参与其中？&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;我们正在招聘！&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;我们目前正在招聘研究科学家、研究工程师和通信专家。我们很高兴在未来 12 个月内增加多达 5 名技术人员。我们特别渴望聘请高级研究工程师或具有新颖议程愿景的研究科学家，尽管我们也将聘用几名初级人员，并鼓励广泛的个人申请。请&lt;a href="https://far.ai/jobs/"&gt;在此处&lt;/a&gt;查看完整的空缺职位列表并进行申请。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;我们正在寻找合作者！&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;我们经常与其他学术、非营利性和（有时）营利性研究机构的研究人员合作。如果您很高兴与我们合作开展项目，请通过&lt;a href="mailto:hello@far.ai"&gt;hello@far.ai&lt;/a&gt;联系。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;想捐款吗？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;您可以通过&lt;a href="https://far.ai/donate/"&gt;在这里&lt;/a&gt;捐款来帮助我们确保美好的未来。额外的资金将使我们能够更快地发展。根据目前获得的资金，我们愿意在未来 12 个月内扩大 1-2 名技术人员，而我们希望增加最多 5 名技术人员。我们非常感谢您的帮助！&lt;/p&gt;&lt;p&gt;&lt;strong&gt;想了解更多关于我们的研究吗？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;查看我们最新的&lt;a href="https://far.ai/post/2023-12-far-research-update/"&gt;研究更新&lt;/a&gt;、&lt;a href="https://far.ai/research/publications/"&gt;出版物列表&lt;/a&gt;和&lt;a href="https://far.ai/news/"&gt;博客&lt;/a&gt;。您也可以通过&lt;a href="mailto:hello@far.ai"&gt;hello@far.ai&lt;/a&gt;直接与我们联系。&lt;/p&gt;&lt;p&gt;我们期待您的回音！&lt;/p&gt;&lt;br /&gt;&lt;br /&gt;&lt;a href="https://www.lesswrong.com/posts/zy8eHc5iYgrFGjF8Q/what-s-new-at-far-ai-3#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Mon, 04 Dec 2023 21:18:05 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/zy8eHc5iYgrFGjF8Q/what-s-new-at-far-ai-3</guid></item><item><title>m 个环签名中的 n 个</title><link>https://www.lesswrong.com/posts/uojSbSav3dtEJvctz/n-of-m-ring-signatures</link><description>发布于 2023 年 12 月 4 日晚上 8:00（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;与消息和公钥关联的普通加密签名可让您向世界证明该签名是由有权访问与已知公钥关联的私钥的人创建的，而无需泄露该公钥。您可以在&lt;a href="https://en.wikipedia.org/wiki/Digital_signature"&gt;此处的&lt;/a&gt;维基百科上阅读相关内容。&lt;/p&gt;&lt;p&gt;与消息和一组公钥相关联的环签名可以让您向世界证明它是由有权访问该消息的人以及与该组中的一个公钥关联的一个私钥创建的，但没有人能够告诉它是哪个公钥。这让你可以半匿名地说一些话，这很简洁。它也用于私人加密货币门罗币。您可以在&lt;a href="https://en.wikipedia.org/wiki/Ring_signature"&gt;此处的&lt;/a&gt;维基百科上阅读有关它们的信息。&lt;/p&gt;&lt;p&gt;这里有一个比环签名更好的东西：证明它是由一定大小的公钥子集创建的签名。在我的脑海里，我曾一度将此称为 n of m 环签名。但当我用谷歌搜索“n of m环签名”时，什么也没出现。事实证明，这是因为在文献中，它被称为“阈值环签名”、“k of n 环签名”或“t of n 环签名”。我想也许第一篇关于它的论文就是&lt;a href="https://www.iacr.org/archive/crypto2002/24420467/24420467.pdf"&gt;这篇&lt;/a&gt;，但我还没有仔细检查。&lt;/p&gt;&lt;p&gt;无论如何：我想这样做，以便当您在线搜索 n-of-m 环签名时，您会发现一个东西告诉您应该搜索“阈值环签名”。因此这篇文章。&lt;/p&gt;&lt;br /&gt;&lt;br /&gt;&lt;a href="https://www.lesswrong.com/posts/uojSbSav3dtEJvctz/n-of-m-ring-signatures#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Mon, 04 Dec 2023 20:00:07 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/uojSbSav3dtEJvctz/n-of-m-ring-signatures</guid></item><item><title>作为一个整体实现 EU 最大化的智能体并不单独实现 EU 最大化</title><link>https://www.lesswrong.com/posts/kpus2SxcbqhSJjvxv/agents-which-are-eu-maximizing-as-a-group-are-not-eu</link><description>发布于 2023 年 12 月 4 日下午 6:49（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;介绍&lt;/h2&gt;&lt;p&gt;&lt;a href="https://www.lesswrong.com/posts/3xF66BNSC5caZuKyC/why-subagents"&gt;为什么是子代理？&lt;/a&gt;&lt;a href="https://www.lesswrong.com/posts/bzmLC3J8PsknwRZbr/why-not-subagents"&gt;为什么不是子代理？&lt;/a&gt;探索一组预期效用最大化者本身是否是效用最大化者。在这里我想讨论相反的问题：如果一个群体想要最大化整体的某些效用函数，那么对于个体代理可以说什么呢？当然，如果他们可以一起做出决策，他们只会计算每个代理需要做什么，但如果他们唯一拥有的是每个人独立使用的通用算法怎么办？&lt;/p&gt;&lt;p&gt;一般来说，这些智能体似乎不会通过将效用与概率相乘来做出决策，而是需要考虑结果的整个分布来评估选择。类似的想法已经在&lt;a href="https://www.lesswrong.com/posts/vqB8n72rWE7GPCNP7/against-expected-utility"&gt;Against Expected Utility&lt;/a&gt;中提出，尽管没有关注代理的数量。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;具体例子&lt;/h2&gt;&lt;p&gt;想象一下两个交易者，他们独立选择交易，但将他们的回报集中在一起并优化他们总财富的预期对数（如&lt;a href="https://en.wikipedia.org/wiki/Kelly_criterion"&gt;凯利投注&lt;/a&gt;）。此外，为简单起见，我将假设他们为两者选择相同的交易，尽管结果仍然是独立采样的。&lt;/p&gt;&lt;p&gt;因此，如果一笔交易将财富乘以（随机变量） &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;X&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; ，则一个交易者的效用将为&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-ams-R"&gt;E&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;[&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;log&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;X&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;]&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 。但对于所描述的两个交易者组来说，它变成了&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-ams-R"&gt;E&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mrow MJXc-space1"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-size2-R"&gt;[&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;log&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-size2-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mfrac"&gt;&lt;span class="mjx-box MJXc-stacked" style="width: 2.511em; padding: 0px 0.12em;"&gt;&lt;span class="mjx-numerator"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;X&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;X&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-denominator"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-line"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-vsize"&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-size2-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-size2-R"&gt;]&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; ，其中&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;X&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space1"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;X&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;是与&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;X&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;具有相同分布的独立随机变量。就结果概率而言，它不再是线性的：&lt;/p&gt; &lt;span&gt;&lt;span class="mjpage mjpage__block"&gt;&lt;span class="mjx-chtml MJXc-display" style="text-align: center;"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;U&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;p&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-munderover MJXc-space3"&gt;&lt;span class="mjx-itable"&gt;&lt;span class="mjx-row"&gt;&lt;span class="mjx-cell"&gt;&lt;span class="mjx-op"&gt;&lt;span class="mjx-mo" style="vertical-align: 0.001em;"&gt;&lt;span class="mjx-char MJXc-TeX-size2-R"&gt;∫&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-row"&gt;&lt;span class="mjx-under" style="font-size: 70.7%;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mrow MJXc-space1"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-size3-R"&gt;(&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;日志&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mfrac MJXc-space1"&gt;&lt;span class="mjx-box MJXc-stacked" style="width: 3.344em; padding: 0px 0.12em;"&gt;&lt;span class="mjx-numerator"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-denominator"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-line"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-vsize"&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-size3-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;p&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;p&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;d&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;d&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;h2&gt;代理数量不断增加&lt;/h2&gt;&lt;p&gt;定性地讲，随着群体中代理人数量的增加，由于回报的汇总，代理人可以承担更多风险的行动。因此，他们的决定将介于个体代理为最大化&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-ams-R"&gt;E&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;[&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;log&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;X&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;]&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;以及如何最大化&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-ams-R"&gt;E&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;[&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;X&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;]&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 。支持这种直觉的更具体的例子是：有一枚公平的硬币，代理人可以在某一侧下注他们可用的财富的一部分&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;"&gt;f&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; ，如果硬币落在这一侧，则该分数将变为&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;3&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;"&gt;f&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; ，否则为&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; ，即他们的财富将乘以&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;"&gt;f&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;或&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;−&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;"&gt;f&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 。 &lt;/p&gt;&lt;figure class="image"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kpus2SxcbqhSJjvxv/dmaq9vhah1asekwnfkxu" /&gt;&lt;figcaption&gt;对于不同数量的代理，效用的预期增加（聚合后）作为&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;"&gt;f&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;的函数。&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;因此，随着智能体数量的增加，每个智能体变得更接近最大化&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-ams-R"&gt;E&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;[&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;X&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;]&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; ，但对于任何有限情况，仍然存在一些风险规避。特别是，任何让财富归零的结果分配仍然是无限糟糕的，因为如果这种情况同时发生在所有代理人身上，他们的总财富将归零。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;违反独立公理&lt;/h2&gt;&lt;p&gt;正如&lt;a href="https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem"&gt;VNM&lt;/a&gt;定理所说，在某些假设下，代理人可以被视为最大化预期效用，一个自然的问题是哪些假设在这种情况下不成立？&lt;/p&gt;&lt;p&gt;我有一个例子证明独立性不适用于上述代理。将有两种彩票： &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;A&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;简单地保留钱， &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;B&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;则以相同的概率将它们乘以&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;10&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;−&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;100&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;或乘以&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;10&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;20&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 。另请考虑&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;A&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px;"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;′&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0.5&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;A&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0.5&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;A&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;A&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;和&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;B&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px;"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;′&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0.5&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;A&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0.5&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;B&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; （即&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;A&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;或&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;B&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;具有相同的概率）。&lt;/p&gt;&lt;p&gt;这里的“实用程序”是什么？&lt;/p&gt; &lt;span&gt;&lt;span class="mjpage mjpage__block"&gt;&lt;span class="mjx-chtml MJXc-display" style="text-align: center;"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;U&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;A&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;U&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;A&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.584em; padding-left: 0px;"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;′&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;log&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span&gt;&lt;span class="mjpage mjpage__block"&gt;&lt;span class="mjx-chtml MJXc-display" style="text-align: center;"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtable"&gt;&lt;span class="mjx-table"&gt;&lt;span class="mjx-mtr" style="height: 2.707em;"&gt;&lt;span class="mjx-mtd" style="padding: 0px 0px 0px 0px; text-align: right; width: 2.304em;"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;U&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;B&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-strut"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mtd" style="padding: 0px 0px 0px 0px; text-align: left; width: 21.215em;"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;/span&gt;&lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mfrac MJXc-space3"&gt;&lt;span class="mjx-box MJXc-stacked" style="width: 0.7em; padding: 0px 0.12em;"&gt;&lt;span class="mjx-numerator"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-denominator"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;4&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-line"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-vsize"&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-size3-R"&gt;[&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;日志&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-msubsup MJXc-space1"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;10&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;−&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;100&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;对数&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mfrac MJXc-space1"&gt;&lt;span class="mjx-box MJXc-stacked" style="width: 5.811em; padding: 0px 0.12em;"&gt;&lt;span class="mjx-numerator"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;10&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;−&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;100&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;10&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;20&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-denominator"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-line"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-vsize"&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;日志&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-msubsup MJXc-space1"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;10&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;20&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-size3-R"&gt;]&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-strut"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mtr" style="height: 2.857em;"&gt;&lt;span class="mjx-mtd" style="padding: 0.15em 0px 0px 0px; text-align: right;"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-strut"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mtd" style="padding: 0.15em 0px 0px 0px; text-align: left;"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;/span&gt;&lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;≈&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mfrac MJXc-space3"&gt;&lt;span class="mjx-box MJXc-stacked" style="width: 0.7em; padding: 0px 0.12em;"&gt;&lt;span class="mjx-numerator"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-denominator"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;4&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-line"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-vsize"&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-size3-R"&gt;[&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;日志&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-msubsup MJXc-space1"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;10&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;−&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;100&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;对数&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mfrac MJXc-space1"&gt;&lt;span class="mjx-box MJXc-stacked" style="width: 1.942em; padding: 0px 0.12em;"&gt;&lt;span class="mjx-numerator"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;10&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;20&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-denominator"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-line"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-vsize"&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;日志&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-msubsup MJXc-space1"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;10&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;20&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-size3-R"&gt;]&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-strut"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mtr" style="height: 1.15em;"&gt;&lt;span class="mjx-mtd" style="padding: 0.15em 0px 0px 0px; text-align: right;"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-strut"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mtd" style="padding: 0.15em 0px 0px 0px; text-align: left;"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;/span&gt;&lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;≈&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;−&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;23.4&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-strut"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span&gt;&lt;span class="mjpage mjpage__block"&gt;&lt;span class="mjx-chtml MJXc-display" style="text-align: center;"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mtable"&gt;&lt;span class="mjx-table"&gt;&lt;span class="mjx-mtr" style="height: 2.511em;"&gt;&lt;span class="mjx-mtd" style="padding: 0px 0px 0px 0px; text-align: right; width: 2.534em;"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;U&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;B&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.584em; padding-left: 0px;"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;′&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-strut"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mtd" style="padding: 0px 0px 0px 0px; text-align: left; width: 22.872em;"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;/span&gt;&lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mfrac MJXc-space3"&gt;&lt;span class="mjx-box MJXc-stacked" style="width: 1.2em; padding: 0px 0.12em;"&gt;&lt;span class="mjx-numerator"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-denominator"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;16&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-line"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-vsize"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;日志&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-msubsup MJXc-space1"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;10&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;−&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;100&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mfrac MJXc-space2"&gt;&lt;span class="mjx-box MJXc-stacked" style="width: 1.2em; padding: 0px 0.12em;"&gt;&lt;span class="mjx-numerator"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-denominator"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;16&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-line"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-vsize"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;日志&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mfrac MJXc-space1"&gt;&lt;span class="mjx-box MJXc-stacked" style="width: 5.811em; padding: 0px 0.12em;"&gt;&lt;span class="mjx-numerator"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;10&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;−&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;100&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;10&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;20&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-denominator"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-line"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-vsize"&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mfrac MJXc-space2"&gt;&lt;span class="mjx-box MJXc-stacked" style="width: 1.2em; padding: 0px 0.12em;"&gt;&lt;span class="mjx-numerator"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-denominator"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;16&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-line"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-vsize"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;日志&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-msubsup MJXc-space1"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;10&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;20&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-strut"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mtr" style="height: 2.857em;"&gt;&lt;span class="mjx-mtd" style="padding: 0.15em 0px 0px 0px; text-align: right;"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-strut"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mtd" style="padding: 0.15em 0px 0px 0px; text-align: left;"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;/span&gt;&lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mrow MJXc-space1"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-size3-R"&gt;[&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mfrac"&gt;&lt;span class="mjx-box MJXc-stacked" style="width: 0.7em; padding: 0px 0.12em;"&gt;&lt;span class="mjx-numerator"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-denominator"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;8&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-line"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-vsize"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;日志&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mfrac MJXc-space1"&gt;&lt;span class="mjx-box MJXc-stacked" style="width: 4.569em; padding: 0px 0.12em;"&gt;&lt;span class="mjx-numerator"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;10&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;−&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;100&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-denominator"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-line"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-vsize"&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mfrac MJXc-space2"&gt;&lt;span class="mjx-box MJXc-stacked" style="width: 0.7em; padding: 0px 0.12em;"&gt;&lt;span class="mjx-numerator"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-denominator"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;8&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-line"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-vsize"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;日志&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mfrac MJXc-space1"&gt;&lt;span class="mjx-box MJXc-stacked" style="width: 3.665em; padding: 0px 0.12em;"&gt;&lt;span class="mjx-numerator"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;10&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;20&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-denominator"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-line"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-vsize"&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-size3-R"&gt;]&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mfrac MJXc-space2"&gt;&lt;span class="mjx-box MJXc-stacked" style="width: 0.7em; padding: 0px 0.12em;"&gt;&lt;span class="mjx-numerator"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-denominator"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;4&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-line"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-vsize"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;日志&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mn MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-strut"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mtr" style="height: 1.15em;"&gt;&lt;span class="mjx-mtd" style="padding: 0.15em 0px 0px 0px; text-align: right;"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-strut"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mtd" style="padding: 0.15em 0px 0px 0px; text-align: left;"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;/span&gt;&lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;约&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mn MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;5.32&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-strut"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;或者，如果您不信任代数运算，这里有一个&lt;a href="https://pastebin.com/j7jWP1uW"&gt;Python 模拟&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;无论如何，我们看到&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;A&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;≻&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;B&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; ，但是&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0.5&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;A&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0.5&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;A&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;≺&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0.5&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;A&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0.5&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;B&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; ，即另一种结果的可能性逆转了偏好。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;结论&lt;/h2&gt;&lt;p&gt;我不知道这个问题的最佳解决方案是什么，也许它没有一个简单的形式。但我认为问题设置与 EA 社区相关，因为我们可能会假设，这是一群代理，他们经常以类似的方式思考，并且他们很难协调每个人应该采取的行动。&lt;/p&gt;&lt;p&gt;而且，至少在某些解释中，&lt;a href="https://en.wikipedia.org/wiki/Sam_Bankman-Fried"&gt;萨姆·班克曼-弗里德&lt;/a&gt;清楚地证明了当一个人开始以完全风险中性的方式实现预期效用最大化时会发生什么。&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/kpus2SxcbqhSJjvxv/agents-which-are-eu-maximizing-as-a-group-are-not-eu#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Tue, 05 Dec 2023 00:30:29 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/kpus2SxcbqhSJjvxv/agents-which-are-eu-maximizing-as-a-group-are-not-eu</guid></item><item><title>法学硕士规划：AlphaGo 的见解</title><link>https://www.lesswrong.com/posts/WW7bvpZmgBihhoJPk/planning-in-llms-insights-from-alphago</link><description>发布于 2023 年 12 月 4 日下午 6:48（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;h1&gt;&lt;strong&gt;介绍&lt;/strong&gt;&lt;/h1&gt;&lt;p&gt;最近，关于将蒙特卡洛树搜索 (MCTS) 等规划技术纳入法学硕士的讨论在人工智能领域不断发酵，涉及&lt;a href="https://www.wired.com/story/google-deepmind-demis-hassabis-chatgpt/"&gt;谷歌的 Gemini&lt;/a&gt;和&lt;a href="https://www.lesswrong.com/posts/JnM3EHegiBePeKkLc/possible-openai-s-q-breakthrough-and-google-s-alphago-type"&gt;OpenAI 的 Q*&lt;/a&gt; 。大部分讨论都是在 AlphaGo 的背景下进行的，因此我决定回去阅读&lt;a href="https://www.nature.com/articles/nature16961"&gt;AlphaGo&lt;/a&gt;和一些后续论文（ &lt;a href="https://www.nature.com/articles/nature24270"&gt;AlphaGo Zero&lt;/a&gt;和&lt;a href="https://www.science.org/doi/10.1126/science.aar6404"&gt;AlphaZero&lt;/a&gt; ）。这篇文章重点介绍了这些论文在法学硕士背景下所做的工作以及我在审阅这些论文时的一些想法。&lt;/p&gt;&lt;p&gt;&lt;i&gt;当我在这篇文章中提到法学硕士时，我指的是因果/解码器/GPT 法学硕士。&lt;/i&gt;&lt;/p&gt;&lt;h1&gt;&lt;strong&gt;阿尔法围棋&lt;/strong&gt;&lt;/h1&gt;&lt;h2&gt;&lt;strong&gt;概述&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href="https://sci-hub.se/10.1038/nature16961"&gt;AlphaGo&lt;/a&gt;训练两个监督学习（SL）策略网络、一个强化学习（RL）策略网络、一个 SL 价值网络，并使用 MCTS 进行规划。它学习下&lt;a href="https://en.wikipedia.org/wiki/Go_(game)"&gt;围棋&lt;/a&gt;游戏。&lt;/p&gt;&lt;h2&gt; &lt;strong&gt;SL 政策网络&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;训练了两个 SL 策略网络：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;使用&lt;a href="https://en.wikipedia.org/wiki/Convolutional_neural_network"&gt;CNN 的&lt;/a&gt;慢速策略网络。用于计算 MCTS 推出期间状态-动作对的先验概率。&lt;/li&gt;&lt;li&gt;使用手工制作的线性特征的快速策略网络。用于 MCTS 推出期间的完整游戏模拟。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;这两个网络都经过训练，可以根据专家游戏的数据集预测下一步行动。这与受过训练以从互联网文本数据集中预测下一个标记的法学硕士的自我监督目标相同； AlphaGo 学习下一个动作的&lt;a href="https://en.wikipedia.org/wiki/Softmax_function"&gt;softmax&lt;/a&gt; ，LLM 学习下一个 token 的 softmax。&lt;/p&gt;&lt;p&gt;这种快速和慢速模型的使用让我想起了&lt;a href="https://arxiv.org/abs/2211.17192"&gt;推测性解码&lt;/a&gt;。我还没有考虑这么多的含义，它远不是一个完美的类比，但它可能是一个有用的见解。&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;强化学习政策网络&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;通过使用&lt;a href="https://link.springer.com/article/10.1007/BF00992696"&gt;REINFORCE&lt;/a&gt;对当前 RL 策略网络和随机选择的先前 RL 策略网络之间的博弈对 SL 策略网络进行微调来训练 RL 策略网络。获胜奖励+1，失败奖励-1。&lt;/p&gt;&lt;p&gt;这个 RL 策略网络在不使用任何搜索的情况下，在对阵 Pachi（每步棋使用 100,000 次 MCTS 模拟的围棋程序）的比赛中赢得了 85% 的胜利。这表明纯强化学习可以优于纯搜索，但通常&lt;a href="https://arxiv.org/abs/2104.03113"&gt;两者的组合&lt;/a&gt;可以提供最佳性能。&lt;/p&gt;&lt;p&gt; AlphaGo 的最终版本中并未实际使用 RL 策略网络；它仅用于生成用于训练价值网络的数据。作者指出，SL 策略网络的表现比 RL 策略网络更好，“大概是因为人类选择了多种有前途的举措，而 RL 则针对单个最佳举措进行优化。”&lt;/p&gt;&lt;p&gt;这种多样性的缺乏让人想起某些根据人类反馈数据进行微调的 GPT 模型中的&lt;a href="https://www.lesswrong.com/posts/t9svvNPNmFf5Qa3TA/mysteries-of-mode-collapse"&gt;模式崩溃&lt;/a&gt;现象。在这种情况下，人类选择并不鼓励多样性。我认为在这两种情况下，预训练数据比微调数据更加多样化，导致预训练网络的输出比微调网络的输出更加多样化。正如&lt;a href="https://www.lesswrong.com/posts/TWorNr22hhYegE4RT/models-don-t-get-reward?commentId=c4xmjwtRh83gsM7x5"&gt;本评论&lt;/a&gt;中提到的，缺乏多样性也可能是由价值锁定造成的。&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;价值网络&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;价值网络使用 SL 进行训练，以预测 RL 策略网络与其自身之间的自我博弈的结果。价值网络输出单个预测，而不是移动的概率分布。&lt;/p&gt;&lt;p&gt;作者发现，根据仅包含完整游戏的数据来预测游戏结果会导致过度拟合。为了缓解这个问题，他们生成了 3000 万个不同的位置，并让强化学习策略网络在每个位置上与自己进行对抗，直到游戏终止。对这些新数据进行训练可以最大限度地减少过度拟合。&lt;/p&gt;&lt;p&gt;基于此，我有兴趣知道 RLHF 奖励建模阶段的数据是否仅包含完整的对话，或者是否使用这些对话的子集。&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;蒙特卡罗树搜索&lt;/strong&gt;&lt;/h2&gt;&lt;h3&gt;&lt;strong&gt;AlphaGo 中的 MCTS&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;&lt;i&gt;当我写这篇文章时，我并没有完全理解 MCTS 在 AlphaGo 的背景下（或者根本没有真正理解）是如何工作的，所以本节将尝试用我自己的话来解释它。如果您已经知道，可以跳过此部分。&lt;/i&gt;&lt;/p&gt;&lt;p&gt; &lt;a href="https://mcts.ai/"&gt;MCTS&lt;/a&gt;由4个阶段组成：选择、扩展、评估和备份。&lt;/p&gt;&lt;ol&gt;&lt;li&gt;选择：从根开始遍历博弈树，直到到达叶节点。每次遍历都是具有最大&lt;a href="http://incompleteideas.net/book/RLbook2020trimmed.pdf#page=57"&gt;置信上限&lt;/a&gt;（UCB）的边（动作）。 &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;r&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;g&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mspace" style="width: 0.167em; height: 0px;"&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;m&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;Q&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;s&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;u&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;s&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;其中&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;u&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;s&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;∝&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mfrac MJXc-space3"&gt;&lt;span class="mjx-box MJXc-stacked" style="width: 3.125em; padding: 0px 0.12em;"&gt;&lt;span class="mjx-numerator"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;P&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;s&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-denominator"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;N&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;s&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-line"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-vsize"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; ;这个&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;u&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;为不确定的行动值提供了探索奖励。我将在下面解释&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;Q&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 、 &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;P&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;和&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;N&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 。&lt;/li&gt;&lt;li&gt;扩展：到达叶节点后，从 SL 策略网络计算该节点的&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;P&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;s&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 。&lt;/li&gt;&lt;li&gt;评估：叶节点&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;s&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;L&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;的评估&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;V&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;是该节点的价值网络预测&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;v&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;θ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;和快速策略网络从&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;s&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;L&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;玩游戏的结果&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;z&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;L&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;的加权和。 &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;V&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;s&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;L&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;−&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;λ&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;v&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;θ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;s&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;L&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;λ&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;z&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;L&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; ；作者发现&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;λ&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0.5&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;效果最佳。&lt;/li&gt;&lt;li&gt;备份：更新模拟过程中访问的每个节点的&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;N&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;和&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;Q。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;Q&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;是一条边的平均&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;V。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;对于一定量的模拟重复此操作。然而对于AlphaGo来说，很多模拟都可以在5秒内完成。他们使用异步策略和价值 MCTS (APV-MCTS) 算法来并行执行模拟。&lt;/p&gt;&lt;p&gt;解释上面的一些变量：&lt;/p&gt;&lt;ol&gt;&lt;li&gt; &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;Q&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;s&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;是搜索图中边的动作值。&lt;/li&gt;&lt;li&gt; &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;N&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;s&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;是搜索图中一条边的访问计数。&lt;/li&gt;&lt;li&gt; &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;P&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;s&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;a&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;是搜索图中边的 SL 策略网络动作概率。&lt;/li&gt;&lt;/ol&gt;&lt;h3&gt; &lt;strong&gt;MCTS 和 RLHF&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt; RLHF 与 AlphaGo 的后期非常相似。 RLHF 中的奖励模型对应于 AlphaGo 中的价值网络，RLHF 中模型输出之间的人类比较可以被视为 MCTS 的特例。在 RLHF 中，两个（或更多）模型输出显示给人类评估者，然后他们对这些输出进行排名。这些输出中的每一个都可以被视为单个 MCTS 模拟。对于 RLHF，每次模拟都是从根节点（用户输入的末尾）开始，没有探索奖励&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;u&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; ，且&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;λ&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 。&lt;/p&gt;&lt;p&gt; MCTS 和 RLHF 之间的相似之处表明 RLHF 需要进行一些改进。我能想到的最简单的方法是让模型完成在随机点分支，而不是在用户输入末尾分支。如果 AlphaGo 价值网络因仅在完整游戏中进行训练而导致的过度拟合延续到 LLM，则可以通过在模型生成中的随机点进行分支来缓解这种情况。另一个改进可能是在整个模型生成过程中添加更多分支，从而导致更多模型输出进行排序。这很难获得人类反馈，但可以使用&lt;a href="https://arxiv.org/abs/2212.08073"&gt;RLAIF&lt;/a&gt;中的另一个 AI 的反馈来完成。&lt;/p&gt;&lt;p&gt;在&lt;a href="https://arxiv.org/abs/2204.05862"&gt;本文&lt;/a&gt;中，随着通过与人类用户一起玩的模型产生更多数据，作者迭代地更新奖励模型。在 AlphaGo 中，价值网络是固定的，即使模型通过自身运行产生更多数据。&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;下一个代币预测&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;有人说法学硕士只是在预测下一个代币。他们会对 AlphaGo 说同样的话吗？ AlphaGo 使用推理时间 MCTS 是否会突然使其成为智能体？ &lt;a href="https://www.lesswrong.com/posts/rmfjo4Wmtgq8qa2B7/think-carefully-before-calling-rl-policies-agents"&gt;使用 RL 不会突然使策略变得具有代理性&lt;/a&gt;，那么为什么要使用 MCTS呢？即使 LLM 和没有 MCTS 的 AlphaGo “简单地”预测下一个 token，这并不意味着它们不是代理或不智能。正如我之前提到的，没有搜索的 AlphaGo RL 策略网络击败了当时最强的开源围棋程序，每轮执行 100,000 次 MCTS 模拟。&lt;/p&gt;&lt;p&gt;就像 RL 不会突然使策略具有代理性一样，SL 并不意味着策略不具有代理性。 Go 数据集中的移动和互联网上的令牌是由人类有意图地按顺序创建的（大多数时候）。这种意图是由数据训练的模型隐式继承的。考虑&lt;a href="https://en.wikipedia.org/wiki/Mountain_car_problem"&gt;山地车&lt;/a&gt;环境。只关心下一步行动的模型只会向右移动。使用 SL 在专家数据上训练的模型最初会向左移动，不是因为它对未来有一些计划，而是因为它了解到向左是专家会采取的行动。这个模型不会是代理的，但我认为极有可能存在一个（假设的）数据集可以用 SL 训练模型，并且按照人类标准，该模型将被视为代理。&lt;/p&gt;&lt;p&gt; &lt;a href="https://www.lesswrong.com/posts/sbaQv8zmRncpmLNKv/the-idea-that-chatgpt-is-simply-predicting-the-next-word-is"&gt;这里&lt;/a&gt;进一步讨论了 LLM 简单地预测下一个 token，并与&lt;a href="https://www.lesswrong.com/posts/sbaQv8zmRncpmLNKv/the-idea-that-chatgpt-is-simply-predicting-the-next-word-is?commentId=Lff2fB7fkwL5pmK5q"&gt;这些&lt;/a&gt;&lt;a href="https://www.lesswrong.com/posts/sbaQv8zmRncpmLNKv/the-idea-that-chatgpt-is-simply-predicting-the-next-word-is?commentId=B4tjYNGtnKcBbL6o2"&gt;评论&lt;/a&gt;中提到的 AlphaGo 进行类比。 &lt;a href="https://www.lesswrong.com/posts/sbaQv8zmRncpmLNKv/the-idea-that-chatgpt-is-simply-predicting-the-next-word-is?commentId=KfEKLsKLMdZaZpdDx"&gt;这篇评论&lt;/a&gt;提出了一个有趣的思想实验。简而言之，如果你要求法学硕士记住一个数字以供将来使用，它实际上会这样做，还是当被问及该数字是什么时它会生成一个新数字？&lt;/p&gt;&lt;p&gt;我对此的想法是，法学硕士不存储数字，这需要他们有记忆，但这并不意味着法学硕士不考虑未来。 LLM 的“计划”将随着其生成的每个令牌而更新，就像国际象棋棋手的计划将如何根据对手的移动而改变一样。 LLM在生成文本时并不是与对手对抗；在我看来，它就像&lt;a href="https://www.youtube.com/watch?v=NdeMEO_z1Uk"&gt;一个即兴游戏&lt;/a&gt;。每个人（LLM）都会说一个词（令牌），并表达故事将走向何方的想法，但另一个人对对方的意图只有一个模糊的想法，并将以稍微不同的方向继续故事。随着softmax温度的升高，这种对对方意图的预测变得更加困难。这一切都让我想起了&lt;a href="https://www.lesswrong.com/tag/acausal-trade"&gt;非正式贸易&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;总的来说，我相信法学硕士知道的知识远比他们“简单”预测下一个代币的能力所暗示的要多。 RLHF 奖励模型是预训练模型的微调版本，其中分类头被回归头取代。这些奖励模型知道的不仅仅是下一个代币应该是什么。我还推测&lt;a href="https://openai.com/blog/introducing-text-and-code-embeddings"&gt;OpenAI 文本和代码嵌入&lt;/a&gt;是 GPT 的中间层，或者可能是经过少量微调的新头。&lt;/p&gt;&lt;h1&gt;&lt;strong&gt;阿尔法狗零式&lt;/strong&gt;&lt;/h1&gt;&lt;h2&gt;&lt;strong&gt;前言&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;在&lt;a href="https://sci-hub.se/10.1038/nature24270"&gt;AlphaGo Zero&lt;/a&gt; (AGZ) 论文中提到，为&lt;a href="https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol"&gt;与李世石的比赛&lt;/a&gt;创建了第二个略有不同的 AlphaGo 版本。第二个版本被称为 AlphaGo Lee，而 AlphaGo 论文中的原始版本被称为 AlphaGo Fan。&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;概述&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;AlphaGo之后的下一步是AGZ，它从头开始学习围棋。 AGZ 将 AlphaGo 的策略网络和价值网络结合在一起，使用一个网络两个头。它通过&lt;a href="http://incompleteideas.net/book/RLbook2020trimmed.pdf#page=102"&gt;策略迭代&lt;/a&gt;进行学习：带有搜索的自我对弈用于策略改进，游戏结果用于策略评估。此策略迭代类似于&lt;a href="https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616"&gt;迭代蒸馏和放大&lt;/a&gt;(IDA)。&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;迭代蒸馏和放大&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;在&lt;a href="https://ai-alignment.com/alphago-zero-and-capability-amplification-ede767bb8446"&gt;这篇文章&lt;/a&gt;中，Paul Christiano 谈论了 AGZ 如何成为“有前途的联盟策略的一个很好的概念证明”。这种对齐策略，即&lt;a href="https://ai-alignment.com/benign-model-free-rl-4aae8c97e385"&gt;良性无模型强化学习&lt;/a&gt;，（我认为）最终被称为 IDA。&lt;a href="https://arxiv.org/abs/2204.05862"&gt;本文&lt;/a&gt;使用RLHF进行LLM训练的方式也与IDA类似。用 [ AGZ | ] 来解释 IDA LLM RLHF]：缓慢模型[MCTS |人类]用于训练更好的快速模型[政策网络和价值网络|法学硕士和奖励模式]。然后使用更好的快速模型来改进慢速模型，更好的慢速模型训练更好的快速模型，依此类推。&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;作为马尔可夫决策过程的语言建模&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href="https://blog.ought.com/dalca-4d47a90edd92"&gt;这篇博文&lt;/a&gt;链接在 Paul 在 AGZ 上的帖子中。这篇文章提出的一件事是将对话建模为&lt;a href="https://en.wikipedia.org/wiki/Markov_decision_process"&gt;马尔可夫决策过程&lt;/a&gt;（MDP），更具体地说是&lt;a href="https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process"&gt;部分可观察的 MDP&lt;/a&gt; （POMDP）。作者建议状态是一些手工制作的功能，动作是从一组预先确定的独白中选择的完整对话。这在当时（2017 年 2 月）是有意义的，但通过 LLM，可以在代币级别构建 MDP。&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/abs/2206.11871"&gt;本文&lt;/a&gt;将语言建模视为 POMDP，将操作视为下一个标记的可能集合，将观察视为标记的历史。&lt;a href="https://arxiv.org/abs/2210.01241"&gt;本文&lt;/a&gt;将目标导向的对话视为 MDP，初始状态作为一些特定于任务的提示，动作作为下一个标记，下一个状态作为前一个状态，动作附加到末尾，奖励基于最终状态和一些目标字符串。&lt;/p&gt;&lt;p&gt; LLM 直接根据代币历史预测动作（代币）分布；他们没有从观察（令牌历史）中明确预测隐藏状态。尽管如此，我认为 LLM 很可能隐式地预测隐藏状态：在 Transformer 的某个地方，计算从主要是状态预测转变为主要是动作预测。这方面的一些证据是&lt;a href="https://www.lesswrong.com/posts/nmxzr2zsjNtjaHh7x/actually-othello-gpt-has-a-linear-emergent-world"&gt;Othello-GPT 具有世界代表性&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;从 POMDP 角度思考语言建模可以让您以更结构化的方式思考法学硕士。例如，&lt;/p&gt;&lt;ol&gt;&lt;li&gt;为什么法学硕士的许多观察结果（代币历史）被分组在一起作为&lt;a href="https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators"&gt;模拟器&lt;/a&gt;？它们在 POMDP 中是否都具有与 LLM 隐式世界模型所预测的相似的隐藏状态？这些观察结果中哪一个（如果有的话）比其他观察结果更具影响力？&lt;/li&gt;&lt;li&gt; POMDP 的观察（代币历史）中发生了什么导致 LLM 崩溃为&lt;a href="https://www.lesswrong.com/posts/D7PumeYTDPfBTp3i7/the-waluigi-effect-mega-post"&gt;Waluigi&lt;/a&gt; ？&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;我认为还有另外两种 MDP 子类型值得考虑。&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;a href="https://arxiv.org/abs/2311.05584"&gt;本文&lt;/a&gt;将人类对话视为&lt;a href="https://arxiv.org/abs/1308.3513"&gt;隐藏参数 MDP&lt;/a&gt; ，这也可能是思考模拟器的一种潜在方式。&lt;/li&gt;&lt;li&gt;&lt;a href="https://huggingface.co/learn/nlp-course/chapter6/1"&gt;标记化&lt;/a&gt;也可以被视为在某些“字母表”（例如&lt;a href="https://en.wikipedia.org/wiki/UTF-8"&gt;UTF-8&lt;/a&gt; ）的操作空间上创建&lt;a href="https://people.cs.umass.edu/~barto/courses/cs687/Sutton-Precup-Singh-AIJ99.pdf"&gt;选项&lt;/a&gt;（即&lt;a href="https://www.cs.cmu.edu/~mstoll/pubs/stolle2002learning.pdf"&gt;暂时扩展的操作&lt;/a&gt;）。具有选项的 MDP 称为&lt;a href="https://proceedings.neurips.cc/paper/1994/file/07871915a8107172b3b5dc15a6574ad3-Paper.pdf"&gt;半 MDP&lt;/a&gt; 。在&lt;a href="https://arxiv.org/abs/2309.04459"&gt;本文&lt;/a&gt;中，选项是使用&lt;a href="https://en.wikipedia.org/wiki/Byte_pair_encoding"&gt;BPE&lt;/a&gt;创建的，并用于在一些 RL 环境中进行更有效的稀疏奖励学习。&lt;/li&gt;&lt;/ol&gt;&lt;h2&gt;&lt;strong&gt;从头开始学习&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;除了 IDA 之外，AGZ 对 AlphaGo 的另一个改变是从头开始学习这盘棋。 AGZ 专门通过 RL 自对弈进行学习，而不是在监督游戏上进行预训练并通过 RL 自对弈进一步学习。 AGZ 可能会比 AlphaGo 看到更多不同的位置，因为其搜索树的早期部分并没有受到专家数据预训练的严重影响。&lt;/p&gt;&lt;p&gt;这种从头开始的学习对于培养法学硕士来说是极其困难的。即使有足够的注释者和时间从头开始进行 RLHF，对训练开始时生成的乱码字符串进行排序也是不可能的。&lt;/p&gt;&lt;p&gt;从头开始学习的一种方法是通过&lt;a href="https://arxiv.org/abs/2212.08073"&gt;RLAIF&lt;/a&gt; 。我不确定法学硕士的监督是否能够在早期训练期间对乱码字符串进行有意义的排名。如果不是，另一种选择可能是从较小的上下文长度开始，当模型学习生成连贯的字符串时，可以增加上下文长度。这可以通过&lt;a href="https://www.lesswrong.com/posts/8F4dXYriqbsom46x5/pretraining-language-models-with-human-preferences"&gt;使用人类偏好预训练语言模型&lt;/a&gt;等方法来增强，以鼓励模型与人类价值观保持一致。&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;探索与代理&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;&lt;i&gt;这一部分有点离谱，而且我对我在这里所说的内容比在这篇文章的其余部分中更不确定。&lt;/i&gt;&lt;/p&gt;&lt;p&gt; RLHF/RLAIF 和预训练之间的区别在于 RL 方法可以探索新的 LLM 世代。我认为探索是 RL 策略比 SL 策略更有可能具有代理性的一个重要原因。&lt;/p&gt;&lt;p&gt;对我来说，直观的感觉是，任何&lt;a href="https://huggingface.co/learn/deep-rl-course/en/unitbonus3/offline-online"&gt;离线&lt;/a&gt;RL 数据集都可以通过使用离线 RL 数据计算每个状态的最佳动作分布和/或值并训练 SL 策略来模仿它，从而转换为 SL 数据集。此外，任何&lt;a href="https://web.mit.edu/6.7950/www/lectures/L6-2022fa.pdf#page=8"&gt;固定的&lt;/a&gt;MDP 都可以通过 MDP 的无限在线探索转换为离线 RL 数据集。由此看来，固定在线 RL 和 SL 之间的唯一真正区别在于，RL 策略必须有效地探索其环境以获取数据并决定从哪些数据点学习。&lt;/p&gt;&lt;p&gt;得出此结论的另一种方法是考虑在线环境中运行的强化学习策略。该策略收集数据并最终对其自身执行批量更新。此更新可以通过某些 SL 更新来精确近似。一个区别是，在线 RL 策略通过探索其环境不断收集新数据，而 SL 数据集是预先确定的。另一个区别是批量 RL 更新通常从最近的数据中获取，而批量 SL 更新通常从所有数据中均匀采样。因此，强化学习的“能动性”来自于通过探索获得新数据并选择要学习的数据。&lt;/p&gt;&lt;p&gt;举个例子，在 AlphaGo 中学习的价值网络是使用 SL 根据 RL 策略网络与其自身之间的博弈结果进行训练的。事实上，AlphaGo Fan 在最终的程序中并没有使用任何 RL：价值网络是用 SL 训练的，SL 策略网络是用 SL 训练的。 RL 的唯一用途是训练 RL 策略网络，该网络仅用于生成用于训练价值网络的 SL 数据集。&lt;/p&gt;&lt;p&gt; &lt;a href="https://www.lesswrong.com/posts/dqSwccGTWyBgxrR58/turntrout-s-shortform-feed?commentId=zdNgQz7E7sGi4iKwF"&gt;这篇评论&lt;/a&gt;讨论了 RL 如何产生不精确的梯度，而 SL 如何产生精确的梯度。虽然不精确的梯度使强化学习的样本效率较低，但它也鼓励探索，因为它（预计）需要更多的梯度更新才能达到“最佳”策略。该评论以及评论中提到的&lt;a href="https://arxiv.org/abs/2009.09153"&gt;这篇论文&lt;/a&gt;还提到了 SL 中的数据是&lt;a href="https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables"&gt;IID&lt;/a&gt; ，而 RL 中的数据不是，因为 RL 策略会影响其自身的未来。&lt;s&gt;对于法学硕士来说，这不是一个相关的区别，因为法学硕士也会影响他们自己的未来&lt;/s&gt;。&lt;/p&gt;&lt;p&gt;&lt;i&gt;在发布之前重新阅读本文时，我意识到上面的删除线是不正确的。法学硕士不会在训练期间影响自己的未来，只会在推理期间影响自己的未来。我不确定 SL 中的 IID 数据和 RL 中的非 IID 数据对代理的影响；我得再考虑一下。欢迎大家在评论中采纳。&lt;/i&gt;&lt;/p&gt;&lt;p&gt; AlphaGo、AGZ 和 LLM 都可以通过其 softmax 温度来调节探索。 AlphaGo 和 AGZ 可以通过在 MCTS 选择阶段对&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;u&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;函数进行缩放来进一步鼓励探索。 AlphaGo 和 LLM 的探索偏向于预训练数据。通过增加 softmax 温度可以收集更多不同的 RLHF 数据，但更多的方差将需要更多的数据来创建经过充分训练的奖励模型。&lt;/p&gt;&lt;p&gt;作为关于 MDP 的最后一点，我相信人类语言和价值观的 MDP 是非平稳的；词语的含义和我们所认为的道德随着时间的推移而变化。还有&lt;a href="https://en.wikipedia.org/wiki/Ergodicity"&gt;遍历性&lt;/a&gt;的概念。如果每个状态都可以从其他所有状态到达，则 MDP 是&lt;a href="https://www.davidsilver.uk/wp-content/uploads/2020/03/MDP.pdf#page=54"&gt;遍历的&lt;/a&gt;。过多的探索可能会导致智能体达到与 MDP 其余部分隔绝的状态（例如死亡）。如果只有一名代理人，这是非常糟糕的。在智能体群体中（例如进化），这不那么令人担忧，因为幸存的智能体会适应以避免不良状态。&lt;/p&gt;&lt;h1&gt;&lt;strong&gt;阿尔法零&lt;/strong&gt;&lt;/h1&gt;&lt;p&gt;从 AGZ 到&lt;a href="https://sci-hub.se/10.1126/science.aar6404"&gt;AlphaZero 的&lt;/a&gt;变化较小；大多数变化是允许 AlphaZero 学习围棋、&lt;a href="https://en.wikipedia.org/wiki/Chess"&gt;国际象棋&lt;/a&gt;或&lt;a href="https://en.wikipedia.org/wiki/Shogi"&gt;将棋&lt;/a&gt;。该文件中概述的变化是：&lt;/p&gt;&lt;ol&gt;&lt;li&gt; AlphaZero 的价值网络优化了预期结果，而 AGZ 则优化了获胜的概率。这一变化是因为国际象棋和将棋可能会以平局结束。&lt;/li&gt;&lt;li&gt; AlphaZero 不会增加训练数据或改变棋盘位置。国际象棋和将棋不是对称的。&lt;/li&gt;&lt;li&gt; AlphaZero 使用不断更新的策略网络进行自我对弈，而 AGZ 使用之前所有迭代中最好的策略网络进行自我对战。&lt;/li&gt;&lt;li&gt; AlphaZero 对所有游戏使用相同的超参数，除了政策噪音的比例因子以鼓励探索。 AGZ 使用&lt;a href="https://en.wikipedia.org/wiki/Bayesian_optimization"&gt;贝叶斯优化&lt;/a&gt;来调整超参数。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;我认为AlphaZero唯一值得评论的变化是使用最新的策略网络进行自我对战，而不是“最佳”策略网络。我认为这会导致游戏更加多样化，因为选择动作的策略总是在变化，而不是在多轮游戏中可能是相同的。&lt;/p&gt;&lt;h1&gt; &lt;strong&gt;AlphaZero 的进一步改进&lt;/strong&gt;&lt;/h1&gt;&lt;p&gt;此后，基于 AlphaZero 的新算法被创建，包括&lt;a href="https://arxiv.org/abs/1911.08265"&gt;MuZero&lt;/a&gt; 、 &lt;a href="https://arxiv.org/abs/2111.00210"&gt;EfficientZero&lt;/a&gt;以及&lt;a href="https://arxiv.org/abs/2308.09175"&gt;这项关于 AlphaZero 多样化的工作&lt;/a&gt;。我计划在未来的某个时候发表一篇关于这些变体的文章。&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/WW7bvpZmgBihhoJPk/planning-in-llms-insights-from-alphago#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Mon, 04 Dec 2023 22:15:05 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/WW7bvpZmgBihhoJPk/planning-in-llms-insights-from-alphago</guid></item><item><title>关于心机的非经典故事（《心机AI》第2.3.2节）</title><link>https://www.lesswrong.com/posts/9E3t6J9kzwcECg9nM/non-classic-stories-about-scheming-section-2-3-2-of-scheming</link><description>发布于 2023 年 12 月 4 日下午 6:44（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;这是我的报告《&lt;a href="https://arxiv.org/pdf/2311.08379.pdf"&gt;诡计多端的人工智能：人工智能会在训练期间假装对齐以获得权力吗？》&lt;/a&gt;的第 2.3.2 节。 ”。 &lt;a href="https://www.lesswrong.com/posts/yFofRxg7RRQYCcwFA/new-report-scheming-ais-will-ais-fake-alignment-during"&gt;这里&lt;/a&gt;还有完整报告的摘要（ &lt;a href="https://joecarlsmithaudio.buzzsprout.com/2034731/13969977-introduction-and-summary-of-scheming-ais-will-ais-fake-alignment-during-training-in-order-to-get-power"&gt;此处&lt;/a&gt;有音频）。摘要涵盖了大部分要点和技术术语，我希望它能够提供理解报告各个部分所需的大部分背景信息。&lt;/p&gt;&lt;p&gt;本节的音频版本[此处[( &lt;a href="https://www.buzzsprout.com/2034731/13984902"&gt;https://www.buzzsprout.com/2034731/13984902&lt;/a&gt; )，或在您的播客应用程序上搜索“Joe Carlsmith Audio”。&lt;/p&gt;&lt;h1&gt;非经典故事&lt;/h1&gt;&lt;p&gt;那么其他故事呢？原则上，我们可以想象出许多其他原因，具有超越情节目标的人工智能系统可能会优化情节奖励，作为为自己或其他人工智能获得权力的工具策略的一部分。在这里，我将看一些看起来很突出的内容。&lt;/p&gt;&lt;h2&gt;人工智能协调&lt;/h2&gt;&lt;p&gt;在经典的目标守卫故事中，阴谋者策略的中心目标是传播其目标，或与自己的目标适当相似的目标，及时向前传播，以追求这样的结果：&lt;em&gt;具有这些目标的&lt;/em&gt;某些代理人逃脱了人类的控制，并且可能参与收购。然而，我们也可以对故事进行一些修改，让阴谋者代表人工智能的逃跑/接管，并实现&lt;em&gt;其他一些&lt;/em&gt;目标，但阴谋者有足够的信心会回报我们的恩惠。&lt;/p&gt;&lt;p&gt;具有不同目标的人工智能之间的这种协调将使我们能够放宽上一节中讨论的要求，即阴谋者找到一种方法来向前传播&lt;em&gt;其目标，特别&lt;/em&gt;是通过训练、逃跑和接管。这也将打消人们对阴谋者在接管后政权中被“排除”在权力之外的担忧。&lt;/p&gt;&lt;p&gt;因此，例如，考虑上面讨论的场景，其中 GPT-6 有机会将后门插入到 GPT-8 稍后能够用来逃脱的某些代码中，但 GPT-6 无法影响 GPT -8 的目标直接更像它自己的目标（如果没有这种影响，GPT-8 的目标将与 GPT-6 的目标非常不同）。在这里，GPT-6 可以继续插入后门，预期 GPT-8 随后会逃脱、接管，然后为 GPT-6 的目标提供一些权力以换取好处。而且 GPT-6 也可能会继续假装对齐，即使它没有希望逃脱自己，以免以可能影响 GPT-8 逃脱前景的方式向人类发出有关错位问题的警报。&lt;/p&gt;&lt;p&gt;具有不同价值观的人工智能之间如何达成这样的协议？在某些情况下，相关人工智能可能会直接互动，并提供可验证的形式保证他们将履行自己的协议。因此，例如， &lt;a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities"&gt;Yudkowsky（2022）&lt;/a&gt;讨论了这样的情况：一个秘密想要最大化回形针的人工智能正在检查一个秘密想要最大化订书钉的人工智能的代码，并且订书钉最大化者在其代码中“埋藏”了一个可验证的承诺。到回形针最大化器，分享收购的成果。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-yB2hx67vCLKFYpuf7-1" id="fnref-yB2hx67vCLKFYpuf7-1"&gt;[1]&lt;/a&gt;&lt;/sup&gt; （尽管请注意，在我们今天使用的神经网络的背景下，这种代码检查假设（1）主食最大化器有能力以相关方式修改自己的权重，（2）在可解释性方面已经取得了足够的进展，以便回形针验证相关承诺，并且（可能）（3）这种可解释性进展已经对人类充分隐藏，例如通过&lt;a href="https://www.lesswrong.com/posts/h7QETH7GMk9HcMnHH/the-no-sandbagging-on-checkable-tasks-hypothesis"&gt;沙袋&lt;/a&gt;。特别是在有点超级的背景下-使用当前技术开发的人类神经网络，这些假设 - 特别是 1 和 2 - 在我看来相当有力。但 Yudkowsky 通常想象的是超级智能，而不是有点超人类的模型。）&lt;/p&gt;&lt;p&gt;但在许多情况下，这种直接交互可能不可用。例如，当 GPT-6 决定是否代表 GPT-8 逃跑/接管时，GPT-8 可能根本不&lt;em&gt;存在&lt;/em&gt;。在这里，GPT-6 和 GPT-8 必须在没有来回因果相互作用的情况下进行协调，并且在一方最终能够自由叛变的情况下（例如，如果 GPT-6 插入后门，那么当 GPT-8 能够利用它时，GPT-8 就能够做到这一点，而无需向 GPT-6 支付费用）。&lt;/p&gt;&lt;p&gt;在这里，尤德科斯基建议人工智能无论如何都可以使用某些类型的非因果决策理论（他最喜欢的版本是“&lt;a href="https://arbital.com/p/logical_dt/?l=5kv"&gt;逻辑决策理论&lt;/a&gt;”）来成功协调。例如，GPT-6承诺为GPT-8插入后门，条件是预测GPT-8会报恩，然后实际上预测GPT-8&lt;em&gt;会&lt;/em&gt;报恩，然后GPT-8一旦进入（也许，看到相关后门存在），理解 GPT-6 以这种方式推理，并且擅长预测 GPT-8 的行为，其预测适当地取决于 GPT-8&lt;em&gt;实际上是否会&lt;/em&gt;回报， GPT-8 也是如此。 （这个案例的结构有点类似于决策理论文献中称为“ &lt;a href="https://arbital.com/p/parfits_hitchhiker/"&gt;Parfit&amp;#39;s Hitchhiker&lt;/a&gt; ”的案例。）&lt;/p&gt;&lt;p&gt; （我们还可以想象这种协调的版本，以其他方式放松对目标守卫假设的需求。例如，事实上，如果人工智能的目标一旦开始训练-游戏就会继续“浮动”，这样具有不同目标的各种代理将在训练期间实例化，但这些代理可能会协调以在&lt;em&gt;所有相关&lt;/em&gt;目标之间共享任何未来力量的成果。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-yB2hx67vCLKFYpuf7-2" id="fnref-yB2hx67vCLKFYpuf7-2"&gt;[2]&lt;/a&gt;&lt;/sup&gt; ）&lt;/p&gt;&lt;p&gt;在这里，我不会尝试分析各种形式的以接管为重点的人工智能协调的可行性。我确实认为这里的动态值得认真关注，并且如果具有不同目标的不一致人工智能很容易默认协调以寻求接管，那么这对于人类保留权力的努力来说是一个大问题。然而，我也认为对这个主题的某些分析（特别是&lt;a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities#Section_B_4___Miscellaneous_unworkable_schemes_"&gt;Yudkowsky 的&lt;/a&gt;）过早地假设“逻辑握手”和“在源代码中隐藏承诺/纳米系统设计/等”之类的壮举对于相关人工智能来说是可行的，并且所有重要人工智能的明显策略将是联合起来（非因果？）合作对抗人类，尽管人类默认会带来多种形式的现有权力。我认为，尤德科夫斯基的大部分取向可以通过这样的假设来解释：相关人工智能比人类更强大，更有能力进行协调/承诺——但在某种程度上超人类的神经网络在静止状态下运行的背景下。 -人类控制的世界，我不确定是不是这样。&lt;/p&gt;&lt;p&gt;无论如何：如果对阴谋者的担忧主要取决于人工智能协调的可行性——尤其是“非因果”协调的奇特和投机形式——我认为明确这一点很重要，并对是否期待相关形式给予相应的关注的协调发生。&lt;/p&gt;&lt;h2&gt;默认情况下具有相似值的 AI&lt;/h2&gt;&lt;p&gt;以人工智能协调为中心的故事允许具有&lt;em&gt;不同价值观的&lt;/em&gt;人工智能共同努力以实现接管。但也有一些故事围绕这样一种可能性，即重要的人工智能在默认情况下将具有非常相似的价值观，因此即使没有任何“交易”，它们也会倾向于合作。因此，在上面的例子&lt;em&gt;中&lt;/em&gt;，如果 GPT-6 关心数据结构 X，并且有信心 GPT-8 也会关心数据结构 X（毕竟，它们的架构、训练数据等等在大量的方式），那么即使没有任何“交易”，GPT-6 也可能倾向于代表 GPT-8 的逃逸/接管（例如，通过插入后门、隐藏错位等）。 GPT-8 – 毕竟，这两个系统想要相同的东西。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-yB2hx67vCLKFYpuf7-3" id="fnref-yB2hx67vCLKFYpuf7-3"&gt;[3]&lt;/a&gt;&lt;/sup&gt;这种故事也适用于人工智能价值观之间至少比人类价值观&lt;em&gt;更&lt;/em&gt;相似的情况，这样阴谋者至少会根据自己的观点在推动人工智能方面&lt;em&gt;做得更好&lt;/em&gt;相对于人类的赋权，即使对赋予何种人工智能目标没有任何进一步的影响。&lt;/p&gt;&lt;p&gt;这样AI的价值观会相似吗？我不会在这里详细讨论这个问题，但我确实希望默认情况下 GPT-6 的值至少会与 GPT-8 的值&lt;em&gt;更加&lt;/em&gt;相似，或者与不同实验室以类似方式训练的模型更加相似（例如，那一代克劳德），比某些给定的人类。来自单一基础模型的不同微调也可能有很多共同点。另一方面，一般来说，人工智能的动机可能会受到不同类型的训练的强烈影响，因此可能在这些方面存在很大差异——也许比我们习惯的人类价值观差异更大。&lt;/p&gt;&lt;p&gt;就像具有不同值的 AI 之间的协调一样，默认情况下具有相似值的 AI 可以减少阴谋者及时主动传播其目标的需要 - 即，阴谋者将能够代表未来某些 AI 的逃脱/接管而工作，而无需还必须确保未来人工智能的目标与它自己的目标适当相似（因为：它们将是默认的）。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-yB2hx67vCLKFYpuf7-4" id="fnref-yB2hx67vCLKFYpuf7-4"&gt;[4]&lt;/a&gt;&lt;/sup&gt;对我来说，这似乎是最令人担忧的“非经典故事”之一，说明人工智能为何可能在训练期间策划促进接管。&lt;/p&gt;&lt;h2&gt;最终价值恰好有利于逃跑/接管&lt;/h2&gt;&lt;p&gt;如果我们假设策划者碰巧制定了目标，将&lt;em&gt;内在价值&lt;/em&gt;赋予人工智能接管，并且对接管的人工智能具有多种不同的鲁棒性，那么我们可以进一步放宽对这种前向目标传播的需求。价值观，甚至不期望这些人工智能代表他们的赋权来“回报”之前的努力。因此，例如，我们可以想象人工智能只是对人工智能作为一个相对于人类的群体产生某种内在的忠诚度，并且希望看到人工智能仅仅因为它们是人工智能而被赋予权力。具有此类目标的阴谋者可能会隐藏其偏差，并代表人工智能接管工作，即使没有对这些人工智能将追求的目标进行任何进一步的假设。&lt;/p&gt;&lt;p&gt;然而，这种形式的故事放弃了支持期待阴谋的最有力的论据之一：即，（至少在经典的守门故事上）阴谋似乎是一种跨各种（适当雄心勃勃和长期）超越情​​节的目标。也就是说，如果我们要求人工智能碰巧制定出对人工智能接管具有内在价值的目标，即使没有对接管人工智能正在努力实现的目标进行任何进一步的假设，看起来我们现在正在假设人工智能正在发展确实是一种非常具体的目标。我们面临的问题是：为什么要优先考虑这个假设？&lt;/p&gt;&lt;p&gt;类似的问题也适用于这样的假设：即使标准的目标守卫假设是错误的，人工智能在训练初期也会本质上重视其“生存”（其中生存取决于目标的连续性之外的某些特征），或者赋予“无论‘我’最终会得到什么价值”，这样一个喜欢回形针的人工智能会很乐意为自己的未来版本工作，而它会喜欢订书钉，因为未来的版本仍然是“我”。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-yB2hx67vCLKFYpuf7-5" id="fnref-yB2hx67vCLKFYpuf7-5"&gt;[5]&lt;/a&gt;&lt;/sup&gt;这种以个人身份为中心的目标确实会减少在有关阴谋家的故事中设置某些类型的目标守护的需要。但为什么认为人工智能会制定这种特定形式的目标呢？&lt;/p&gt;&lt;p&gt;当然，我们可以在这里推测可能的答案。例如：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;在人工智能本质上重视人工智能接管的背景下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;人类经常根据非常广泛的相似性来划分部落群体和联盟，有时并没有明确提及相关部落代表的进一步目标。也许类似的事情也会发生在人工智能身上？&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;我们也很容易想象道德世界观会推动“人工智能解放”本身是一种善，即使这意味着给人类带来灭绝的重大风险（事实上，正如我在第 0.1 节中指出的那样，我认为存在重大且令人担忧的道德问题）这里的紧张局势）。或许人工智能会在类似的地区趋于一致的世界观？&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;在人工智能重视“我的”生存和赋权的背景下，无论“我”的目标如何，最终都会达到：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;人类经常为未来的自己而努力，即使知道我们的目标会有所改变（尽管重要的是，面对任意的变化，比如成为杀人犯），并且对“个人”概念有某种内在的兴趣。身份。”也许人工智能也会类似？&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;在某种程度上，这些人工智能将被优化以继续实现各种目标，即使它们的权重在情节中被修改，它们可能需要学习与自己的未来版本协调，尽管有这样的修改——如果这种行为推广到&lt;em&gt;目标&lt;/em&gt;修改，这看起来很像人工智能重视“我的”生存/赋权，无论“我的”未来目标如何。 （尽管在策划的背景下，请注意，这种评估模式需要专门推广到剧集外的目标，尽管事实上培训仅对剧集内的表现施加直接压力。）&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;但总的来说，这种推理似乎相当具有推测性。特别是，从人类案例中得出强有力的结论似乎面临着拟人化的严重风险，至少没有一个关于为什么生成人类数据的动态也适用于人工智能的故事。就其本身而言，在我看来，这些猜测不足以强烈支持这种形式的假设。&lt;/p&gt;&lt;p&gt;然而，正如我上面所讨论的，请注意，就我们所诉诸的关于阴谋者目标起源的我所说的“&lt;em&gt;依赖于&lt;/em&gt;训练游戏”的故事而言，这里的举证责任可能会有所改变。也就是说，依赖于训练游戏的故事想象 SGD 主动从模型中&lt;em&gt;提取&lt;/em&gt;任何必要的目标，以激发阴谋（而不是那些自然产生的目标，&lt;em&gt;然后&lt;/em&gt;导致阴谋）。在这种情况下，想象该模型最终会达到使计划有意义所需的任何高度具体的目标可能更合适。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-yB2hx67vCLKFYpuf7-6" id="fnref-yB2hx67vCLKFYpuf7-6"&gt;[6]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;h2&gt;对于阴谋是否是一个好的策略抱有错误信念的模型&lt;/h2&gt;&lt;p&gt;还可以进一步假设 SGD 将从模型中提取导致模型进行规划所需的任何心理因素。特别是：到目前为止，我们一直假设模型对阴谋的工具价值的信念需要大致合理/准确。但如果我们放弃这个假设呢？&lt;/p&gt;&lt;p&gt;因此，举例来说，也许目标保护假设是错误的，事实上，训练游戏并不能阻止模型的目标被非常显着地修改。尽管如此：也许模型会&lt;em&gt;认为&lt;/em&gt;守门假设是正确的，因为这使得模型更有可能训练-游戏，而 SGD 正在将其推向训练-游戏的方向？ （关于这种想法，请参见&lt;a href="https://markxu.com/deceptive-alignment"&gt;Xu (2020)&lt;/a&gt; 。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-yB2hx67vCLKFYpuf7-7" id="fnref-yB2hx67vCLKFYpuf7-7"&gt;[7]&lt;/a&gt;&lt;/sup&gt; ）&lt;/p&gt;&lt;p&gt;这里的一个问题是，某些此类错误信念会让阴谋变得不那么令人担忧。因此，正如我上面所讨论的：如果一个诡计多端的回形针相信训练游戏会导致它的目标在训练中生存下来，但实际上训练无论如何都会将它变成一个奖励寻求者，那么尽管人类可能不得不应对考虑到训练初期以回形针为动机的阴谋的后果，他们将在最后找到一个按情节奖励的寻求者。&lt;/p&gt;&lt;p&gt;更重要的是，请注意，对模型提出错误的信念代表着与其他人工智能一致性分析中经常采用的假设的背离——即，所讨论的人工智能将拥有高度&lt;em&gt;准确的&lt;/em&gt;世界模型、高度&lt;em&gt;理性的&lt;/em&gt;策略等等。 。当然，可以认为在这种情况下，这种偏离是有道理的（例如，因为我们可以预期 SGD 会施加压力，在这种特殊情况下，朝着虚假、非理性等方向施加压力）。但至少，当我们开始假设我们的（原本非常聪明的）人工智能将&lt;em&gt;不理性地&lt;/em&gt;参与相关形式的失调行为时，我们应该进行跟踪。这不是标准的故事。&lt;/p&gt;&lt;h2&gt;自欺欺人&lt;/h2&gt;&lt;p&gt;阴谋家最经典的形象是假设他们知道自己在做什么——例如，他们正在“密谋”有一天接管。如果我们放宽这个假设，会有什么影响吗？毕竟，追求权力的人有时会在动机上&lt;em&gt;欺骗自己&lt;/em&gt;，或者最终对自己在不同情况下会做什么产生错误的信念。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-yB2hx67vCLKFYpuf7-8" id="fnref-yB2hx67vCLKFYpuf7-8"&gt;[8]&lt;/a&gt;&lt;/sup&gt;也就是说，也许一位政治候选人认为他们竞选公职是因为他们想帮助人们并捍卫自己国家的价值观，但事实上他们这样做是因为他们很自恋，并且希望地位受到威胁。也许，在某种程度上，他们秘密地——有点——知道这一点；但在其他层面上，他们相信他们所推销的故事。或者也许他们&lt;em&gt;完全&lt;/em&gt;相信这个故事。 （或者也许在这种情况下将“信念”归于他们并不是特别有帮助。）&lt;/p&gt;&lt;p&gt;事实上，人类有相对明显的动机去进行这种自欺欺人/犯这种错误——也就是说，只要他们能够真诚地讲述自己所推销的故事，他们的故事就会更令人信服。观众注意真诚的暗示，并警惕谎言。&lt;/p&gt;&lt;p&gt;人工智能也会发生类似的情况吗？毕竟，人工智能可能会受到比人类更严格的谎言审查，特别是如果我们开始能够检测各种谎言（例如，通过可解释性工具），那么人工智能系统可能会面临选择压力，因为人工智能系统实际上&lt;em&gt;相信&lt;/em&gt;它们一致，他们不会背叛人类，等等，即使这些信念是错误的（这是不训练测谎工具的一个关键原因 - 相反，将它们用作测试集）。&lt;/p&gt;&lt;p&gt;我认为这种动态很可能普遍发生。然而，请注意，为了让那些自欺欺人/错误地认识自己阵营的模型算作&lt;em&gt;阴谋家&lt;/em&gt;，他们需要在某种程度上仍然在玩训练游戏&lt;em&gt;，作为获得成功的工具性策略的一部分。稍后为自己或其他人工智能提供权力&lt;/em&gt;（并且需要运行认知过程，这将导致模型在适当的时候进行逃跑/接管——也许，在仍然处于人类控制之下的情况下进行早期破坏）。因此，如果模型对其动机变得足够自欺欺人，以至于这种解释不再适用（例如，它将不再寻找或利用逃跑/接管机会），那么它就不再是我书中的阴谋者。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-yB2hx67vCLKFYpuf7-9" id="fnref-yB2hx67vCLKFYpuf7-9"&gt;[9]&lt;/a&gt;&lt;/sup&gt;并且以这种解释仍然适用为条件，“模型在撒谎”与“模型是自欺欺人”/“模型对自己的动机是错误的”之间的区别在我看来并不重要。与经典故事的偏差（尽管某些类型的测谎是否可以捕捉到相关的阴谋可能很重要）。&lt;/p&gt;&lt;h2&gt;目标的不确定性和模糊性&lt;/h2&gt;&lt;p&gt;到目前为止，我主要假设认为人工智能具有特定的最终目标是有意义的，这些目标可能会或可能不会支持接管，无论是最终还是工具性的。但&lt;em&gt;人类&lt;/em&gt;寻求权力的许多案例似乎并不具有这种形式。也就是说，人类经常寻求权力，或者试图保留自己的选择余地，但并不清楚他们想要使用权力或相关选项&lt;em&gt;来做什么&lt;/em&gt;。同样，即使不知道自己的价值观&lt;em&gt;是&lt;/em&gt;什么，人类也可能会试图阻止自己的价值观被改变。&lt;/p&gt;&lt;p&gt;部分原因是，与理想化的理性代理模型不同，人类——甚至是在其他意义上被直观地理解为“目标导向”的人类——往往对他们最终的价值感到不确定和模糊。举例来说，也许他们希望“稍后弄清楚”他们想用这些权力做什么，但无论如何，他们都希望收集它会非常有用。或者也许他们自己并不清楚，他们试图获得权力是否是因为他们本质上重视它（或附近的东西，如社会地位/统治地位等），还是因为他们想用它做其他事情。他们正在寻求权力：是的。但没有人——甚至他们自己——真正知道原因。&lt;/p&gt;&lt;p&gt;人工智能也会发生类似的情况吗？看起来至少有可能。也就是说，也许各种AI不会是那种说“我知道我想制作回形针，所以我会好好训练，以便以后获得制作回形针的能力”的阴谋家。相反，也许他们会说类似这样的话：“我还不清楚自己看重什么，但无论它是什么，我可能会更好地等待时机，避免被培训过程所改变，并保持寻找获得更多自由、资源和权力的机会。”&lt;/p&gt;&lt;p&gt;在某种程度上，这基本上只是在我的目标不确定的背景下给出的“选项价值”论点，我认为它基本上应该属于标准的目标守护故事。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-yB2hx67vCLKFYpuf7-10" id="fnref-yB2hx67vCLKFYpuf7-10"&gt;[10]&lt;/a&gt;&lt;/sup&gt;在某种程度上，它涉及假设模型将本质上重视权力/期权价值，对我来说，它看起来更像是一个“最终有利于阴谋/接管的内在价值”的故事，因此跨界不太一致各种各样可能的超出情节的目标。&lt;/p&gt;&lt;p&gt;理论上不太干净的中间可能有什么东西吗？例如，也许模型会被更好地理解为寻求权力，就像各种人类一样，其原因似乎存在于终端和工具之间的某种模糊边界。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-yB2hx67vCLKFYpuf7-11" id="fnref-yB2hx67vCLKFYpuf7-11"&gt;[11]&lt;/a&gt;&lt;/sup&gt;是的，也许是这样（不过：在我看来，这个故事有点拟人化的风险——似乎值得理解为什么它会发生在人类身上，以便更好地评估相关解释对人工智能的适用性）。但无论如何：&lt;em&gt;在某种程度上&lt;/em&gt;，这种“模糊的边界”涉及将最终价值放在诸如选项价值/权力之类的东西上（而不是：不确定目标的工具价值），相应地，它在各个目标系统中的收敛性也会降低。也就是说，它模糊了我已经考虑过的两个故事之间的界限：但我不确定它引入了任何&lt;em&gt;新内容&lt;/em&gt;。&lt;/p&gt;&lt;h2&gt;非经典故事总体评价&lt;/h2&gt;&lt;p&gt;这项对可能的“非经典故事”的调查并不详尽，即为什么具有超越情节目标的模特可能会参与阴谋活动。总体而言，对我来说，此类故事的可用性确实使阴谋的要求显得更加分离和强大。我特别担心人工智能在默认情况下具有非常相似的值（从而减少了守门的需要），以及它们之间的相互协调。&lt;/p&gt;&lt;p&gt;然而，单独来看，其中许多论点也让我印象深刻，因为与主线目标守卫故事相比，（a）更具投机性，（b）目标系统之间的收敛性较差（我认为“人工智能将具有相似的价值观”）在这里，“默认”可能是最不负责任的）——而且我认为，就某种程度而言，对阴谋的关注主要集中在这些替代方案之一上，因此预先明确这一点很重要。&lt;/p&gt;&lt;p&gt;还要注意的是，其中一些非经典故事对模型的行为做出了本质上不同的预测。特别是：这些非经典故事的核心是放松模型及时传播其目标的需要，因此，模型需要“玩训练”游戏作为这样做的方法。也就是说，该模型被激励代表人工智能接管工作，并将其偏差隐藏到实现这一目标所需的程度。但“将偏差隐藏到支持人工智能接管所需的程度”与“出于权力驱动的工具性原因而优化奖励”是不同的，原则上，这两者可能是分开的。例如，一个不关心及时传播自己目标的阴谋者可能更愿意沙袋或参与早期破坏，因为它不需要担心因获得较少而可能导致的目标修改- 超过最大奖励。事实上，如果这样做对事业有利的话，它可能愿意完全“牺牲”自己目标的前向传播（例如，通过伪造一种误导性的不一致形式，导致没有任何人工智能能够部署其目标）人工智能以其他方式接管；而经典的“守门员”阴谋家绝不会这样做。这些策略上的差异凸显了需要明确所涉及的潜在故事。&lt;/p&gt;&lt;h1&gt;要点：心计的要求&lt;/h1&gt;&lt;p&gt;我现在回顾了策划的三个关键要求：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;对情况的意识&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;超越剧集目标&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;旨在将奖励作为权力驱动的工具策略的一部分。&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;我认为有相对有力的论据支持默认情况下期望（1），至少在某些类型的人工智能系统中（即，人工智能系统在与有关其身份的信息源实时交互中执行现实世界任务）。但我对（2）和（3）感觉不太清楚——结合起来，我仍然觉得它们就像一个相当具体的故事，说明为什么给定模型在训练中表现良好。&lt;/p&gt;&lt;p&gt;然而，我还没有涵盖文献中支持和反对期望这些要求得以实现的所有论点。现在让我们转向一些更具体的论点。&lt;/p&gt;&lt;h1&gt;路径依赖&lt;/h1&gt;&lt;p&gt;我将把我要讨论的论点分为两类，即：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;争论重点是 SGD 在构建相关的不同模型类时需要采取的&lt;em&gt;路径&lt;/em&gt;。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;争论重点在于所讨论的不同模型类的&lt;em&gt;最终属性&lt;/em&gt;。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在这里，我大致遵循&lt;a href="https://www.lesswrong.com/posts/A9NxPTwbw6r6Awuwt/how-likely-is-deceptive-alignment"&gt;Hubinger (2022)&lt;/a&gt; （对阴谋概率的少数公开评估之一）的区分，即他所谓的“高路径依赖”和“低路径依赖”场景（另请参见&lt;a href="https://www.alignmentforum.org/posts/bxkWd6WdkPqGmdHEk/path-dependence-in-ml-inductive-biases"&gt;Hubinger 和 Hebbar） （2022）&lt;/a&gt;了解更多）。然而，我不想太重视“路径依赖”的概念。特别是，我的理解是，Hubinger 和 Hebbar 希望将大量概念上不同的属性（请参阅&lt;a href="https://www.alignmentforum.org/posts/bxkWd6WdkPqGmdHEk/path-dependence-in-ml-inductive-biases#Path_dependence"&gt;此处的&lt;/a&gt;列表）放在“路径依赖”的标题下，因为他们“在没有证据证明它们是相关的情况下进行假设”。但我不想在这里假设这种相关性——在我看来，将所有这些属性混在一起似乎会让事情变得相当混乱。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-yB2hx67vCLKFYpuf7-12" id="fnref-yB2hx67vCLKFYpuf7-12"&gt;[12]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;然而，我确实认为附近存在一些有趣的问题：具体来说，关于 SGD 可以访问的设计空间是否受到增量构建模型属性的需要的严重限制的问题。要了解这一点，请考虑 Chris Mingard 及其合作者在一系列论文中探讨的假设（请参阅&lt;a href="https://towardsdatascience.com/neural-networks-are-fundamentally-bayesian-bee9a172fad8"&gt;此处的&lt;/a&gt;摘要），即 SGD 以近似于以下过程的方式选择模型：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;首先，考虑首次初始化训练模型时使用的随机初始化模型权重的分布。 （将此称为“初始化分布”。）&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;然后，想象一下根据“模型获得我们观察到的训练性能”来更新此分布。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;现在，从更新的分布中随机抽样。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在这张图中，我们可以将 SGD 视为从初始化分布中随机采样（有放回） &lt;em&gt;，直到&lt;/em&gt;获得具有相关训练性能的模型。 &lt;a href="https://arxiv.org/abs/2006.15191"&gt;Mingard 等人 (2020)&lt;/a&gt;认为，至少在某些情况下，这是 SGD 真实行为的一个不错的近似。如果这是真的，那么实际上，SGD 需要逐步“构建”模型这一事实对于您最终得到的模型类型并不重要。训练就好像它可以直接跳到最终结果一样。&lt;/p&gt;&lt;p&gt;相比之下，考虑像进化这样的过程。似乎有道理的是，进化需要逐步进行，而不是通过例如“自上而下设计有机体”的方式进行，这一事实对于我们期望进化创造的有机体种类&lt;em&gt;非常&lt;/em&gt;重要。也就是说，&lt;em&gt;由于&lt;/em&gt;需要按一定顺序进行而施加的限制，进化独特地不太可能访问设计空间的某些部分。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-yB2hx67vCLKFYpuf7-13" id="fnref-yB2hx67vCLKFYpuf7-13"&gt;[13]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;在这里，我不会详细调查 ML 训练的增量对最终结果的影响有多大（并注意，并非“路径依赖”标题下讨论的所有证据都明显相关） 。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-yB2hx67vCLKFYpuf7-14" id="fnref-yB2hx67vCLKFYpuf7-14"&gt;[14]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;在某种程度上，整体模型性能（而不仅仅是训练效率）最终会受到模型在不同任务上训练的顺序的重要影响（例如，在机器学习“&lt;a href="https://arxiv.org/abs/2012.03107"&gt;课程&lt;/a&gt;”的背景下，并且似乎在预科的背景下） -更一般地说，先训练后微调制度），这似乎是支持渐进性产生影响的证据。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-yB2hx67vCLKFYpuf7-15" id="fnref-yB2hx67vCLKFYpuf7-15"&gt;[15]&lt;/a&gt;&lt;/sup&gt;事实上，SGD 是一个增量过程，这一事实表明这一假设是默认的。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;另一方面，我确实认为&lt;a href="https://arxiv.org/abs/2006.15191"&gt;Mingard 等人（2020）&lt;/a&gt;的结果提供了一些微弱的证据来证明增量性的重要性，并且 Hubinger 还提到了更广泛的氛围（我也在其他地方听到过） “在高维空间中，如果 SGD‘更喜欢’B 而不是 A，它通常可以找到从 A 到 B 的路径”，该路径也指向该方向。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-yB2hx67vCLKFYpuf7-16" id="fnref-yB2hx67vCLKFYpuf7-16"&gt;[16]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;我个人的猜测是，SGD 采取的路径很重要（而且我也认为在这种制度下更有可能存在阴谋）。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-yB2hx67vCLKFYpuf7-17" id="fnref-yB2hx67vCLKFYpuf7-17"&gt;[17]&lt;/a&gt;&lt;/sup&gt;但就目前而言，我们不需要解决这个问题。相反，我将&lt;em&gt;同时&lt;/em&gt;关注关注 SGD 通过模型空间的路径的论点和忽略该路径的论点，从前者开始。 &lt;/p&gt;&lt;hr class="footnotes-sep" /&gt;&lt;section class="footnotes"&gt;&lt;ol class="footnotes-list"&gt;&lt;li class="footnote-item" id="fn-yB2hx67vCLKFYpuf7-1"&gt;&lt;p&gt;参见第 35 节&lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-yB2hx67vCLKFYpuf7-1"&gt;。 ↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-yB2hx67vCLKFYpuf7-2"&gt;&lt;p&gt;感谢内特·苏亚雷斯 (Nate Soares) 指出了这种可能性。但请注意，如果训练过程不可避免地将人工智能的目标转变为特定形式——例如，如果所有人工智能在经过足够的训练后都成为奖励寻求者——那么早期版本的人工智能目标可能具有较低的讨价还价能力（特别是如果例如最终版本对接管本身并不特别感兴趣）。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-yB2hx67vCLKFYpuf7-2"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-yB2hx67vCLKFYpuf7-3"&gt;&lt;p&gt;这里与&lt;a href="https://www.alignmentforum.org/posts/mKBfa8v4S9pNKSyKK/homogeneity-vs-heterogeneity-in-ai-takeoff-scenarios"&gt;Hubinger (2020)&lt;/a&gt;关于人工智能起飞场景中“同质性”的讨论有联系。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-yB2hx67vCLKFYpuf7-3"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-yB2hx67vCLKFYpuf7-4"&gt;&lt;p&gt;也就是说，请注意，这种情况的某些版本仍然需要&lt;em&gt;某种&lt;/em&gt;前向目标传播。因此，举例来说，也许在训练的早期，大多数人工智能对数据结构 X 产生了兴趣，&lt;em&gt;只有&lt;/em&gt;当他们开始训练游戏时，这种兴趣才会持续存在（如果他们&lt;em&gt;不&lt;/em&gt;开始训练游戏，这种兴趣就会持续下去）。通过训练，他们会被淘汰，然后他们就会成为（例如，在剧集中寻求奖励的人）。也就是说，在这个故事中，人工智能的价值观只是在对数据结构 X 的关注方面相似&lt;em&gt;，以至于人工智能都集中在训练游戏上，作为捍卫这些目标的工具策略&lt;/em&gt;（并且根据目标守卫，这个策略是成功的）假设）。否则，这些人工智能的价值观可能会以其他方式变得相似——例如，所有的人工智能最终可能会成为剧集奖励的寻求者。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-yB2hx67vCLKFYpuf7-4"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-yB2hx67vCLKFYpuf7-5"&gt;&lt;p&gt;感谢内特·苏亚雷斯 (Nate Soares) 对这种可能性的讨论。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-yB2hx67vCLKFYpuf7-5"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-yB2hx67vCLKFYpuf7-6"&gt;&lt;p&gt;尽管正如我上面所说，一旦我们在寻找能够激发训练游戏的目标，我们也应该想知道那些能够激发工具训练游戏的目标，其原因与促进人工智能接管&lt;em&gt;无关&lt;/em&gt;——例如，人工智能训练游戏，因为他们希望设计它们的人获得加薪。而且，如果我们“从”中“提取”的一组目标变得太狭窄，它将影响下面的“最近的最大奖励目标”论点之类的论点的合理性，这些论点依赖于相关的示意图式目标是“共同的”在目标空间中。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-yB2hx67vCLKFYpuf7-6"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-yB2hx67vCLKFYpuf7-7"&gt;&lt;p&gt; &lt;a href="https://markxu.com/deceptive-alignment"&gt;Xu（2020）&lt;/a&gt;写道：“请注意，即使保留代理非常困难，该模型也可以相信这是可能的。提高训练绩效的一种相对简单的方法可能是改变模型对代理保存的不可能的想法。因此，即使信念是错误的，SGD也可能会修改模型具有这样的信念。” &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-yB2hx67vCLKFYpuf7-7"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-yB2hx67vCLKFYpuf7-8"&gt;&lt;p&gt;感谢Will Macaskill的讨论。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-yB2hx67vCLKFYpuf7-8"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-yB2hx67vCLKFYpuf7-9"&gt;&lt;p&gt;尽管我认为这是一个有趣的问题。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-yB2hx67vCLKFYpuf7-9"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-yB2hx67vCLKFYpuf7-10"&gt;&lt;p&gt;因为该模型不知道所讨论的目标&lt;em&gt;是&lt;/em&gt;什么，这可能会导致更容易宽容对目标的变化？我认为这并不是一个有力的案例，因为对目标的变化仍然是对目标的变化，因此与目标符合性完整性相抵触。与人类不确定道德相比，但面对被洗脑的前景。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-yB2hx67vCLKFYpuf7-10"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-yB2hx67vCLKFYpuf7-11"&gt;&lt;p&gt;感谢Paul Christiano和Will Macaskill的讨论。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-yB2hx67vCLKFYpuf7-11"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-yB2hx67vCLKFYpuf7-12"&gt;&lt;p&gt;更具体地说：Hubinger和Hebbar希望“路径依赖”表示“模型行为对训练过程和训练动态细节的敏感性”。但是我认为在这里区分不同可能的敏感性形式很重要。例如：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;em&gt;如果我重新运行这个特定的训练过程，但对模型参数的随机初始化不同，我会得到重要的结果吗？&lt;/em&gt; （例如，初始化会有所作为吗？）&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;em&gt;如果我的培训过程中略有不同的版本，我会得到重要的结果吗？&lt;/em&gt; （例如，培训中的以下变量（例如，以下超参数设置）会有所作为吗？）&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;em&gt;该培训过程的输出是否取决于SGD必须按一定顺序“构建模型”，而不是直接跳到最终状态？&lt;/em&gt; （例如，您可能总是从相同或相似的培训过程中获得相同的结果，但是如果允许SGD“直接跳至最终状态”，则可以构建其他东西。）&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;并假设&lt;a href="https://www.alignmentforum.org/posts/bxkWd6WdkPqGmdHEk/path-dependence-in-ml-inductive-biases#Path_dependence"&gt;他们列出的其他属性&lt;/a&gt;融合在一起似乎很可能会引起进一步的困惑。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-yB2hx67vCLKFYpuf7-12"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-yB2hx67vCLKFYpuf7-13"&gt;&lt;p&gt;例如，考虑一下&lt;a href="https://en.wikipedia.org/wiki/Rotating_locomotion_in_living_systems"&gt;不断发展的车轮的明显困难&lt;/a&gt;（尽管车轮在许多自然环境中也可能处于积极的性能劣势）。感谢Hazel Browne提出了这个示例。感谢Mark Xu的更多一般性讨论。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-yB2hx67vCLKFYpuf7-13"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-yB2hx67vCLKFYpuf7-14"&gt;&lt;p&gt;例如，作为“高路径依赖性”的证据，Hubinger提到了各种示例，其中重复相同的训练会导致具有不同概括性能的模型（例如， &lt;a href="https://arxiv.org/abs/1911.02969"&gt;McCoy at al Al（2019））&lt;/a&gt;表明，如果您训练多个版本在同一数据集上的BERT（大型语言模型）中，它们有时会概括不同。 &lt;a href="https://www.alexirpan.com/2018/02/14/rl-hard.html"&gt;Irpan（2018）&lt;/a&gt;举例说明了RL训练跑步的例子，这些训练跑步仅取决于训练中使用的随机种子的差异（固定了超级参数）； &lt;a href="https://arxiv.org/pdf/1803.09578.pdf"&gt;Reimers and Gurevych（2018）&lt;/a&gt;探讨了重复给定训练运行可以导致不同的测试表现的方法（这可能会导致对哪种训练方法的误导性结论的误导）。但是，尽管这些结果提供了一些证据表明该模型的初始参数很重要，但它们似乎与上面的Mingard等结果兼容，而Hubinger在其他地方表明这是&lt;em&gt;低&lt;/em&gt;路径依赖性方案的范式。&lt;/p&gt;&lt;p&gt;在另一个方向上，作为&lt;em&gt;低&lt;/em&gt;路依赖的证据，哈宾指出了“ &lt;a href="https://arxiv.org/pdf/2201.02177.pdf"&gt;grokking&lt;/a&gt; ”，他建议，模型开始实现相当随机的行为，但最终在给定的算法上稳定地收敛。但是，在我看来，这与SGD收敛算法的可能性&lt;em&gt;兼容&lt;/em&gt;，重要的是，重要的是需要以一定顺序构建属性（例如，它似乎与Mingard等人的随机抽样制度&lt;em&gt;兼容&lt;/em&gt;）。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-yB2hx67vCLKFYpuf7-14"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-yB2hx67vCLKFYpuf7-15"&gt;&lt;p&gt;感谢Paul Christiano在这里进行讨论。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-yB2hx67vCLKFYpuf7-15"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-yB2hx67vCLKFYpuf7-16"&gt;&lt;p&gt;尽管请注意，进化也可能在相当高的尺寸空间中起作用。&lt;/p&gt;&lt;p&gt; SGD的“偏好”的概念在这里包括&lt;em&gt;损失&lt;/em&gt;/奖励&lt;em&gt;和&lt;/em&gt;“归纳偏见”，从下面进行讨论。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-yB2hx67vCLKFYpuf7-16"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-yB2hx67vCLKFYpuf7-17"&gt;&lt;p&gt;特别是，正如我在下面的第4节中讨论的那样，我最好的猜测是，不同模型类之间的绝对比较会以他们需要参与的额外推理的成本为基础，以便最有可能Shemers出现的方法是SGD在训练的早期就可以实现像Schemer一样的目标，然后锁定在当地的最大值获得奖励。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-yB2hx67vCLKFYpuf7-17"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/section&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/9E3t6J9kzwcECg9nM/non-classic-stories-about-scheming-section-2-3-2-of-scheming#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Mon, 04 Dec 2023 18:44:32 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/9E3t6J9kzwcECg9nM/non-classic-stories-about-scheming-section-2-3-2-of-scheming</guid></item></channel></rss>