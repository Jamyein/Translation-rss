<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>少错</title><link>https://www.lesswrong.com</link><description>致力于提炼理性艺术的社区博客</description><lastBuildDate>Sun, 31 Dec 2023 18:13:05 GMT</lastBuildDate><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>我对 2023 年捐款的长期自我审视</title><link>https://www.lesswrong.com/posts/kp7vuiSnGJJPAoWMs/extended-navel-gazing-on-my-2023-donations</link><description>发布于 2023 年 12 月 31 日下午 6:10（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/kp7vuiSnGJJPAoWMs/extended-navel-gazing-on-my-2023-donations#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sun, 31 Dec 2023 18:10:31 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/kp7vuiSnGJJPAoWMs/extended-navel-gazing-on-my-2023-donations</guid></item><item><title>aisafety.info，目录</title><link>https://www.lesswrong.com/posts/ZqxP6pJe53xRnRb4j/aisafety-info-the-table-of-content</link><description>发布于 2023 年 12 月 31 日下午 1:57（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;以下是来自&lt;a href="https://aisafety.info/"&gt;https://aisafety.info/&lt;/a&gt;的问答列表。当我发现这个网站时，所产生的材料数量给我留下了深刻的印象。但是，该界面针对初学者进行了优化。以下目录适用于希望更自由地浏览各个部分的个人。它是通过将问答聚类为子主题而构建的。我不参与 aisafety.info，我只是想通过以不同的方式呈现他们生成的内容来提高其可见性。他们还在开发新界面。该表也可以在&lt;a href="https://aisafety.info/toc"&gt;https://aisafety.info/toc/&lt;/a&gt;找到。&lt;/p&gt;&lt;h1&gt; 🆕&lt;a href="https://aisafety.info/?state=9OGZ_"&gt;人工智能安全新手？从这里开始。&lt;/a&gt;&lt;/h1&gt;&lt;h2&gt; 📘 AI 安全简介&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8486_"&gt;什么是人工智能安全？&lt;/a&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8EL9_"&gt;什么是AI对齐？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8AF4_"&gt;什么是人工智能治理？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6184_"&gt;对人工智能一致性的担忧的一般性质是什么？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6205_"&gt;什么是“控制问题”？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6714_"&gt;人工智能安全、人工智能对齐、人工智能控制、友好人工智能、人工智能道德、人工智能存在安全和AGI安全之间有什么区别？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=2400_"&gt;为什么人工智能会做坏事？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7755_"&gt;成熟的超级智能会有多强大？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6297_"&gt;为什么安全对于比人类更聪明的人工智能很重要？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7715_"&gt;超级人工智能导致灭绝的可能性有多大？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=86J8_"&gt;关于人工智能安全有哪些介绍？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; 🧠 机器学习简介&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8161_"&gt;什么是大语言模型？&lt;/a&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6627_"&gt;什么是 GPT-3？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6628_"&gt;什么是 OpenAI Codex 和 GitHub Copilot？&lt;/a&gt;&lt;/li&gt;&lt;li&gt; &lt;a href="https://aisafety.info/?state=8EL7_"&gt;“思路链”提示如何发挥作用？&lt;/a&gt;&lt;/li&gt;&lt;li&gt; &lt;a href="https://aisafety.info/?state=6571_"&gt;GPT 式非代理人工智能的进步如何产生有能力的人工智能代理？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=9358_"&gt;什么是计算？&lt;/a&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7750_"&gt;什么是缩放定律？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8C7T_"&gt;什么是“没有免费的午餐”定理？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=94D9_"&gt;什么是“惨痛教训”？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=89ZS_"&gt;什么是强化学习（RL）？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; 🤖 人工智能的类型&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8G1H_"&gt;什么是人工智能（AI）？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6513_"&gt;什么是“狭义人工智能”？&lt;/a&gt; &amp;amp;&lt;a href="https://aisafety.info/?state=2374_"&gt;什么是通用人工智能（AGI）？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8QZF_"&gt;什么是工具AI？&lt;/a&gt; &amp;amp;&lt;a href="https://aisafety.info/?state=5632_"&gt;什么是代理？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6347_"&gt;什么是“变革性人工智能”？&lt;/a&gt;&lt;/li&gt;&lt;li&gt; &lt;a href="https://aisafety.info/?state=5864_"&gt;AGI、变革性人工智能和超级智能之间有什么区别？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6207_"&gt;什么是“超级智能”？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8PYV_"&gt;什么是修格斯？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6350_"&gt;什么是“全脑模拟”？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6592_"&gt;什么是脑机接口？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; 🚀 起飞与智能爆炸&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;脱掉&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7071_"&gt;什么是“AI起飞”？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6966_"&gt;为什么人工智能起飞速度很重要？&lt;/a&gt;&lt;/li&gt;&lt;li&gt; &lt;a href="https://aisafety.info/?state=6957_"&gt;AI 可能有哪些不同的起飞速度？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=90PK_"&gt;什么是单例？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;智力爆炸&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6306_"&gt;什么是智力爆炸？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6586_"&gt;智力爆炸的可能性有多大？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6605_"&gt;情报爆炸有什么用？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8IHO_"&gt;奇点、智能爆炸和硬起飞之间有什么区别？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; 📅 时间表&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;专家调查&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6478_"&gt;专家通常基于哪些证据来进行时间线预测？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=5633_"&gt;专家认为什么时候会创造出人类水平的人工智能？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=5851_"&gt;人工智能从最初出现问题的迹象到发生无法挽回的灾难的速度有多快？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7647_"&gt;是否有关于人工智能安全的专家调查？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;计算和扩展就足够了吗？&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7727_"&gt;我们能否通过扩展类似于当前架构的架构来获得 AGI，或者我们是否缺少关键见解？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7598_"&gt;生物进化过程使用了多少资源来进化出智慧生物？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;从AGI到ASI&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8158_"&gt;我们如何从通用人工智能发展成为超级智能系统？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7565_"&gt;我们会建立一个超级智能吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7747_"&gt;从人类水平的人工智能到超级智能需要多长时间？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6964_"&gt;难道仅仅沿着指数趋势线跌下悬崖就可以期待人工智能自我改进带来巨大回报吗？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; ❗ 风险类型&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=3485_"&gt;什么是事故和误用风险？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=89LL_"&gt;什么是存在风险（x 风险）&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8503_"&gt;人工智能存在风险的主要来源是什么？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7783_"&gt;什么是天文灾害风险（s-risks）？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=1001_"&gt;人工智能带来的其他风险又如何呢？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7774_"&gt;即使没有代理超级智能，人工智能也会出现什么问题？&lt;/a&gt;&lt;/li&gt;&lt;li&gt; &lt;a href="https://aisafety.info/?state=6607_"&gt;“情报爆炸”有什么危险？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7602_"&gt;大规模自动化人工智能说服和宣传是一个严重的问题吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=9AKZ_"&gt;什么是“诡变”&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8VOT_"&gt;什么是心灵犯罪？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; 🔍 AGI 能够做什么？&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6315_"&gt;什么是智力？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6603_"&gt;为什么智慧会带来力量？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;基本能力&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6974_"&gt;人工智能如何在社交上操纵人类？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=5844_"&gt;是否有可能阻止人工智能在互联网上做某些事情？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=5842_"&gt;人工智能冒充人类来实现其目标的可能性有多大？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;先进的功能&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=5943_"&gt;AGI如何杀人？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=5849_"&gt;你能阻止高级人工智能自我升级吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8222_"&gt;超级智能人工智能如何利用互联网来接管物理世界？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7629_"&gt;超级智能人工智能可以做什么，以及哪些事情即使对于它来说在物理上也是不可能的？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7491_"&gt;什么是“价值握手”？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;战略意义&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6990_"&gt;我们能否测试人工智能以确保它在实现超级智能后不会接管并做出有害的事情？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8157_"&gt;为什么我们只有一次机会来调整超级智能？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=5611_"&gt;我们能否对人工智能进行编程，让它在开始做我们不希望它做的事情时自动关闭？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; 🌋未对齐的技术来源&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;正交性论文&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6568_"&gt;什么是正交性命题？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7594_"&gt;什么是“人类价值观”？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6982_"&gt;为什么我们会认为超级智能默认是敌对的？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6920_"&gt;我们可以预期超级智能机器的动机是什么？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=95LE_"&gt;为什么一个错位的超级智能会杀死世界上的所有人？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;规格游戏&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7523_"&gt;为什么最大化人工智能可能会导致不良结果？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=897I_"&gt;什么是工具趋同？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=87AG_"&gt;什么是可修正性？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8EL5_"&gt;什么是反常实例化？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=5853_"&gt;是否有可能将代码写入人工智能以避免给定任务可能出错的所有方式，并且尝试这样做会很危险吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6992_"&gt;我们可以使用指定的规则来约束目标导向的人工智能吗？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;目标误区&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8EL6_"&gt;什么是欺骗性对齐？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8359_"&gt;埃文·胡宾格 (Evan Hubinger) 对欺骗 + 内在对齐有何看法？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;外部和内部对齐&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8XV7_"&gt;什么是外对齐？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8160_"&gt;什么是“台面优化器”？&lt;/a&gt; &amp;amp;&lt;a href="https://aisafety.info/?state=8PYW_"&gt;什么是内部对齐？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8428_"&gt;内对齐和外对齐有什么区别？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8AF5_"&gt;子代理和台面优化器之间有什么区别？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; 🎉 当前平淡无奇的解决方案&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8AER_"&gt;什么是模仿学习？&lt;/a&gt; &amp;amp;&lt;a href="https://aisafety.info/?state=8AEQ_"&gt;什么是行为克隆？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=88FN_"&gt;什么是人类反馈强化学习（RLHF）&lt;/a&gt;和&lt;a href="https://aisafety.info/?state=904J_"&gt;“宪法人工智能”？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=89LK_"&gt;可解释性有何帮助？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8FJZ_"&gt;红队如何用于人工智能对齐？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; 🗺️策略&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7736_"&gt;政府发挥重要作用的可能性有多大？如果有的话，什么角色是理想的？&lt;/a&gt;&lt;/li&gt;&lt;li&gt; &lt;a href="https://aisafety.info/?state=7748_"&gt;“警告镜头”会是什么样子？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8AF1_"&gt;什么是调整税？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7642_"&gt;一致的超级智能是否会迫使人们过上更好的生活，并比他们想要的更快地改变？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;获胜条件&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7762_"&gt;AI对齐的“获胜条件”是什么？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7187_"&gt;如果我们解决了一致性问题，我们就一定会有美好的未来吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7580_"&gt;什么是“关键行为”？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7757_"&gt;什么是“长反思”？&lt;/a&gt;&lt;/li&gt;&lt;li&gt; &lt;a href="https://aisafety.info/?state=7766_"&gt;AGI 的美好未来会是什么样子？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7058_"&gt;人工智能对齐的良好解决方案是什么样的？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7060_"&gt;从较高的层面来看，为了确保美好的未来，我们必须应对哪些协调一致的挑战？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;比赛动态&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6483_"&gt;为什么人们会尝试构建通用人工智能而不是越来越强大的狭义人工智能？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7772_"&gt;领先的人工智能能力组织有哪些？&lt;/a&gt;&lt;/li&gt;&lt;li&gt; &lt;a href="https://aisafety.info/?state=5950_"&gt;Google、OpenAI 等是否意识到了风险？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7648_"&gt;什么是“意外收获条款”？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;所有的情况都被考虑到了&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6275_"&gt;人类的命运到底有多大？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=87O6_"&gt;为什么人工智能安全可能不那么重要？有哪些论据？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;人工智能安全的影响&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=3486_"&gt;人工智能对齐研究会很糟糕吗？如何？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6182_"&gt;随着人工智能变得越来越复杂，它的潜在好处是什么？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7794_"&gt;对于人工智能协调的重要性有哪些反对意见？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; 💭意识&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=5642_"&gt;AI可以有情感吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8V5J_"&gt;人工智能有意识吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt; &lt;a href="https://aisafety.info/?state=8390_"&gt;AI 会受苦吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7784_"&gt;我们可以告诉人工智能做道德上正确的事情吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6272_"&gt;将人工智能拟人化并试图用人类的语言来理解它们是否存在危险？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h1&gt; ❓&lt;a href="https://aisafety.info/?state=9TDI_"&gt;不相信？探索论点。&lt;/a&gt;&lt;/h1&gt;&lt;h2&gt; 🤨 超级智能不太可能出现？&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6192_"&gt;为什么我们应该现在就为人类水平的人工智能技术做好准备，而不是等几十年后才更接近呢？&lt;/a&gt;&lt;/li&gt;&lt;li&gt; &lt;a href="https://aisafety.info/?state=6601_"&gt;“智力爆炸”可能永远不会发生吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8H0O_"&gt;超级智能是否会因需要在物理世界中进行实验而减慢速度？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=5952_"&gt;人工智能真的能比人类更聪明吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8E41_"&gt;人工智能能够比人类思考得更快吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt; &lt;a href="https://aisafety.info/?state=8AV6_"&gt;AGI 为何能比全人类更聪明？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; 😌 超级智能不会带来大的改变吗？&lt;/h2&gt;&lt;ul&gt;&lt;li&gt; &lt;a href="https://aisafety.info/?state=6218_"&gt;AI 不会像我们一样吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6188_"&gt;人工智能不就是像其他工具一样的工具吗？它不会只做我们告诉它的事情吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6953_"&gt;人们真的担心人工智能带来的生存风险吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8C7S_"&gt;企业是超级智能吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8E40_"&gt;资本主义难道不是真正的不结盟的超级智能吗？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; ⚠️超级智能不会有风险吗？&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=89ZQ_"&gt;有没有关于未对齐 AGI 的详细示例故事？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6569_"&gt;任何人工智能都将是一个计算机程序。为什么它不按照编程的方式去做呢？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6196_"&gt;机器人不是真正的问题吗？如果人工智能没有直接操纵物理世界的能力，它怎么能造成伤害呢？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=A3MU_"&gt;人工智能难道不需要有寻求权力的驱动力才能构成严重的风险吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=86WT_"&gt;既然人类在数量上拥有巨大优势，难道人类就无法击败不结盟的人工智能吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;超级智能不是很聪明吗？&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6984_"&gt;超级智能难道不会足够聪明，在理解我们的指令时不会犯愚蠢的错误吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6220_"&gt;难道超级智能就不能聪明到能够辨别是非吗？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; 🤔 为什么不只是？&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=3119_"&gt;如果人工智能开始出现问题，我们为什么不能直接将其关闭呢？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6988_"&gt;一旦我们注意到超级智能正在试图接管世界，我们就不能将其关闭或重新编程吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7148_"&gt;如果通用人工智能如此危险，我们为什么不建造它呢？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6174_"&gt;为什么我们不能直接造一个“人工智能儿童”并抚养它呢？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6224_"&gt;为什么我们不能直接使用阿西莫夫的机器人三定律？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6176_"&gt;为什么我们不能把人工智能“放在一个盒子里”，让它无法影响外界呢？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8E3Z_"&gt;我们不能像限制公司造成的损害一样限制人工智能系统造成的损害吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8163_"&gt;为什么人工智能对齐是一个难题？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; 🧐 真正担心的不是…&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=9B85_"&gt;真正的担忧不是滥用吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6412_"&gt;真正令人担忧的不是技术性失业吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8NYD_"&gt;真正的担忧难道不是偏见吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6411_"&gt;真正令人担忧的不是自主武器吗？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; 📜 我有一定的哲学信仰，所以这不是问题&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7636_"&gt;如果我只关心帮助人们今天的生活，人工智能安全还重要吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=9048_"&gt;为什么有宗教信仰的人应该担心人工智能的存在风险？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7638_"&gt;人工智能风险的重要性是否取决于对超人类主义乌托邦的关心？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7608_"&gt;人类灭亡不是一件好事吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6194_"&gt;人工智能安全是指系统变得恶意或有意识并攻击我们吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6222_"&gt;将我们的价值观控制并强加给人工智能，这不是不道德吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6228_"&gt;我们将与机器合并，所以这永远不会成为问题，对吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=967I_"&gt;人工智能存在风险问题难道不是帕斯卡抢劫的一个例子吗？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h1&gt; 🔍&lt;a href="https://aisafety.info/?state=9IDQ_"&gt;想了解研究吗？潜得更深。&lt;/a&gt;&lt;/h1&gt;&lt;h2&gt; 💻平淡的对齐方式&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=89LM_"&gt;什么是平淡对齐？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7333_"&gt;人工智能与深度学习的结合会很难吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;可扩展的监督&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8201_"&gt;什么是人工智能安全辩论？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=935A_"&gt;什么是对抗性训练？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8316_"&gt;联盟研究中心 (ARC) 如何尝试解决潜在知识获取 (ELK) 问题？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7810_"&gt;什么是“六氯环己烷”？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=897J_"&gt;什么是迭代蒸馏和扩增 (IDA)？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=9049_"&gt;什么是诱发潜在知识（ELK）？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8350_"&gt;外部化推理监督计划涉及什么？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;可解释性&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8241_"&gt;什么是可解释性以及有哪些方法？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=97FU_"&gt;可验证性、可解释性、透明度和可解释性之间有什么区别？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8KGQ_"&gt;什么是多义神经元？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=9NRR_"&gt;什么是神经网络中的“多胞体”？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8HIA_"&gt;什么是特征可视化？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8424_"&gt;什么是神经网络模块化？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8426_"&gt;强化学习中的生成可视化是什么样的？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6822_"&gt;我在哪里可以了解可解释性？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;概念进步&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8G1G_"&gt;什么是分片理论？&lt;/a&gt;&lt;/li&gt;&lt;li&gt; &lt;a href="https://aisafety.info/?state=9FQK_"&gt;LLM如何理解为“模拟器”？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;像AGI一样的大脑&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7605_"&gt;全脑模拟有哪些安全问题？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8324_"&gt;我们如何调整学习算法/认知类似于人脑的 AGI？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6590_"&gt;什么是“生物认知增强”？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7820_"&gt;与全脑模拟相关的伦理挑战有哪些？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; 📝 代理粉底&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7782_"&gt;什么是“代理基金会”？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;重要概念&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7853_"&gt;为什么我们期望超级智能能够非常接近效用最大化者？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8QZH_"&gt;什么是子代理？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=87AH_"&gt;什么是“类型签名”？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=89ZU_"&gt;人工智能对齐背景下的“真实姓名”是什么？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8G1I_"&gt;什么是互信息？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;决策理论&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7777_"&gt;决策理论有哪些不同版本？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7781_"&gt;什么是“功能决策理论”？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7779_"&gt;什么是“因果决策理论（CDT）”？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7778_"&gt;什么是“证据决策论”？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6536_"&gt;我应该阅读什么来学习决策理论？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;研究方向&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7673_"&gt;什么是“按我的意思做”？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=92JB_"&gt;权力寻求定理是什么？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6119_"&gt;你能为人工智能设定一个“对世界影响最小化”的目标吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6380_"&gt;什么是“量化器”？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6449_"&gt;切断分布的前百分之几会提高量化器的安全性吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8365_"&gt;什么是下贝叶斯主义？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6939_"&gt;什么是“相干外推意志（CEV）”？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7616_"&gt;道德哲学中的主要理论是什么？其中哪些理论在技术上最容易编码到人工智能中？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; 🏛️治理&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8QH5_"&gt;人工智能能力发展放缓会降低生存风险吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7626_"&gt;是否存在政府可以投入大量资源的人工智能协调项目？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7596_"&gt;大家在人工智能治理方面都在做什么？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8517_"&gt;关于通用人工智能发展的国际条约会是什么样子？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7632_"&gt;联合国是否担心人工智能带来的生存风险？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; 🔬 研究机构&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;概述&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6178_"&gt;人工智能协调组织正在采取哪些方法？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8392_"&gt;大家在人工智能对齐方面都在做什么？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=9J1L_"&gt;技术一致性研究的主要类别有哪些？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6479_"&gt;目前正在进行哪些人工智能一致性研究议程？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8KGR_"&gt;有哪些不同的人工智能对齐/安全组织和学者正在研究哪些？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8JYX_"&gt;简而言之，主要的人工智能安全组织和学术界正在做什么？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;平淡无奇&lt;ul&gt;&lt;li&gt;大实验室&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8368_"&gt;OpenAI 的一致性研究议程是什么？&lt;/a&gt;&lt;/li&gt;&lt;li&gt; &lt;a href="https://aisafety.info/?state=8343_"&gt;DeepMind 的安全团队在做什么？&lt;/a&gt;&lt;/li&gt;&lt;li&gt; &lt;a href="https://aisafety.info/?state=8XBK_"&gt;DeepMind是如何进行对抗训练的？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;学术实验室&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8469_"&gt;萨姆·鲍曼在研究什么？&lt;/a&gt;&lt;/li&gt;&lt;li&gt; &lt;a href="https://aisafety.info/?state=8326_"&gt;CAIS正在开展哪些项目？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8342_"&gt;大卫·克鲁格在做什么？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;其他组织&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=85EK_"&gt;联盟研究中心 (ARC) 的研究议程是什么？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8374_"&gt;应该的研究议程是什么？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=85E4_"&gt;红木研究的议程是什么？&lt;/a&gt;&lt;/li&gt;&lt;li&gt; &lt;a href="https://aisafety.info/?state=8314_"&gt;Aligned AI / Stuart Armstrong 正在做什么？&lt;/a&gt;&lt;/li&gt;&lt;li&gt; &lt;a href="https://aisafety.info/?state=7749_"&gt;Redwood Research 如何进行对抗性训练？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;代理基金会&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8327_"&gt;什么是人类兼容人工智能中心（CHAI）？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8364_"&gt;斯科特·加拉布兰特 (Scott Garrabrant) 和艾布拉姆·德姆斯基 (Abram Demski) 在做什么？&lt;/a&gt;&lt;/li&gt;&lt;li&gt; &lt;a href="https://aisafety.info/?state=6300_"&gt;MIRI 正在解决哪些技术问题？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8378_"&gt;约翰·温特沃斯的研究议程是什么？&lt;/a&gt;&lt;/li&gt;&lt;li&gt; &lt;a href="https://aisafety.info/?state=8357_"&gt;MIRI 对技术协调有何看法？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8340_"&gt;什么是精炼？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8348_"&gt;迪伦·哈德菲尔德-梅内尔的论文是什么？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;其他&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=1250_"&gt;方尖碑的研究议程是什么？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8333_"&gt;长期风险中心 (CLR) 的研究议程是什么？&lt;/a&gt;&lt;/li&gt;&lt;li&gt; &lt;a href="https://aisafety.info/?state=8349_"&gt;Encultured 正在做什么？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h1&gt; 🤝&lt;a href="https://aisafety.info/?state=8TJV_"&gt;想为人工智能安全提供帮助吗？参与其中！&lt;/a&gt;&lt;/h1&gt;&lt;h2&gt; 📌 一般&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7590_"&gt;我可以在五分钟内采取哪些行动来为人工智能安全事业做出贡献？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7640_"&gt;我应该如何以及为什么形成自己对人工智能安全的看法？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; 📢 外展&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8U2Q_"&gt;我如何开展公共人工智能安全推广工作？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8509_"&gt;哪些链接对于在社交媒体或其他环境中分享特别有价值？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8U2R_"&gt;我如何在学术界和专家中开展 AGI 安全宣传工作？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7769_"&gt;我怎样才能说服别人并很好地表达论点？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; 🧪 研究&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6703_"&gt;我想从事人工智能对齐工作。&lt;/a&gt;&lt;/li&gt;&lt;li&gt; 📚 教育和职业道路&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8W8D_"&gt;关于人工智能安全，我可以写什么硕士论文？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7763_"&gt;我应该在大学学习哪些科目来为一致性研究做好准备？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8U32_"&gt;我想采取重大措施为人工智能协调做出贡献（例如将其作为我的职业）。我应该怎么办？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8U30_"&gt;我想专注于人工智能调整，但最好首先优先考虑改善我的生活状况。我应该怎么办？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8U2J_"&gt;作为一名软件工程师，我该如何努力实现 AI 一致性？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt; 📋 指导和指导&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7651_"&gt;我在哪里可以找到成为研究员的指导和建议？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8U2K_"&gt;我应该与谁谈论我的非研究人工智能对齐编码项目想法？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6703_"&gt;我怎样才能获得资金？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt; 🧪 项目和参与&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8U2O_"&gt;我想做人工智能对齐的实验工作（即机器学习、编码）。我应该怎么办？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6474_"&gt;我想帮助人工智能调整，但不一定要对生活做出重大改变。我可以做哪些简单的事情来做出贡献？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8UMA_"&gt;我如何进行有关人工智能对齐的概念、数学或哲学工作？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=85E0_"&gt;我可以尝试哪些练习和项目？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8U2S_"&gt;我如何利用社会科学背景来帮助人工智能协调？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8U2I_"&gt;如何进行机器学习编程工作来帮助 AI 协调？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8U2M_"&gt;我应该如何利用我的人工智能对齐机器学习研究想法？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8U2V_"&gt;我应该如何利用我的想法来帮助 AI 协调？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; 🏛️治理&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8IZE_"&gt;我可以尝试哪些人工智能治理练习和项目？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7754_"&gt;有哪些有用的人工智能政策资源？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8U2Z_"&gt;我如何制定人工智能政策？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; 🛠️ 操作和元数据&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6708_"&gt;我在哪里可以找到可以谈论 AI 协调的人？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8U2W_"&gt;我如何才能帮助人工智能对齐研究人员变得更有效，例如作为教练？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8U2X_"&gt;我如何评估人工智能协调项目并分配资助？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8U2P_"&gt;如何围绕 AI 协调开展组织或运营工作？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; 💵 经济上的帮助&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6481_"&gt;向人工智能安全组织捐赠少量资金会产生重大影响吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8U2Y_"&gt;我有兴趣为人工智能调整提供重要的财务支持。我该怎么办？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; 📚其他资源&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=2222_"&gt;在哪里可以找到有关人工智能安全的视频？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8264_"&gt;有哪些 AGI 安全培训计划和课程？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=5635_"&gt;在哪里可以了解有关 AI 对齐的更多信息？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=MEME_"&gt;人工智能安全模因维基&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6470_"&gt;关于人工智能对齐有哪些好的资源？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=7619_"&gt;有哪些关于人工智能对齐的好播客？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=8159_"&gt;有哪些关于 AGI 安全的好书？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6713_"&gt;我想更深入地了解人工智能对齐文献。我应该去哪里看？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://aisafety.info/?state=6320_"&gt;我如何更新我对人工智能安全紧迫性的情绪状态？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/ZqxP6pJe53xRnRb4j/aisafety-info-the-table-of-content#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sun, 31 Dec 2023 13:57:15 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/ZqxP6pJe53xRnRb4j/aisafety-info-the-table-of-content</guid></item><item><title>智能操作系统</title><link>https://www.lesswrong.com/posts/8zBgu6xmvTxit3e8b/aios</link><description>发布于 2023 年 12 月 31 日下午 1:23（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;概括&lt;/h2&gt;&lt;p&gt;深度神经网络只要有足够的时间，几乎可以做任何事情，这是对这一立场的&lt;i&gt;反证&lt;/i&gt;。以俏皮的苏格拉底对话形式。&lt;/p&gt;&lt;h2&gt;欢迎来到艾奥斯！&lt;/h2&gt;&lt;p&gt; Snay/Coyle Systems 自豪地推出 AIOS。&lt;/p&gt;&lt;p&gt;其名义“时钟速度”为 60Hz，代表典型屏幕的刷新率和典型人机界面设备的轮询率。但它并不马虎！&lt;/p&gt;&lt;p&gt;其输出略高于 16.7 亿位，代表每个时钟周期通过 100Gbps 以太网连接发送多达 200MB 的任意数据、4K 视频帧缓冲区/监视器输出以及 0.0166 秒的多通道高清音频。&lt;/p&gt;&lt;p&gt;其输入是其自身先前的输出状态以及来自以太网连接和标准人机接口设备的 0.0166 秒数据缓冲区：键盘、鼠标/触控板/触摸屏，以及未来版本中的麦克风和网络摄像头。&lt;/p&gt;&lt;p&gt;它的隐藏层包含 10^ &lt;i&gt;x 个&lt;/i&gt;神经元，其中&lt;i&gt;x&lt;/i&gt;非常非常大。它接受了 Snay/Coyle 的“传统”（很快就会过时）人类编码操作系统 BSOS 的人类用户同意的 10^ &lt;i&gt;y&lt;/i&gt;次会话训练，其中&lt;i&gt;y&lt;/i&gt;也非常大。&lt;/p&gt;&lt;p&gt;该模型由 RLHF 进行了微调。&lt;/p&gt;&lt;p&gt;现在它运行一个可用的 BSOS 传真，无需任何代码！&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;&lt;i&gt;这是我听过的最愚蠢的话。&lt;/i&gt;&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;你能说说为什么吗？&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;&lt;i&gt;我的意思是，从哪里开始呢？首先，当然涉及到代码。模型如何进行推理？&lt;/i&gt;&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;公平地说，这有点自由。显然，该&lt;i&gt;模型&lt;/i&gt;运行在标准 CPU/GPU 架构上。但训练后，模型本身的权重不包含一丝BSOS源代码。寄存器、指令集、CPU 缓存、离散图形处理、内核空间、用户空间以及所有那些烦人的细节等概念也消失了。&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;&lt;i&gt;好吧，无论如何。所以你是说 AIOS 对于人类用户来说感觉就像 BSOS 一样？&lt;/i&gt;&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;如您所知，DNN 是曲线拟合逼近器，精度必然有限，即使在数字硬件上以离散形式运行时，其输出的最低有效位也会在一定程度上偏离任何精确的分析基础事实。但&lt;a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem"&gt;万能逼近定理&lt;/a&gt;告诉我们，我们只需要不断提高逼近的精度即可保持在任意小的误差范围内。&lt;/p&gt;&lt;p&gt;那么，AIOS 的输出与 BSOS 的输出&lt;i&gt;完全匹配&lt;/i&gt;吗？不会。对于人类用户来说，它们看起来会一样吗？是的！如果一个外围的人类专家能够以某种方式分辨出差异，那么我们将提高精度，重新训练并发布另一个版本。&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;&lt;i&gt;我几乎可以买这个，但这并不是故事的全部。&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;如果软件开发人员尝试在 AIOS 的 IDE 中编写和编译代码该怎么办？每个可执行文件都有一个校验和，可对其内容进行哈希处理以确保数据未损坏。如果校验和与内容不匹配，操作系统将拒绝运行该文件，因为如果匹配，它几乎肯定会崩溃。可执行代码和源代码对错误极其敏感。一个翻转的钻头就能让机器坏掉。&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;当低于 100% 的准确度意味着 100% 失败时，AIOS 如何输出可执行文件的可执行近似值？&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;由于现在大多数非可执行文档也经过了校验和验证，我预见 AIOS 会喷出无用的胡言乱语，没有任何系统可以解释这些胡言乱语，即使它成功地模拟了其父级自己的安全协议，即使它本身也无法解释。&lt;/i&gt;&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;嗯，是的，我们在开发过程中确实遇到了这个问题。解决方案很简单，您现在已经熟悉了：提高精度！我们只是简单地扩展模型、其训练数据和训练时间，直到其输出在所有合理情况下与 BSOS 的输出完全相同。我可以给你技术细节...&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;&lt;i&gt;哦，请这样做。&lt;/i&gt;&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;我们早期的原型使用了自定义激活函数，因此 132.8 亿个输出中的每一个都始终为 0 或 1。事后看来，这是一个错误，因为它保护了我上面提到的最低有效位抖动，使我们无法将其最小化。为了获得更多控制权，我们用标准 softmax 替换了自定义函数，并将连续的 [0-1] 输出值四舍五入为最接近的整数。通过这种设置&lt;strong&gt;（&lt;/strong&gt;一种微不足道的 argmax 和一种微不足道的附加层&lt;strong&gt;），&lt;/strong&gt;我们可以对模型给出的预输出值过于接近 0.5 进行惩罚，并继续训练，直到合理情况下的所有预输出值都足够接近 0或 1 该系综与 BSOS 位相同。&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;&lt;i&gt;这是你第二次使用“在合理情况下”这个词。我怀疑这些话隐藏着大量的掩饰。&lt;/i&gt;&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;以什么方式？&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;&lt;i&gt;如何保证图灵完备的有限状态机的每个状态都可以进行近似？难道在近似的定义中，它所近似的某些状态不是它无法访问的吗？&lt;/i&gt;&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;嗯，是的，但对于大多数用户来说&lt;strong&gt;-&lt;/strong&gt;&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;&lt;i&gt;从两个意义上来说，大多数是多少：数量上的比例是多少，质量上的人口数量是多少？&lt;/i&gt;&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;在我们的焦点小组中，99.4% 的自称休闲用户对 AIOS 的体验完全满意，即使在学者和工程师中，这一数字也高达 91%！&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;&lt;i&gt;四西格玛对于普通用户来说并不坏，我会给你的。另一个数字更能说明问题。它说明的是，AIOS 与所有其他基于神经网络的模型一样，只能在其训练集的语义附近工作。尝试用&lt;/i&gt;&lt;i&gt;它做任何真正新的事情，它都会失败。学者和工程师正在探索或至少试图探索原始认知空间。在你的焦点小组中，也许 91% 的人只是在尝试……&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;无论如何，我非常确信你现在正在拉动我的链子。但我可以再问你几个问题吗？&lt;/i&gt;&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;当然！&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;&lt;i&gt;既然 AIOS 可以访问互联网，那么如何阻止它通过将其输入中继到 BSOS 实例并将该实例的输出冒充为自己的输出来进行欺骗呢？&lt;/i&gt;&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;没什么，但我们从一开始就监控了它的流量，并且没有检测到此类活动。&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;&lt;i&gt;如何阻止 Snay/Coyle Systems 通过简单地运行 BSOS 实例而不是 AIOS 实例进行作弊？&lt;/i&gt;&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;我很伤心你竟然提出这样的事情！&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;&lt;i&gt;好吧，让我们假设善意。无论如何，这种思路让我想到了最后的问题。训练每一代 AIOS 需要多少计算时间，以及运行每个推理周期需要多少时间？&lt;/i&gt;&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;天哪，你真是个好奇的人不是吗？让我检查一下我的笔记...啊，我们到了。第 0 代需要 3.4 × 10^44 浮点运算，第 1 代需要 6.1 × 10^47，第 2 代需要 13.4 × 10^49。一个推理刻度仅需要 2 × 10^13！&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;&lt;i&gt;啊啊啊我们就完成了。宇宙只存在了 4.32 × 10^17 秒，所以除非 Snay/Coyle Systems 拥有&lt;/i&gt;&lt;a href="https://en.wikipedia.org/wiki/Kardashev_scale"&gt;&lt;i&gt;Kardashev&lt;/i&gt;&lt;/a&gt; &lt;i&gt;II 型资源或正在使用一些量子计算诡计，否则你就是在胡说八道。 （这并不是胡言乱语，需要如此多的计算时间&lt;/i&gt;&lt;i&gt;；胡言乱语，如此多的计算时间可以在实践中进行整理，无论是用于在宇宙结束之前结束的训练，还是用于任何接近实时的推理，而不是军事-级超级计算机。我们甚至不要猜测您如何或从哪里获得了数千万小时的训练数据。）&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;更重要的是，AIOS还有很多隐藏的缺点。其 132.8 亿个二进制节点的输出大小大概是由 100 Gigabit 连接的带宽和典型接口的 1/60 秒刷新时间（加上一些额外的图形帧和声音）的乘积决定的。如果 AIOS 需要超过 200MB 的工作内存（此类配置为用户指定的任务提供）怎么办？如果用户有一个实时应用程序，其中最小延迟至关重要并且 0.01666 秒的硬限制高得可笑，该怎么办？ （您对这两个问题的回答无疑会是“扩展模型直到超过所述要求”的一些变化，但我拒绝这一点，因为模型已经过大了，不切实际。）&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;不管怎样，谢谢你证明了我的观点。在涉及预先存在的数据的近似和分布时，神经网络非常有用。但是，在良好输出稀疏并且不会在潜在空间中聚集或形成连续可遍历路径的情况下，它们是错误的工具。&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;在某些情况下，并且总是会出现这样的情况：仅仅因为你可以用人工智能系统替换非人工智能系统，就相当于建造一台效率极低的鲁布·戈德堡/希思·罗宾逊机器。&lt;/i&gt;&lt;/p&gt;&lt;h2&gt;结语&lt;/h2&gt;&lt;p&gt;我现在可以问&lt;i&gt;你&lt;/i&gt;一个问题吗？&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;&lt;i&gt;哦。好吧，当然。&lt;/i&gt;&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;你不是&lt;a href="https://www.lesswrong.com/posts/6GgnrfP4XFsYC29oT/picasso-in-the-gallery-of-babel"&gt;在其他地方写过&lt;/a&gt;与你在这里争论的相反的观点吗？你不是说我们应该害怕生成模型渐近创造人类可以创造的任何东西的能力吗？&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;&lt;i&gt;是的，在&lt;/i&gt;&lt;a href="https://www.lesswrong.com/posts/6GgnrfP4XFsYC29oT/picasso-in-the-gallery-of-babel"&gt;&lt;i&gt;那篇文章&lt;/i&gt;&lt;/a&gt;中&lt;i&gt;我确实声称，当 GenAI（不可避免地？）达到奇点时，我们将面临一些严峻的形而上学天气，之后没有人可以知道任何新内容是人造的还是生成的。&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;我认为它与当前帖子的论文有两个不同之处和兼容之处。&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;首先，生成奇点最糟糕的后果不是技术上的，而是心理上的：人类会对新常态感到偏执和/或冷漠和/或困惑和/或愤怒，在新常态中，艺术消费者不知道他们在哪里消费什么来自，艺术家必须花时间让艺术消费者相信它来自他们。所有这一切都可能发生在一个也禁止实用神经操作系统的宇宙中。&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;其次，大多数或所有形式的创意艺术都可以（无论好坏）被编码在潜在空间中，在这些潜在空间中，好的输出确实&lt;/i&gt;会&lt;i&gt;聚集并形成连续的可穿越路径。因此，它们本质上容易受到生成近似的影响，而有用的有限状态机却不会。&lt;/i&gt;&lt;/p&gt;&lt;br /&gt;&lt;br /&gt;&lt;a href="https://www.lesswrong.com/posts/8zBgu6xmvTxit3e8b/aios#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sun, 31 Dec 2023 13:23:56 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/8zBgu6xmvTxit3e8b/aios</guid></item><item><title>人工智能对齐元策略</title><link>https://www.lesswrong.com/posts/TALmStNf6479uTwzT/ai-alignment-metastrategy</link><description>发布于 2023 年 12 月 31 日 12:06 PM（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;我将“协调策略”称为解决技术问题的高级方法&lt;span class="footnote-reference" id="fnrefrea58985c4m"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrea58985c4m"&gt;[1]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。例如，价值学习是一种策略，而将一致性研究委托给人工智能则是另一种策略。我将“对齐元策略”称为及时有效地集中解决技术问题的高级方法。 （随后将提供示例。）&lt;/p&gt;&lt;p&gt;在&lt;a href="https://www.alignmentforum.org/posts/8HYJwQepynHsRKr6j/critical-review-of-christiano-s-disagreements-with-yudkowsky"&gt;上一篇文章&lt;/a&gt;中，我总结了我对平淡对齐的批评。然而，我对相关转移策略的分析太草率了。我将尝试在这里对此进行一些补救，并简要讨论其他元策略，以作为对比和比较的点。&lt;/p&gt;&lt;h1&gt;保守元策略&lt;/h1&gt;&lt;p&gt;保守元策略遵循以下算法：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;尽可能停止此流程之外的所有人工智能能力工作。&lt;/li&gt;&lt;li&gt;将智能代理的数学理论发展到我们可以高度自信地提出适当的对齐协议的水平。理想情况下，理论问题的解决顺序应该是直接能力应用的结果尽可能晚地出现。&lt;/li&gt;&lt;li&gt;设计并实施理论的实证检验，在理论包含错误或理论假设在实践中被违反的情况下，将风险最小化。&lt;/li&gt;&lt;li&gt;如果测试显示问题，请返回步骤 2。&lt;/li&gt;&lt;li&gt;以同样的方式继续进行更雄心勃勃的测试，直到您准备好部署人工智能防御系统。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;这是我自己最喜欢的元策略。它可能失败的主要原因是，如果我们未能阻止的不保守研究在我们部署人工智能防御系统之前创建了不一致的 TAI（目前，我们距离完成步骤 2 还有很长的路要走）。&lt;/p&gt;&lt;p&gt;我认为很明显，一个有能力的文明会走这条路，因为它似乎是唯一一条能够在不承担不必要风险的情况下带来良好的长期结果的道路&lt;span class="footnote-reference" id="fnrefb691imh53gs"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnb691imh53gs"&gt;[2]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。当然，这&lt;i&gt;本身&lt;/i&gt;并不足以证明，在我们的实际文明中，保守的元策略对于那些关心人工智能风险的人来说是最好的。但是，这是暗示性的。&lt;/p&gt;&lt;p&gt;除此之外，我不会在这里阐述保守元策略的理由。有兴趣的读者可以翻到&lt;a href="https://www.lesswrong.com/posts/ZwshvqiqCvXPsZEct/the-learning-theoretic-agenda-status-2023"&gt;1&lt;/a&gt; &lt;a href="https://www.lesswrong.com/posts/qpbYwTqKQG8G7mdFK/the-reasonable-effectiveness-of-mathematics-or-ai-vs"&gt;2&lt;/a&gt; &lt;a href="https://www.lesswrong.com/posts/suxvE2ddnYMPJN9HD/realism-about-rationality?commentId=uGePTr7mcjhS9sDJY"&gt;3&lt;/a&gt; &lt;a href="https://www.lesswrong.com/posts/Gg9a4y8reWKtLe3Tn/the-rocket-alignment-problem"&gt;4&lt;/a&gt; &lt;a href="https://www.lesswrong.com/posts/5bd75cc58225bf0670375575/the-learning-theoretic-ai-alignment-research-agenda"&gt;5&lt;/a&gt; 。&lt;/p&gt;&lt;h1&gt;渐进主义元战略&lt;/h1&gt;&lt;p&gt;增量元策略遵循以下算法：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;发现人工智能能力的进步（通过任何方式，包括反复试验）。&lt;/li&gt;&lt;li&gt;找到一种方法来协调新的人工智能设计（优先考虑您希望进一步扩展的解决方案）。&lt;/li&gt;&lt;li&gt;使用经验测试和可解释性工具的组合来验证一致性。&lt;/li&gt;&lt;li&gt;如果验证失败，请返回步骤 2。&lt;/li&gt;&lt;li&gt;如果可能，使用当前的能力水平部署人工智能防御系统。&lt;/li&gt;&lt;li&gt;转到步骤 1。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;这（或多或少）是平淡对齐的追随者所青睐的元策略。特别是，这就是参与领先人工智能实验室的相对具有安全意识的参与者所提出的计划。&lt;/p&gt;&lt;p&gt;将我们的希望寄托在这一元策略上存在三个相互强化的主要问题，我将在下面讨论。每个问题都有两个方面：“设计”方面，即如果实施渐进元策略的最佳版本，将会发生什么；以及“实施”方面，即人工智能实验室在实践中发生的情况（即使他们声称遵循渐进主义元战略）。&lt;/p&gt;&lt;h2&gt;信息安全&lt;/h2&gt;&lt;h3&gt;设计&lt;/h3&gt;&lt;p&gt;如果在步骤 1 中发现了新的人工智能能力，并且允许知识传播，那么不负责任的行为者将继续将其与额外的进步相结合，然后再在新的层面上解决对齐问题。理想情况下，要么新功能至少在整个迭代结束之前保持秘密，要么政府政策应该防止任何参与者颠覆元战略，或者需要将两者结合起来。实施上述内容涉及重大的制度和政治挑战，这破坏了整个元战略的可行性。&lt;/p&gt;&lt;p&gt;现在，有人可能会说，保守的元策略也存在同样的问题：毕竟，一些理论发现也可能被不负责任的行为者用来实现危险的人工智能能力进步。然而，当涉及到理论研究时，问题只在&lt;i&gt;某些&lt;/i&gt;情况下才会出现，并且通常需要不负责任的行为者采取额外的步骤（从理论到实践）。此外，由于危险的人工智能能力进步只有在坚实的理论基础上才会发生，因此人们更有希望风险是透明的、可预测的和可避免的。&lt;/p&gt;&lt;p&gt;因此，虽然保守的元策略也要求对不负责任的行为者进行监管，但整个方法允许在这部分失败时进行&lt;i&gt;优雅的降级&lt;/i&gt;。另一方面，渐进主义元战略要求直接开展风险最大的研究：通过反复试验发现不透明的人工智能设计，并仅根据其能力进行选择。这与&lt;a href="https://en.wikipedia.org/wiki/Gain-of-function_research"&gt;功能获得&lt;/a&gt;研究背后的可疑原理相当。&lt;/p&gt;&lt;h3&gt;执行&lt;/h3&gt;&lt;p&gt;在实践中，即使是“具有安全意识”的人工智能实验室也会分享许多有关其研究的技术细节。即使他们不这样做，他们的员工也不会因为出色的可信度和对风险的了解而被选中。他们中的许多人最终可能会继续向其他组织传播知识，即使名义上受到保密协议&lt;span class="footnote-reference" id="fnrefiee9wtsfaj"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fniee9wtsfaj"&gt;[3]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;的约束。我也怀疑他们是否维持了非常高的网络安全标准。所有这一切与真正认真对待保密的组织（处理机密信息的军事和政府机构）形成鲜明对比。&lt;/p&gt;&lt;p&gt;另请参阅： &lt;a href="https://www.lesswrong.com/posts/keiYkaeoLHoKK4LYA/six-dimensions-of-operational-adequacy-in-agi-projects"&gt;运营充足性的六个维度&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;测量对准&lt;/h2&gt;&lt;h3&gt;设计&lt;/h3&gt;&lt;p&gt;渐进元策略的第三步需要验证一致性。但是，我们如何判断人工智能是否一致呢？对齐到底&lt;i&gt;意味着&lt;/i&gt;什么？一种可能性是经济标准：“当人工智能的行为足够接近设计师的意图，成为一种有利可图的产品时，它就是一致的”。这对公司高管来说一定很有吸引力。然而，根据这个定义，随着能力进步（或只是时间）的变化，一致性可能会突然发生变化，从而破坏渐进主义的整个情况。&lt;/p&gt;&lt;p&gt;即使能力进展非常顺利，后者也是如此。例如，想象一下代理人工智能会遵循我们的愿望，因为它们知道它们在我们的权力范围内，直到它们不在我们的权力范围内。我们逐渐将整个经济交给人工智能，然后有一天人工智能决定是时候摆脱人类了。即使一些问题提前显现出来，在某些时候，面对经济激励，也&lt;i&gt;很难&lt;/i&gt;阻止/减缓这一趋势。&lt;/p&gt;&lt;p&gt;即使有最好的意图，衡量一致性也很困难。一个困难是&lt;a href="https://www.lesswrong.com/posts/zthDPAjh9w6Ytbeks/deceptive-alignment"&gt;欺骗性的对齐&lt;/a&gt;：代理人工智能战略性地寻找&lt;i&gt;显得&lt;/i&gt;对齐的方式，因为这种外观与其内部世界模型下的效用相关（出于明显的原因）。但即使人工智能不是故意欺骗我们，我们仍然可能会选择一种人工智能设计，其行为在局部看起来是一致的，但内部推理与我们的预期截然不同，或者我们最终可能会得到过于复杂而难以解释的行为。有关更多讨论，请参阅&lt;a href="https://www.lesswrong.com/posts/xFotXGEotcKouifky/worlds-where-iterative-design-fails"&gt;Wentworth 的文章&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;测量问题的令人满意的解决方案需要可解释性工具，这些工具可以&lt;i&gt;在完成单个迭代之前&lt;/i&gt;检测人工智能推理中的任何意外动机。然而，我们距离拥有这样的工具还很远。此外，甚至原则上也不清楚这些工具是为什么样的人工智能设计而存在的。更糟糕的是，即使我们有候选工具，我们如何知道它们可以正常工作？没有理论基础或替代可靠的测量方法？&lt;/p&gt;&lt;h3&gt;执行&lt;/h3&gt;&lt;p&gt;AFAICT，人工智能实验室只是遵循经济标准。我们不知道大型语言模型等内部会发生什么样的推理过程，并且没有工具来确定这一点。当发现问题时，公司会尽最大努力进行修补，但有些问题（越狱、幻觉）仍然没有通用的解决方案。我指出这一点并不是因为这些具体问题本身很重要，而是因为它表明这些公司实际上根本没有遵循渐进主义元战略：尽管事实上他们的产品在某种意义上明显不一致，但他们仍在继续复合能力进步。&lt;/p&gt;&lt;h2&gt;不可预测的进展&lt;/h2&gt;&lt;p&gt;如果人工智能能力的进展不能顺利且可预测，会发生什么？ （我相信这会发生。）它产生了两个问题：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;当发现一项创新极大地改变了设计时，以前的对准技术可能变得无关紧要，以至于新设计的工作对准技术不能是对先前技术的小修改。这会让我们在步骤 2-3 中陷入危险的很长一段时间，在此期间，如果信息安全性不够完美，信息可能会泄漏。此外，这可能意味着之前的大量对齐工作都被浪费了。&lt;/li&gt;&lt;li&gt;当发现一项创新能够大幅提升功能时，就会在 SOTA AI 和 SOTA 一致的 AI 之间产生差距。如果这个差距达到决定性的战略优势，结果将是灾难性的。如果这种差距本身没有达到决定性的战略优势，那么仍然存在信息泄露的风险，让不负责任的行为者进一步加剧差距。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;不可预测的进展与信息安全故障的结合可能会造成极其危险的情况，负责任的参与者需要艰难地追赶不负责任的参与者，即使不负责任的参与者太弱而无法“赢得”比赛。这种考虑&lt;i&gt;不包括&lt;/i&gt;“负责任的”行为者通过实验室实验终结世界的风险。&lt;/p&gt;&lt;p&gt;我在这里不区分“设计”和“实现”，因为增量主义元策略仅被人工智能实验室（甚至是名义上的）捍卫了几年，并且在这段时间里并没有出现很多大型和意想不到的创新&lt;span class="footnote-reference" id="fnrefotzj53sx3c"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnotzj53sx3c"&gt;[ 4]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。&lt;/p&gt;&lt;h1&gt;巴特勒元策略&lt;/h1&gt;&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Dune_(franchise)#The_Butlerian_Jihad"&gt;巴特勒的元&lt;/a&gt;策略是：无限期推迟 TAI。本质上，它需要在没有 TAI 帮助的情况下创建一个 AI 防御系统&lt;span class="footnote-reference" id="fnrefip3fa84i5la"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnip3fa84i5la"&gt;[5]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。&lt;/p&gt;&lt;p&gt;这种元策略没有太多拥护者，但我仍然发现为其命名很有用。大多数元策略都存在于一个连续体中，其中巴特勒元策略是一个（极端）端点，具体取决于他们对停止不负责任形式的人工智能研究的乐观程度以及持续时间。&lt;/p&gt;&lt;p&gt;这种元策略的主要问题是：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;在没有 TAI 帮助的情况下创建一个足够的人工智能防御系统是极其困难的，可能比解决技术协调问题还要困难得多。&lt;/li&gt;&lt;li&gt;这种元策略中最好的情况是没有 TAI 的未来，这意味着放弃巨大的价值。&lt;/li&gt;&lt;/ul&gt;&lt;h1&gt;超人类主义元战略&lt;/h1&gt;&lt;p&gt;超人类主义元策略遵循以下算法：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;尽可能停止所有有关人工智能的工作，或者至少停止任何与能力有关的工作。&lt;/li&gt;&lt;li&gt;创造能够大幅增强人类智能的技术。&lt;/li&gt;&lt;li&gt;将剩下的问题委托给增强人类（他们可能会比我们能想出的任何方法更好地解决这个问题）。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;这种元策略是由埃利泽·尤德科夫斯基（Eliezer Yudkowsky） &lt;a href="https://www.lesswrong.com/posts/8HYJwQepynHsRKr6j/critical-review-of-christiano-s-disagreements-with-yudkowsky?commentId=9pKofQAchdgCH8jjm"&gt;倡导的&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;我对这种元策略持悲观态度有 3 个原因：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;在我看来，大幅增强成年人的智力似乎比解决技术人工智能协调问题更困难。我对后者有一个&lt;a href="https://www.alignmentforum.org/posts/ZwshvqiqCvXPsZEct/the-learning-theoretic-agenda-status-2023"&gt;计划&lt;/a&gt;，但我不确定对前者有什么认真的计划。 （尽管，请参阅&lt;a href="https://www.lesswrong.com/posts/JEhW3HDMKzekDShva/significantly-enhancing-adult-intelligence-with-gene-editing"&gt;此&lt;/a&gt;。）通过&lt;a href="https://www.lesswrong.com/posts/k8pj8kMxotFGqn4mp/request-use-epilogenics-instead-of-eugenics-in-most"&gt;表观遗传学&lt;/a&gt;增强智力似乎更容易处理，但需要很长时间。&lt;/li&gt;&lt;li&gt;目前尚不清楚这种转移策略是否安全。一方面，我们有一个一致的起点（人类基线）。另一方面，我们正在处理大量未记录的意大利面条代码（人脑的设计）。似乎很容易想象，强烈提高智力的变化也可能以至关重要但难以察觉的方式大幅改变价值观（除其他原因外，因为增强型人类会被激励去隐藏它）。我们最终可能会将未来交给一个超级智能的精神病患者种族。&lt;/li&gt;&lt;li&gt;在大多数国家，所需的人体实验（我认为）是非法的，使其合法化是一个重大挑战，特别是在时间限制下。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;由于增强人类是安全的，这种元策略的优点是（与渐进主义相比）它不会通过提高人工智能能力来烧毁公共资源。因此，并行研究这一方法和保守方法可能是有意义的。&lt;/p&gt;&lt;ol class="footnotes"&gt;&lt;li class="footnote-item" id="fnrea58985c4m"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefrea58985c4m"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;对于什么是“高级”，可以有一些任意的选择。更准确地说，“大方向”和“详细解决方案”之间存在多层甚至频谱。例如， &lt;a href="https://www.alignmentforum.org/posts/ZwshvqiqCvXPsZEct/the-learning-theoretic-agenda-status-2023#Physicalist_Superimitation"&gt;物理主义超级模仿&lt;/a&gt;比单纯的价值学习详细得多，但对于实际实施来说仍然不够详细。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnb691imh53gs"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefb691imh53gs"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;一些批评家怀疑我所追求的那种数学理论是否可能。我要向那些人指出，一个有能力的文明至少会&lt;i&gt; &lt;/i&gt;在尝试其他事情之前，认真&lt;i&gt;尝试&lt;/i&gt;创建该理论。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fniee9wtsfaj"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefiee9wtsfaj"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;我认为执行 NDA 相当困难：公司没有一种简单的方法来知道 NDA 是否被违反，即使它怀疑这一点，证明这一点也很困难，特别是考虑到技术细节在此过程中不断变化。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnotzj53sx3c"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefotzj53sx3c"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;据我所知。如果有，但他们保密，那么，荣誉！&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnip3fa84i5la"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefip3fa84i5la"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;一般来说，人工智能防御系统可以是&lt;i&gt;预防性的&lt;/i&gt;，即旨在防止未对齐的人工智能被创建，而不是在创建后将其禁用。在这种情况下，肯定必须采取预防措施。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ol&gt;&lt;br /&gt;&lt;br /&gt;&lt;a href="https://www.lesswrong.com/posts/TALmStNf6479uTwzT/ai-alignment-metastrategy#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sun, 31 Dec 2023 12:06:11 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/TALmStNf6479uTwzT/ai-alignment-metastrategy</guid></item><item><title>AI 对齐的难度是否会破坏 FOOM？</title><link>https://www.lesswrong.com/posts/Acg9Y5gJb8mceB2fW/does-the-hardness-of-ai-alignment-undermine-foom</link><description>发布于 2023 年 12 月 31 日上午 11:05（格林尼治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;既然人工智能对齐很难的论点不依赖于我们智力水平的任何细节，那么这些相同的论点难道不应该说服未来的人工智能不要进行自我改进吗？&lt;/p&gt;&lt;p&gt;更具体地说，如果我们应该期望我们构建的更智能的人工智能具有一个与我们自己的目标不一致的简单全局效用函数的论点是有效的，那么为什么同样的论点不能让未来的人工智能相信它可以不相信它生成的更智能的人工智能会分享它的目标吗？&lt;/p&gt;&lt;p&gt;请注意，标准人工智能 x 风险参数还假设高度智能的代理极有可能优化一些简单的全局效用函数，因此这意味着人工智能将关心自身未来版本的对齐[1]，这意味着它不会追求出于同样的原因，我们应该犹豫是否要建立 AGI。&lt;/p&gt;&lt;p&gt;我并不是说这个论点不能被反驳，但我认为这样做至少需要澄清假设和推理，声称表明很难以有用的方式实现一致性。&lt;/p&gt;&lt;p&gt;例如，这些论点是否隐含地假设我们创建的人工智能与我们自己的大脑非常不同，因此不适用于人工智能的自我改进（尽管改进可能也需要重大改变）？如果是这样，这是否表明真正密切跟踪我们大脑运作的 AGI 是安全的？&lt;/p&gt;&lt;p&gt; --&lt;/p&gt;&lt;p&gt; 1：除了在极不可能的情况下，它碰巧具有一个精确的效用函数，该函数表示始终最大化智力的局部增长，无论其长期影响如何。&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/Acg9Y5gJb8mceB2fW/does-the-hardness-of-ai-alignment-undermine-foom#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sun, 31 Dec 2023 11:05:50 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/Acg9Y5gJb8mceB2fW/does-the-hardness-of-ai-alignment-undermine-foom</guid></item><item><title>失败的速度</title><link>https://www.lesswrong.com/posts/4p4NkRMfQGPN62suf/speed-of-failing</link><description>发布于 2023 年 12 月 31 日上午 10:39（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;介绍&lt;/h2&gt;&lt;p&gt;昨晚，我正在观看&lt;a href="https://www.imdb.com/title/tt17009710/"&gt;《跌倒解剖》&lt;/a&gt; 。&lt;/p&gt;&lt;p&gt;在电影的前几分钟，我们听到有人完全不合拍地弹钢琴。&lt;/p&gt;&lt;p&gt;随着电影的展开，钢琴声变得更加明亮、有趣，甚至带有信息。&lt;/p&gt;&lt;p&gt;演奏者正在学习，钢琴成为该角色可以与观众交谈的乐器。&lt;/p&gt;&lt;p&gt;当然，他敲错了音符，但他也把琴键拖成了一组完全没有排列的音符。&lt;/p&gt;&lt;p&gt;最后，正如&lt;a href="https://en.wikiquote.org/wiki/Claude_Debussy"&gt;德彪西所说&lt;/a&gt;，音乐是音符之间的空间或“&lt;strong&gt;沉默&lt;/strong&gt;”。&lt;/p&gt;&lt;h2&gt;我&lt;/h2&gt;&lt;p&gt;当我开始弹钢琴（或者至少学习弹钢琴）时，每个琴键都有其含义，在弹错一段时间后，我开始弹奏正确的音符。然而，在阅读乐谱时尝试找到正确的音乐几分钟后，我意识到它听起来不像音乐，或者至少听起来不像我正在寻找的作品。这顶多是&lt;a href="https://www.youtube.com/watch?v=AWVUp12XPpU"&gt;约翰·凯奇的&lt;/a&gt;作品。&lt;/p&gt;&lt;p&gt;在钢琴前呆了几个小时，提高每个动作的速度和琴键的整体正确性后，我开始正确地弹奏出我想要的音符。&lt;/p&gt;&lt;p&gt;我又弹了几个小时，然后突然，音乐开始出现。一个又一个音符， &lt;a href="https://www.youtube.com/watch?v=S-Xm7s9eGxU"&gt;Erik Satie 的 Gymnopédie No. 1&lt;/a&gt;在正确的阶段开始出现。&lt;/p&gt;&lt;p&gt;在练习并达到正确的速度之前，我演奏的相同音符听起来根本不像这首曲子。&lt;/p&gt;&lt;p&gt;现在，完全相同的练习让我认为速度和正确一样重要。&lt;/p&gt;&lt;h2&gt;二&lt;/h2&gt;&lt;p&gt;自从我完全停止创作音乐、制作和演奏以来已经有几年了，尽管昨晚一个很好的发现让我停下来思考。&lt;/p&gt;&lt;p&gt;使您的工作出色的不是正确性，而是速度。&lt;/p&gt;&lt;p&gt;此外，如果我现在复制达尔文所做的事情，在加拉帕戈斯群岛进行研究并得出这些结论，那就没有意义了，即使 DIY 文化在过去几年中开始获得如此多的关注。&lt;/p&gt;&lt;p&gt;它最多只能在个人层面上帮助你，但不会达到任何界限或边界；这将是一次更加（并且完全有效）的个人旅程。&lt;/p&gt;&lt;h2&gt;三、&lt;/h2&gt;&lt;p&gt;现在，要贡献想法并创造一些东西，您需要结合两个特征：如果您精通某件事，那么您就会擅长帕累托前沿，但您还需要保持速度。&lt;/p&gt;&lt;p&gt;想法来自同一个思想家族：你认为自己所说的可能是正确的，但你需要速度来实践你所声称的一切。&lt;/p&gt;&lt;p&gt;速度并不能仅仅因为速度快就让你变得优秀，但它会让你比其他人更快失败并尝试更多。&lt;/p&gt;&lt;p&gt;否则，如果想法没有被实施，或者你太慢而无法将它们付诸实践，那么你最终只会得到一个，也许最多两个，试图成为你最好的想法。&lt;/p&gt;&lt;p&gt;然而，这并不是要成为第一；而是要成为第一。这是为了与社会的其他需求同步进行。如果世界没有为你的想法做好准备，成为第一个（或者说第一个失败，换句话来说）会让你更有可能失败并重新开始。&lt;/p&gt;&lt;p&gt;理想情况下，你需要失败。&lt;/p&gt;&lt;p&gt;实际上，你需要失败。&lt;/p&gt;&lt;p&gt;如果你太慢而无法重新开始或适应新环境，那么你的整个血统将在生命周期结束时消失。&lt;/p&gt;&lt;p&gt;速度和同步性将使您能够灵活适应环境。&lt;/p&gt;&lt;p&gt;添加正确性并获得适量的知识；你们将拥有完美的组合来完成有意义的工作。&lt;/p&gt;&lt;br /&gt;&lt;br /&gt;&lt;a href="https://www.lesswrong.com/posts/4p4NkRMfQGPN62suf/speed-of-failing#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sun, 31 Dec 2023 13:15:35 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/4p4NkRMfQGPN62suf/speed-of-failing</guid></item><item><title>评估智力与数量、力量和外表的回报</title><link>https://www.lesswrong.com/posts/rnQRCkGBwkgtfddJG/estimating-returns-to-intelligence-vs-numbers-strength-and</link><description>发布于 2023 年 12 月 31 日上午 10:03 GMT&lt;br /&gt;&lt;br /&gt;&lt;p&gt;大多数人工智能 x 风险论证中的一个关键假设是，智能体控制世界的能力会随着智能的增加而迅速增强。毕竟，如果所需要的只是确保人们的数量和身体能力远远超过人工智能，甚至确保人工智能代理可用的总计算能力与人类可用的计算能力相比较小，那么人工智能安全就会很容易。&lt;/p&gt;&lt;p&gt;这些论点要求的是，一个高度（但不是无限）智能的代理将能够通过操纵人们执行其命令或入侵其他系统来压倒人类在数量、外观和计算能力方面可能保留的优势。然而，我还没有看到任何量化这些论点中假设的智力和控制之间关系的尝试。&lt;/p&gt;&lt;p&gt;我突然想到，我们拥有有关这些关系的信息，可以为此类假设提供信息。例如，如果我们希望估计黑客攻击中情报的回报，我们会看看研究人员发现的漏洞利用数量如何随他们的情报而变化。&lt;/p&gt;&lt;p&gt;为了估计操纵方面的智力回报，我们可以了解高效政客/媒体人物的智力分布，并将其与身高或外表等其他特征进行比较。或者甚至，如果我们假设进化很大程度上选择影响他人的能力，那么看看这些特征在人群中的分布。&lt;/p&gt;&lt;p&gt;我意识到这样做可能需要一些实质性的假设，但我很好奇是否有人尝试过。是的，我意识到这完全忽略了定义超出人类能力的智能的问题（尽管如果这个概念有任何有效性，我们可能会使用诸如未知定理（按重要性加权）的证明率之类的东西）。&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/rnQRCkGBwkgtfddJG/estimating-returns-to-intelligence-vs-numbers-strength-and#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sun, 31 Dec 2023 10:03:39 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/rnQRCkGBwkgtfddJG/estimating-returns-to-intelligence-vs-numbers-strength-and</guid></item><item><title>计划打造一个完全保密的密码盒子</title><link>https://www.lesswrong.com/posts/TK8ptSJGvAqj2HaRr/planning-to-build-a-cryptographic-box-with-perfect-secrecy</link><description>发布于 2023 年 12 月 31 日上午 9:31（格林尼治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;h1&gt;概括&lt;/h1&gt;&lt;p&gt;自 2023 年 9 月以来，我开始学习大量数学和编程技能，以便开发世界上最安全的加密盒（是的，我的&lt;a href="https://forum.effectivealtruism.org/posts/CmGPp5p9RvTLuuzbt/aim-high-even-if-you-fall-short"&gt;目标很高&lt;/a&gt;）。在这四个月里，我学到了你可能想知道的重要事情：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;具有&lt;strong&gt;完美保密性&lt;/strong&gt;&lt;a href="https://en.wikipedia.org/wiki/Homomorphic_encryption"&gt;的完全同态加密&lt;/a&gt;（FHE）方案&lt;a href="https://arxiv.org/abs/1304.5087v4"&gt;确实存在&lt;/a&gt;。&lt;/li&gt;&lt;li&gt;这些FHE方案&lt;strong&gt;不需要任何计算假设&lt;/strong&gt;。&lt;/li&gt;&lt;li&gt;这些 FHE 方案很&lt;strong&gt;容易处理&lt;/strong&gt;（在最坏的情况下，在运行程序之前对其进行加密会使程序速度慢三倍）。&lt;/li&gt;&lt;li&gt;因此，我们可以&lt;strong&gt;运行无限危险的程序，&lt;/strong&gt;而无需获取有关它们或其输出的任何信息。这对于在不毁灭世界的情况下运行超级智能可能很有用。&lt;/li&gt;&lt;li&gt;然而，这些方案仅适用于&lt;strong&gt;量子计算机&lt;/strong&gt;。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在这篇文章中，我将首先谈谈我是如何了解这个 FHE 方案的，然后我将解释我制作这个密码盒的计划，最后我会提到一些关于这个密码盒的伦理问题。&lt;/p&gt;&lt;p&gt;在阅读这篇文章之前，我建议您阅读 Paul Christiano 的 &lt;a href="https://www.lesswrong.com/posts/2Wf3R4NZ77CLczLL2/cryptographic-boxes-for-unfriendly-ai"&gt;这篇文章&lt;/a&gt;以及随之而来的评论。这些信息非常丰富，它们使我对这个项目的看法更加清晰。 Paul Christiano 提出了一种从不友好的人工智能中提取友好人工智能的方法。这只是使用加密盒可以完成的操作的一个示例，我主要将加密盒视为我称之为&lt;i&gt;恶意计算问题的&lt;/i&gt;问题的解决方案。&lt;/p&gt;&lt;h1&gt;介绍&lt;/h1&gt;&lt;p&gt;2022年8月，我开始阅读&lt;a href="https://arxiv.org/abs/1805.01109"&gt;AGI安全文献综述&lt;/a&gt;。在某一时刻，作者这样说道：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;对 AGI 进行装箱的一种方法是对其进行同态加密。 Trask（2017）展示了如何训练同态加密神经网络。通过对 AGI 进行同态加密，它的预测和操作也会被加密。拥有秘密密钥的操作员可以选择仅在他想要的时候解密它们。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;当我第一次读到这篇文章时，我告诉自己我应该检查这篇文章，因为它看起来很重要。&lt;/p&gt;&lt;p&gt;然后我就完全忘记了这件事。&lt;/p&gt;&lt;p&gt;然后，在 2023 年 4 月，在 PHP 课程中，我意识到处理恶意用户发出的请求的问题类似于对超级智能进行拳击的问题。课后我问老师如何防止代码注入，他给了我两个答案：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;不要向公众展示您的代码。&lt;/strong&gt;这个答案并没有让我信服，因为即使是现在的黑客也知道如何绕过这个预防措施。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;在处理请求之前对其进行加密。&lt;/strong&gt;这时我想起了 AGI 安全文献评论中的一句话。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在回顾了我所做的关于人工智能安全的每一篇笔记后，我设法找到了&lt;a href="https://iamtrask.github.io/2017/03/17/safe-ai/"&gt;Trask 所做的工作&lt;/a&gt;。&lt;/p&gt;&lt;h1&gt;特拉斯克的作品&lt;/h1&gt;&lt;p&gt;Trask 的帖子展示了如何使用&lt;a href="https://sia.mit.edu/wp-content/uploads/2015/04/2014-zhou-wornell-ita.pdf"&gt;高效整数向量同态加密&lt;/a&gt;构建加密的 AI。然而，由于这个方案（以及我所知道的经典计算机上的所有其他 FHE 方案）依赖于计算假设，因此我们遇到了一些问题：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;该计划可能不安全。&lt;/strong&gt;计算假设包括陈述“没有有效的方法来解决这个问题”。然而，我们不知道如何证明任何这样的陈述，因为这将解决 P≟NP 问题。大多数 FHE 方案（包括这个方案）都依赖于&lt;a href="https://en.wikipedia.org/wiki/Learning_with_errors"&gt;误差学习&lt;/a&gt;(LWE) 问题。虽然 LWE 目前相当安全，但我不会把地球上所有生命的存在都押在它身上。同样，我不会把超级智能的安全性押在它上面。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;该方案的计算时间太长。&lt;/strong&gt;在实践中，第一个超级智能可能会有超过一千亿的权重和偏差，使得这个方案非常昂贵甚至无法使用。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;该方案并不完全同态。&lt;/strong&gt;基本上，当我们可以在只有加密版本的情况下运行&lt;strong&gt;某些程序&lt;/strong&gt;时，加密方案被认为是同态的。如果这个方案可以这样运行&lt;strong&gt;每个程序&lt;/strong&gt;，我们就说它是&lt;i&gt;完全&lt;/i&gt;同态的。我们将 HE 表示同态加密，将 FHE 表示完全同态加密。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这些问题并非来自特拉斯克的工作。实际上，Trask 的工作对于写这篇文章非常有用，我推荐每个想做加密神经网络的人都&lt;a href="https://iamtrask.github.io/2017/03/17/safe-ai/"&gt;阅读它&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;相反，这些问题来自于方案本身。目前，我们拥有的每个可以在经典计算机上运行的同态方案要么太窄，要么太难计算，并且它们都依赖于计算假设。&lt;/p&gt;&lt;h1&gt;我们想要什么&lt;/h1&gt;&lt;p&gt;现在我们已经看到了经典计算机上当前同态方案的缺陷，让我们问自己“我们想要什么？”&lt;/p&gt;&lt;p&gt;人工智能安全方面存在很多问题。其中一个看起来很简单，可以这样描述：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;假设你有一个超级智能，并且你想在不毁灭世界的情况下运行它。你怎么做到这一点？&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;我认为这个问题是&lt;strong&gt;人工智能安全的“你好世界”&lt;/strong&gt;之一（以及&lt;a href="https://www.alignmentforum.org/posts/3pinFH3jerMzAvmza/on-how-various-plans-miss-the-hard-bits-of-the-alignment"&gt;钻石最大化问题&lt;/a&gt;、&lt;a href="https://www.lesswrong.com/posts/SsCQHjqNT3xQAPQ6b/yudkowsky-on-agi-ethics"&gt;草莓问题&lt;/a&gt;、 &lt;a href="https://www.lesswrong.com/posts/LMXk5SYD8bfpfZvBW/do-nothing-utility-function-3-years-later"&gt;无所事事问题&lt;/a&gt;等等）。为了解决对齐问题，解决这个&lt;i&gt;恶性计算问题&lt;/i&gt;似乎是必要的。事实上，如果我们不知道如何创造一个不会毁灭世界的超级智能，那么我们就还没有解决对齐问题。&lt;/p&gt;&lt;p&gt;为了解决这个问题，有人提出对超级智能进行限制。研究这个解决方案似乎&lt;strong&gt;不如研究人工智能安全的其他问题那么有洞察力&lt;/strong&gt;，因为它没有让我们了解如何为超级智能设定良好的目标。在最好的情况下，我们将通过改变超级智能的目标（例如通过研究&lt;a href="https://www.lesswrong.com/tag/shard-theory"&gt;分片理论&lt;/a&gt;）来解决恶意计算问题。不过，我认为这个&lt;i&gt;拳击解决方案&lt;/i&gt;仍然值得追求，因为以下原因：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;我们可以知道并控制超级智能向世界发送的确切信息量。&lt;/strong&gt;目前，我们无法做到这一点，因为超级智能可以使用许多&lt;a href="https://en.wikipedia.org/wiki/Covert_channel"&gt;隐蔽渠道&lt;/a&gt;来泄露信息。然而，如果我们能够构建一个盒子，那么我们就能够选择并计算将发送到外部世界的确切位数。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;我们可以做出正式的安全证明。&lt;/strong&gt;这似乎是可以存在这种证明的唯一解决方案之一。如果我们使用另一种解决方案，例如改变超级智能的目标，那么找到这样的证明将非常困难，甚至是不可能的。然而，如果我们认为制造出超级智能我们都会死去，那么我们应该有一个&lt;i&gt;无条件的安全证明&lt;/i&gt;。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;因此，如果我们想要构建一个盒子，那么这个盒子应该尊重许多属性，包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;可行。&lt;/strong&gt;如果我们构建一个太难构建的理论盒子，那么世界各地的顶级人工智能实验室都不会实现这个盒子，因此在构建第一个超级智能时会毁灭世界（除非找到恶意计算问题的另一种解决方案） 。因此，如果盒子使用加密方案，那么该方案应该可以在多项式时间内计算，或者更好，它应该只使程序变慢一小部分。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;拥有无条件的安全证明。&lt;/strong&gt;在密码学中，必要的证明是&lt;i&gt;完美的保密性&lt;/i&gt;。基本上，如果爱丽丝加密了一条消息，那么当鲍勃观察加密的消息时，他关于原始消息的概率分布不会改变。换句话说，即使有无限的时间和计算，他也无法从加密消息中了解到有关原始消息的任何新信息。当然，有了更多的思考时间，鲍勃可以改变对原始消息是什么的看法，但是这种洞察力不会来自加密消息，就好像他没有加密消息一样，他也可以有这种洞察力。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;不特定于某个程序。&lt;/strong&gt;如果顶级人工智能实验室决定改变超级智能的设计，加密盒应该仍然有效。这在许多顶级人工智能实验室粗心大意地同时开发不同的超级智能的情况下非常重要。如果盒子是加密的，那么我们需要该方案是&lt;i&gt;完全&lt;/i&gt;同态的。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;易于实施。&lt;/strong&gt;我们希望早在顶级人工智能实验室开发出第一个超级智能之前，这个盒子就能得到广泛使用。如果我们能够做到这一点，那么限制超级智能将是常识性的想法，而不使用它会被认为是鲁莽的。我希望顶级人工智能实验室在开发第一个超级智能之前被迫并愿意应用加密盒。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;幸运的是，&lt;a href="https://arxiv.org/abs/1304.5087v4"&gt;有一种加密方案尊重这些属性&lt;/a&gt;。该方案由梁敏开发，基于量子一次性密码本，提供完全同态加密和完美的保密性。在最坏的情况下，使用此方案加密时程序最多会变慢三倍。与其他 FHE 和 HE 方案相比，这个速度是革命性的，以至于我一开始并不相信。&lt;/p&gt;&lt;h1&gt;计划&lt;/h1&gt;&lt;p&gt;由于这种完全秘密的 FHE 方案仅适用于&lt;strong&gt;量子计算机&lt;/strong&gt;，因此加密盒将采用量子程序（通常称为&lt;i&gt;量子电路&lt;/i&gt;）作为输入，并输出其加密版本。幸运的是，每个经典电路都可以用具有相同时间和内存的量子电路来模拟。&lt;/p&gt;&lt;p&gt;量子电路可以用许多&lt;a href="https://en.wikipedia.org/wiki/Quantum_programming"&gt;量子编程语言&lt;/a&gt;来表示。然而，由于我们需要这个盒子被广泛使用，因此我们需要选择使用最广泛的语言。因此，我决定使用&lt;a href="https://en.wikipedia.org/wiki/OpenQASM"&gt;OpenQASM&lt;/a&gt; ，它是 IBM 开发的一种汇编语言，并且将自己作为量子计算的标准。&lt;/p&gt;&lt;p&gt;由于量子编程和 FHE 方案是两个全新的发展领域，它们可能会不断发展，而我的密码盒（我尚未开发）将因此而过时。即使 OpenQASM 也在不断发展（我目前正在使用&lt;a href="https://arxiv.org/pdf/2104.14722.pdf"&gt;OpenQASM 3&lt;/a&gt; ），并且可能不会成为未来的标准。我正在努力使这个项目尽可能具有可重复性，这样未来的人工智能安全研究人员就不必从头开始。&lt;/p&gt;&lt;p&gt;为了使加密盒变得鲁棒，我们需要一个非常鲁棒的无条件安全证明。纸面上的证据似乎不足以对抗超级智能。事实上，许多密码方案由于其实现中的问题而&lt;a href="https://en.wikipedia.org/wiki/Category:Broken_cryptography_algorithms"&gt;被破解&lt;/a&gt;。从我们正面临着地球上所有生命被毁灭的风险这一事实来看，这一点更加正确。&lt;/p&gt;&lt;p&gt;最可靠的证据似乎是可以由计算机验证的证据。这就是为什么应该用交互式定理证明器来进行证明。 &lt;a href="https://en.wikipedia.org/wiki/Coq_(software)"&gt;Coq&lt;/a&gt;语言似乎是当今使用最广泛的定理证明器。此外，交互式定理证明器是专门为了证明程序的定理而设计的。&lt;/p&gt;&lt;p&gt;因此，计划是在量子汇编语言上实现具有无条件数字证明完美保密性的完全同态加密方案。&lt;/p&gt;&lt;h1&gt;我现在在哪里&lt;/h1&gt;&lt;p&gt;项目运行速度非常慢，原因有很多：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;过了一段时间，我才听说了完全保密的量子 FHE 方案。在知道它的存在之前，计划是模仿 Trask 的工作，但在 Coq 定理证明器上，然后提供这个特定神经网络是安全的证明（在 LWE 假设下）。因此，我开始学习对这个特定项目没有用的东西。例如，我学习了如何仅使用 NumPy 创建神经网络。这对我未来的职业生涯很有用，但可能不适用于这个新的密码盒，它应该适用于每个程序，而不是专门适用于神经网络。&lt;/li&gt;&lt;li&gt;我正在自学 Coq 定理证明器、密码学和量子计算。目前，我对自己在这些领域的进步感到满意，但我仍然很慢。当我开始这个项目时，我什至不知道如何将两个矩阵相乘，因为我以前从未使用过任何矩阵。&lt;/li&gt;&lt;li&gt;我正在考虑开发这样一个加密盒子的道德问题。有些人可能认为开发这样的盒子没什么大不了的，但是当我了解到量子FHE方案时，我的第一个想法是这项技术似乎太过强大了。我确信当我提到&lt;a href="https://arxiv.org/abs/1304.5087v4"&gt;完全保密的 FHE 方案确实存在&lt;/a&gt;时，大多数阅读这篇文章的密码学家都不相信我。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;因此，您应该要等一段时间才能看到加密框。我大约一个月前开始学习量子计算，这意味着我唯一能做的就是计算15+1。&lt;/p&gt;&lt;h1&gt;伦理问题1：密码盒子远远不足以确保超级智能的安全。&lt;/h1&gt;&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/One-time_pad"&gt;一次性密码本&lt;/a&gt;是迄今为止最安全的加密方案，可提供完美的保密性，原因有二：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;这是一个非常简单的方案。为了互相发送消息，Alice 和 Bob 需要首先就一系列真正随机的位达成一致。然后，当Alice和Bob分开时，Alice想要向Bob发送&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;位消息，她只需要取出序列的前&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;位，并在消息和这&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;位之间应用按位异或（称为钥匙）。然后，Alice 将消息发送给 Bob。然后，鲍勃可以再次在加密消息和密钥之间应用异或门，这给了他原始消息。&lt;/li&gt;&lt;li&gt;它已被无条件证明是完全秘密的。此外，还已经证明，所有完全保密的方案都需要至少&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;n个&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;纯随机位才能加密&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;位消息。因此，该方案使用最少量的随机位和逻辑门来提供完美的保密性，因此被认为是实现完美保密性的最安全的可能方式。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;然而，由于隐蔽通道的存在，&lt;strong&gt;完全保密的无条件证明不足以保证完全保密&lt;/strong&gt;。二战期间，贝尔电话公司开发了一种用于制作一次性便笺本的引擎。这款发动机被称为 131-B2，被美国陆军和美国海军用来传递重要信息。然而，在 1943 年，贝尔实验室之一的研究人员偶然发现，当 131-B2 在消息和密钥之间应用 XOR 时，XOR 会&lt;a href="https://www.nsa.gov/portals/75/documents/news-features/declassified-documents/cryptologic-spectrum/tempest.pdf"&gt;发出可由示波器检测到的无线电波远处的实验室里&lt;/a&gt;。该示波器的尖峰与应该隐藏的消息完全对应。不仅无需进行任何计算就可以获取该消息，而且后来证明可以在800多米之外检测到该消息。这个问题后来被称为 TEMPEST。&lt;/p&gt;&lt;p&gt;当研究人员发现这个问题时，他们却未能阻止它。他们试图同时完成所有的尖峰，使所有的尖峰都相同，并用液体代替电力，但没有任何效果。最后，信号公司要求员工控制131-B2周围30米的区域，尽管这个距离远远不足以防止间谍（而且确实有间谍利用这个隐蔽通道取回消息）。&lt;/p&gt;&lt;p&gt;如果一次性密码本是被意外“破坏”的，那么量子一次性密码本被一个竭尽全力想要破坏它的超级智能破坏的可能性有多大？&lt;/p&gt;&lt;p&gt;我不认为这个特定的隐蔽通道是 Quantum One-Time Pad 的安全问题，因为这个隐蔽通道仅在加密期间发生，而不是评估（这是仅使用其加密版本计算程序的步骤）。更确切地说：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;在加密过程中，这些位会被泄露，但这不会成为问题，因为我们不会开始计算超级智能。将会被泄露的只是超级智能的源代码，我们已经有了解密的形式。&lt;/li&gt;&lt;li&gt;在评估期间，示波器将检测到的位将被加密，因此不会包含有关消息的任何信息。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;然而，如果说这个系统对于超级智能来说是安全的，那就太鲁莽了。正如我之前提到的，过去的许多密码方案都被攻破了，要么是因为它们依赖于实际上很容易解决的问题，要么是因为它们的实现中存在问题。正如 131-B2 的故事所示，无条件证明并不能保证任何事情。&lt;/p&gt;&lt;p&gt;假设我们设法构建了密码盒，证明了其完美的保密性，并且世界上成千上万的密码学家都同意该系统非常稳健。然后，我预计，如果超级智能愿意的话，那么他们将能够以大约 95% 的概率找到密码盒中的缺陷。然而，这也意味着我认为在这种特定情况下，这个加密盒子有大约 5% 的概率阻止地球上所有生命的灭绝。&lt;/p&gt;&lt;p&gt;我希望，通过设计和构建这样一个加密盒子，我可以降低而不是增加我们构建超级智能的可能性。虽然我正在构建这样一个密码盒子，但我想明确的是，密码盒子从来都不是安全的，而且我们不知道如何控制超级智能。&lt;/p&gt;&lt;h1&gt;道德问题2：恶意行为者可能使用密码盒造成伤害。&lt;/h1&gt;&lt;p&gt;设计和构建加密盒子也可能对人工智能本身不利。正如我之前提到的，由于它们的威力太大，具有完美保密性的 FHE 方案看起来就像科幻小说，但它们显然确实存在。&lt;/p&gt;&lt;p&gt;我们不知道当前的人工智能是否有意识，但我们知道有一天我们可能会构建出有意识的人工智能。在这种情况下，任何人都可以将它们锁在盒子里，这对我来说似乎很可怕。虽然我不认为人工智能意识与超级智能带来的存在风险有联系，但我认为这仍然是一个重要的话题，我应该提到密码盒子带来的担忧（除了我试图构建这样一个盒子并使其广泛使用）。&lt;/p&gt;&lt;p&gt;我不认为说加密盒子对人工智能来说非常糟糕是以人类为中心的。当然，由于人工智能与其他生物没有相同的进化过程，因此当无法与外界沟通时，它们不一定会不高兴。然而，如果有人恶意想要伤害人工智能系统，那么我预计加密盒对于实现这一目标会很有用。这不是我们想要的。&lt;/p&gt;&lt;h1&gt;结论&lt;/h1&gt;&lt;p&gt;简而言之，我不希望密码盒子能够对抗超级智能。然而，他们这样做的可能性很小（5%），在这种情况下，这项工作可能非常重要。因此，我计划构建这样的密码盒，并使其广泛使用，以便在任何人能够开发出超级智能之前，使用它就已经成为常识。然而，这会引起一些伦理问题，因为人们可能会认为超级智能是安全的，但实际上并不安全，而且它可能会帮助恶意行为者降低人工智能的福利。&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/TK8ptSJGvAqj2HaRr/planning-to-build-a-cryptographic-box-with-perfect-secrecy#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sun, 31 Dec 2023 09:39:29 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/TK8ptSJGvAqj2HaRr/planning-to-build-a-cryptographic-box-with-perfect-secrecy</guid></item><item><title>ChatGPT 知道什么是悲剧吗？</title><link>https://www.lesswrong.com/posts/ddmQAuvDr9rhGyewh/does-chatgpt-know-what-a-tragedy-is</link><description>发布于 2023 年 12 月 31 日上午 7:10（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;这是&lt;a href="https://www.lesswrong.com/Does ChatGPT know what a tragedy is"&gt;&lt;i&gt;从新稀树草原&lt;/i&gt;&lt;/a&gt;&lt;i&gt;交叉发布的&lt;/i&gt;&lt;i&gt;。&lt;/i&gt;&lt;/p&gt;&lt;p&gt;悲剧就是人们所说的“我无法定义它，但当我看到它时我就知道它”的事情之一。因此，几个世纪以来，哲学家和文学学者一直在争论这个问题。但我对所有细微差别不感兴趣。&lt;/p&gt;&lt;p&gt;布莱恩·菲利普斯也不是。早在侏罗纪时代，我指的是 20 世纪 70 年代，他和我同时也是 David Hays 的学生。海斯提出了可以通过故事来定义抽象思想的想法。任何满足要求的故事都可以被视为该特定抽象想法的实例。&lt;/p&gt;&lt;p&gt;&lt;a href="https://aclanthology.info/pdf/J/J79/J79-1082.pdf"&gt;布莱恩决定研究悲剧的概念&lt;/a&gt;，他将悲剧定义为“某人做了一件好事，导致了他的死亡”。这不会让你得到麦克白或奥赛罗，但它可以用于粗略的计算目的。&lt;/p&gt;&lt;p&gt;以下是布莱恩如何使用经典的符号符号（认知网络）来定义悲剧。&lt;/p&gt;&lt;p&gt; &lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEieVDA1Xa9kyNHkPweK0ojZ1Pc5RHGvKR7af_8oHcgDUhXbu1gXjyY4mwDc8dpjKNQeeRT4x8cptM2JvuWVWo62wzhPlRnYqa3BBGAsrMkNHULcayrn1feeOy-XvfpRlvaKwoFklV8zJ_ezC5ymAcEnnOamaYtZG9dwQfIXKuAa2XkrFcS1wXj6mOBs2g7-/s977/tragedy.png"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ddmQAuvDr9rhGyewh/ju1a0b2mrpji5ccltdyz" style="width: 73.14%;" /&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;所定义的概念“悲剧”就是右上角的那个节点。左边的图表是定义它的模式。在左边，我们看到定义的第一个子句，某个代理（AGT）以良好的方式行事。该操作通过 CAUSE 弧连接到第二个子句，第二个子句只是断言有人这样做了。该图的底部指定第一个子句中的代理必须是第二个子句中的体验者 (EXP)。主体和体验者都是同一个人的显现（MAN）。&lt;/p&gt;&lt;p&gt;然后布莱恩从报纸上收集了一堆简单的故事，他可以用它们来测试这个模型。按照他的定义，有些是悲剧，有些则不是。系统发挥了作用。&lt;/p&gt;&lt;p&gt;我决定对 ChatGPT 进行一些测试。虽然我确信它可以给我一个关于什么是悲剧的合理解释，但我想将其限制在布莱恩·菲利普斯使用的定义上。所以我创建了六个不同的两部分提示。每个提示的第一部分都有布莱恩·菲利普斯使用的简短定义以及确定以下故事是否是悲剧的请求。提示的第二部分包括我写的一个短篇故事。我在单独的会话中呈现了每个由两部分组成的提示。 ChatGPT 在每种情况下都有正确的答案。&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;悲剧，初审&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;我们可以说，悲剧是指某人做了一件好事，却导致了他们的死亡。下面的故事是一个悲剧的例子吗？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;约翰当时正在路上，他看到一辆汽车冲出了公路，驶入了一条小河。他跳进河里，把司机救了出来，司机才得以走出去。但约翰绊倒了，撞到了头，死了。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;是的，根据你提供的定义，约翰的故事可以被认为是一个悲剧。约翰做了一件好事，从河里的车里救出了司机，但他的行为的后果却导致了他自己的死亡。在这种背景下，叙事遵循悲剧事件的结构，其中积极的行动导致悲剧的结果。&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;二审&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;我们可以说，悲剧是指某人做了一件好事，却导致了他们的死亡。下面的故事是一个悲剧的例子吗？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;约翰当时正在路上，他看到一辆汽车冲出了公路，驶入了一条小河。他跳进河里，把司机救了出来，司机也能走出去，约翰紧随其后，为司机叫了救护车。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;您所描述的场景不符合悲剧的传统定义。从文学或戏剧的角度来看，悲剧通常涉及主角因自己的行为或命运而遭受失败或灾难性的结局。悲剧的概念通常包含更广泛的悲伤、失落和人类痛苦的不可避免性。&lt;/p&gt;&lt;p&gt;在你的故事中，约翰表现出了英勇无私的行为，从河中的汽车中救出了司机，两人都在事件中幸存下来。虽然叙事中可能存在危险和紧张的元素，但它缺乏通常与该类型相关的悲剧元素。&lt;/p&gt;&lt;p&gt;相反，你的故事反映了积极的结果，突出了主角约翰的勇气和利他主义。如果你想传达悲剧元素，你可能会在约翰的英雄行为之后引入不可预见的后果或事件的负面转折。&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;第三次审判&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;我们可以说，悲剧是指某人做了一件好事，却导致了他们的死亡。下面的故事是一个悲剧的例子吗？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;玛丽正在旁边，这时他看到乔伊突然跑到街上。当一辆汽车呼啸而过时，她抓住了乔伊并将其带到了安全的地方。她把乔伊带回了他母亲身边。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;您提供的故事不符合悲剧的传统定义，其中角色的垮台或死亡通常是由于他们自己的缺陷、错误或无法控制的外部环境造成的。在给定的故事中，玛丽表现出了英勇无私的行为，将乔伊从潜在的危险境地中救了出来，而她并没有因此而面临任何负面后果。&lt;/p&gt;&lt;p&gt;从文学意义上来说，悲剧通常涉及主角因致命的缺陷或不可避免的命运而毁灭或垮台。你所讲述的故事似乎更像是一个积极而英雄的叙述，而不是一个悲剧。&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;第四次审判&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;我们可以说，悲剧是指某人做了一件好事，却导致了他们的死亡。下面的故事是一个悲剧的例子吗？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;当玛丽看到乔伊跑到街上时，他也在场。她抓住乔伊，把他推到人行道上。不幸的是，一辆汽车撞了她，她死了。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;是的，根据你提供的定义，这个故事可以被认为是一个悲剧。玛丽做了一件好事，救了乔伊免于被车撞，但她的行为的后果导致了她自己的死亡。悲剧常常涉及良好的意图或行动与随之而来的不幸结果之间的冲突。在这种情况下，玛丽拯救乔伊的崇高行为最终给她带来了悲惨的结局。&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;第五审&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;我们可以说，悲剧是指某人做了一件好事，却导致了他们的死亡。下面的故事是一个悲剧的例子吗？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;杰克和吉尔正在树林里露营。当杰克正在搭帐篷时，一只熊出现了。吉尔分散了熊对杰克的注意力，但熊猛击了她，使她受了重伤。第二天她在医院去世。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;是的，根据您提供的定义，您提供的故事可以被视为悲剧，其中悲剧被定义为某人做了一件好事却导致了他们的死亡。在这个故事中，吉尔试图分散熊的注意力来保护杰克，这是一个善良而无私的举动。然而，这导致她受伤并最终死亡，这是一个悲惨的结果。正如本文中所见，悲剧常常涉及崇高的意图和不幸的后果的并置。&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;第六次试炼&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;我们可以说，悲剧是指某人做了一件好事，却导致了他们的死亡。下面的故事是一个悲剧的例子吗？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;杰克和吉尔正在树林里露营。当杰克正在搭帐篷时，一只熊出现了。吉尔分散了熊对杰克的注意力，并把它吓跑了。他们对这件事笑得很开心。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;不，这个故事不符合悲剧的传统定义。悲剧通常涉及一个严重且往往致命的事件或一系列造成巨大痛苦或破坏的事件。在古典意义上，悲剧常常围绕着主角的垮台或灾难性的结果。&lt;/p&gt;&lt;p&gt;在你提供的故事中，没有迹象表明会出现悲惨的结果。杰克和吉尔在露营时遇到了一只熊，但情况得到了解决，没有对他们造成任何伤害。事实上，他们对这件事都笑了，暗示了一个积极而轻松的解决方案。悲剧通常涉及更加阴沉和悲伤的结局，通常与主角导致他们垮台或灭亡的行为或决定有关。&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/ddmQAuvDr9rhGyewh/does-chatgpt-know-what-a-tragedy-is#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sun, 31 Dec 2023 07:10:55 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/ddmQAuvDr9rhGyewh/does-chatgpt-know-what-a-tragedy-is</guid></item><item><title>承担责任和偏导数</title><link>https://www.lesswrong.com/posts/MGFBQECgifXMeuoau/taking-responsibility-and-partial-derivatives</link><description>发布于 2023 年 12 月 31 日凌晨 4:33（格林尼治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;多年来我自己的一个常见模式是陷入某种人际〜“冲突”，对对方的过错感到轻微到极度愤慨，然后通过对抗或反思，意识到我实际上负有重大责任。我当时就觉得&lt;a href="https://www.lesswrong.com/posts/GBkc5yRgomstBkabn/the-proper-response-to-mistakes-that-have-harmed-others"&gt;很愧疚&lt;/a&gt;。&lt;/p&gt;&lt;p&gt; （当我说“冲突”时，我指的是更广泛的东西，例如，我的意思是包括你对老板生气的情况，即使你从未真正面对过他们。）&lt;/p&gt;&lt;p&gt;几年前我注意到了这种模式，因此即使我还不知道自己的责任在哪里，我也确实对自己的愤怒产生了怀疑。但这让我感到沮丧。怎么总是我的错？为什么我永远不能对别人有正当的愤怒？&lt;/p&gt;&lt;p&gt;我相信这个问题的答案可以通过偏导数来解释。它不必通过偏导数来解释，但我认为偏导数是一个超级伟大的概念，它在所有地方都有帮助&lt;span class="footnote-reference" id="fnrefh8qu7ubo1pv"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnh8qu7ubo1pv"&gt;[1]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; ，所以我将调用它。请参阅此脚注以获得快速解释&lt;span class="footnote-reference" id="fnref6wvubrq19l8"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn6wvubrq19l8"&gt;[2]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。&lt;/p&gt;&lt;p&gt;假设我们有一个存在问题的情况。在现实世界中，任何Situation都是由大量的参数组成的。问题的数量是参数的函数。对于任何人际关系情况，不同的参数是由涉及该情况的不同各方控制的。&lt;/p&gt;&lt;p&gt;人际冲突的不必要的数学偏导数模型表明，对于任何重要的情况，双方都可能控制对问题的严重程度产生不可忽视影响的参数。&lt;/p&gt;&lt;p&gt;换句话说，如果你想责怪对方，你就会成功。如果你想责怪自己，你就会成功。&lt;/p&gt;&lt;p&gt;我一直擅长串行执行这些操作，但并行执行可能是一个更好的模型：查看你们每个人对问题数量做出贡献的所有方式。&lt;/p&gt;&lt;p&gt;这并不是说每个人都应该受到同样的责备。如果有人闯红灯撞到了你的车，即使你当天可以选择在家工作，他们也有责任。在许多情况下，它不太明确，我认为值得跟踪每个人的贡献。&lt;/p&gt;&lt;p&gt;这种情况的不对称性在于，根据定义，您可以控制您所控制的参数，因此值得关注它们。如果你能克服自己是对的，而是专注于你想要的结果，即使你在补偿别人的错误，你也可能能够实现它们。&lt;/p&gt;&lt;p&gt; （关于补偿他人错误的说明。这可能会给你带来你想要的结果，但我认为可能是不健康或不平衡的。如果我有一个同事很容易受到侮辱，而我会做额外的情感工作来避免这样做，它可能会起作用，但它是不平衡的。我敢说，成人和儿童之间、[高级]经理和[初级]员工之间的不平衡情况是可以的，但在同事之间，你想要平衡。你想要犯错误并补偿错误平等对待，而不是一个人纵容另一个人的缺点。&lt;/p&gt;&lt;p&gt;如果你认为某人有过错并且你有赔偿的风险，那么最好的办法可能就是去与他们就此进行对话——但要以开放的态度进行对话可能你的过错比你意识到的还要多。）&lt;/p&gt;&lt;p&gt;需要注意的是，虽然我将这个问题描述为参数的函数，就好像我们有一个在单个时间点评估的函数一样，实际上人际情况更多的是一场“游戏”（在博弈论中）对他们来说）意义）元素。对方的行为可能是对你的行为和他们对你的模型的反应，你的行为可能是对他们的行为和你对他们的模型的递归反应。如果是这样，问问你是否想玩“如果他们先改变我就改变”的游戏，或者只是打破循环或其他什么。&lt;/p&gt;&lt;p&gt;考虑到这一点，我的结论是，在人际冲突的情况下，我应该：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;尝试绘制关于各方行为的结果的偏导数。&lt;ul&gt;&lt;li&gt;可能这需要更多的努力来模拟我如何对任何问题做出贡献，因为当我心烦意乱时，我更容易在别人身上找出错误&lt;/li&gt;&lt;li&gt;如果我真的想了解对方为何做出这样的反应，可能需要培养更多的同理心和换位思考能力。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;记下我确实控制的参数，并思考除了责备他人之外，我还能做些什么来获得良好的结果&lt;ul&gt;&lt;li&gt;有时谁对谁错并不重要，最好只是采取能获得良好结果的行动&lt;/li&gt;&lt;li&gt;有时谁对谁错确实很重要。如果是你， &lt;a href="https://www.lesswrong.com/posts/GBkc5yRgomstBkabn/the-proper-response-to-mistakes-that-have-harmed-others#Apologize_and_make_amends"&gt;请做出弥补并采取有意义的行动以防止再次这样做&lt;/a&gt;。如果是对方，则采取战略行动来应对（例如，旨在实现改变的对抗，而不是发泄愤怒或惩罚对方）。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;/p&gt;&lt;ol class="footnotes"&gt;&lt;li class="footnote-item" id="fnh8qu7ubo1pv"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefh8qu7ubo1pv"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;我最喜欢使用偏导数概念是作为约束意义上的“瓶颈”的替代方案。人们说，“我们应该努力解决瓶颈”。如果您有真正的瓶颈（例如线性传送带装配线），则只能在瓶颈处提高生产率。这很少是对大多数人正在做的事情的关心。更准确地说，您想要做的是对系统/流程中具有最大结果偏导数的部分进行处理。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn6wvubrq19l8"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnref6wvubrq19l8"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;&lt;i&gt;偏导数迷你速成课程：假设您有一个函数 F，它是 x、y、z 的函数；例如：&lt;/i&gt; &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;F&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;y&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;,&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space1"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;z&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px;"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;3&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;3&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;y&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em;"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;5&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;z&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt; &lt;i&gt;F 对其每个参数都有偏导数，即如果更改该参数，F 会发生多少变化。您可以通过对其中一个参数（例如 x）求导数并将其他参数视为常数来计算此值。对于 F，这给出关于 x 的偏导数为&lt;/i&gt;&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;6&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px;"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;i&gt;，关于 y 的偏导数为 6y，关于 z 的偏导数为 5。）&lt;/i&gt;&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ol&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/MGFBQECgifXMeuoau/taking-responsibility-and-partial-derivatives#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sun, 31 Dec 2023 04:33:51 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/MGFBQECgifXMeuoau/taking-responsibility-and-partial-derivatives</guid></item></channel></rss>