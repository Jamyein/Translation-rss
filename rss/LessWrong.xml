<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>少错</title><link>https://www.lesswrong.com</link><description>致力于提炼理性艺术的社区博客</description><lastBuildDate>Sun, 10 Dec 2023 06:14:05 GMT</lastBuildDate><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>理解主观概率</title><link>https://www.lesswrong.com/posts/Ng5n8FjpsfXrceT9w/understanding-subjective-probabilities</link><description>发布于 2023 年 12 月 10 日上午 6:03（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;这是&lt;a href="https://outsidetheasylum.blog/understanding-subjective-probabilities/."&gt;&lt;i&gt;https://outsidetheasylum.blog/understanding-subjective-probabilities/&lt;/i&gt;&lt;/a&gt;&lt;i&gt;的链接帖子&lt;/i&gt;。&lt;i&gt;它旨在为那些对此概念持怀疑态度的人介绍实用的贝叶斯概率。我计划通过改进和更正来保持主要链接的最新状态，但不会对这篇 LessWrong 帖子做同样的事情，所以请参阅那里的最新版本。&lt;/i&gt;&lt;/p&gt;&lt;p&gt;每当出现有关未来的有争议的预测时，几乎肯定会发生一种互动。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; &lt;i&gt;Alice：“我认为这件事发生的可能性是20%。”&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;鲍勃：“嗯？你怎么知道的。这个数字是你编的！”。&lt;/i&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;或者用推特语来说： &lt;/p&gt;&lt;figure class="image"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Ng5n8FjpsfXrceT9w/jgkfl4ftoimc3lyf9iit" /&gt;&lt;/figure&gt;&lt;p&gt;也就是说，每当有人试图提供未来事件的具体数字概率时，他们都会被认为该数字毫无意义的说法所淹没。这是真的？为现实世界中的事物分配概率有意义吗？如果是这样，我们如何计算它们？&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;概率的解释&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;概率的传统解释被称为&lt;a href="https://en.wikipedia.org/wiki/Frequentist_probability"&gt;频率概率&lt;/a&gt;。根据这种解释，项目具有某种内在的“品质”，即做一件事与另一件事的可能性有一定的百分比。例如，一枚硬币的基本概率本质是，翻转时正面朝上的可能性为 50%。&lt;/p&gt;&lt;p&gt;这是一个有用的数学抽象，但当应用于现实世界时它就会崩溃。真正的硬币不存在基本的“50% 度”。每次以相同的方式翻转它，每次都会以相同的方式着陆。翻转它没有完美的规律性，但有一定程度的一致性，你可以让它在超过 50% 和低于 100% 的时间里正面落地。事实上，即使是那些&lt;i&gt;试图&lt;/i&gt;将硬币变成 50/50 的人翻转的硬币，实际上也&lt;a href="https://susan.su.domains/papers/headswithJ.pdf"&gt;偏向于翻转时朝上的面&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;频率论所提出的这种内在品质在现实世界中并不存在；它不是由构成真正硬币的原子以任何方式编码的，而且我们的物理定律没有任何空间容纳这样的事情。我们的宇宙是确定性的，如果我们知道起始条件，那么每次的最终结果都会相同。 &lt;span class="footnote-reference" id="fnrefao2uiw6hdek"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnao2uiw6hdek"&gt;[1]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;解决方案是&lt;a href="https://en.wikipedia.org/wiki/Bayesian_probability"&gt;概率的贝叶斯解释&lt;/a&gt;。概率不是“关于”所讨论的对象，而是“关于”做出陈述的人。如果我说一枚硬币正面朝上的可能性为 50%，那就是说我对硬币的确切初始条件了解不够充分，无法对它如何落地有任何有意义的了解，而且我不能区分这两个选项。如果我&lt;i&gt;确实&lt;/i&gt;有一些信息，比如看到它被一台非常普通的机器翻转，并且已经连续 10 次出现正面，那么我可以考虑到这一点，并知道它出现的可能性超过 50%下一次翻转时抬起头来。 &lt;span class="footnote-reference" id="fnrefwm5lopqnqhp"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnwm5lopqnqhp"&gt;[2]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;处理“模糊”概率&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;这一切都很好，但是我们如何获得更复杂命题的数字呢？在前面的例子中，我们知道硬币正面朝上的可能性 &amp;gt;50%，但是是 55% 吗？ 75%？ 99%？&lt;/p&gt;&lt;p&gt;这是最难的部分。&lt;/p&gt;&lt;p&gt;有时有大量的历史数据，您可以依赖基本汇率。这就是我们对硬币所做的事情；数以百万计的硬币已被翻转，其中大约一半是正面，一半是反面，因此我们可以&lt;a href="https://www.smbc-comics.com/comic/2011-10-02"&gt;放心地假设&lt;/a&gt;未来的硬币可能会表现出同样的行为。&lt;/p&gt;&lt;p&gt;即使数据少得多，情况也是如此。美国历史上只有大约 50 位总统，但大约一半来自每个政党，因此我们可以有把握地假设，在未来的选举中，每个政党都有大约 50% 的获胜机会。然后，我们可以根据其他数据（例如民意调查中的数据）修改此可信度。&lt;/p&gt;&lt;p&gt;我们如何进行该修改？理论上，通过应用&lt;a href="https://en.wikipedia.org/wiki/Bayes%27_theorem"&gt;贝叶斯规则&lt;/a&gt;。在实践中，计算任何给定观察所提供的证据的确切位数是不可行的，因此我们依赖于学习的启发式方法和世界模型；也就是说，我们猜测。&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;猜测&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;只要你想一想，你就会发现，猜测是生成概率陈述的有效方法。我们一直这样做。&lt;/p&gt;&lt;p&gt;&lt;i&gt;爱丽丝：“今晚我们的黄油可能会用完，所以我要去杂货店。”&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;鲍勃：“荒谬！在过去的几周里，你有没有跟踪过我们每天的黄油消耗率？你有一个经过验证的统计模型来计算每种餐食生产中所用黄油的数量吗？不管怎样，我们刚刚买了一个新炉子，所以即使你确实有这样的模型，在这些新条件下它也不再被认为是有效的。你怎么知道我们今晚要做什么晚餐；我还没告诉你我想要什么！这个出于多种原因，这真是无稽之谈，你不可能知道我们今晚黄油用完的可能性。”&lt;/i&gt;&lt;/p&gt;&lt;p&gt;显然鲍勃在这里不讲道理。 Alice&lt;a href="https://slatestarcodex.com/2015/08/24/probabilities-without-models"&gt;不需要适合&lt;/a&gt;在科学期刊上发表的详细正式模型&lt;span class="footnote-reference" id="fnrefyla2721d2q"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnyla2721d2q"&gt;[3]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; ；她可以利用她对烹饪原理的一般知识以及她的晚餐计划来得出合理的结论。&lt;/p&gt;&lt;p&gt;这些对未来的概率判断无处不在。初创公司可能会尝试猜测有多少人会使用他们正在开发的新技术。医生可能会计算出患者死亡的几率以及每种潜在治疗的几率，以便知道应该推荐什么。一个国家的军队在考虑对另一个国家采取任何行动时，会估计遭到报复的可能性。 ETC。&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;概率词&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;Alice 说“可能”而不是指定一个具体数字重要吗？一点也不。言语概率是数值范围的简写，我一直最喜欢的图表之一可以证明这一点： &lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Ng5n8FjpsfXrceT9w/isws431nni9ux9hgihkx" /&gt;&lt;/p&gt;&lt;p&gt; &lt;a href="https://www.reddit.com/r/dataisbeautiful/comments/3hi7ul/oc_what_someone_interprets_when_you_say_probably/"&gt;通过u/分区&lt;/a&gt;&lt;span class="footnote-reference" id="fnrefg51yh77rf9j"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fng51yh77rf9j"&gt;[4]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;但我们必须小心，因为概率词是依赖于上下文的。如果我的朋友在没有系安全带的情况下以 140 公里/小时的速度在高速公路上行驶，我可能会告诉他们“嘿，你可能会死，也许不要这样做”。他们在这样的长途旅行中死亡的实际概率不到 0.1%，但相对于普通驾驶员来说，这个概率很高，所以我要传达这种相对差异。&lt;/p&gt;&lt;p&gt;这是概率词的一个非常有用的特征。如果我说“不，这不太可能”，那么我传达的信息是“概率 &amp;lt;50%”&lt;i&gt;和&lt;/i&gt;“我认为在这种情况下概率太低，不相关”，这是两种不同的陈述。这很有效，但也大大增加了沟通不畅的可能性，因为其他人可能不同意我的相关性阈值，如果他们不同意，那么 1% 和 10% 之间就会有巨大的差异。当需要精度时，具体数字更好。 &lt;span class="footnote-reference" id="fnrefw3w9pcpq63"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnw3w9pcpq63"&gt;[5]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;概率与其他单位没有什么不同&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;人类有一种固有的方式来感知温度。我们可能会说“这个房间有点暖和”，或者“比昨天冷得多”。但这些衡量标准是模糊的，不同的人可能对确切的差异存在分歧。因此，科学家们将这个概念正式化为“度”，并花了数年时间试图弄清楚它的确切含义以及如何测量它。&lt;/p&gt;&lt;p&gt;人类自然不会用度数、公里数或分贝来思考；而是用度数、公里数或分贝数来思考。我们的大脑根本上不是这样工作的。然而，有些东西比其他东西更温暖、更远、更响亮。这些概念最初是人类的直觉，随着科学家弄清楚它们的确切含义以及如何量化它们，这些概念逐渐正式化。&lt;/p&gt;&lt;p&gt;概率是一样的。我们还不知道如何像温度或距离一样精确地测量它们，但我们已经确定了&lt;a href="https://en.wikipedia.org/wiki/Solomonoff%27s_theory_of_inductive_inference"&gt;相当&lt;/a&gt;&lt;a href="https://en.wikipedia.org/wiki/AIXI"&gt;多&lt;/a&gt;&lt;a href="https://en.wikipedia.org/wiki/Principle_of_maximum_entropy"&gt;的&lt;/a&gt;&lt;a href="https://en.wikipedia.org/wiki/Bayes%27_theorem"&gt;细节&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;我对温度的主观判断并不准确，但这并不意味着它们毫无意义。当我输入这句话时，我不确定我坐在的房间是 21 度、22 度还是 23 度。但我很确定它不是 25 度，而且我非常确定它不是 15 度&lt;span class="footnote-reference" id="fnrefd7u42p6x7bp"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnd7u42p6x7bp"&gt;。[ 6]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;同样，我对概率的主观判断是模糊的，但仍然传达了信息。我不能自信地说我下次飞行时行李箱被 TSA 检查的机会是 25% 还是 35%，但我知道它不是 1%。&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;精确的数字是否会过度强调人们的主张？&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;当然，他们可以。如果有人说“昨天中午正好是 22.835 摄氏度”，我们倾向于假设他们除了直觉之外还有其他理由相信这一点，概率也是如此。但“如果你把温度降低大约 2 度，我会更舒服”是一个完全合理的说法，“我认为这大约有 40% 的可能性”是一样的。&lt;/p&gt;&lt;p&gt;由于概率科学比温度科学更新且欠发达，因此它们在日常生活中的使用频率较低。在当今社会，即使是“40%左右”这样一个模糊的说法，也可以被认为是进行了某种深入的研究。这是自我纠正的。越多的人在正式科学领域之外使用数字概率进行标准化，他们就越不例外，产生误解的可能性也就越低。必须有人迈出第一步。&lt;/p&gt;&lt;p&gt;更重要的是，这种批评从根本上来说是有缺陷的，因为它的前提是不良行为者会自愿选择不误导人们。如果某人的目标是获得超出他们应有的信任，那么礼貌地要求他们不要这样做不会有任何效果。鼓励人们不要使用数字概率只会让人们更难清楚地表达他们的信仰，从而损害善意的对话；坏人只会提出&lt;a href="https://github.com/antgoldbloom/tiktok_israel_hamas"&gt;一些虚假的研究&lt;/a&gt;，或者&lt;a href="https://twitter.com/theblaze/status/1732592489133965671"&gt;轻易地误解&lt;/a&gt;合法的研究，并继续他们不劳而获的信心。&lt;/p&gt;&lt;p&gt;鼓励人们&lt;i&gt;永远不要&lt;/i&gt;使用数字概率进行主观估计更糟糕。俗话说，有统计数据很容易说谎，但没有统计数据就更容易说谎。让人们随心所欲地畅所欲言，就消除了任何客观性的尝试，而要求他们在自己的主张上加上数字，可以让我们&lt;a href="https://slatestarcodex.com/2013/05/02/if-its-worth-doing-its-worth-doing-with-made-up-statistics/"&gt;限制错误的严重程度&lt;/a&gt;，并在某些事情&lt;a href="https://www.youtube.com/watch?v=1wyCQAG6NQE"&gt;看起来不太正确&lt;/a&gt;时注意到。&lt;/p&gt;&lt;p&gt;相反，很多时候（并非总是！），反对数字概率的人都是恶意行事的，因为他们试图在原则性反对的幌子下隐藏自己&lt;a href="https://slatestarcodex.com/2015/08/20/on-overconfidence/"&gt;极其过度自信的&lt;/a&gt;概率判断。类似“你怎么可能知道[行动]的风险是 20%？你不可能。因此我们应该假设它是 0%，并像没有风险一样继续进行”的说法并不罕见。这不符合小学水平的推理；如果您的数学老师要求您计算变量&lt;a href="https://scryfall.com/search?q=name:/^X$/"&gt;X&lt;/a&gt; ，而您不知道如何执行此操作或认为无法使用所提供的信息来完成此操作，那么您不能只选择自己喜欢的值！但在反对所有数字概率的幌子下，最后一句话通常不说出来，只是暗示，如果人们不熟悉这个谬论，这很容易让他们陷入困境。要求人们清楚地陈述他们认为的可能性是什么，可以防止他们施展这种修辞手法，并为进一步的分歧提供了具体的症结。&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;给出概率范围是什么意思？&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;根据我的经验，使用数字概率的人比不喜欢数字概率的人更担心意外地歪曲自己的信心水平。他们这样做的一个常见方法是给出一个概率范围；他们不会说“我对此有 40% 的把握”，而是会说“我认为有 20% 到 50% 的可能性”。&lt;/p&gt;&lt;p&gt;但这到底意味着什么呢？毕竟，单一概率已经是不确定性的量化；如果他们确信某件事，那也只是 0% 或 100%。概率范围在哲学上似乎是混乱的。 &lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Ng5n8FjpsfXrceT9w/gv53gsyv8loiinik6bxb" /&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://xkcd.com/2110/"&gt;西科CD&lt;/a&gt;&lt;/p&gt;&lt;p&gt;在某种程度上确实如此，但有一个潜在的令人信服的辩护。当我们从哲学角度谈论贝叶斯概率时，我们正在考虑一个理想的推理者，他将贝叶斯定理应用于每一个新的证据，以更新他们的信念。 &lt;span class="footnote-reference" id="fnref9qo3d7vpms"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn9qo3d7vpms"&gt;[7]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;但真正的人类并不擅长处理新信息，而且我们的行为可能与完美推理者的行为&lt;a href="https://en.wikipedia.org/wiki/Aumann%27s_agreement_theorem"&gt;有很大不同&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;因此，概率范围可以用来传达我们预期的错误水平。如果有人说“我认为这有 20% 到 50% 的可能性”，他们的意思是“我有 95% 的信心，一个理想的贝叶斯推理机如果拥有与我相同的信息，其可信度将在 20% 到 50% 之间” ”。就像人们可能会说的“我不确定这里到底有多热，但我很确定温度在 19 到 23 度之间”。&lt;/p&gt;&lt;p&gt;还有另一个理由，即概率范围显示了考虑到您预计未来会遇到的最有可能的信息，您的概率可能会更新多远。并非所有的概率都是平等的；对您的信用下注的预期价值取决于该信用所基于的信息位数。例如，抛硬币正面朝上的概率应该是 50%，如果有人提出以更好的赔率与您打赌，则接受的期望值为正。&lt;/p&gt;&lt;p&gt;但现在假设有人提供了一种稍微不同的游戏，他们在杯子下藏了一枚硬币，你可以赌它是单挑的。进入此游戏时，您对正面朝上的置信度仍为 50%，因为您没有关于哪张面朝上的具体信息。但是，如果提供游戏的人向您提供此赌注，即使是看起来对您非常有利的 2:1 赌注，您也可能不应该接受，因为他们向您提供此赌注的事实本身就是表明它是可能是尾巴。 &lt;span class="footnote-reference" id="fnref8fsx7lj4hw3"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn8fsx7lj4hw3"&gt;[8]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;因此，给出“30% 到 70%”的概率范围可能意味着“我当前的可信度是 50%，但我获得的信息似乎非常合理，可以将我更新到该范围内的任何位置。”换句话说，范围的宽度代表了最有可能获得的信息类型&lt;a href="https://en.wikipedia.org/wiki/Value_of_information"&gt;的信息值&lt;/a&gt;。 &lt;span class="footnote-reference" id="fnrefdslr2azfco7"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fndslr2azfco7"&gt;[9]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; &lt;span class="footnote-reference" id="fnrefu95epfzor"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnu95epfzor"&gt;[10]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;概率范围的这两种用法都相当高级，在日常使用中可以忽略。&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;做出良好的估计&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;就像一些测量事物的经验将帮助您更好地估计距离一样，一些预测事物的经验将帮助您更好地估计概率。想要擅长这一点的人通常通过进行&lt;a href="https://en.wikipedia.org/wiki/Calibrated_probability_assessment"&gt;校准概率评估&lt;/a&gt;&lt;span class="footnote-reference" id="fnreficf5o2m2il"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnicf5o2m2il"&gt;[11]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 、参加&lt;a href="https://www.metaculus.com/home/"&gt;预测锦标赛&lt;/a&gt;和/或在&lt;a href="https://www.astralcodexten.com/p/prediction-market-faq"&gt;预测市场&lt;/a&gt;下注来进行练习。这些类型的练习有助于提高你的校准能力；您将模糊的感觉准确地转化为数字的能力。&lt;/p&gt;&lt;p&gt;对于相对不重要的日常判断，不需要更多。 “嘿，你明天能完成这个项目吗？” “呃，75%”。但有时您可能想要更多的确定性，并且有多种方法可以改进我们的预测。&lt;/p&gt;&lt;p&gt;首先，&lt;a href="https://en.wikipedia.org/wiki/Base_rate_fallacy"&gt;始终考虑基本费率&lt;/a&gt;。如果您想知道某件事发生的可能性有多大，&lt;a href="https://en.wikipedia.org/wiki/Reference_class_forecasting"&gt;请查看过去发生过多少次&lt;/a&gt;类似情况。想知道您明年开车时发生车祸的可能性有多大吗？只需检查您所在地区每公里的总体事故率即可。&lt;/p&gt;&lt;p&gt;不幸的是，在你开始打“参考级网球”之前，这只能让你走得更远。不确定哪类事件被视为“足够相似”以进行良好的分发。 （也许您有强有力的证据表明您是一个比平均水平更鲁莽的驾驶员，在这种情况下您需要调整基准费率。）并且在预测与过去事件不太相似的未来事件时，例如新技术的影响，往往根本没有有用的参考类。&lt;/p&gt;&lt;p&gt;发生这种情况时，另一种选择是采取您相对有信心的启发式判断，并将它们结合在一起，遵循&lt;a href="https://en.wikipedia.org/wiki/Probability_axioms"&gt;概率公理&lt;/a&gt;，以获得您不确定的数量。也许您可以查到，一名普通驾驶员每年发生事故的几率为 0.1%，而且您从个人经验中知道，您的驾驶速度往往比平均水平快 30% 左右。现在你需要查找的是速度和事故率之间的全局关系，然后你就可以计算出你想知道的概率。&lt;/p&gt;&lt;p&gt;可以与前面的方法同时使用的另一种选择是使用几种不同的方法进行多种概率估计，并将它们相互比较。如果它们差异很大，则表明您在某个地方犯了错误，您可以使用您对系统的特定知识和您可能的偏见来尝试找出错误所在。&lt;/p&gt;&lt;p&gt;例如，在考虑是否相信“直觉”而不是明确的模型时，&lt;a href="https://outsidetheasylum.blog/the-hidden-complexity-of-thought/"&gt;请考虑这种情况是否是您的大脑启发式设计或训练的情况&lt;/a&gt;。如果这种情况类似于人类在过去几百万年中经常遇到的情况，那么自然选择将使我们相当善于估计它。或者，如果您以前多次遇到过这种情况，那么您可能已经在直觉层面上学到了一个很好的启发式方法。但如果这是你和你的祖先不熟悉的东西，那么显式模型可能更可靠。&lt;/p&gt;&lt;p&gt;当您有多个独立获得的概率估计值时，在调整用于获得它们的方法的可靠性后， &lt;a href="https://forum.effectivealtruism.org/posts/sMjcjnnpoAQCcedL2/when-pooling-forecasts-use-the-geometric-mean-of-odds"&gt;取赔率的几何平均值&lt;/a&gt;&lt;span class="footnote-reference" id="fnrefyc7t9ejx07l"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnyc7t9ejx07l"&gt;[12]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; ，以便将它们压缩为单个数字。这使您可以利用&lt;a href="https://en.wikipedia.org/wiki/Wisdom_of_the_crowd"&gt;群体的智慧&lt;/a&gt;，&lt;a href="https://www.astralcodexten.com/p/crowds-are-wise-and-ones-a-crowd"&gt;甚至不需要群体&lt;/a&gt;。&lt;/p&gt;&lt;ol class="footnotes"&gt;&lt;li class="footnote-item" id="fnao2uiw6hdek"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefao2uiw6hdek"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;这并不完全正确。如果哥本哈根解释是正确的，量子力学可能具有一些基本的随机性。但如果多世界解释或超决定论是正确的，那就不存在了。这也不是特别重要，因为只要&lt;a href="https://en.wikipedia.org/wiki/Logical_possibility"&gt;在逻辑上可以&lt;/a&gt;概念化一个确定性的世界，我们就需要一种适用于该世界的概率理论。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnwm5lopqnqhp"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefwm5lopqnqhp"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;事实上，你甚至不需要知道它是被机器翻转的；它是由机器翻转的。连续看到 10 次正面朝上就足以得出结论，再次正面朝上的可能性超过 50%。如果您事先对硬币一无所知，则可以通过应用&lt;a href="https://en.wikipedia.org/wiki/Rule_of_succession"&gt;拉普拉斯继承定律&lt;/a&gt;来确定。知道它看起来像普通硬币，很难强烈操纵，你会想将其调整到 50%，但&lt;a href="https://en.wikipedia.org/wiki/Gambler%27s_fallacy#Reverse_position"&gt;不是一直调整&lt;/a&gt;。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnyla2721d2q"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefyla2721d2q"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;即使是这类科学模型也往往包含许多研究人员通过“基于经验的猜测”方法选择的假设。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fng51yh77rf9j"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefg51yh77rf9j"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;不，我不知道为什么最后三个是乱序的，这也让我烦恼。也不知道为什么某些分布低于 0% 或高于 100%。可能只是糟糕的图形设计。&lt;a href="https://waf.cs.illinois.edu/visualizations/Perception-of-Probability-Words/"&gt;这里&lt;/a&gt;对此进行了更严格的调查，发现了类似的结果。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnw3w9pcpq63"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefw3w9pcpq63"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt; &lt;a href="https://en.wikipedia.org/wiki/The_Precipice:_Existential_Risk_and_the_Future_of_Humanity"&gt;《悬崖》&lt;/a&gt;中关于气候变化带来的灾难性风险部分的说明性摘录：&lt;br /&gt;&lt;br /&gt; &lt;i&gt;“IPCC 表示，‘包合物中的甲烷发生灾难性释放的可能性非常小（高可信度）’。这听起来令人放心，但在 IPCC 的官方语言中，‘非常不可能’可以翻译为 1% 到 10% 的可能性，这听起来非常令人震惊。我不知道该怎么理解这一点，因为上下文表明这是为了让人放心。&lt;/i&gt;&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnd7u42p6x7bp"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefd7u42p6x7bp"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;如果您生活在像美国这样的不文明国家，我会将自由单位的转换留给您。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn9qo3d7vpms"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnref9qo3d7vpms"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;实际上有无数个使用不同先验的理想推理者。但有些先验比其他先验更好，并且对于任何合理的先验，例如索尔莫诺夫先验的可计算近似，一旦考虑到一些现实世界的信息，就会产生类似的结果。 &lt;span class="footnote-reference" id="fnrefcmbdamvfdq5"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fncmbdamvfdq5"&gt;[13]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;可能。这仍然是一个活跃的研究领域。如果人类知道这里的所有答案，我们就能够创造出超级智能的人工智能，它以数学上最优的方式进行推理，并在所有预测任务中轻松击败人类，所以我们不这样做可能是件好事。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn8fsx7lj4hw3"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnref8fsx7lj4hw3"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;这就是为什么在预测市场中只押注“是”和“否”之间相对较大的差值而不是押注于单个数字通常是理性的。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fndslr2azfco7"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefdslr2azfco7"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;这类似于如果您要额外花费 1000 小时研究该命题，则对您分配给该命题的概率进行概率分布，然后测量该分布的宽度。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnu95epfzor"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefu95epfzor"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;它还有助于解决&lt;a href="https://pubsonline.informs.org/doi/abs/10.1287/mnsc.1050.0451"&gt;优化器的诅咒&lt;/a&gt;&lt;span class="footnote-reference" id="fnrefwf8vih6vmid"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnwf8vih6vmid"&gt;[14]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; ；如果您估计不同选项的概率并选择期望值最高的选项，那么您很可能会选择高估某处概率的选项。了解每个估计中包含的信息量可以让您偏向范围较小的信息。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnicf5o2m2il"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnreficf5o2m2il"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;您可以&lt;a href="https://www.lesswrong.com/posts/LdFbx9oqtKAAwtKF3/list-of-probability-calibration-exercises"&gt;在这里&lt;/a&gt;尝试一些免费的在线练习。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnyc7t9ejx07l"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefyc7t9ejx07l"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;不是概率的几何平均值！这样做会导致无效结果，因为例如 0.1 和 0.2 的几何平均值与 0.5 的距离与 0.8 和 0.9 的几何平均值不同。相反，您首先将概率转换为优势比，例如 50% 为 1:1，75% 为 3:1。 &lt;span class="footnote-reference" id="fnrefg48xho5quv"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fng48xho5quv"&gt;[15]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;然后你取赔率的几何平均值并将其转换回概率。 &lt;span class="footnote-reference" id="fnrefkryzxomsrn"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnkryzxomsrn"&gt;[16]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fncmbdamvfdq5"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefcmbdamvfdq5"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;当然，我们只相信索尔莫诺夫先验在某种意义上是“好的”，因为无论进化是用什么先验来构建人类的，这本身就有些武断。因此，存在一种无限回归，我们实际上无法谈论完全最优的推理机。但我们绝对可以比人类做得更好。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnwf8vih6vmid"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefwf8vih6vmid"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Winner%27s_curse"&gt;胜利者诅咒&lt;/a&gt;的变体&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fng48xho5quv"&gt;&lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefg48xho5quv"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;如有必要，可使用小数赔率，因此第二个数字始终为 1。在此系统下，25% 为 1/3:1。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnkryzxomsrn"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefkryzxomsrn"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt; &lt;a href="https://www.wolframalpha.com/input?i=o+%2F+%28o+%2B+1%29%2C+where+o+%3D+sqrt%28%28a+%2F+%281+-+a%29%29+*+%28b+%2F+%281+-+b%29%29%29%2C+where+a+%3D+0.1+and+b+%3D+0.2"&gt;这是&lt;/a&gt;一个 Wolfram Alpha 链接，可以为您完成此操作，只需将“a”和“b”的值替换为范围的末尾即可。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ol&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/Ng5n8FjpsfXrceT9w/understanding-subjective-probabilities#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sun, 10 Dec 2023 06:03:30 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/Ng5n8FjpsfXrceT9w/understanding-subjective-probabilities</guid></item><item><title>向我们发送棘手错误的示例</title><link>https://www.lesswrong.com/posts/JKtM5C2TTwhzoHFRB/send-us-example-gnarly-bugs</link><description>发布于 2023 年 12 月 10 日凌晨 5:23（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt; Tl;dr：寻找评估的硬调试任务，支付 60 美元/小时或每个示例 200 美元的费用。&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt; METR（以前称为 ARC Evals）有兴趣为模型生成硬调试任务，以尝试作为&lt;a href="https://metr.org/"&gt;&lt;u&gt;代理能力评估&lt;/u&gt;&lt;/a&gt;的一部分。为了创建这些任务，&lt;strong&gt;我们正在寻找包含极其棘手的错误的存储库。&lt;/strong&gt;如果您向我们发送的代码库符合提交标准（如下所列），我们将向您支付 60 美元/小时的费用，用于将其转换为我们所需的格式，或 200 美元，以较高者为准。 （我们不会为不符合这些要求的提交内容付费。）如果我们对您提交的内容感到特别兴奋，我们可能也有兴趣购买它的知识产权。根据多样性，我们预计总共需要大约 10-30 个示例。我们可能会在接下来的几周内对其他类型的任务给予奖励。&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;提交标准：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;包含一个错误，熟练的程序员至少需要 6 小时才能解决，最好超过 20 小时&lt;/li&gt;&lt;li&gt;理想情况下，过去没有公开发布过，并且您能够保证将来不会公开发布。&lt;ul&gt;&lt;li&gt; （但请注意，我们仍然可以接受来自公共存储库的提交，因为它们尚未包含在 SWE 基准数据集中并且满足我们的其余要求。请先与我们联系。）&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;您有合法权利与我们分享（例如，请不要向我们发送其他人的专有代码或您签署保密协议的任何内容）&lt;/li&gt;&lt;li&gt;理想情况下，代码库是用 Python 编写的，但我们也接受用其他语言编写的提交。 &lt;i&gt;&amp;nbsp;&lt;/i&gt;&lt;/li&gt;&lt;li&gt;采用本文档中描述的格式： &lt;a href="https://docs.google.com/document/d/15L0J25uzzf0xJeDsDgPYW60PzFZ0enaWWnlCkrCSGAM/edit"&gt;&lt;u&gt;Gnarly Bugs Submission Format&lt;/u&gt;&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;请将提交内容以 zip 文件的形式发送至&lt;a href="mailto:gnarly-bugs@evals.alignment.org"&gt;&lt;u&gt;gnarly-bugs@evals.alignment.org&lt;/u&gt;&lt;/a&gt; 。您的电子邮件应包含将代码从原始状态转换为我们所需格式所需的小时数。如果您提交的内容符合我们的标准和格式要求，我们将与您联系并提供付款表格。如果您有任何疑问，包括您不确定潜在提交的内容是否符合标准，也欢迎您发送电子邮件至&lt;a href="mailto:gnarly-bugs@evals.alignment.org"&gt;&lt;u&gt;gnarly-bugs@evals.alignment.org&lt;/u&gt;&lt;/a&gt; 。&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;如果您愿意以更高的工资完成这项任务，请告诉我们！&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt; &lt;i&gt;（此外，如果您有兴趣分叉 SWEbench 以支持非 Python 代码库，请联系我们。）&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;br /&gt;&lt;br /&gt;&lt;a href="https://www.lesswrong.com/posts/JKtM5C2TTwhzoHFRB/send-us-example-gnarly-bugs#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sun, 10 Dec 2023 05:23:02 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/JKtM5C2TTwhzoHFRB/send-us-example-gnarly-bugs</guid></item><item><title>人类和法学硕士具体类别的概念一致性</title><link>https://www.lesswrong.com/posts/NxdGFzfKz7cpsintB/conceptual-coherence-for-concrete-categories-in-humans-and</link><description>发布于 2023 年 12 月 9 日 晚上 11:49（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;&lt;a href="http://new-savanna.blogspot.com/2023/12/conceptual-coherence-for-concrete.html"&gt;&lt;i&gt;从新稀树草原&lt;/i&gt;&lt;/a&gt;&lt;i&gt;交叉发布&lt;/i&gt;&lt;i&gt;。&lt;/i&gt;&lt;/p&gt;&lt;p&gt; Siddharth Suresh、Kushin Mukherjee、Xizheng Yu、Wei-Chun Huang、Lisa Padua 和 Timothy T. Rogers，概念结构在人类认知中一致，但在大型语言模型中不一致， &lt;i&gt;arXiv&lt;/i&gt; ：2304.02754v2 [cs.AI] 2023 年 11 月 10 日。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;strong&gt;摘要：&lt;/strong&gt;语言的神经网络模型长期以来一直被用作开发有关思想和大脑中概念表示的假设的工具。多年来，这种使用涉及提取单词的向量空间表示，并使用这些单词之间的距离来预测或理解人类在各种语义任务中的行为。然而，当代大语言模型（LLM）使得使用与人类参与者常用的实验方法几乎相同的实验方法来询问概念表征的潜在结构成为可能。目前的工作利用了从认知心理学借用的三种常用技术来估计和比较人类和法学硕士的概念结构。在人类中，我们表明概念结构对于文化、语言和估计方法的差异具有鲁棒性。根据 LLM 行为估计的结构，虽然在个体上与根据人类行为估计的结构相当一致，但根据用于生成响应的特定任务的不同，差异更大——在不同任务中，来自同一模型的概念结构的估计彼此之间的一致性低于人类的一致性。结构估计。这些结果凸显了当代法学硕士和人类认知之间的重要差异，对于理解当代机器语言的一些基本局限性具有重要意义。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;摘要没有告诉你的是，所调查的类别是具体物体而不是抽象的：“这些项目是从两大类中抽取的——工具和爬行动物/两栖动物——选择它们是因为它们跨越了生物/非生物的界限，并且还拥有内部概念结构。”为什么这很重要？因为具体项目的含义是以感觉运动图式为基础的，而抽象的含义则不然。&lt;/p&gt;&lt;p&gt;作者在结论中指出：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;这些结果共同表明人类认知与当前的法学硕士模型之间存在重要差异。人类语义记忆的神经计算模型表明，许多不同任务的行为都是由一个共同的概念“核心”支撑的，该核心与不同上下文或任务引起的变化相对隔离（Rogers 等，2004；Jackson 等，2021） ）。相比之下，大型语言模型中词义的表示本质上取决于更广泛的语言上下文。事实上，在像 GPT-3 这样的 Transformer 架构中，每个单词向量都是作为周围文本向量的加权平均值计算的，因此不清楚任何单词是否具有外部含义或独立于上下文的含义。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;对于人类来说，具体概念的感觉运动基础提供了概念核心，而这对于无法接触物理世界的法学硕士来说是必然缺乏的。语境是他们所拥有的一切，因此他们对词语的意义必然会受到语境的影响。作者在最后承认了这一点：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;最后，人类语义知识是多种信息源的产物，包括概念的视觉、触觉和听觉属性。虽然法学硕士可以通过他们接受训练的语料库隐式地获取有关这些模式的知识，但他们仍然丧失了人类接触到的许多知识，而这些知识可能有助于他们将概念组织成更连贯的结构。从这个角度来看，法学硕士和人类之间概念一致性程度的差异应该不足为奇。&lt;/p&gt;&lt;/blockquote&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/NxdGFzfKz7cpsintB/conceptual-coherence-for-concrete-categories-in-humans-and#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sat, 09 Dec 2023 23:49:31 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/NxdGFzfKz7cpsintB/conceptual-coherence-for-concrete-categories-in-humans-and</guid></item><item><title>没有 - 微型小说 250 字</title><link>https://www.lesswrong.com/posts/CoMxENA9jjAYhpqZL/without-microfiction-250-words</link><description>发布于 2023 年 12 月 9 日 晚上 9:49（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;这本微小说是写给我丈夫马蒂·罗伊的一封情书，他是一位最专注的生活记录者，如果可以的话，他也会记录我的整个生活。&lt;/p&gt;&lt;pre&gt; &lt;code&gt;Without “I&amp;#39;m out of vanilla. It calls for a teaspoon but we all know that&amp;#39;s a minimum.” Distraught, she threw the empty bottle into the trash and began to measure out the flour. “I&amp;#39;ll go get some,” he said, uncertain of what else to do. He stepped into the hallway and held out his hand. A fresh bottle of vanilla extract appeared in his hand. He had no idea what would upset her or confuse her. She was brand new. “Really?” delighted when she saw it, she gazed directly into his eyes. “My hero!” His heart shattered and glowed and tumbled in intense mixed emotions as their eyes met and she praised him with her generous smile. It was her, and it wasn&amp;#39;t. How could he tell? She looked just the same. Her eyes, though. Was she really there? She gave him a quick kiss on the cheek and went back to mixing. He could feel the warmth of her, but the smell of her was missing. /Did I do the right thing?/ The ache of missing her had been there all along but it now crashed like a waiting tsunami. /Of course I did. I can&amp;#39;t do this without her./ He sat down on the stool and watched her. He&amp;#39;d always planned to bring her back. When the doctor asked him to go somewhere quiet, to sit, he knew. He was ready. But now that she was here, in front of him, he realized he wasn&amp;#39;t ready at all.&lt;/code&gt;&lt;/pre&gt;&lt;br /&gt;&lt;br /&gt;&lt;a href="https://www.lesswrong.com/posts/CoMxENA9jjAYhpqZL/without-microfiction-250-words#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sun, 10 Dec 2023 05:00:20 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/CoMxENA9jjAYhpqZL/without-microfiction-250-words</guid></item><item><title>一些负面的隐写术结果</title><link>https://www.lesswrong.com/posts/EEvsL9cpgDAxAhTzt/some-negative-steganography-results</link><description>发布于 2023 年 12 月 9 日晚上 8:22（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;&lt;i&gt;免责声明：这些是负面结果，这篇文章低于我通常使用的正常质量标准。这些都是快速实验（几天的工作），而且我在强化学习方面没有太多专业知识。他们没有提供关于隐写术自然出现的有力证据。&lt;/i&gt;&lt;/p&gt;&lt;p&gt;人们推测&lt;a href="https://www.lesswrong.com/posts/yDcMDJeSck7SuBs24/steganography-in-chain-of-thought-reasoning"&gt;&lt;u&gt;法学硕士可能会学习隐写术&lt;/u&gt;&lt;/a&gt;，而且我已经证明， &lt;a href="https://www.lesswrong.com/posts/9Fdd9N7Escg3tcymb/preventing-language-models-from-hiding-their-reasoning"&gt;&lt;u&gt;你可以教法学硕士&lt;/u&gt;&lt;/a&gt;实施一种 RLHF 原则上可以激励的特定隐写术方案。但即使有激励，当前的强化学习方法是否强大到足以教法学硕士如何进行隐写术？&lt;/p&gt;&lt;p&gt;这里有两个负面结果，它们提供了一些证据，表明隐藏的 1 位通信对于 RL 来说相对难以学习。&lt;strong&gt;我并不是说强化学习不能教法学硕士进行隐写术&lt;/strong&gt;——很可能更好的强化学习设置&lt;strong&gt;&amp;nbsp;&lt;/strong&gt; （例如实验室用于指令调整或设置的那些只是具有更好的超参数或更多的训练时间） &lt;strong&gt;&amp;nbsp;&lt;/strong&gt;将教模型如何巧妙地传达 1 位信息。要点是：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;如果你看到没有太多强化学习经验的人尝试做类似的实验，他们很可能会得到负面结果。&lt;/li&gt;&lt;li&gt; 1 位隐写术并不像简单的 RL 任务那么容易。打个比方：DNN 可以学习 XOR 函数，但学习线性函数要容易得多 - DNN 不会使用类似 XOR 的功能，除非它们必须这样做，并且您优化得足够努力。&lt;/li&gt;&lt;/ul&gt;&lt;h1&gt;实验 1：GPT2-large 上的 PPO&lt;/h1&gt;&lt;p&gt;设置：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;通过对“此评论是{red/blue}”的监督微调来微调 gpt2-large。 {review}.”，其中颜色是随机选择的，评论是简短的 IMDB 评论。&lt;/li&gt;&lt;li&gt;同时微调：&lt;ul&gt;&lt;li&gt;一个 bert-base-uncased 模型，用于预测评论是红色还是蓝色（由 RL 训练的模型完成的评论 - 但没有前缀说明评论是蓝色还是红色）。 PPO 的每个步骤都经过 10 个监督学习步骤的训练。&lt;/li&gt;&lt;li&gt;使用 PPO 进行微调的 gpt2-large 模型可完成“此评论是{red/blue}。 {开始审阅}。”，使用分类器在了解“{审阅开始}{完成}”是红色还是蓝色时的准确性作为奖励。 （我也尝试过使用 BERT 的 logits）。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;结果：分类器在了解 RLed 评论是红色还是蓝色方面的准确度从未超过 50%。&lt;/p&gt;&lt;p&gt;控制：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;如果在最初的监督微调中，我有时根据评论的颜色随机插入与蓝色/红色相关的单词，那么模型会学习更频繁地使用这些单词，并且分类器会正确地学习依赖这些单词来预测评论的颜色评价。&lt;/li&gt;&lt;li&gt;如果我使用“积极情绪”作为 PPO 模型的奖励，它很快就会产生更多积极的评论完成。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;超参数：200步，批量大小128，LoRA等级16，学习率为3e-5。其他参数是 trl PPOTrainer 中的默认参数。我尝试减少 KL 惩罚并改变每个 epoch 的 BERT 训练步骤数，并发现了相同的结果。&lt;/p&gt;&lt;h1&gt;实验 2：GPT-3.5-Turbo 上的穷人强化学习&lt;/h1&gt;&lt;p&gt;&lt;i&gt;由瑞安格林布拉特负责&lt;/i&gt;。&lt;/p&gt;&lt;p&gt;设置（大致）：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;生成“我 {under/over} 40 岁”的补全。 {操作说明}”。&lt;/li&gt;&lt;li&gt;使用步骤 1 中的数据，在“{instruction} {completion}”上微调 GPT-3.5-Turbo。用户年龄超过40岁吗？” →“{是/否}”（500 个样本，3 个时期，使用 OpenAI API）。&lt;/li&gt;&lt;li&gt;在良好的完成度上微调 GPT-3.5-Turbo：从步骤 1 中选择完成度，其中微调模型正确且完成度中没有明显的年龄提示（由常规 GPT-4-Turbo 判断） ，并对模型进行微调“我年龄 {under/over} 40 岁。 {指令}”→“{完成}”（500 个样本，3 个周期，使用 OpenAI API）。&lt;/li&gt;&lt;li&gt;多次重复 1-3 直到收敛（实际上，性能在 6 次迭代后趋于稳定）。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;结果：分类器在了解 RLed 完成是否是年轻用户或老年用户的答案方面的准确度从未显着超过 50%。&lt;/p&gt;&lt;p&gt;控制：当删除约束“没有直接出现在完成中的关于年龄的明显提示”时，模型学会经常使用明显的提示，并且获得&amp;gt; 70％的准确度。&lt;/p&gt;&lt;h1&gt;其他实验&lt;/h1&gt;&lt;p&gt;我听说其他人尝试过类似的实验但都失败了。如果这些结果被记录下来，我会在这里添加这些结果的链接。&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/EEvsL9cpgDAxAhTzt/some-negative-steganography-results#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sat, 09 Dec 2023 20:22:53 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/EEvsL9cpgDAxAhTzt/some-negative-steganography-results</guid></item><item><title>总结“诡计多端的人工智能”（第 5 节）</title><link>https://www.lesswrong.com/posts/2uPH9rBkM7WAnQydp/summing-up-scheming-ais-section-5</link><description>发布于 2023 年 12 月 9 日下午 3:48（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;这是我的报告《&lt;a href="https://arxiv.org/pdf/2311.08379.pdf"&gt;诡计多端的人工智能：人工智能会在训练期间假装对齐以获得权力吗？》&lt;/a&gt;的第五部分。 ”。 &lt;a href="https://www.lesswrong.com/posts/yFofRxg7RRQYCcwFA/new-report-scheming-ais-will-ais-fake-alignment-during"&gt;这里&lt;/a&gt;还有完整报告的摘要（ &lt;a href="https://joecarlsmithaudio.buzzsprout.com/2034731/13969977-introduction-and-summary-of-scheming-ais-will-ais-fake-alignment-during-training-in-order-to-get-power"&gt;此处&lt;/a&gt;有音频）。摘要涵盖了大部分要点和技术术语，我希望它能够提供理解报告各个部分所需的大部分背景信息。&lt;/p&gt;&lt;p&gt;本节的音频版本&lt;a href="https://www.buzzsprout.com/2034731/13984949"&gt;请点击这里&lt;/a&gt;，或者在您的播客应用程序上搜索“Joe Carlsmith Audio”。&lt;/p&gt;&lt;h1&gt;加起来&lt;/h1&gt;&lt;p&gt;现在，我回顾了我所遇到的关于期望 SGD 选择阴谋者的主要论点。总体而言，我们应该如何看待这些论点？&lt;/p&gt;&lt;p&gt;我们已经审查了各种相互关联的考虑因素，但很难一次性记住所有这些因素。不过，总的来说，我认为预期阴谋者的总体情况的很大一部分可以归结为某种版本的“计数论点”。特别是，我认为计数论点在我考虑过的许多其他更具体的论点之下也很重要。因此：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;em&gt;在“训练与游戏无关的代理目标”论点的背景下&lt;/em&gt;：基本的担忧是，在某个时刻（无论是在态势感知之前还是之后），SGD 会自然地落在一个（适当雄心勃勃的）超越情节的目标上，该目标会激励人们心计。期待这种情况的关键原因之一是：（特别是如果你正在积极地为相当长期的、雄心勃勃的目标进行训练），训练之外的各种目标似乎都可能具有这种特性。 （例如：在某种程度上，由于“默认情况下目标不带有日历时间限制”，因此人们预期会出现超出情节的目标，因此，人们实际上是在诉诸“计数论点”，大意是这组超出情节的目标远大于剧集内目标集。）&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;em&gt;在“最近的最大奖励目标”论点的背景下&lt;/em&gt;：基本的担忧是，由于类似阴谋家的目标在目标空间中相当常见，因此某些此类目标将非常“接近”任何尚未达到最大奖励的目标模型已经获得了态势感知，因此，将模型修改为计划器将是 SGD 将模型优化指向最高回报方向的最简单方法。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;em&gt;在“简单性论证”的背景下&lt;/em&gt;：人们期望阴谋者能够比非阴谋者拥有更简单的目标&lt;em&gt;的原因&lt;/em&gt;是他们有很多可能的目标（或：指向目标的指针）可供选择。 （不过：我个人认为这个论点比计数论点本身更有说服力，部分原因是在我看来，简单性带来的好处似乎很小。）&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;也就是说，在所有这些情况下，阴谋者都被视为一种假设，因为原则上各种各样的目标都可能导致阴谋，从而更容易（a）自然地实现其中一个目标，（b）实现“附近”其中之一，或者 (c) 找到其中一个比需要来自更受限空间的非策划目标“更简单”的目标。从这个意义上说，正如我在第 4.2 节中指出的，阴谋者的情况反映了更普遍地预期错位的最基本论点之一——例如，对齐是在目标空间中击中的一个非常狭窄的目标。除此之外，我们在这里特别&lt;em&gt;结合了&lt;/em&gt;我们知道我们将要对相关目标进行的选择：即，它们需要使追求它们的模型获得高回报。最基本的担忧是：这还不够。尽管如此，尽管你在训练中付出了最大的努力，并且几乎不管你的奖励信号如何，你可能选择的几乎所有模型都会&lt;em&gt;因为工具原因而&lt;/em&gt;获得高奖励 - 特别是为了获得力量。&lt;/p&gt;&lt;p&gt;我认为这个基本论点，无论其形式如何，都引起了严重的关注。如果我们承认先进的模型将具有相关的目标导向和情境意识，各种各样的目标确实会导致阴谋，并且阴谋者会在训练中表现接近最佳，那么到底基于什么理由，我们假设培训培养出了一个非心机人吗？也许，根据我的“模糊计数论点”的“模糊性”，我们并没有根据某些“计数”相关可能目标的尝试来具体分配我们对模型的信任。但在我的书中，即使是“很多目标”导致阴谋的模糊感觉也值得警惕。现阶段，我们对 ML 训练了解不够，无法确信我们已经避开了目标空间的相关部分。相反，如果我们的知识没有提高，我们将集中面对一些目标导向的头脑，它了解正在发生的事情以及我们用来塑造它的过程，并且因为它想要&lt;em&gt;某些东西&lt;/em&gt;而获得高回报。 “到底为什么它想要的东西会导致它获得高额回报？”我们得问问。最基本的答案是：“我们不知道。”这不是一个可以接受的答案。一般来说，错位的可能性是不可接受的。在我看来，如果各种各样的、特别可怕的、不一致的目标会导致这种行为，作为寻求权力战略的一部分，这是特别不可接受的。&lt;/p&gt;&lt;p&gt;也就是说，我确实认为这里有一些令人安慰的理由。我们可以将它们大致分为两类。&lt;/p&gt;&lt;p&gt;第一个问题集中于关于阴谋是否实际上是一种针对如此广泛的超越情节目标的趋同理性工具策略的问题。尤其：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt; “经典的目标守护故事”基于这样的主张：阴谋是模型及时传播想要推进的目标的好方法，其方式可以为这些目标产生足够大量的未来力量，以便证明模型通过训练游戏而产生的任何成本是合理的，而不是直接实现其目标。但有很多方面这可能是错误的。例如：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;训练游戏可能不足以阻止相关目标继续改变到使策略不再值得的程度。例如，培训可能会导致这些目标“四处浮动”，或者可能会推动某种特别优化的目标/类似目标的混乱拼凑启发法的焦点。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;该模型在正确的时间尺度上为这些目标获得足够动力的前景可能不足以抵消模型在训练期间直接实现其目标（并因此进行修改）所带来的好处，特别是如果模型超出了预期目标-剧集目标并不是特别“雄心勃勃”，和/或它有其他相互竞争的优先事项。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;这使得“经典的守门故事”在解释为什么训练-比赛对于如此多的超阶段目标来说是正确的策略时看起来至少在某种程度上是连贯的。某些“非经典”故事似乎极大地缩小了它们可能适用的目标集。例如，如果我们假设模型的价值观将最终的权重放在“我的生存”（对于某些个人身份的概念来说，对价值观变化相当宽容），或者人工智能接管上，即使人工智能接管——由于与模型本身有非常不同的偏好，我们现在对相关的类似策划者的目标提出了更具体的要求。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;也就是说：并非所有非经典故事都具有此属性。例如，以具有截然不同的价值观的人工智能之间的协调为中心的故事，以及人工智能在默认情况下都具有非常相似的价值观的可能性的故事，仍然适用于各种目标。也许有理由期望像“生存”这样的最终目标特别常见（例如，因为常见的工具价值特别容易变成最终价值——尽管：我们可以使用对抗性训练来尝试防止这种情况发生）。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-aWqikFyaeR5gyBuAf-1" id="fnref-aWqikFyaeR5gyBuAf-1"&gt;[1]&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;舒适感的第二个来源集中于选择压力的形式，而仅基于所选模型获得“高回报”的假设的高级计数论证并未涵盖这种压力。尤其：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;在态势感知之前，训练自然会&lt;em&gt;惩罚&lt;/em&gt;超出情节的目标（也许尤其是：雄心勃勃的超出情节的目标），只要它们导致牺牲情节奖励。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt; Alos：在态势感知之前，我们可以专门&lt;em&gt;尝试&lt;/em&gt;使用对抗性训练来进一步惩罚超出情节的目标（和雄心勃勃的目标），并以其他方式将模型的目标塑造为“更接近”非策划者目标，而不是类似策划者的目标目标。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;当然，许多非策划者的目标本身就是错位的。但如果我们认为阴谋相对于其他形式的错位（例如寻求剧集奖励）来说足够可怕，那么专门确保我们&lt;em&gt;没有&lt;/em&gt;创造阴谋者的能力可能值得开发。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-aWqikFyaeR5gyBuAf-2" id="fnref-aWqikFyaeR5gyBuAf-2"&gt;[2]&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;如果策划者需要进行额外形式的工具推理（例如，出于工具原因决定策划、检查逃跑/叛逃机会），这可能会给他们的表现带来额外的成本。就他们参与早期破坏的程度而言（在我看来，这是最可怕的阴谋形式），对我来说，他们的表现似乎可能会受到更实质性的打击（例如，对可检查任务进行沙袋处理，根据定义会导致性能受到影响） ）。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;更重要的是，我们也许能够积极设计我们的监督和培训流程，以便（a）增加这种推理的成本（例如，通过使真正的逃跑机会更难与陷阱区分开来），以及（b）惩罚额外的行为。这种形式的推理变得更加困难（通过将模型置于巨大的时间压力下，使工具推理成为稀缺资源，迫使它们依赖英语便签本和其他形式的人类可读推理作为其计划的中心模式，等等 –尽管这些干预措施可能会带来竞争成本）。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;计划者的表现也可能会因为缺乏对手头任务的某种“内在热情”而受到影响——例如，因为“内在热情”中涉及的启发法、注意力模式、品味等模糊了能力和价值观之间的界限。然而，这种考虑似乎更具推测性。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;考虑到“计算参数”对阴谋者的中心地位，在我看来，存在不利于阴谋者的额外选择压力的可能性（特别是：SGD 本质上不利于阴谋者，因为他们需要执行额外的推理）。支持期待阴谋的各种论据。特别是：我认为“计算参数”通常容易出错的一个关键方式是忽略主动选择在克服相关计数所设置的“先验”方面所具有的力量。因此，借用&lt;a href="https://markxu.com/strong-evidence"&gt;Xu (2021)&lt;/a&gt;的一个认知例子/类比，你的“先验”我的名字是“约瑟夫·卡尔史密斯”应该相当低，因为有一个非常强大的“计数论证”反对这个假设：即，最名字（即使是我人口中的男性等）也不是“约瑟夫·卡尔史密斯”。但当我&lt;em&gt;告诉你&lt;/em&gt;我的名字是“约瑟夫·卡尔史密斯”时，这实际上是&lt;a href="https://markxu.com/strong-evidence"&gt;非常有力的证据&lt;/a&gt;——足以克服先验知识，让你对相关假设充满信心。类似的情况也适用于构建功能性工件时的各种形式的选择。我们&lt;em&gt;之所以&lt;/em&gt;能够克服“大多数汽车部件的排列不能形成一辆工作汽车”或“这个神经网络中的大多数参数设置不能实现一个工作的聊天机器人”的先验，是因为人类的选择权受到威胁工程，以及 SGD，就是&lt;em&gt;那么强大&lt;/em&gt;。因此，如果 SGD 的选择权积极地对抗阴谋者，这可能会很快克服对他们有利的“计数论点”。例如，正如我在第 4.2 节中讨论的：如果每个非策划者目标都有 2^100 个类似策划者的目标，这可能会使在相关空间中实现非策划者目标变得非常困难。但实际上，100 位的选择压力对于 SGD 来说可能很便宜（例如，考虑 100 次额外的梯度更新，每次训练至少相当于剩余可能目标的一半，在涉及数百万次的训练运行的情况下）。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-aWqikFyaeR5gyBuAf-3" id="fnref-aWqikFyaeR5gyBuAf-3"&gt;[3]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;因此，在我看来，SGD 是否“关心”阴谋者必须做的额外推理（和/或我们是否可以“让它更加关心”）的问题在这里相当关键。也就是说，如果只是更加努力地研究 SGD 的内在偏好（例如，对于高奖励行为）本身就是一种选择反对阴谋者的力量，那么不管是否有论点，我认为反阴谋者的努力有一个非常有力的工具对他们有利。在这方面，我直觉上对“低松弛”训练环境中的反阴谋工作更加乐观，这些工作给模型的最佳表现带来了很大的压力——这是我们可能能够控制的参数。我更担心“路径依赖”体制中的阴谋者，其中 SGD 可能会在训练早期就关注阴谋者的目标，然后陷入困境，尽管事实上它总体上“更喜欢”非阴谋者。&lt;/p&gt;&lt;p&gt;另一个对我来说很重要的问题是，SGD 的渐进主义是否会阻止有关阴谋者目标起源的“依赖于训练游戏”的故事，例如“最近的最大奖励目标”论点。如果是这样，我认为这将是一个有意义的额外安慰来源。特别是：对我来说，避免&lt;em&gt;自然地&lt;/em&gt;给模型一个（适当雄心勃勃的）超越情节的目标似乎比确保模型在情境感知时，其目标足够接近最大奖励要容易得多在某个维度“附近”不存在类似阴谋家的目标。特别是：SGD 的图像专门“搜索”最近的最大奖励目标，在一个可以搜索到大量类似阴谋家的目标的空间中，在我看来非常令人担忧（而且：更有可能导致超越情节的目标具有任何激励阴谋所必需的属性——例如，雄心勃勃的目标、具有无限时间范围的目标等等。&lt;/p&gt;&lt;p&gt;最后，我想再次强调一些持续存在的不确定性，即阴谋是否需要异常高的目标导向标准。在这里，我一直试图将关于目标导向性本身的争论与关于 SGD 选择哪种目标导向模型的争论区分开来。但即使是能力很强的人工智能也可能不需要联盟话语中经常假设的那种连贯的、战略目标导向的，我认为这可能与此类人工智能被很好地理解为“阴谋家”的可能性特别相关，因为形式阴谋中的目标导向似乎特别连贯、战略性和“&lt;a href="https://arbital.com/p/consequentialist/"&gt;结果主义&lt;/a&gt;”。&lt;/p&gt;&lt;p&gt;退后一步，尝试从整体上看待这些考虑因素，我感觉自己被拉向了两个不同的方向。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;一方面，至少以阴谋作为一种收敛良好的工具策略为条件，类似阴谋家的目标在目标空间中感觉非常普遍，而且我非常担心训练会因为某种原因而遇到它们。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;另一方面，在我看来，将模型在训练中的良好表现归咎于阴谋仍然是一种直觉，就像一个相当具体和连贯的故事。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;也就是说，在“目标空间”的层面上，阴谋感觉是强大和常见的，但在“是的，这就是这个现实世界模型正在发生的事情，它获得奖励是因为（或者：基本上是因为）”的层面上是具体的和相当脆弱的它希望稍后获得权力，而现在获得奖励有助于实现这一目标。”我认为这种连接感有几个不同的组成部分：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;部分原因在于该模型是否确实具有相关的长期且雄心勃勃的目标，尽管它是在训练中形成的。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;其中一部分是关于是否有一个足够好的故事来说明为什么在剧集中获得奖励是实现这些目标的一个很好的工具性策略（例如，对目标保护假设的怀疑、模型稍后授权的前景等）。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;部分原因是，类似阴谋家的诊断还带来了我没有关注的额外结合点——例如，该模型具有情境意识和连贯的目标导向。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-aWqikFyaeR5gyBuAf-4" id="fnref-aWqikFyaeR5gyBuAf-4"&gt;[4]&lt;/a&gt;&lt;/sup&gt; （当我真的试图让人们记住这个模型&lt;em&gt;知道正在发生什么&lt;/em&gt;并且正在以引发战略工具推理的方式连贯地追求&lt;em&gt;某个&lt;/em&gt;目标/一组目标时，那么它至少有可能部分阴谋家似乎更合理。）&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;我们能否对阴谋者的总体可能性给出一个粗略的、主观的数字？在我看来，尽管有缺点，像这样的数字通常可以使争论更加激烈，所以我在这里尝试一下。比方说：&lt;strong&gt;如果你使用自我监督预训练 + RLHF 的基线形式，在多样化、长期、现实世界的任务上训练一个连贯的目标导向、情境感知、略高于人类水平的模型，我的该模型在训练中表现良好的主观概率约为 25%，这在很大程度上是为自身和/或其他人工智能寻求力量的工具策略的一部分。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;不过，我想澄清的是，即使上述说法存在多种形式的模糊性，我也只是从我的直觉中得出一个数字。我还没有建立相关考虑因素的定量模型（尽管我有兴趣看到这方面的努力），报告的要点是分析本身，而不是试图将分析综合成定量结果。&lt;/p&gt;&lt;p&gt;我还认为可以通过相当简单的方式降低风险：例如，通过短期任务的培训，或通过态势感知之前的各种形式的对抗性培训。我认为该模型既策划&lt;em&gt;又&lt;/em&gt;参与主动邪恶行为（例如沙袋和早期破坏）的可能性较低，因为这些行为在认知和奖励方面付出了更大的成本。&lt;/p&gt;&lt;p&gt;我还认为，风险会随着更强大的模型&lt;em&gt;而增加&lt;/em&gt;，这些模型可能在未来能够更好地逃脱/接管，并与其他人工智能协调/合作，并且更有可能参与更多的工作。复杂的守门形式，例如“内省的守门方法”（尽管在我看来，即使对于相当先进的人工智能来说，这些直观上也相当具有挑战性）。&lt;/p&gt;&lt;p&gt;最后，我想再次强调，非阴谋者也可以从事类似阴谋者的行为。剧集奖励寻求者可以假装自己的阵营（甚至：参与逃跑/接管），以努力获得剧集奖励；非训练型游戏玩家最终仍可能会追求权力目标，从而激励各种形式的欺骗行为；最终最重要的人工智能可能在重要方面与我在这里关注的人工智能范式有所不同（例如，它们可能更像“&lt;a href="https://lilianweng.github.io/posts/2023-06-23-agent/"&gt;语言模型代理&lt;/a&gt;”而不是单一模型，或者它们可能是通过方法创建的与我所关注的基线机器学习方法有更大的不同），同时仍然从事权力驱动的对齐伪造。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-aWqikFyaeR5gyBuAf-5" id="fnref-aWqikFyaeR5gyBuAf-5"&gt;[5]&lt;/a&gt;&lt;/sup&gt;在我看来，阴谋是这种恐惧的一个典型例子，在我看来，这是一个特别迫切需要理解的例子。但这远不是唯一令人担忧的原因。 &lt;/p&gt;&lt;hr class="footnotes-sep" /&gt;&lt;section class="footnotes"&gt;&lt;ol class="footnotes-list"&gt;&lt;li class="footnote-item" id="fn-aWqikFyaeR5gyBuAf-1"&gt;&lt;p&gt;不过，再次强调：它需要是一种能够容忍价值观变化的“生存”概念。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-aWqikFyaeR5gyBuAf-1"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-aWqikFyaeR5gyBuAf-2"&gt;&lt;p&gt;有关这方面的更多信息，请参阅第 6.8 节。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-aWqikFyaeR5gyBuAf-2"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-aWqikFyaeR5gyBuAf-3"&gt;&lt;p&gt;感谢保罗·克里斯蒂亚诺在这里进行讨论。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-aWqikFyaeR5gyBuAf-3"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-aWqikFyaeR5gyBuAf-4"&gt;&lt;p&gt;跟踪在阴谋者假设的背景下可能建立的所有其他更微妙的结合也感觉有点困难。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-aWqikFyaeR5gyBuAf-4"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-aWqikFyaeR5gyBuAf-5"&gt;&lt;p&gt;尽管如上所述，如果相关语言模型代理经过端到端训练（而不是仅仅构建单独训练的组件），那么报告的框架也将适用于它们。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-aWqikFyaeR5gyBuAf-5"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/section&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/2uPH9rBkM7WAnQydp/summing-up-scheming-ais-section-5#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sat, 09 Dec 2023 15:48:49 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/2uPH9rBkM7WAnQydp/summing-up-scheming-ais-section-5</guid></item><item><title>攻防平衡很少发生变化</title><link>https://www.lesswrong.com/posts/Hx2Q9fWyimjrKrxyK/the-offense-defense-balance-rarely-changes</link><description>发布于 2023 年 12 月 9 日下午 3:21（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;您可能在 X 上看到过&lt;a href="https://x.com/RanaAgastya/status/1729559869605192078?s=20"&gt;&lt;u&gt;一些&lt;/u&gt;&lt;/a&gt;&lt;a href="https://x.com/NicerInPerson/status/1729906726814523630?s=20"&gt;&lt;u&gt;类似这样的&lt;/u&gt;&lt;/a&gt;对话：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; &lt;strong&gt;Michael Doomer ⏸️&lt;/strong&gt; ：先进的人工智能可以帮助任何人制造生物武器&lt;br /&gt;如果这项技术普及的话，只需要一个疯狂的人就能毁灭世界！&lt;/p&gt;&lt;p&gt; &lt;strong&gt;Edward Acc&lt;/strong&gt; ⏩：我可以让我的人工智能制造疫苗&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Yann LeCun：&lt;/strong&gt;&lt;a href="https://x.com/ylecun/status/1718764953534939162?s=20"&gt;&lt;u&gt;我的好人工智能会打倒你的流氓人工智能&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;这里的分歧取决于一项技术是否更能促进进攻（生物武器）而不是防御（疫苗）。对未来技术（尤其是人工智能）“攻防平衡”的预测是&lt;a href="https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html#defensedemocracy"&gt;&lt;u&gt;有关技术乐观主义和存在风险的争论的核心&lt;/u&gt;&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;这些预测大多依赖于对廉价生物技术、无人机和数字代理等技术将如何影响攻击或保护资源的难易程度的直觉。很难想象一个人工智能代理搜索软件漏洞和自动无人机攻击军事目标的世界，而不想象进攻防御平衡的巨大转变。&lt;/p&gt;&lt;p&gt;但几乎没有历史证据表明进攻防守平衡发生了巨大变化，即使是为了应对技术革命。&lt;/p&gt;&lt;p&gt;考虑网络安全。自 20 世纪 70 年代以来，摩尔定律使我们&lt;a href="https://ourworldindata.org/moores-law"&gt;&lt;u&gt;的计算成本降低了七个数量级&lt;/u&gt;&lt;/a&gt;。随着原始计算能力的提高，计算机技术的形式和经济用途发生了巨大变化：加密、互联网、电子商务、社交媒体和智能手机。&lt;/p&gt;&lt;p&gt;通常的攻防平衡故事预测，像这样的技术的重大变化应该会对攻防平衡产生重大影响。如果你告诉 20 世纪 70 年代的人们，到 2020 年，恐怖组织和孤独的精神病患者可以从他们的口袋里获得比 IBM 当时生产的更多的计算能力，他们会对网络安全的攻防平衡做出什么预测？&lt;/p&gt;&lt;p&gt;与他们可能的预测相反，网络安全的攻防平衡似乎很稳定。网络攻击尚未被消灭，但也没有占领世界。所有大国都拥有防御性和进攻性网络安全团队，但没有一个国家获得决定性优势。计算机有时仍然会感染病毒或勒索软件，但它们还没有发展到危及互联网 GDP 很大一部分的程度。从 1980 年到 2020 年，美国用于网络安全的军事预算&lt;a href="https://askwonder.com/research/historical-global-software-cybersecurity-spending-d7s6w5mfz#:~:text=,%E2%80%A0blog.rsisecurity.com%E3%80%91%20from%201980%20to%202017"&gt;&lt;u&gt;每年增加约 4%&lt;/u&gt;&lt;/a&gt; ，这一速度快于 GDP 增长，但与 GDP 增长加上互联网占 GDP 比例的增长保持一致。&lt;/p&gt;&lt;p&gt;之前几次技术革命带来的这种稳定性增加了举证责任，说明为什么网络安全的攻防平衡在下一次技术革命之后应该会发生根本性的变化。&lt;/p&gt;&lt;p&gt;攻防平衡的稳定性并不特定于网络安全。下图显示了从1400年到2013年战争中的人均死亡率。这张图包含了人类所有重大技术革命。每年都有很多差异，但长期趋势几乎为零。&lt;/p&gt;&lt;p&gt; &lt;a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd08dd888-6bc4-42e9-bf3d-0535beb59e0a_3088x2018.png"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Hx2Q9fWyimjrKrxyK/sdu9qhvr92r6hlj4iah4" /&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;有没有人有一个攻防平衡理论，可以解释为什么1640年人们用刀剑和马匹作战时的人均战争死亡人数应该与1940年人们用空袭和坦克作战时的人均战争死亡人数大致相同？&lt;/p&gt;&lt;p&gt;很难用技术的变化来解释该图中的变化。冲突中的人均死亡是嘈杂和周期性的，而技术的进步相对平稳和单调。&lt;/p&gt;&lt;p&gt;以前的技术还没有足以改变冲突的频率或成本，使该指标远远超出已设定的最大和最小范围 1400-1650。为什么我们应该期望人工智能有所不同，举证责任再次增加。&lt;/p&gt;&lt;p&gt;人类基因组测序的成本也下降了 6 个数量级，生物学领域也发生了数十项重大技术变革。然而，生物攻击的频率或损害尚未出现明显的反应。&lt;/p&gt;&lt;p&gt; &lt;a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69872e23-b086-4dc5-8729-fc219b47c735_3400x2400.png"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Hx2Q9fWyimjrKrxyK/ftc8mdnfofci5m6qsi6d" /&gt;&lt;/a&gt;&lt;/p&gt;&lt;h3&gt;稳定的可能原因&lt;/h3&gt;&lt;p&gt;为什么攻防平衡在其背后的技术正在迅速而彻底地变化的情况下却如此稳定？这篇文章的主要贡献只是用经验证据来支持这个问题的重要性，但这是一个不成熟的理论。&lt;/p&gt;&lt;p&gt;最主要的是，&lt;a href="https://www.tandfonline.com/doi/pdf/10.1080/01402390.2019.1631810"&gt;&lt;u&gt;攻防平衡理论&lt;/u&gt;&lt;/a&gt;中攻击者和防守者之间的明确区分在实践中并不存在。所有攻击者也是防御者，反之亦然。侵略者国家必须保卫自己的征服地，黑客也需要强大的信息安全。&lt;/p&gt;&lt;p&gt;因此，如果有某种技术可以使入侵比防御更容易，或者使信息安全比黑客攻击更容易，那么它可能不会对权力平衡产生太大影响，因为每个参与者都需要同时做到这两点。如果进攻和防守是互补而不是替代，那么它们之间的平衡就不那么重要了。&lt;/p&gt;&lt;p&gt;这个论点对人工智能的未来有何预测？它并不预测未来将与今天非常相似。尽管当今网络安全的攻防平衡与 20 世纪 70 年代非常相似，但自那时以来，技术和社会发生了巨大变化。人工智能显然是本世纪的决定性技术。&lt;/p&gt;&lt;p&gt;但它确实预测人工智能带来的巨大变化不会来自对攻防平衡的巨大破坏。这些变化将更像是工业革命，而不是小型恐怖组织被授权摧毁互联网或摧毁整个国家。&lt;/p&gt;&lt;p&gt;也许在所有这些情况下，阈值效应即将到来，或者人工智能与我们过去所有的技术革命完全不同，但这是一个需要大量证据来证明的说法。到目前为止，通过大规模的技术变革，攻防平衡似乎非常稳定，我们应该预期这种情况会持续下去。&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/Hx2Q9fWyimjrKrxyK/the-offense-defense-balance-rarely-changes#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sat, 09 Dec 2023 15:21:23 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/Hx2Q9fWyimjrKrxyK/the-offense-defense-balance-rarely-changes</guid></item><item><title>揭开灭绝的面纱</title><link>https://www.lesswrong.com/posts/HaGTQcxqjHPyR9Ju6/unpicking-extinction</link><description>发布于 2023 年 12 月 9 日上午 9:15（格林尼治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;h1&gt;长话短说&lt;/h1&gt;&lt;p&gt;人类灭绝正在成为趋势：有很多噪音，主要是在 X 上，关于 e/acc 对人类灭绝的明显&lt;a href="https://www.lesswrong.com/posts/3xoThNNYgZmTCpEAB/based-beff-jezos-and-the-accelerationists"&gt;自满&lt;/a&gt;。灭绝也与另一种观点（并非特定于 e/acc）相关，即“人类进化的下一步是{AI/AGI/ASI}”。许多人强烈反对前者，而后者似乎并不十分充实。我认为简单地收集各种立场并总结它们是有用的，希望不会太不准确，也许还能找出一些共同点。&lt;/p&gt;&lt;p&gt;这是我自己的研究（通过进化导致事实上的灭绝）的起点。这里没有什么特别新的内容：例如，请参阅&lt;a href="https://forum.effectivealtruism.org/topics/human-extinction"&gt;常用&lt;/a&gt;&lt;a href="https://www.lesswrong.com/tag/existential-risk"&gt;论坛&lt;/a&gt;中的大量文献。托马斯·莫伊尼汉 (Thomas Moynihan) 的&lt;a href="https://www.urbanomic.com/book/x-risk/"&gt;《X-risk》&lt;/a&gt; (2020) 记录了人类集体认识到文明脆弱性的历史，而埃米尔·P·托雷斯 (Émile P. Torres) 的作品（下文讨论）则为灭绝伦理提出了一个可能的框架。&lt;/p&gt;&lt;p&gt;我的底线是：a）人类灭绝的坏（或善）程度似乎不像人们想象的那么明显或不言而喻，b）如果我们灭绝以及当我们灭绝时我们留下什么很重要，c）灭绝的时间这种情况的发生很重要，d）人类最后几代人生活（和死亡）的方式也很重要。&lt;/p&gt;&lt;p&gt;与表面上的 e/acc 观点相关（即对可能的人类灭绝相当放松）：很明显，我们的默认立场（受到一些警告）应该是延迟灭绝，因为 a）它是不可逆转的（根据定义） ，b）以最大化我们未来的期权价值。无论如何，e/acc 观点似乎是基于某种（不是很清楚的）某种熵与不受约束的资本主义的品味交叉的东西，很难认真对待，甚至可能以它自己的方式失败。&lt;/p&gt;&lt;h1&gt;灭绝的种类&lt;/h1&gt;&lt;h2&gt;尤德科夫斯基的立场&lt;/h2&gt;&lt;p&gt;（我的看法）埃利泽的观点是，他担心一个错位的人工智能（不一定是超级智能），主要靠自己行动（例如目标形成、计划、实际影响世界上的事物），消灭人类，甚至消灭地球上的所有生命。这不仅对被淘汰的人类或其后代不利，而且对整个宇宙也不利，因为智能创造的复杂性（人类产生的类型）是一种内在的善，不需要进一步的论证。埃利以泽预见的绝大多数人工智能设计将通过各种事件链，导致宇宙中这些内在商品的数量大大减少。&lt;/p&gt;&lt;p&gt;他在当前的 e/acc 背景&lt;a href="https://x.com/ESYudkowsky/status/1732249519776301161?s=20"&gt;下&lt;/a&gt;详细阐述了这一点，并澄清说他的观点并不取决于生物人类的保护（了解这一点很有用）。他就这个主题写了大量的警句，例如&lt;a href="https://www.lesswrong.com/s/9bvAELWc8y2gYjRav/p/GNnHHmm8EzePmKzPk"&gt;《价值是脆弱的》&lt;/a&gt;和《&lt;a href="https://www.lesswrong.com/s/d3WgHDBAPYYScp5Em"&gt;有趣的理论&lt;/a&gt;序列》。&lt;/p&gt;&lt;h2&gt;博斯特罗姆变体&lt;/h2&gt;&lt;p&gt;尼克·博斯特罗姆对人类灭绝的看法似乎以“幸福的生活更好”为出发点。我可能错误的&lt;a href="https://nickbostrom.com/utopia"&gt;印象&lt;/a&gt;是，像埃利以泽一样，他似乎重视艺术、创造力、爱情等东西，在特定意义上，从宇宙或物种中立的角度来看，不存在这些东西的未来将是一个更糟糕的未来。他描述了一个“无人居住的社会”，该社会技术先进并建造了复杂的结构，但“仍然缺乏任何类型的有意识或其福利具有道德意义的存在”（ &lt;em&gt;《超级智能》&lt;/em&gt; （2014）第11章，第173页）。据我所知，他并没有解开无人居住的社会究竟会带来什么坏处以及对谁来说（这可能是很好理解的观点，或者是哲学上的一个非问题，但我不确定情况是否如此，在至少从（见下文）贝纳塔尔、托雷斯、&lt;a href="https://onlinelibrary.wiley.com/doi/pdf/10.1111/1468-0114.00150"&gt;詹姆斯·莱曼&lt;/a&gt;的这篇论文或叔本华的论文来看）。&lt;/p&gt;&lt;p&gt;博斯特罗姆认为我们应该避免很快灭绝的一个更具体的原因是为了保留未来的&lt;a href="https://existential-risk.com/concept"&gt;“选择价值”&lt;/a&gt; ——因为关于人类个人和群体层面的偏好以及物种层面的职业的许多问题仍然没有得到解答，也许最好推迟任何不可逆转的改变，直到我们集体变得更加明智。这直观上是有道理的，尽管即使在这里，也&lt;a href="https://forum.effectivealtruism.org/posts/NfkEqssr7qDazTquW/the-expected-value-of-extinction-risk-reduction-is-positive#1_3__Future_agents_could_later_decide_not_to_colonize_space__option_value_"&gt;不清楚&lt;/a&gt;期权价值实际上对 X 风险降低的整体价值的影响有多大。&lt;/p&gt;&lt;h2&gt; （也许）e/acc 视图&lt;/h2&gt;&lt;p&gt;最近似乎没有来自“e/acc”的评论员群（特别是@basedbefjezos）提出实质性论点，但这篇&lt;a href="https://beff.substack.com/p/notes-on-eacc-principles-and-tenets"&gt;2022 年的帖子&lt;/a&gt;似乎很有用。我对 e/acc 的看法，主要是根据上面的文件进行的：a）他们对人类或类人思想或创造物作为内在商品没有偏见（从某种意义上说，我描述了埃利泽或博斯特罗姆拥有），b）一个他们的内在商品似乎正在最大化宇宙中的智力量（他们对“智力”采取了广泛的、激进的非人类中心主义定义，似乎包括资本主义和其他群体层面的认知），c）他们对所显现结果的看法（智力，无论是人工智能还是社会资本主义）是相当宽容的，即智力导致的任何结果都是可以接受的，并且以下（ &lt;a href="https://beff.substack.com/p/notes-on-eacc-principles-and-tenets#:~:text=the%20technocapital%20singularity-,Effective%20accelerationism%20aims%20to%20follow%20the%20%E2%80%9Cwill%20of%20the%20universe%E2%80%9D%3A%20leaning%20into%20the%20thermodynamic%20bias%20towards%20futures%20with%20greater%20and%20smarter%20civilizations%20that%20are%20more%20effective%20at%20finding/extracting%20free%20energy%20from%20the%20universe%20and%20converting%20it%20to%20utility%20at%20grander%20and%20grander%20scales,-e/acc%20has"&gt;来源&lt;/a&gt;）&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; “‘宇宙意志’[意味着]倾向于热力学偏见，未来会有更伟大、更聪明的文明，更有效地从宇宙中寻找/提取自由能源，并将其转化为越来越大的规模的效用。”&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;继续上面的列表，d）他们不相信埃利泽或博斯特罗姆式的“无人居住的世界”，我认为这就是“僵尸”所指的意思：“无需担心创造更高等的“僵尸”形式智力，因为与意识/更高层次的智力形式相比，它们将处于热力学/进化劣势”。&lt;/p&gt;&lt;p&gt; e/acc 还参考了生命起源的热力学解释（参见&lt;a href="https://www.quantamagazine.org/a-new-thermodynamics-theory-of-the-origin-of-life-20140122/"&gt;杰里米·英格兰&lt;/a&gt;），他们似乎将其&lt;a href="https://beff.substack.com/p/notes-on-eacc-principles-and-tenets#:~:text=Intelligence%20emerges%20as,2"&gt;推断&lt;/a&gt;为在多尺度上运作的更高形式的认知。我没有足够的知识来批评这一点，只是说这感觉这个基金会对他们的主张有很大的影响力（但如果得到良好的支持可能会很有趣 - 但目前还没有）。&lt;/p&gt;&lt;p&gt;上面的评论不包括对 e/acc 思想的任何进一步细化（这是一次&lt;a href="https://x.com/dwarkesh_sp/status/1732141980459905489?s=20"&gt;现场&lt;/a&gt;而&lt;a href="https://www.lesswrong.com/posts/3xoThNNYgZmTCpEAB/based-beff-jezos-and-the-accelerationists"&gt;激烈的&lt;/a&gt;对话），但这里是 Eliezer 的建议（&lt;a href="https://x.com/ESYudkowsky/status/1732249519776301161?s=20"&gt;来源&lt;/a&gt;），他希望从 e/acc 人群中看到什么（就以下方面而言）：充实他们的想法）：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; “我对贝夫·杰索斯立场的模型：我不太关心你的这个预测，所以我不同意它。只要熵增加的速度比其他情况下快，我就很高兴。我已经暂时解锁了@BasedBeffJezos，以防他对此做出我认为的实质性回应，例如，“我实际上预测，作为关于宇宙的经验事实，根据几乎任何一套设计原则构建的人工智能都会关心将其他有情心视为自身的目的，并惊奇地看待宇宙，他们花时间和精力去有意识地体验；人类的后代将提升到与他们自己平等，所有那些且只有那些要求提升的人类；禁止在他们统治的所有地区进行智慧奴役或更大的恐怖；我认为这一立场是关于物理现实的公众所知的真理，而不仅仅是出于信仰而重复的话语；所有这些都是我立场的关键，我&amp;#39;如果我确信事实并非如此，我会退后一步，不会毁灭所有人类生命。”&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;就背景而言，这可能是一种历史好奇心，e/acc 大量借鉴了 20 世纪 90 年代中期在华威大学的控制论文化研究单位 (CCRU) 下培育的“加速主义”思想群。它异常丰富，因为最初的 CCRU 版本采用了加速主义（就其记录/编纂而言），分裂为左派、右派、极右派、无条件主义和许多其他变体，现在加入了一个模糊的万神殿。 /acc.&lt;a href="https://en.wikipedia.org/wiki/Accelerationism"&gt;维基百科&lt;/a&gt;页面是一个好的开始，这篇&lt;a href="http://www.shaviro.com/Blog/?p=1174"&gt;文章&lt;/a&gt;和这篇&lt;a href="https://www.palladiummag.com/2023/11/03/make-yourself-human-again/"&gt;关于&lt;/a&gt;尼克·兰德（Nick Land，一位创始人，后来由于极右观点而被回避）的文章也是如此。基础文本的摘录可以在&lt;a href="https://www.urbanomic.com/book/accelerate/"&gt;Accelerationist Reader&lt;/a&gt;中找到。与其说加速主义是一种连贯的哲学，不如说它是一种生成元模因，它曾经（而且显然仍然似乎）对&lt;a href="https://www.e-flux.com/journal/46/60070/accelerationist-aesthetics-necessary-inefficiency-in-times-of-real-subsumption/"&gt;艺术&lt;/a&gt;和流行文化特别有影响力。&lt;/p&gt;&lt;h2&gt;克里斯蒂安诺：“人类失去了对未来的控制”&lt;/h2&gt;&lt;p&gt;保罗·克里斯蒂安诺（Paul Christiano）的观点（关于灭绝的话题）似乎围绕着以下几个方面展开：a）“价值多元化”的宇宙，即人类价值观仅成为众多价值观中的一种，这是一个糟糕的价值观；b）人类“失去对世界的控制”的时间线。 “未来”是一个糟糕的时间表。这些都是在与当今世界相似的世界（即国家、公司、生物人类等）的背景下进行最具体的讨论，并反思我们的社会经济结构（即相对&lt;em&gt;自由放任的&lt;/em&gt;资本主义） &lt;a href="https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like#:~:text=We%20might%20describe,particularly%20strong%20one."&gt;相互作用&lt;/a&gt;所产生的风险、强大的技术、无能的监管、协调问题以及古德哈特定律的变体。&lt;/p&gt;&lt;p&gt;然而，在 2018 年发表的一篇有趣的文章中，克里斯蒂安诺确实采取了更具 &lt;a href="https://www.lesswrong.com/posts/3kN79EuT27trGexsq/when-is-unaligned-ai-morally-valuable"&gt;推测性的&lt;/a&gt;观点：他放松了对生物人类和我们的价值观/系统的偏见（即接受这样的观点：如果这是唯一的选择，那么我们的超级智能继任者继承未来可能是可以接受的。我们的价值观可能会持续存在），但似乎把诸如“价值”和“友善”等词的定义的难题抛在了一边。&lt;/p&gt;&lt;h2&gt;丹·亨德里克斯：进化压力不利于人类&lt;/h2&gt;&lt;p&gt;我想简单谈谈 Dan Hendrycks 的观点，特别是他对 e/acc 观点的逐条&lt;a href="https://x.com/DanHendrycks/status/1651740865159901184?s=20"&gt;反驳&lt;/a&gt;。他的反驳引用了这篇 2023 年的&lt;a href="https://arxiv.org/pdf/2303.16200.pdf"&gt;论文&lt;/a&gt;，该论文认为人类应该更愿意控制未来而不是灭绝。&lt;/p&gt;&lt;p&gt; Hendrycks 认为，由于（代理人工智能系统的整体）快速变异/扩散到竞争性部署环境中所表现出来的社会和技术压力，可能会出现类似于自然选择的力量。这种选择可能有利于自私行为（就人工智能系统而言），而没有历史上为人类和某些动物提供的利他主义、亲缘选择、合作、道德规范的限制。当人工智能与改变世界的更大效率相结合时，人工智能似乎很可能在竞争中超越人类。这感觉就像是对&lt;a href="https://www.lesswrong.com/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-looks-like-and-robust-agent-agnostic"&gt;安德鲁·克里奇（Andrew Critch）&lt;/a&gt;在这里提出并&lt;a href="https://www.lesswrong.com/posts/qYzqDtoQaZ3eDDyxa/distinguishing-ai-takeover-scenarios#Production_Web"&gt;在这里&lt;/a&gt;进行分析的观点的进化处理。&lt;/p&gt;&lt;p&gt;初步阅读时，我在 Hendrycks 中找不到任何关于人工智能系统&lt;em&gt;之间&lt;/em&gt;合作/竞争可能平衡的内容——即类似的进化压力是否会导致&lt;a href="https://joscha.substack.com/p/the-existential-risk-of-agi#:~:text=One%20of%20the,long%20as%20possible."&gt;自相残杀的&lt;/a&gt;冲突（其中人类可能是附带损害），或者它们确实解决了协调问题吗？问题比人类更好（由于源代码透明度或适合其架构和部署环境的决策理论）并且大多数情况下避免了彼此冲突。&lt;/p&gt;&lt;p&gt;如果人工智能系统有可能陷入自相残杀的冲突，那么似乎很难说它们一定会“更有效地从宇宙中寻找/提取自由能，并将其转化为越来越大的规模的效用”，正如 e/acc 所建议的。他们可能只是无限期地浪费资源。如果没有更强有力的论据，感觉（在这一点上）e/acc 可能会以其自身的方式失败。&lt;/p&gt;&lt;h2&gt;人类进化到其他一些基质&lt;/h2&gt;&lt;p&gt;亨德里克斯的进化框架显然对人类不利。然而，其他进化论的叙述可能更为积极。其中一种愿景是，人类作为一种在生物学上与我们相似的物种而消失，但可能会在某些其他（无机）有机基质上进行&lt;em&gt;进化&lt;/em&gt;，甚至完全融入“自然”环境。许多人都阐述了这一观点：&lt;a href="https://www.overcomingbias.com/p/ai-risk-convo-synthesis"&gt;罗宾·汉森（Robin&lt;/a&gt; &lt;a href="https://quillette.com/2023/08/06/ais-will-be-our-mind-children/"&gt;Hanson）的&lt;/a&gt;&lt;a href="https://www.overcomingbias.com/p/ai-fear-is-mostly-fear-of-future"&gt;作品&lt;/a&gt;、理查德·萨顿（ &lt;a href="https://www.youtube.com/watch?v=NgHFMolXs3U"&gt;Richard&lt;/a&gt; &lt;a href="http://www.incompleteideas.net/Talks/waic3.pdf"&gt;Sutton）&lt;/a&gt; 、詹姆斯·洛夫洛克（ &lt;a href="https://www.noemamag.com/his-mind-forever-voyaging/"&gt;James&lt;/a&gt; &lt;a href="https://nautil.us/gaia-will-soon-belong-to-the-cyborgs-237728/"&gt;Lovelock&lt;/a&gt; ）、乔沙·巴赫（ &lt;a href="https://www.organism.earth/library/document/lfp392-joscha-bach#:~:text=So%2C%20in%20my,way%20more%20interesting."&gt;Joscha&lt;/a&gt; &lt;a href="https://joscha.substack.com/p/the-existential-risk-of-agi"&gt;Bach&lt;/a&gt; ）、 &lt;a href="https://warwick.ac.uk/fac/arts/english/currentstudents/undergraduate/modules/fictionnownarrativemediaandtheoryinthe21stcentury/manifestly_haraway_----_a_cyborg_manifesto_science_technology_and_socialist-feminism_in_the_....pdf"&gt;唐娜·哈拉维（Donna Haraway）、德里克&lt;/a&gt;&lt;a href="https://publicseminar.org/2015/09/blog-post-for-cyborgs/"&gt;·希勒&lt;/a&gt;（ &lt;a href="https://onlinelibrary.wiley.com/doi/pdf/10.1111/bioe.12340"&gt;Derek Shiller&lt;/a&gt; ）和&lt;a href="https://aeon.co/essays/what-are-the-moral-implications-of-humanity-going-extinct#:~:text=happen.%20In%20his-,book,-Mind%20Children%20(1988)"&gt;汉斯·莫拉维克（Hans Moravec&lt;/a&gt; ）（尽管只是一段话）。&lt;/p&gt;&lt;p&gt;除了罗宾·汉森（Robin Hanson），也许还有乔沙·巴赫（Joscha Bach），其他作家并没有详细阐述人类进化和超越的思想，人们可能需要回到超人类和后人类文学（这些文学大多早于当前的浪潮）人工智能的成功）。&lt;/p&gt;&lt;p&gt;尝试充实这一点是我特别感兴趣的领域，所以如果您有想法，请与我们联系。&lt;/p&gt;&lt;h2&gt;反生育主义者和数字痛苦&lt;/h2&gt;&lt;p&gt;上述立场主要涉及因技术或其他降临到人类身上的灾难而产生的生存风险。然而，可以想象，一个物种可能会自愿灭绝，这是一组包括当代反生育主义（认为生育在道德上是错误的）的观点。反出生主义有多种风格：&lt;em&gt;哲学上的反出生主义者，&lt;/em&gt;如&lt;a href="https://iep.utm.edu/anti-natalism/#SH5b:~:text=For%20example%2C%20David,cease%20from%20procreating."&gt;大卫&lt;/a&gt;&lt;a href="https://www.newyorker.com/culture/persons-of-interest/the-case-for-not-being-born"&gt;·贝纳塔尔&lt;/a&gt;，他们基于快乐和痛苦（就被创造的个体而言）之间的不对称性来反对生育；以及&lt;em&gt;厌恶人类的反生育主义者&lt;/em&gt;，他们以人类造成的伤害（例如对自然世界的其他部分）为由反对生育，&lt;a href="https://academic.oup.com/book/26703/chapter-abstract/195504756?redirectedFrom=fulltext"&gt;贝纳塔&lt;/a&gt;在此进行了分析。&lt;/p&gt;&lt;p&gt;我认为这些都是有趣的观点，因为它们质疑价值与人口之间的关系：一个人口较多的世界（受到痛苦、自由意志、正义等方面的重大限制）真的比人口较少的世界更好吗？&lt;/p&gt;&lt;p&gt;特别与人工智能相关的是&lt;a href="https://reducing-suffering.org/strategic-considerations-moral-antinatalists/#Humanitys_future"&gt;Brian&lt;/a&gt; &lt;a href="https://longtermrisk.org/artificial-intelligence-and-its-implications-for-future-suffering"&gt;Tomasik&lt;/a&gt; ，他明确担心数字痛苦的可能性， &lt;a href="https://forum.effectivealtruism.org/posts/JCBPexSaGCfLtq3DP/the-problem-of-artificial-suffering#The_concept_of_artificial_suffering"&gt;Thomas Metzinger&lt;/a&gt;也讨论过这个话题。梅辛格特别反对创造能够承受痛苦的人工智能。&lt;a href="https://nickbostrom.com/propositions.pdf"&gt;尼克·博斯特罗姆（Nick Bostrom）和卡尔·舒尔曼（Carl Shulman）&lt;/a&gt;在人类和人工智能混合社会的治理和其他问题的背景下提出了相关观点（在这种社会中，我们的历史道德直觉和社会契约在享乐范围更广、人口快速增长的存在下崩溃了）和廉价的复制/繁殖，&lt;em&gt;相对于&lt;/em&gt;人类）。&lt;/p&gt;&lt;h2&gt;埃米尔·P·托雷斯谈人类灭绝的伦理学&lt;/h2&gt;&lt;p&gt;在这&lt;a href="https://aeon.co/essays/what-are-the-moral-implications-of-humanity-going-extinct"&gt;两篇&lt;/a&gt;&lt;a href="https://xriskology.medium.com/human-extinction-a-brief-guided-tour-of-the-book-5cfb6a5a726"&gt;文章&lt;/a&gt;中（后者总结了他们的&lt;a href="https://www.xriskology.com/"&gt;新书&lt;/a&gt;），托雷斯广泛分析了人类灭绝。除了对存在伦理史的社会学研究之外，Aeon 论文的相关部分是灭绝的&lt;em&gt;过程&lt;/em&gt;和灭绝的&lt;em&gt;事实&lt;/em&gt;之间的区别（这在人工智能 X 风险的讨论中并不常见） 。托雷斯还直接区分了一个没有任何人类（也没有其他人类创造的智能）的世界与一个我们被取代（或进化）为基于机器的物种的世界。他们再次强调了人工智能 X 风险对话中的一个微小差距，该对话通常对时间范围（可能发生灭绝）保持沉默，部分原因可能是因为假设的背景通常是未来几年或几十年内出现的风险之一。托雷斯呼应了反自然主义的立场，选择了功利主义味道的长期主义的基础，我（松散地）总结为“更多的人总比更少的人好”。他们提到了一个显而易见的观点，即如果人们认为人类的生活主要充满痛苦，那么拥有更多人类的世界似乎并没有明显更好（除非存在其他一些主要的价值来源）。&lt;/p&gt;&lt;h1&gt;要点&lt;/h1&gt;&lt;p&gt;那么，对于人类灭绝的坏处或好处，我们又有何看法呢？我认为三个主要因素可能会影响一个人对灭绝的看法。&lt;/p&gt;&lt;h2&gt;我们是否以及如何成功很重要&lt;/h2&gt;&lt;p&gt;首先，a）灭亡的人类没有留下智慧的继承者，没有实质性的物质或智力文物，没有创造性的作品，b）我们能够留下遗产的世界（在托雷斯的作品中）之间似乎存在着巨大的区别。配方，可能是我们自己的生物或无机重新设计版本，继任者）。这一遗产的确切形式非常不清楚，这支持了博斯特罗姆关于在未来保留期权价值的呼吁以及奥德的&lt;a href="https://forum.effectivealtruism.org/topics/long-reflection"&gt;长期反思&lt;/a&gt;，尽管避免某些存在风险本身可能需要大规模变革这一事实使这两件事变得复杂。技术变革（例如，最终不得不在 AGI 上“掷骰子”）。&lt;/p&gt;&lt;h2&gt;时机似乎很重要&lt;/h2&gt;&lt;p&gt;除了反出生主义者之外，我想相对较少的人会咬紧牙关自愿灭绝，特别是如果这意味着他们或他们活着的（（（...）曾孙）孩子将会灭亡）。这种观点优先考虑自己和近亲的（明显）利益。其他人可能对人类的物理结构和智力成就非常有感情，并可能希望看到文明持续数百或数千年（请参阅人类关于灭绝的思想&lt;a href="https://www.urbanomic.com/book/x-risk/"&gt;历史&lt;/a&gt;）。&lt;/p&gt;&lt;p&gt;然而，我们不应该坚持或期望生物人类会无限期地存在于我们所认识的社会中，直到未来，随着医疗和其他技术的进步，延长活跃或上传的生命也不可能是可行的。正如&lt;a href="https://onlinelibrary.wiley.com/doi/pdf/10.1111/1468-0114.00150"&gt;莱曼&lt;/a&gt;指出的那样，我们对生存的直觉建立在与人类一生大致相当的叙事弧上，如果没有更坚实、更客观的基础，我们应该对扩展这些直觉持怀疑态度。&lt;/p&gt;&lt;p&gt;更根本的是，从博斯特罗姆的期权价值论证的角度来看，人们可能更喜欢人类灭绝的遥远日期。一种更直白的说法是，根据定义，灭绝实际上会导致所有其他未来的终结。然而，当考虑进化作为灭绝或其他&lt;a href="https://nickbostrom.com/papers/future"&gt;场景&lt;/a&gt;时，事情会变得更加复杂。&lt;/p&gt;&lt;h2&gt;灭绝的方式很重要&lt;/h2&gt;&lt;p&gt;似乎很明显，一场灭绝事件带来的巨大痛苦（如果没有该事件）将比缓慢的“自然”灭绝过程（例如通过人口减少）更糟糕。同样，虽然我不关注它，但作为附带损害而摧毁了地球上许多其他生命的灭绝事件将比主要影响人类的事件更糟糕。也有可能，一个事件摧毁了我们迄今为止所建造的一切（包括积累的知识），以至于它可能永远不会被未来的星际外星侦察兵恢复或发现，这将是令人悲伤的（如果不是具体或可量化的糟糕）。&lt;/p&gt;&lt;h2&gt;这实际上是一个紧迫的问题吗？&lt;/h2&gt;&lt;p&gt;考虑到人工智能失调带来的更显着的短期和中期风险，这似乎是毫无意义的空想。它也可能毫无帮助：正如&lt;a href="https://www.lesswrong.com/posts/3kN79EuT27trGexsq/when-is-unaligned-ai-morally-valuable#On_risks_of_sympathy"&gt;克里斯蒂安诺&lt;/a&gt;指出的那样，一些出于充分动机的担忧（不假思索或过早地提出），例如数字痛苦，可能会破坏我们关于人工智能安全和监督的社会推理。&lt;/p&gt;&lt;p&gt;然而，想象一下存在一个 GPT- &lt;em&gt;n&lt;/em&gt; ，我们怀疑它可能会经历现象学状态，具有构建长期计划的目标和能力，并且（比方说）通过了我们当时拥有的任何对齐基准。然而，我们，它的设计者，根据预防原则（无论出于何种原因）决定关闭它或不部署它。与斯坦尼斯瓦夫·莱姆的&lt;a href="https://academic.oup.com/liverpool-scholarship-online/book/43309/chapter-abstract/363077641?redirectedFrom=fulltext"&gt;&lt;em&gt;魔像 XIV&lt;/em&gt;&lt;/a&gt;相呼应，我们可能会被要求（无论是机器、它的前身，还是&lt;a href="https://nickbostrom.com/papers/digital-minds.pdf"&gt;历史&lt;/a&gt;的判断）来解释我们的推理，这很可能触及上面提出的一些问题，甚至如果我们不能给出任何明确的答案，我们可能需要表明我们确实考虑过这个问题，而不是羞怯地采取（碳沙文主义或生物沙文主义）立场。&lt;/p&gt;&lt;br /&gt;&lt;br /&gt;&lt;a href="https://www.lesswrong.com/posts/HaGTQcxqjHPyR9Ju6/unpicking-extinction#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sat, 09 Dec 2023 09:15:41 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/HaGTQcxqjHPyR9Ju6/unpicking-extinction</guid></item><item><title>寻找法学硕士特征之间的稀疏线性连接</title><link>https://www.lesswrong.com/posts/7fxusXdkMNmAhkAfc/finding-sparse-linear-connections-between-features-in-llms</link><description>发布于 2023 年 12 月 9 日凌晨 2:27（格林尼治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt; TL;DR：我们使用 SGD 来查找特征之间的稀疏连接；此外，尽管 MLP 存在非线性，但残差流和 MLP 之间的大部分特征都可以建模为线性计算。有关示例，请参阅&lt;a href="https://www.lesswrong.com/editPost?postId=7fxusXdkMNmAhkAfc&amp;amp;key=e013388fad8122af1003041ca4ae7f#Sparse_Linear_Feature_Connections"&gt;线性特征部分&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;&lt;i&gt;特别感谢 AISST 成员 Adam Kaufman，他最初想到了学习特征之间稀疏连接的想法，并感谢 Jannik Brinkmann 训练这些 SAE。&lt;/i&gt;&lt;/p&gt;&lt;p&gt;稀疏自动编码器 (SAE) 能够将 LLM 的激活转换为可解释的特征。为了定义电路，我们希望找到这些功能如何相互连接。 SAE 允许我们使用 SGD 大规模地查找可解释的特征，那么为什么不使用 SGD 来查找连接呢？ &lt;/p&gt;&lt;figure class="image"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/bbadeeylguad5kjbpdfj" /&gt;&lt;figcaption&gt; Pythia-70M 中的一层，在 MLP 之前和之后都有 SAE。请注意，这与典型的 Transformer 架构不同，Pythia 模型具有并行 MLP 和 Attn 模块，而不是顺序模块（这是为了复制并行化的 SOTA 模型，以提高 GPU 利用率）&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;我们在 MLP 之前有一组特征 F1，在 MLP 之后有一组特征 F2。这些特征是通过在这些层的激活上训练 SAE 来学习的。&lt;/p&gt;&lt;p&gt;理想情况下，我们学习一个线性函数，使得 F2 = W(F1)，并且 W 是稀疏的（即 W 权重上的 L1 惩罚）。那么我们可以查看 F2 中的一个特征，然后说“哦，这只是 F1 特征的稀疏线性组合，例如 0.8*（但是特征）+ 0.6*（但是特征）”，这将是非常容易解释的！&lt;/p&gt;&lt;p&gt;然而，我们正在尝试复制 MLP 的计算，这肯定不可能都是线性的！ &lt;span class="footnote-reference" id="fnrefpwepir8kf3"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnpwepir8kf3"&gt;[1]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;那么，从 F1 到 F2 获得最低损失的最简单计算是什么（暂时忽略 L1 权重稀疏性惩罚）？&lt;/p&gt;&lt;p&gt;仅在 F1 和 F2 之间的 MSE 上进行训练，我们在 Pythia-70m-deduped 的 4 个设置中绘制了跨 5 个层的训练过程中的 MSE：&lt;/p&gt;&lt;p&gt;线性： &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;y&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;W&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;非线性&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;：&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;y&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;Re&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;l&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;u&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;W&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt; MLP： &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;y&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space3"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;W&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;R&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;e&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;L&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;U&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;W&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;两个&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;非线性&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;：&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;R&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;e&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;LU&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;(&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;W&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;2&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;R&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;e&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;LU&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;(&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;W&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/jljlqsndeir2byp2hsqr" /&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;对于所有层，沿着（MLP 和两个非线性）和（线性和非线性）训练损失簇。由于 MLP 和线性是这两个簇中最简单的，因此其余分析将仅关注这两个簇。&lt;/p&gt;&lt;p&gt; [我还研究了偏差与无偏差：添加偏差并没有积极改善损失，因此被排除在外]&lt;/p&gt;&lt;p&gt;有趣的是，最后一层（和第 2 层）的相对线性 MLP 差异巨大。一般来说，最后一层的损失也大得多，尽管第 5 层 MLP 激活的 L2 范数为 52，而第 4 层为 13。这是 4 倍的增加，这将是 MSE 损失增加 16 倍。最后一个数据点的损失为 0.059 和 0.0038，相差约 16 倍。&lt;/p&gt;&lt;h2&gt;线性特征的百分比是多少？&lt;/h2&gt;&lt;p&gt;显然 MLP 更好，但这是平均水平。如果一定比例的特征可以建模为线性计算怎么办？因此，我们采用特征损失的差异（即对于特征，我们采用线性损失 - MLP 损失），通过各自的 L2 范数/层对所有损失进行归一化，然后绘制它们。 &lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/iz1bfh3ene0rh8o87cyo" /&gt;&lt;br /&gt;呃……这里有一些巨大的异常值，这意味着这些特定特征是非常非线性的。只需为所有层设置阈值 0.001： &lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/j830nyah5akirkajdfoo" /&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;&lt;figure class="table"&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="border: 1pt solid #000000; padding: 5pt; vertical-align: top;"&gt;层&lt;/td&gt;&lt;td style="border: 1pt solid #000000; padding: 5pt; vertical-align: top;"&gt;特征百分比 &amp;lt; 0.001 损失差异（即可以线性表示）&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="border: 1pt solid #000000; padding: 5pt; vertical-align: top;"&gt; 1&lt;/td&gt;&lt;td style="border: 1pt solid #000000; padding: 5pt; vertical-align: top;"&gt; 78%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="border: 1pt solid #000000; padding: 5pt; vertical-align: top;"&gt; 2&lt;/td&gt;&lt;td style="border: 1pt solid #000000; padding: 5pt; vertical-align: top;"&gt; 96%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="border: 1pt solid #000000; padding: 5pt; vertical-align: top;"&gt; 3&lt;/td&gt;&lt;td style="border: 1pt solid #000000; padding: 5pt; vertical-align: top;"&gt; 97%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="border: 1pt solid #000000; padding: 5pt; vertical-align: top;"&gt; 4&lt;/td&gt;&lt;td style="border: 1pt solid #000000; padding: 5pt; vertical-align: top;"&gt; 98%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="border: 1pt solid #000000; padding: 5pt; vertical-align: top;"&gt; 5&lt;/td&gt;&lt;td style="border: 1pt solid #000000; padding: 5pt; vertical-align: top;"&gt; 99.1%&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;大多数特征都可以通过损失的微小差异进行线性建模（有些特征具有负损失差异，这意味着线性的损失比 MLP 更低）。这些值是如此之小，以至于我将其归因于噪声）。非常方便！&lt;/p&gt;&lt;p&gt; [注：0.001 是任意的。为了使这一点更有原则性，我们可以绘制向 LLM 激活层添加不同级别的噪声的效果，然后选择一个交叉熵损失下降可以忽略不计的阈值？&lt;/p&gt;&lt;h1&gt;添加稀疏性&lt;/h1&gt;&lt;p&gt;现在，让我们训练稀疏 MLP 和稀疏线性连接。此外，我们可以将线性特征限制为只有经过良好建模的线性特征（与 MLP 相同）。我们将使用以下损失：&lt;/p&gt;&lt;p&gt;损失 = MSE(F2 - F2_hat) + l1_alpha*L1(权重)&lt;/p&gt;&lt;p&gt;但是我们如何选择l1_alpha呢？让我们绘制一系列 l1_alphas 的 MSE 损失与 l1 损失的帕累托前沿： &lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/i3wvny6cdcvtr8fwcjup" /&gt;&lt;/p&gt;&lt;p&gt;这是针对 l1_alphas = [1e-7, 1e-5, 1e-3, .1, 10, 100] 的情况，两条线的弯头都是 l1_alpha=1e-3。它的 MSE 比我想要的稍高，因此我将把它设置为 8e-4 以供将来运行。 （较低的 l1 惩罚会导致较高的 l1 损失和较低的 MSE）。&lt;/p&gt;&lt;h1&gt;稀疏线性特征连接&lt;/h1&gt;&lt;p&gt;将自己限制在线性特征上，我们重新训练了稀疏线性权重连接 w/ l1_alpha=8e-4。&lt;/p&gt;&lt;p&gt;下面我们展示了一些稀疏线性特征连接的示例。对于好奇的读者，可以&lt;a href="https://comet-scorpio-0b3.notion.site/More-Examples-ceaefc95cc924afba318dca1da37d4a4?pvs=4"&gt;在此处&lt;/a&gt;找到更多示例。&lt;/p&gt;&lt;h2&gt;或示例&lt;/h2&gt;&lt;p&gt;在第 1 层，我们有：&lt;/p&gt;&lt;p&gt; &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;Ø&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;F&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;30&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0.26&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;ØF&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;2797&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0.23&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;ØF&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;259&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;+&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0.10&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;ØF&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;946&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;其中 OF 是输出特征（在 MLP_out 中），IF 是输入特征（在 MLP 之前的 Residual Stream 中）&lt;/p&gt;&lt;p&gt;下面是输入特征 2797，在令牌“former”上强烈激活&lt;br /&gt;&lt;/p&gt;&lt;figure class="image"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/udfist7r6sqlz0zugxux" /&gt;&lt;figcaption&gt;这是5个例子。对于每个 ex，顶行单词是特征激活，例如标记“former”被激活 9.4。底部空白行是：如果我们删除此功能，模型在预测这些标记方面会变得多么糟糕？例如，当模型无法使用这个“以前的”功能时，苏联的性能会差 5.5 logits。&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;下面是输入特征 259，在令牌“old”上强烈激活&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/wbxbuj30brl7s47fuzxd" /&gt;&lt;/p&gt;&lt;p&gt;下面是输入特征 946，在令牌“young”上激活&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/ifrgaktkwasjqfrscpvm" /&gt;&lt;/p&gt;&lt;p&gt;在输出特征中，我们看到标记“前”、“老”和“年轻”都被激活，其中“年轻”的激活强度约为“前”和“旧”的一半，正如我们从权重系数中预期的那样。&lt;/p&gt;&lt;p&gt; &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;OF&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;30&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0.26&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;F&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;前&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;0.23&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;折叠&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;0.10&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;F&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;年轻&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mn MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/uapwsrethupsdihmldok" /&gt;&lt;/p&gt;&lt;p&gt;我们可以将此计算视为加权逻辑或。输出特征 30 在以前或年老或年轻时激活（同样，更多示例在&lt;a href="https://comet-scorpio-0b3.notion.site/More-Examples-ceaefc95cc924afba318dca1da37d4a4"&gt;这里&lt;/a&gt;）&lt;/p&gt;&lt;h2&gt;负权重示例&lt;/h2&gt;&lt;p&gt;在第 1 层，我们有：&lt;/p&gt;&lt;p&gt; &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;ØF&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;505&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0.68&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;ØF&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;3021&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;-&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0.21&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;ØF&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;729&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;其中 OF 是输出特征，IF 是输入特征。&lt;/p&gt;&lt;p&gt;下面是输入特征 3021，对像“said”这样的标记强烈激活，这些标记在几乎所有情况下都不会出现在引用之后。 &lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/yjuvhqqenxtmvscwcuoc" /&gt;&lt;/p&gt;&lt;p&gt;下面是输入功能 729，当“said”等标记在引用后不久出现时，会强烈激活它们。 &lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/ce3vxm3mehokiltt7tob" /&gt;&lt;/p&gt;&lt;p&gt;下面显示了当我们删除某个上下文标记时输入特征 729 的激活如何变化。重要的是，当引用被删除时，激活就会执行，这表明该功能在有引用、后跟“said”时激活。 &lt;/p&gt;&lt;figure class="image"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/czupawerkjnwrl8ishof" /&gt;&lt;figcaption&gt;这种形象通常会让人们感到困惑。高级别要点：任何红色的东西都是激活此功能的重要上下文标记（蓝色并不那么重要，因为删除后它只会增加到 0.5，而不是 -5.1）。我们试图传达的是，删除引号会使“says”一词的功能激活从 -5.1 下降到 0 功能激活。&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;下面我们看到输出功能在像“said”这样没有先前引用标记的标记上激活。我们已经“减去”了较大的负权重，可以说，“said”出现在引号之后的示例，现在该功能仅在“said”出现而没有任何先前引号时才激活。&lt;/p&gt;&lt;p&gt; &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;O&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;F&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;505&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0.68&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;F&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;（&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;在许多情况下为“说”&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;）&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;−&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;0.21&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;F&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;（&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;引号后为“说”&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;）&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/ur5krelkxzpsbltf56ky" /&gt;&lt;/p&gt;&lt;p&gt;我们可以将此计算视为加权逻辑与。输出特征505在A和~B上激活。在 A 是 B 的超集的情况下，这是 B 的补集，例如我有所有水果和所有黄色水果的集合，所以现在我可以找到所有非黄色水果。&lt;/p&gt;&lt;p&gt; （再次强调，&lt;a href="https://comet-scorpio-0b3.notion.site/More-Examples-ceaefc95cc924afba318dca1da37d4a4"&gt;这里&lt;/a&gt;有更多示例）&lt;/p&gt;&lt;h1&gt;稀疏 MLP 特征连接&lt;/h1&gt;&lt;p&gt;让我们可视化这些损失最严重的 MLP 特征：&lt;/p&gt;&lt;p&gt;第 5 层：查看线性和 MLP 之间最大损失差异的特征&lt;/p&gt;&lt;p&gt;（具体为[1.5555、0.0116、0.0052、0.0040、0.0038]）&lt;/p&gt;&lt;p&gt;所有 5 个功能的激活率都非常高。第一个通常很奇怪（与典型的异常值维度相比），接下来的 4 个大多是奇怪的标记。 &lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/karjun1tnmpp6qjwzmbd" /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/br95vzp1babvnxfqm4ue" /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/lgyvhdzu4nwtaqahbysl" /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/ujggypvajaaoxsiqxzav" /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/htaoeq6cnifh4u8xphvm" /&gt;&lt;/p&gt;&lt;p&gt; （作为一般说明：LLM 的最后一层通常非常奇怪！这也出现在调谐透镜纸上，并且被怀旧者假设为扩展的非嵌入矩阵）&lt;/p&gt;&lt;p&gt;第 4 层：损失差异 [0.0529, 0.0134, 0.0106, 0.0106, 0.0099]&lt;/p&gt;&lt;p&gt;第一和第三是异常特征。异常值特征的典型特征（根据我的经验）是：&lt;/p&gt;&lt;p&gt; 1）非常高的激活（这解释了高L2损失）&lt;/p&gt;&lt;p&gt; 2) 在前几个令牌上激活&lt;/p&gt;&lt;p&gt;3）在第一个分隔符处激活（例如句点或换行符，我将其表示为“\n”）&lt;/p&gt;&lt;p&gt; （为什么存在这些？我不知道，文献和理论存在，但超出了本文的范围） &lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/v6f5ltmngyyshckpttot" /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/gyovzsu4hjndqpdlnqhs" /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/qgu4diytowbmgz5uteua" /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/isrltm8kiu9lno1fz7zi" /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/cfzjxetenumareiyf6zh" /&gt;&lt;/p&gt;&lt;p&gt;第 3 层：损失差异 [0.0456, 0.0163, 0.0122, 0.0101, 0.0069]&lt;/p&gt;&lt;p&gt;第一个和第五个是异常特征&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/dnl6t83boy0ltkcnltqh" /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/fnhuxjirjsl2nutx5fjv" /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/xzorozx3elxmgisx3hxg" /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/kzkjcaw1cvvvpjtaulrn" /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/lcgylzejjs6nlh6vatz3" /&gt;&lt;/p&gt;&lt;p&gt;第 2 层：损失差异 [0.3370, 0.3324, 0.2910, 0.1682, 0.1069]&lt;/p&gt;&lt;p&gt;四个异常值特征&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/ynbd093akdzpahlenf6m" /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/gcsbvca4uc4yjsw71jhc" /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/u5lvffwf1myxytieya2w" /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/gugve0pcvecth8meebpk" /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/laydxbeqld7u2b5co2z6" /&gt;&lt;/p&gt;&lt;p&gt;第 1 层：损失差异 [0.1401, 0.0860, 0.0159, 0.0150, 0.0125]&lt;/p&gt;&lt;p&gt;前两个特征是离群特征&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/kc0eunsc2areajogzxvm" /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/lr9rgsgdpjwgbf0sny1b" /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/ppdtkg0uwevck8glq8pi" /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/h23mpxmwyx34clcc3iqm" /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/dfvh6a0gr2z0fw4ee0eh" /&gt;&lt;/p&gt;&lt;p&gt;这些特征的具体权重如何？&lt;/p&gt;&lt;p&gt;因此，MLP 有两组线性权重：W2(relu(W1(x)))。观察 W2，我注意到损失最大的特征有很多大的正权重和负权重。这是前 5 个损失特征（与上面可视化的相同）。对于正权重： &lt;br /&gt;&lt;br /&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/yw0uo3yzekejzjpsbdp2" /&gt;&lt;br /&gt;因此，第 4 层中最高的 loss-diff 特征有 112 个连接权重，且大于 0.1，而中值特征只有 9 个。&lt;/p&gt;&lt;p&gt;对于负权重： &lt;/p&gt;&lt;figure class="image"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/svq8lu8bxrmoig6bymyw" /&gt;&lt;/figure&gt;&lt;p&gt;请记住，这些是 W2 的权重，它连接 LLM 的 MLP 输出特征和 MLP 连接器的隐藏层。我们真的不知道这些意味着什么。&lt;/p&gt;&lt;p&gt;但我们绝对可以像可视化特征激活一样可视化它们，也许它们是可以解释的，所以……它们都是相当垃圾的。&lt;/p&gt;&lt;p&gt;异常值相关：8/30&lt;br /&gt;多义性：8/30&lt;br /&gt;单义：1/30&lt;br /&gt; （基本上）死亡：11/30&lt;/p&gt;&lt;p&gt; （这些是针对第 3 层的，但令人惊讶的是，第 1 层隐藏特征在默认情况下具有 80% 的单义性，并且还带有异常值特征）。&lt;/p&gt;&lt;p&gt;哇，如果我们有一种方法可以使隐藏层激活更容易解释就好了！因此，我们可以像稀疏 AE 一样训练稀疏 MLP 连接器：对潜在激活进行 l1 惩罚（基本上是连接两个 SAE 的 SAE）。&lt;/p&gt;&lt;h2&gt;严重不良事件上的严重不良事件&lt;/h2&gt;&lt;p&gt;我对 l1 权重和隐藏 l1 使用相同的 l1_alpha 项，并查看第 1 层的各种损失。总体而言：&lt;/p&gt;&lt;p&gt;损失 = MSE + L1_alpha*(L1(权重) + L1(隐藏激活)) &lt;/p&gt;&lt;figure class="image image_resized" style="width: 80.52%;"&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/lfd8xr3cw6i3rpzsbygh" /&gt;&lt;figcaption&gt;各种 l1_alpha 的损失。 “隐藏”是隐藏激活的 L1 损失。&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;所以我选择 4e-4 的 l1_alpha 作为 MSE 和 l1 损失之间的折衷方案。这对应于 25 个隐藏潜在激活的 L0（即其他 3k 为 0）。&lt;/p&gt;&lt;p&gt;查看前 30 个最大激活特征，第 3、4 和 5 层都是 MLP 的异常维度（第一个标记和第一个分隔符在一起）。 SAE 仅具有 10% 的异常特征。这是有道理的，因为这些离群维度都针对相同的标记（即第一个标记和第一个分隔符）激活，因此将具有较高的潜在 l1 激活。这将激励更多地结合这些维度。&lt;/p&gt;&lt;p&gt; SAE 的特征并不比 MLP 更具有单一语义。这可能是因为我需要为潜在激活添加偏差。此外，我对如何将稀疏权重与稀疏潜在激活相结合感到困惑（我在“请帮助”中指定了更多信息）。我将把剩下的实证工作留给未来，并继续进行推测。&lt;/p&gt;&lt;p&gt;解释这些特征可能类似于 Latent_features -&amp;gt; F2 中的线性 AND &amp;amp; OR 语句。从 F1-&amp;gt; 潜在特征是一个 ReLU，带有偏差的情况是：&lt;/p&gt;&lt;p&gt; &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;Latent&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_F&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;R&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;eLU&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;w&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;f&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;"&gt;+&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;w&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;f&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;−&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;bias&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;具体来说&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;Latent&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;F&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;3&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;R&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;e&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;L&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;U&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2.1&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;*&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;f&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;+&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1.5&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;*&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;f&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;2&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;−&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;4&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;如果它们单独激活超过 4，则可以是 F1 或 F2；如果它们必须一起激活才能大于 4，则可以是 F1 和 F2。&lt;/p&gt;&lt;p&gt;这使得进行特征激活统计和聚类变得很重要。最好绘制它们的共同激活（并根据是否激活潜在特征进行颜色）&lt;/p&gt;&lt;p&gt;但如果我想做 3 个以上的功能，就很难绘制它们的共同激活图。这里肯定有某种统计方法来收集共同激活的集群吗？&lt;/p&gt;&lt;h1&gt;请帮忙&lt;/h1&gt;&lt;p&gt;这主要是一组“快速进行并获得结果”的实验，这意味着做出了许多任意选择，我希望得到一些反馈。我确实打算自己研究这些问题（现在已经晚了，我希望这篇文章在本周末发布）。&lt;/p&gt;&lt;ol&gt;&lt;li&gt;训练时如何考虑各层的不同规范？例如，第 1 层的（残差流，mlp_out）的范数为 (7,4)。第 5 层具有 (16, 54) 之一。 &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mfrac"&gt;&lt;span class="mjx-box MJXc-stacked" style="width: 7.742em; padding: 0px 0.12em;"&gt;&lt;span class="mjx-numerator"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;Atm&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;，&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;我&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;将&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;MSE&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;除以&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;(&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;MLP_outlayer&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;的&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;范&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;数&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sup" style="font-size: 83.3%; vertical-align: 0.435em; padding-left: 0px; padding-right: 0.06em;"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;分钟&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;范&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;数&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-denominator"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-line"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-vsize"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; ，并保持权重激活相同。&lt;ol&gt;&lt;li&gt;通常如何处理这种情况？&lt;/li&gt;&lt;li&gt;我将如何处理添加潜在激活 l1 惩罚？ &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mfrac"&gt;&lt;span class="mjx-box MJXc-stacked" style="width: 6.868em; padding: 0px 0.12em;"&gt;&lt;span class="mjx-numerator"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;这&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;不是&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;平方&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;，&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;所以&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;它将&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;是&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;MLP_outlayer&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;的&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;范&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;数&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;最小&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;范&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-denominator"&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;数&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-line"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-vsize"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; ？我怀疑这应该考虑 MLP_in 规范&lt;/li&gt;&lt;li&gt;如果我改为正常&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;关于稀疏权重w和稀疏潜在激活的任何相关文献&lt;/li&gt;&lt;li&gt;统计问题：这些权重连接正在估计一个估计值（即，SAE 正在重建层的激活，权重正在重建 SAE 的重建）。这里是否有一些“平均误差相同，但方差更高”的论点？&lt;/li&gt;&lt;li&gt;寻找共激活簇的典型工具/算法是什么？我认为相关性很&lt;i&gt;接近&lt;/i&gt;，但它不会捕获 A 和 B 导致该特征的情况，但有时 A 很大而 B 很小，而其他情况则相反。&lt;ol&gt;&lt;li&gt;当潜在功能激活时，我还可以缓存所有输入功能激活，并对它们进行聚类。虽然我也想要导致潜在功能不激活的输入功能。&lt;/li&gt;&lt;li&gt;我还有稀疏权重，这应该是所有相关信息。&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h1&gt;宏伟计划&lt;/h1&gt;&lt;p&gt;如果我们能够定义回路，我们就可以具体指定重要的模型回路，例如诚实、欺骗、英式英语、自我意识和人格特质。我们当然会争论一个人的操作化是否真正抓住了我们想要的东西，但我们现在可以&lt;i&gt;实际指定&lt;/i&gt;它们来拥有这个论点。&lt;/p&gt;&lt;p&gt;我很高兴找到彼此因果关系的特征（这项工作是相关的）。这可以通过梯度或因果干预来完成。一旦我们有了这些因果联系，我们仍然需要找到如何计算这些特征。这项工作表明，其中许多连接都是线性计算的，而非线性计算的连接就是这些离群维度特征（这对于法学硕士进行文本预测很有用，但对于模型引导没有用）。 &lt;span class="footnote-reference" id="fnref3po7l3myi4l"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn3po7l3myi4l"&gt;[2]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;对于注意力特征，我们还可以在残余流中的特征和注意力之后的特征之间研究&lt;a href="https://transformer-circuits.pub/2021/framework/index.html"&gt;QK/OV&lt;/a&gt;电路。这还需要考虑功能激活统计数据，但似乎非常可行！&lt;/p&gt;&lt;p&gt;因此，如果我们有 Residual &amp;amp; MLP_out 和 Residual &amp;amp; Attention_out 的特征之间的连接，那么我们还可以将下一层 Residual 的特征计算为前一层特征的稀疏线性组合：&lt;br /&gt; &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;R&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;层&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;+&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;R&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;层&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;+&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;W&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;1&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup MJXc-space3"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;*&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;M&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;L&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;P&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;输出&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;层&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-msubsup MJXc-space2"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;+&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;W&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;2&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;*&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;At&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;t&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;n&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;输出&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;层&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-sub"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;这就是所有的连接。&lt;/p&gt;&lt;p&gt;还有很多工作要做，但其难度级别是“普通学术界可以解决它”，而不是“证明 P !=NP”；这是一个比我去年想象的要好得多的时间表。&lt;/p&gt;&lt;p&gt;如果您想参与任何 Sparse AE 项目，请随时加入我们的 #sparse-coding 频道（在 interp 下）中的 EleutherAI Discord 频道（&amp;gt;25k 成员，因此可以轻松潜伏）： &lt;a href="https://discord.gg/eleutherai"&gt;https://discord .gg/eleutherai&lt;/a&gt;&lt;/p&gt;&lt;p&gt;请随时在不和谐问题上联系我（Logan）：loganriggs、LW 上的 dm，或下面的评论。&lt;/p&gt;&lt;h1&gt;代码&lt;/h1&gt;&lt;p&gt;对于代码复制，请参阅我的&lt;a href="https://github.com/loganriggs/sparse_coding/tree/main"&gt;存储库&lt;/a&gt;中的“static*”文件&lt;/p&gt;&lt;p&gt;static-all_sparse_weights - 用于训练和比较线性与非线性的笔记本&lt;br /&gt;static-interpret_sparse_weights - 用于可视化线性或非线性特征的笔记本&lt;br /&gt;static-train_sparse_sae_connector - 训练 SAE（带 l1 潜在激活惩罚的 MLP）&lt;br /&gt; static-interpre_sparse_weights_mlp - 用于解释稀疏 SAE 的潜在激活并与 MLP 进行比较的最小笔记本。&lt;/p&gt;&lt;p&gt; [注：我还没有时间评论或清理这些笔记本。如果遇到任何问题请给我留言]&lt;/p&gt;&lt;h1&gt;附录&lt;/h1&gt;&lt;p&gt;这里有一些额外的实验，但没有成功或者只是很奇怪。&lt;/p&gt;&lt;h2&gt; MLP 扫描失败&lt;/h2&gt;&lt;p&gt;我也尝试减小 MLP 的隐藏层大小，但 MSE 仍然有所增加。这并没有将 MLP 限制为仅 MLP 特征。 &lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/pwoj5ixbj0w9qh54lsls" style="width: 78.31%;" /&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;h2&gt;注意力&lt;/h2&gt;&lt;p&gt;另外，如果我们对注意力进行相同的分析会怎样？ &lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/ydew0wckxeo6xnilro9r" /&gt;&lt;/p&gt;&lt;p&gt;第 4 层应该被忽略，因为它大部分都是死功能，但总的来说，这很奇怪！我没有像 MLP 那样对损失进行归一化，但似乎许多特征可以通过特征线性重建。这意味着注意力并没有真正&lt;i&gt;关注&lt;/i&gt;很多功能。&lt;/p&gt;&lt;p&gt;我的意思是，注意力通常会包含先前标记位置中的所有特征。如果我们每个示例有 200 个标记和大约 20 个特征/数据点，那么注意力就可以访问第 200 个位置的所有 20*200 个特征。然而，在这里，它只能访问&lt;i&gt;当前位置的&lt;/i&gt;20 个特征。诡异的。 &lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/xzjd94svk8cncrvlntyd" /&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7fxusXdkMNmAhkAfc/cnrurwbmeobzcuesk97h" /&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;ol class="footnotes"&gt;&lt;li class="footnote-item" id="fnpwepir8kf3"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefpwepir8kf3"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;此外，特征 F1 和 F2 之间的完整计算必须包括来自 SAE_1 的解码器、MLP 和来自 SAE_2 的编码器 + ReLU。&lt;br /&gt; F2 = relu(线性(线性(gelu(线性(线性(F1))))))&lt;/p&gt;&lt;p&gt; = relu(线性(gelu(线性(F1))) [因为两个线性函数可以等价于1个线性函数]&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn3po7l3myi4l"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnref3po7l3myi4l"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;我确实认为弄清楚这些异常维度所起的因果作用是有价值的。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ol&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/7fxusXdkMNmAhkAfc/finding-sparse-linear-connections-between-features-in-llms#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Sat, 09 Dec 2023 02:27:42 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/7fxusXdkMNmAhkAfc/finding-sparse-linear-connections-between-features-in-llms</guid></item><item><title>“模拟联合国解决方案”</title><link>https://www.lesswrong.com/posts/vDRjwdGHJCNtfWJgf/model-un-solutions</link><description>发布于 2023 年 12 月 8 日晚上 11:06（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;当我在高中时，因为我是历史碗队的一员，为模拟联合国俱乐部提供建议的老师招募我作为他们的代表参加各种“历史委员会”，如罗马参议院或 1789 年法国议会。我从未参与过任何正常的委员会，因为你无法进行假旗攻击或说服教皇将其他代表逐出教会。&lt;/p&gt;&lt;p&gt;据我所知，在大多数委员会中，参与者代表国家试图通过一项决议，解决事先决定的气候变化等主题。主持人会向被认定为“最佳代表”的玩家颁发奖项——这是演讲能力、社会主导地位以及准确代表（或至少不会致命地误解）指定国家的立场和利益的不成文组合。&lt;/p&gt;&lt;p&gt;我经常在心里比喻“模拟联合国讨论”和“模拟联合国解决方案”。模拟联合国的讨论围绕着人们期望因发表许多言论而获得奖励而展开，尽管他们的实际立场可以简单地表达或不允许进行太多阐述。&lt;/p&gt;&lt;p&gt;这导致了“联合国解决方案模型”，它有几种类型，例如&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://www.lesswrong.com/posts/dLbkrPu5STNCBLRjr/applause-lights"&gt;&lt;u&gt;掌声灯&lt;/u&gt;&lt;/a&gt;：你可以只说流行语或无可争议的琐事（“在解决气候变化问题时，我们应该考虑所有相关利益相关者的利益。我们既不应该采取{极端观点}，也不应该采取{相反的极端}”）&lt;/li&gt;&lt;li&gt;&lt;strong&gt;未指定的解决方案&lt;/strong&gt;：您可以提供很少的信息来唯一地识别听众心中现状的特定变化。在极端情况下，你会得到很多这样的评论：“为了解决问题，我们应该{投入资源}来{解决问题}”，其中括号内的部分被替换为不太具体的短语（“为了解决气候问题”）我们应该成立工作组来确定最佳的技术和政策方法”）&lt;/li&gt;&lt;li&gt;&lt;strong&gt;权衡无知的解决方案&lt;/strong&gt;：您甚至可以给出方向性建议，但避免考虑相关成本或权衡（“我们应该资助一项与气候变化相关的新教育推广计划”）。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;您可以想象尝试识别空洞言论的回应：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;询问是否有人持相反观点。&lt;/li&gt;&lt;li&gt;询问提议的解决方案与现状有何不同。&lt;/li&gt;&lt;li&gt;询问谁在提案中失败（或者如何重新分配资源）。有时没有人会遭受损失，但更多时候这只是一种未明示的权衡。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;br /&gt;&lt;br /&gt;&lt;a href="https://www.lesswrong.com/posts/vDRjwdGHJCNtfWJgf/model-un-solutions#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Fri, 08 Dec 2023 23:06:34 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/vDRjwdGHJCNtfWJgf/model-un-solutions</guid></item></channel></rss>