<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>少错</title><link>https://www.lesswrong.com</link><description>致力于提炼理性艺术的社区博客</description><lastBuildDate>Wed, 06 Dec 2023 00:53:31 GMT</lastBuildDate><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>这些天您对 LessWrong 感觉如何？ [打开反馈线程]</title><link>https://www.lesswrong.com/posts/j2W3zs7KTZXt2Wzah/how-do-you-feel-about-lesswrong-these-days-open-feedback</link><description>发布于 2023 年 12 月 5 日晚上 8:54（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;你好！我是来自 LessWrong / Lightcone 团队的 jacoobjacob。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;这是一个元线程，您可以分享您一直在想的有关 LessWrong 的任何想法、感受、反馈或其他内容。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;您可能分享的内容示例：&lt;/p&gt;&lt;ul&gt;&lt;li&gt; “我真的很喜欢同意/不同意投票！”&lt;/li&gt;&lt;li&gt; “所有这些对话内容是怎么回事？这令人困惑......&lt;/li&gt;&lt;li&gt; “嗯……最近网站上的氛围似乎发生了某种变化……特别是[&lt;i&gt;插入 10 段&lt;/i&gt;]”&lt;/li&gt;&lt;/ul&gt;&lt;p&gt; ...或者其他什么！&lt;/p&gt;&lt;p&gt;这篇文章的目的是让您能够在一个您知道团队成员会倾听的地方分享您想到的任何事情。&lt;/p&gt;&lt;p&gt; （我们是一个小团队，必须优先考虑我们的工作，所以我当然不承诺采取这里提到的所有内容。但我至少会倾听所有这些！）&lt;/p&gt;&lt;p&gt;我已经有一段时间没有看到这样的公共话题了。也许有很多关于这个网站的沸腾的感觉从未表达出来？或者，除了我从阅读正常评论、帖子、指标和对讲评论中发现的内容之外，你们没有什么可以分享的了？好吧，这是找出答案的一种方法！我真的很想询问并了解人们对该网站的感受。&lt;/p&gt;&lt;p&gt;那么，您最近对 LessWrong 感觉如何？欢迎在下面留下您的答案。&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/j2W3zs7KTZXt2Wzah/how-do-you-feel-about-lesswrong-these-days-open-feedback#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Tue, 05 Dec 2023 20:54:45 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/j2W3zs7KTZXt2Wzah/how-do-you-feel-about-lesswrong-these-days-open-feedback</guid></item><item><title>人工智能协调计划批评马拉松</title><link>https://www.lesswrong.com/events/kgyrsAkbBjNxPbicc/critique-a-thon-of-ai-alignment-plans</link><description>发布于 2023 年 12 月 5 日晚上 8:50（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt; AI-Plans.com 将于 12 月 16 日至 18 日举办批评马拉松，参与者将提出并讨论对人工智能调整计划的批评。&lt;/p&gt;&lt;p&gt;评委包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;内特·苏亚雷斯，MIRI 主席&lt;/li&gt;&lt;li&gt;Ramana Kumar，DeepMind 研究员&lt;/li&gt;&lt;li&gt;Peter S Park 博士，麻省理工学院 Tegmark 实验室博士后&lt;/li&gt;&lt;li&gt;Charbel-Raphaël Segerie，EffiSciences 人工智能部门负责人&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;好处：&lt;/h2&gt;&lt;p&gt;批评马拉松提供了一个绝佳的机会，通过深入研究调整计划可能成功或失败的原因，获得对人工智能安全的重要见解。&lt;br /&gt;我们将突出调整计划的关键要素，这些要素将反馈给研究人员并可用于改进他们的计划。&lt;br /&gt;这也是获得专家评委反馈的绝佳机会。&lt;/p&gt;&lt;h2&gt; Critique-a-Thon 活动为期 3 天，分为 2 个阶段：&lt;/h2&gt;&lt;h3&gt;&lt;strong&gt;第一阶段 - 添加评论&lt;/strong&gt;（截至 12 月 16 日）&lt;/h3&gt;&lt;p&gt;我们将在 ai-plans.com 上以两个类别的形式添加对调整计划的批评：优势和弱点。&lt;/p&gt;&lt;p&gt;&lt;br /&gt; “优势”应表明计划如何作为协调解决方案发挥作用。 “漏洞”应该起到相反的作用。&lt;br /&gt;奖品将颁发给那些提出最多批评的人 - 投票给负分的批评将不被计算在内。&lt;br /&gt;&lt;br /&gt;您可以随时开始这个阶段 - 您今天就可以开始添加评论！&lt;br /&gt;截止日期为格林尼治标准时间 12 月 16 日午夜。&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;奖品&lt;/strong&gt;&lt;br /&gt;第一名：100 美元&lt;br /&gt;第二名：60美元&lt;br /&gt;第三名：40美元&lt;/p&gt;&lt;h2&gt;&lt;br /&gt;第二阶段 - 讨论和总结（12 月 17 日至 18 日）&lt;/h2&gt;&lt;p&gt;我们两人一组。每对将选择一个已提出批评的调整计划。一个人会提出理由，另一个人会提出反对理由，优势/弱点是真实和准确的。&lt;br /&gt;然后，第二天，我们将交换立场，并以所提议的优势/弱点的书面形式结束。&lt;br /&gt;请参阅之前获奖评论的示例：&lt;/p&gt;&lt;p&gt; &lt;a href="https://docs.google.com/document/d/16Ww6nNECsDnjMOGaz5fqg1PxCVRPpoOy80UooEa7CZM/edit#heading=h.vdtj4e22pjdv"&gt;&lt;u&gt;八月评论马拉松&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;a href="https://docs.google.com/document/d/1O586rkRZd9BbpI8yqA2K6aG9IIq6E1A9I2H9C5nEYKQ/edit#heading=h.vdtj4e22pjdv"&gt;&lt;u&gt;九月评论马拉松&lt;/u&gt;&lt;/a&gt;&lt;br /&gt;&lt;br /&gt;这些讨论将有助于完善批评并加强论点，并且交换可以减少由于缺乏某个主题的先验知识而造成的任何不利。&lt;br /&gt;如果你的对手对这个话题了解得更多，并提出了你没有想到的观点，你可以在第二天自己使用它们，看看有哪些应对措施 - 并确定它们是否是你的文章中需要提及的内容，这就是将被判断的内容。&lt;br /&gt;这些讨论将&lt;a href="https://discord.gg/aGVtu5JyjJ"&gt;&lt;u&gt;在 Discord 上&lt;/u&gt;&lt;/a&gt;进行。&lt;/p&gt;&lt;p&gt;在这里加入👉 &lt;a href="https://discord.gg/aGVtu5JyjJ"&gt;&lt;u&gt;https://discord.gg/aGVtu5JyjJ&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;&lt;h3&gt;&lt;br /&gt;&lt;strong&gt;奖品&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;第一名：400 美元&lt;br /&gt;第二名：250 美元&lt;br /&gt;第三名：150 美元&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;在此注册加入👉 &lt;a href="https://ai-plans.com/login"&gt;&lt;u&gt;https://ai-plans.com/login&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/events/kgyrsAkbBjNxPbicc/critique-a-thon-of-ai-alignment-plans#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Tue, 05 Dec 2023 20:50:08 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/events/kgyrsAkbBjNxPbicc/critique-a-thon-of-ai-alignment-plans</guid></item><item><title>支持/反对 scheming 的论据集中在 SGD 所采取的路径上（“Scheming AI”的第 3 节）</title><link>https://www.lesswrong.com/posts/KyuMS9XzqaJGMu74f/arguments-for-against-scheming-that-focus-on-the-path-sgd</link><description>发布于 2023 年 12 月 5 日下午 6:48（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;这是我的报告《&lt;a href="https://arxiv.org/pdf/2311.08379.pdf"&gt;诡计多端的人工智能：人工智能会在训练期间假装对齐以获得权力吗？》&lt;/a&gt;的第三部分。 ”。 &lt;a href="https://www.lesswrong.com/posts/yFofRxg7RRQYCcwFA/new-report-scheming-ais-will-ais-fake-alignment-during"&gt;这里&lt;/a&gt;还有完整报告的摘要（ &lt;a href="https://joecarlsmithaudio.buzzsprout.com/2034731/13969977-introduction-and-summary-of-scheming-ais-will-ais-fake-alignment-during-training-in-order-to-get-power"&gt;此处&lt;/a&gt;有音频）。摘要涵盖了大部分要点和技术术语，我希望它能够提供理解报告各个部分所需的大部分背景信息。&lt;/p&gt;&lt;p&gt;本节的音频版本&lt;a href="https://www.buzzsprout.com/2034731/13984918"&gt;请点击这里&lt;/a&gt;，或者在您的播客应用程序上搜索“Joe Carlsmith Audio”。&lt;/p&gt;&lt;h1&gt;支持/反对阴谋论的争论集中在 SGD 所采取的路径上&lt;/h1&gt;&lt;p&gt;在本节中，我将讨论支持/反对计划的论点，这些论点更直接地关注 SGD 在选择训练的最终输出时所采取的路径。&lt;/p&gt;&lt;p&gt;重要的是，这些论点可能并不相关。特别是：如果 SGD 在模型类别之间以某种“直接比较”的方式主动支持或不支持阴谋者，那么 SGD 将“找到一种方法”来选择它在这个意义上所支持的模型类型（例如，因为足够高的模型类型）。维度空间使这样的“方式”可用）， &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-ZwSi8PjDgjvifEocL-1" id="fnref-ZwSi8PjDgjvifEocL-1"&gt;[1]&lt;/a&gt;&lt;/sup&gt;那么足够的训练只会引导你到 SGD 最喜欢的任何模型，而所讨论的“路径”并不重要。&lt;/p&gt;&lt;p&gt;在关于不同模型的最终属性之间的比较的部分中，我将讨论我们可能期望 SGD 出现这种偏袒的一些原因。特别是：阴谋者“更简单”，因为他们可以有更简单的目标，但他们“更慢”，因为他们需要参与各种形式的额外工具推理 - 例如，在决定策划时，检查现在是否是一个好时机缺陷，可能参与和掩盖“早期破坏”等方面的努力（尽管请注意，此处需要执行额外的工具推理，可能会表现为由策划者的权重实现的算法的额外复杂性，因此表现为“简单性”）成本”，而不是“运行该算法更长时间的需要”）。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-ZwSi8PjDgjvifEocL-2" id="fnref-ZwSi8PjDgjvifEocL-2"&gt;[2]&lt;/a&gt;&lt;/sup&gt;我将在下面对此进行更多讨论。&lt;/p&gt;&lt;p&gt;不过，在这里，我想指出，如果 SGD 足够关心简单性和速度等属性，那么 SGD 通常会首先构建一个具有长期追求权力目标的模型，但即使该模型尝试了一个方案 -就像策略一样（在这种情况下，由于预知其失败，它不一定会这样做），它会被无情地磨成一个情节奖励寻求者，因为情节奖励寻求者的速度优势。或者，SGD 通常会首先构建一个按情节奖励的寻求者，但由于 SGD 渴望更简单的目标，该模型将被无情地磨成一个阴谋者。&lt;/p&gt;&lt;p&gt;在本节中，我将假设这种事情不会发生。也就是说，SGD 构建模型的顺序会对训练的最终结果产生持久的影响。事实上，我的一般感觉是，对阴谋者的讨论经常隐含地假设这样的事情——例如，人们通常认为阴谋者会在训练中很早就出现，然后在那之后将自己锁定。&lt;/p&gt;&lt;h2&gt;独立于训练游戏的代理目标故事&lt;/h2&gt;&lt;p&gt;回想一下我上面介绍的区别：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;训练与比赛&lt;em&gt;无关的&lt;/em&gt;赛外目标，其产生与其在训练-比赛中的角色无关，但随后会激励训练-比赛，而不是训练-比赛。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;与训练游戏&lt;em&gt;相关的&lt;/em&gt;赛外目标，SGD 主动&lt;em&gt;创建&lt;/em&gt;这些目标是&lt;em&gt;为了&lt;/em&gt;激励训练游戏。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在我看来，关于专注于与训练、比赛&lt;em&gt;无关的&lt;/em&gt;目标的策划故事似乎更传统。也就是说，这个想法是：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;由于[插入原因]，模型将制定一个（适当雄心勃勃的）与训练中良好表现相关的超集目标（以&lt;em&gt;不&lt;/em&gt;通过训练游戏的方式）。&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;这可能发生在态势感知到来之前或之后。&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;如果之前，那么在一段时间内它可能会被训练出来，并且还没有激发训练-游戏。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;如果之后，它可能会立即开始激励训练-游戏。&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;然后，结合态势感知，这个（适当雄心勃勃的）超集目标将开始激发训练游戏。&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;将此称为“独立于训练游戏的代理目标故事”。&lt;/p&gt;&lt;p&gt;我相当认真地对待这个论点。如果某种适当雄心勃勃的超越情节的代理目标没有被训练（要么因为它在态势感知之前没有得到训练，要么因为它在态势感知之后突然出现），并且如果有一些关于为什么这个目标会产生影响的故事策划一个好的工具性策略适用（例如，经典的守门故事，或者可能是其他一些非经典故事），那么策划似乎确实可能只是一件发生的事情。&lt;/p&gt;&lt;p&gt;我认为最直接的反对意见是：为什么模型要制定这些（适当雄心勃勃的）超越情节的目标？我之前在诸如“目标默认情况下不会有时间折扣”和“模型时间不会像日历时间”之类的论点中讨论过这一点，我不会在这里添加太多内容，除了说假设模型将实现一些适当雄心勃勃的、超越情节的、独立于训练游戏的代理目标，追求这些目标与高奖励相关，在我看来，这似乎是经典故事中较弱的部分之一。&lt;/p&gt;&lt;p&gt;特别是：对阴谋的讨​​论有时集中在“好奇心”和“权力”等代理目标上，这些目标在很多剧集中都显得有用。但这并不意味着在训练中会激励对&lt;em&gt;情节之外的&lt;/em&gt;好奇心和力量的追求。更重要的是，特别是如果你知道/可以控制情境意识何时出现，那么在模型因追求好奇心/力量而受到积极惩罚的训练片段中似乎并不那么困难。非训练游戏模型将直接进行修改。事实上，总的来说，我希望我们能做很多事情来积极阻止模特采取寻求权力的行为，并鼓励其他形式的“谦虚”。&lt;/p&gt;&lt;p&gt;后一点与我之前提到的乐观原因有关：即，独立于训练游戏的代理目标故事允许我们在态势感知之前进行任何我们想要的对抗性训练，以尝试确保模型的目标保持不变情节内且适度适度。&lt;/p&gt;&lt;p&gt;也就是说，试图在态势感知之前通过对抗性训练来避免这种阴谋，面临着一些严重的障碍：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;首先：最重要的是，所讨论的对抗性训练可能不够多样化/彻底。特别是（特别是缺乏可解释性工具）：可能很难想到所有可能的适当雄心勃勃的剧集外代理目标，这些目标与奖励有足够好的相关性，并且您需要使用此类培训来推动。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-ZwSi8PjDgjvifEocL-3" id="fnref-ZwSi8PjDgjvifEocL-3"&gt;[3]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;第二：当态势感知开始发挥作用时，你可能不知道，或者无法很好地控制。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;第三：一旦足够智能的模型经过文本预测的预训练，它可能已经非常接近态势感知，因为它拥有大量相关的常识（即使不一定是自定位知识）。因此，在那之后可能没有太多时间进行没有态势感知的对抗训练。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;最后，&lt;em&gt;在&lt;/em&gt;态势感知启动后，模型有可能制定适当的、雄心勃勃的、超越情节的、策划激励的目标。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-ZwSi8PjDgjvifEocL-4" id="fnref-ZwSi8PjDgjvifEocL-4"&gt;[4]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;例如，也许当模型开始按照我上面描述的方式“反思”时，它已经相当聪明并且具有战略意识——例如，“弄清楚它真正想要什么”，自我解决，学习打破以前尝试的新事物对齐它，等等。 （这要求模型不会因为获得态势感知而立即开始寻求奖励，但这似乎是可能的，特别是如果我们假设训练留下了显着的“余裕”。）&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;当然，仍然存在一个问题，即为什么该模型会将自己打造成具有适当雄心勃勃的目标，特别是超越情节的目标。但在这里，根据我上面对“如果你训练它拥有长期目标会怎样”的讨论，也许我们可以诉诸这样一个事实：我们试图将其指向一项雄心勃勃的长期任务，但这种指向有些不准确。 / 模糊，这样当它弄清楚它想要如何概括时，这种概括就落在了足够长期和雄心勃勃的事情上，从而激发了阴谋。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;事实上，正如我上面指出的，我认为有一个案例表明，当前的调整工作已经&lt;em&gt;在&lt;/em&gt;试图将我们的系统指向长期的、超越情节的目标——例如，避免帮助制造炸弹的“无害”形式即使炸弹数十年不会爆炸。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;因此，总的来说，我认为，以谋划作为追求（适当雄心勃勃的）超越情节目标的良好工具性策略为条件，独立于训练游戏的代理目标故事在我看来是一个真正令人担忧的问题。&lt;/p&gt;&lt;h2&gt; “最近的最大奖励目标”故事&lt;/h2&gt;&lt;p&gt;我们还可以讲述一个不同的故事，重点关注&lt;em&gt;依赖于&lt;/em&gt;训练和比赛的赛外目标。我将其称为“最近的最大奖励目标”故事。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-ZwSi8PjDgjvifEocL-5" id="fnref-ZwSi8PjDgjvifEocL-5"&gt;[5]&lt;/a&gt;&lt;/sup&gt;故事是这样的：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;当模型变得具有情境意识时，它可能不会追求最大奖励目标（即，在训练数据上追求的目标大致会导致与模型的其他功能一致的最大奖励）。相反，它将追求一些低于最大奖励的代理目标。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;但那时，世界模型将包含模型训练游戏所需的所有信息。 （然而，因为我们在这里关注的是关于阴谋的“训练游戏&lt;em&gt;依赖&lt;/em&gt;”故事，我们假设在获得态势感知时，模型的目标还不是激发阴谋——而是进行一些进一步的修改为此需要。）&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;此时，SGD 修改模型以获得最大奖励的最简单方法就是使其成为一个计划者。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;所以：SGD 将使模型成为一个策划者。&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;现在，一种选择是尝试使 (1) 为假。事实上，如上所述，在态势感知之前尝试以良好的方式塑造模型的目标似乎是降低阴谋可能性的关键潜在途径。但请注意，在这种情况下，所面临的挑战比&lt;em&gt;独立于&lt;/em&gt;训练游戏的代理目标更困难。也就是说，对于与训练游戏无关的代理目标，只需在态势感知之前避免给模型提供类似策划者的目标，因此它可以容忍所讨论的目标可能尚未达到最大奖励的&lt;em&gt;其他&lt;/em&gt;方式。相比之下，在“最近的最大奖励目标”论点的背景下，人们需要要么实际创建一个最大奖励、非类似阴谋的目标，要么&lt;em&gt;足够接近&lt;/em&gt;（3）不适用的目标。也就是说，你不能仅仅阻止“自然阴谋”的出现，而必须阻止它的出现。你需要确保当 SGD 稍后“搜索”最近的最大奖励目标时，它不会首先找到阴谋者。&lt;/p&gt;&lt;p&gt;现在，我们假设我们还没有完全掌握这一点，并且（1）是正确的（尽管：我们所达到的掌握&lt;em&gt;程度&lt;/em&gt;对我将在下面讨论的“接近度竞赛”很重要）。 (2) 源自态势感知的定义（或者至少是一个宽松的定义，例如“参与阴谋并认识到阴谋的好处所必需的对世界的理解”），所以我们也承认这一点。&lt;/p&gt;&lt;p&gt;但是（3）呢？我认为，在这里，论证还有更多工作要做。为什么认为将模型打造成阴谋者将是使其获得最大奖励的最简单方法？例如，为什么不把它变成一个训练圣人，或者一个剧集奖励寻求者，或者一个追求某种最大奖励代理目标的错误概括的非训练游戏玩家？根据假设，该模型具有代表&lt;em&gt;任何&lt;/em&gt;这些目标的概念工具。 （请注意，在这方面，它与大多数进化历史中的人类不同，人类&lt;em&gt;没有&lt;/em&gt;概念工具来表示“包容性遗传适应性”等目标。）因此，我们需要一个故事来说明为什么该论点会优先考虑类似阴谋家的人特别是目标。&lt;/p&gt;&lt;p&gt;为了清楚起见：当我谈论 SGD 可以进行某种修改的“轻松”程度，或者所得到的模型的“接近度”时，这是“SGD‘偏好’的那种修改”的代表。本身就是“SGD 实际上将进行的那种修改”的替代品。在机械层面上，这大致意味着：奖励景观中最陡梯度的方向。我经常会想象一种更模糊的感觉，即 SGD 可以做的“工作”预算有限，因此希望在修改模型目标方面做尽可能少的“工作”，以便它可以专注于改进模型认知的其他方面。&lt;/p&gt;&lt;p&gt;那么，根据对“轻松”的理解，我们应该期望哪个模型类对于 SGD 来说是“最容易”创建的，即从一个新的情境感知模型中创建一个低于最大奖励代理目标的模型。自己不能激发阴谋吗？&lt;/p&gt;&lt;p&gt;作为一个有助于激发直觉的松散类比：想象一下将人类技术冻结在当前水平，并让进化选择在人类身上运行更长时间。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-ZwSi8PjDgjvifEocL-6" id="fnref-ZwSi8PjDgjvifEocL-6"&gt;[6]&lt;/a&gt;&lt;/sup&gt;从长远来看，你预计哪种人类（或人类后裔生物）将占据主导地位？特别是：您期望：&lt;/p&gt;&lt;p&gt; (a) 人类本质上重视“&lt;a href="https://en.wikipedia.org/wiki/Inclusive_fitness"&gt;包容性遗传适应性&lt;/a&gt;”或“我的基因被进化选择”（这是对“指定目标”和“奖励过程”的某种组合的松散类比），&lt;/p&gt;&lt;p&gt; （b）人类重视与包容性遗传适应性密切相关的其他事物（例如，“拥有尽可能多的孩子”），但他们没有明确地优化包容性遗传适应性，甚至是工具性的（这是对错误的松散类比）广义的非训练游戏玩家），或&lt;/p&gt;&lt;p&gt;(c) 具有长期目标的人类，他们正在优化包容性遗传适应性，特别是作为一种在长期内为自己的价值观获得权力的方法（这对阴谋家来说是一个宽松的类比）。&lt;/p&gt;&lt;p&gt;在这里，“最近的最大奖励目标”故事的类比会问这样的问题：对现有人类价值观的最小修改是什么（或者更好：特别是对进化最容易的修改）是什么，使我们陷入困境之一如上所述，同时也与所面临的进化选择压力的大小兼容？ &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-ZwSi8PjDgjvifEocL-7" id="fnref-ZwSi8PjDgjvifEocL-7"&gt;[7]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;当然，这个类比带来了大量令人困惑的变量（例如，与进化选择构建学习价值的大脑的基因组而不是价值本身有关的事实，文化积累和模因选择的作用）人口动态、缺乏对未来逃跑/接管机会的明确模拟，等等）。为了使这一类比与机器学习更加相似，最好想象进化直接选择整个人类大脑，配备完整的概念工具来代表所有这些目标，而不是选择构建大脑的基因组。更好&lt;em&gt;地&lt;/em&gt;想象一个从一开始就选择大脑的进化版本，这样我们开始理解进化时的人类价值观可能会完全不同。不管怎样，我对最佳预测没有任何明确的信念。但我认为这是“最近的最大奖励目标”论点的一个令人回味的直觉泵。&lt;/p&gt;&lt;h3&gt; SGD 渐进主义对类似阴谋家的修改的障碍&lt;/h3&gt;&lt;p&gt;不过，让我们关注实际的论证，而不是类比。我们应该期望哪个模型类在相关意义上是“最接近的”？&lt;/p&gt;&lt;p&gt;上面我讨论了阴谋者可能被快速排除的一种方式：即，如果 SGD 无法通过修改模型时遵循的梯度来“注意到”类似阴谋者修改的好处。也就是说，正如我之前讨论的，在寻找最大奖励目标时，SGD 不仅仅“跳”到最近的目标。相反，它需要在梯度计算的基础上逐步实现这样的目标，梯度计算表明模型权重在相关方向上的微小变化会增加奖励。在许多情况下，尚不清楚类似阴谋者的修改是否可以像这样工作。&lt;/p&gt;&lt;p&gt;因此，例如， &lt;a href="https://www.lesswrong.com/posts/A9NxPTwbw6r6Awuwt/how-likely-is-deceptive-alignment#Deceptive_alignment_in_the_high_path_dependence_world"&gt;Hubinger (2022)&lt;/a&gt;考虑了一个例子，其中 SGD 通过将模型从关心剧集中的金币修改为始终关心金币来引发阴谋。不过，在所讨论的示例中，SGD 并不是逐渐延长模型金币关注的时间范围，每次延长都会导致奖励的提高。相反，SGD 只是做了“一个简单的改变”——即完全放弃目标的时间限制——从而创建了一个阴谋者。但问题是：奖励空间的梯度是否反映了这样做的好处？在我看来，发生这种情况的最自然的方式是，是否有能力从一种模型平滑地过渡到另一种模型，这样每次修改都会逐渐获得更多的策划好处。但目前尚不清楚这是否会发生。正如我之前讨论的，如果我们假设 SGD 还需要构建大量新机器来执行策划所需的工具推理（而不是仅仅重定向预先存在的“目标实现引擎”），那么任务变得更具挑战性。&lt;/p&gt;&lt;h3&gt;哪个模型是“最接近的”？&lt;/h3&gt;&lt;p&gt;然而，根据我之前的讨论，我也不觉得我能够排除这种类型的增量转换可能发生的可能性（例如，也许足够高的维空间允许 SGD“找到一种方法”） ”），而且我还没有尝试进行深入分析。因此，虽然我认为这种类型的渐进主义对专注于依赖训练游戏的阴谋者目标的故事提出了相对强烈的反对，但我认为也值得评估这些故事的其他方面。也就是说，假设SGD&lt;em&gt;能够&lt;/em&gt;注意到将“最近的最大奖励目标”故事中的模型制作成schemer的好处，这样的修改是否是获得高奖励的最简单方法？&lt;/p&gt;&lt;h4&gt;目标空间中类似阴谋家的目标的共性&lt;/h4&gt;&lt;p&gt;对此的一个论点可以追溯到我所认为的支持期待阴谋的核心直觉之一：即可能的阴谋者式目标的绝对&lt;em&gt;数量&lt;/em&gt;。粗略地说，这个想法是，因为&lt;em&gt;如此多的目标&lt;/em&gt;可以激发策划，所以很可能会有一些类似策划者的目标非常&lt;em&gt;接近&lt;/em&gt;模型的当前目标（其中“附近”，回想起来，是根据工作量 SGD 来定义的）必须做才能引起相关类型的修改）。也就是说，阴谋者在目标空间中非常常见——因此 SGD 相应地很容易从任何给定的起点“找到”类似阴谋者的目标。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-ZwSi8PjDgjvifEocL-8" id="fnref-ZwSi8PjDgjvifEocL-8"&gt;[8]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;上面描述的“剧集中的金币”→“所有时间的金币”转变就是这种思想的一个例证。也就是说，如果你假设最充分的长期目标会激发阴谋， &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-ZwSi8PjDgjvifEocL-9" id="fnref-ZwSi8PjDgjvifEocL-9"&gt;[9]&lt;/a&gt;&lt;/sup&gt;那么&lt;em&gt;不&lt;/em&gt;成为阴谋家的最显着的方法就是有一个足够的短期目标。但是（假设我们已经抛开上面讨论的“增量主义”问题），任何给定的时间有限的目标似乎都非常“接近”时间无限的版本。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-ZwSi8PjDgjvifEocL-10" id="fnref-ZwSi8PjDgjvifEocL-10"&gt;[10]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;但时间范围并不是这里可能的“邻近性”的唯一来源。例如，如果模型碰巧有一些尚未激发阴谋的&lt;em&gt;长期&lt;/em&gt;目标（例如，因为目标不够“雄心勃勃”），我们可以改变其他相关变量。因此，如果模型开始关心以不受时间限制的方式正确解决一些数学问题，但它可以在密谋接管世界以获得更多计算来检查和重新计算的情节中很好地解决问题检查它的答案似乎不值得，我们可能会想象 SGD，例如，改变模型对得到错误答案的风险的容忍度，或者改变它的目标，以便它想要解决&lt;em&gt;更多的&lt;/em&gt;数学问题，或者你有什么。总的要点是：在尝试创建计划程序时，SGD 有大量选项可供选择，因此很容易在模型起点附近找到一个选项。&lt;/p&gt;&lt;p&gt;因此，在进化和人类的背景下，这种思想的类比是这样的：即使是相当短期的、适度的人类价值观，也可以相对容易地将它们转变为长期的、雄心勃勃的价值观。例如，人类长期主义者似乎会采取“对我周围环境中满足各种条件的人表现出模糊的友善和帮助”之类的价值观，并将其转化为更加雄心勃勃的目标——这些目标似乎有动力明确优化以获取选择-通过进化，如果这是未来电力的关键瓶颈。事实上，在我看来，在实践中，如果你进行进化选择足够长的时间，在长期范围内优化的意识形态群体最终将占据主导地位（许多最接近优化生殖适应性的群体） ，例如，看起来他们有这种味道）。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-ZwSi8PjDgjvifEocL-11" id="fnref-ZwSi8PjDgjvifEocL-11"&gt;[11]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;一旦我们搁置了上面关于渐进主义的反对意见，我认为目标空间中类似阴谋家的目标的共性是一个相当有力的理由，担心新的情境意识的最近的最大奖励修改，仍然不是-最大奖励目标追求模型将走向心计。更重要的是，请注意，一旦我们想象 SGD 在目标空间中主动&lt;em&gt;搜索&lt;/em&gt;会激发阴谋的目标，我们关于阴谋起源的故事就可以更容忍阴谋者般的目标，必须有其他更具体的目标属性，例如资源匮乏性。也就是说，在专注于训练与游戏&lt;em&gt;无关的&lt;/em&gt;目标的环境中，有可能质疑 SGD 落在类似阴谋者的目标上的概率（甚至以它落在更普遍的超越情节目标为条件），凭借不同的超越情节的目标意味着对未来力量、风险等的不同偏好。但在像这样的&lt;em&gt;依赖于&lt;/em&gt;训练游戏的环境中，SGD 正在积极&lt;em&gt;寻找&lt;/em&gt;这样的目标（并且相关目标在目标空间中仍然很常见——例如，您可以“在所有空间和时间上最大化&lt;em&gt;x&lt;/em&gt;的实例”） “ 对于&lt;em&gt;x&lt;/em&gt;的许多值）。因此，如果 SGD 找到了它想要的东西，也就不足为奇了。&lt;/p&gt;&lt;p&gt;然而，我认为这种论点并不是决定性的，原因有二。&lt;/p&gt;&lt;h4&gt;非策划目标的接近程度&lt;/h4&gt;&lt;p&gt;第一个原因是，即使我们坚持纯粹专注于修改模型的目标，并采用“干净的目标导向”框架（我将在下面使这样的焦点复杂化），我认为还有一个抵消的“接近性”考虑这至少支持一些非计划目标——即，在其他条件相同的情况下，模型的初始训练（在态势感知之前）似乎更有可能主动将模型指向这些非计划目标的方向。&lt;/p&gt;&lt;p&gt;这对于指定的目标来说是最清楚的，根据定义，这是通过培训来奖励的。这意味着，即使模型在具有态势感知能力时最终没有&lt;em&gt;完全&lt;/em&gt;达到指定的目标，它也可能最终得到广泛附近的目标。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;因此，举例来说，如果模型正在接受训练以最大化剧集中的金币，并且最终学习了一个目标，例如“最大化剧集中的金币”（这个目标&lt;em&gt;不太&lt;/em&gt;正确，因为有些训练环境包括圆形金币（非硬币），看来 SGD 最简单的修改就是将模型变成&lt;em&gt;金币&lt;/em&gt;on-the-episode-getter（即训练圣人）。 ）。或者至少，就“接近度”而言，“情节中的金币”似乎与“有史以来的圆形金币”&lt;em&gt;具有竞争力&lt;/em&gt;。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;对于更多 RLHF 风格的指定目标，我们可能会提出类似的观点。也就是说，如果您正在训练一个模型以一种广泛有帮助、诚实且无害的方式遵循剧集中的说明​​（为简单起见，您的 RLHF 流程仅在这方面提供准确的反馈），那么即使它当它变得具有情境意识时，它已经学会了一种有点不平衡的概念，例如诚实，改进其诚实概念，直到它更好地适合人类概念，似乎具有竞争性，回复：接近，具有更多类似阴谋家的动机修改，例如“偏离-始终保持诚实，以激励计划逃脱人类控制并夺取权力。”&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;请注意，在人类和进化的背景下，人类确实已经重视与包容性遗传适应性密切相关的各种事物——例如，生孩子、更广泛地拥有繁荣而强大的家庭血统等等。如果我们想象进化选择整个人类大脑配备了代表“包容性遗传适应性”的概念装置，人类最初进化到重视与包容性遗传适应性非常接近的东西似乎更容易想象。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-ZwSi8PjDgjvifEocL-12" id="fnref-ZwSi8PjDgjvifEocL-12"&gt;[12]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;请注意，我们也可以在这里对错误概括的非训练游戏玩家提出类似的观点，只要他们追求最大奖励目标（回想一下，根据我的定义，错误概括的目标可以是最大奖励，前提是特别是&lt;em&gt;训练数据&lt;/em&gt;永远不会惩罚它们——例如，它们只会在模型从未见过的反事实场景中受到奖励过程的惩罚。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;因此，例如，想象一个训练过程，它永远不会区分“在剧集中获得金币”和“在剧集中获得一般金币”之类的目标，因为训练数据中的所有金币都是硬币。假设在态势感知之前，我们的模型学习了一个目标，比如“在剧集中获得金币，但有时也会跟随你对剧集的好奇心，即使这意味着牺牲金币”——这个目标可以实现（但不是最大程度地实现）好吧）在训练中，因为好奇心并不是一种非常有害的行为，有时甚至是一种有益的行为。在这里，似乎只是删除模型的好奇心并只专注于获得剧集中的黄金物品（最大奖励目标，即使不是指定目标）是一个相当“附近”的修改 - 并且是一个看似合理的修改在“接近”方面具有竞争力，例如修改模型，使其始终追求其黄金和好奇心的组合。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;在人类和进化的背景下，如果我们想象，从进化选择的角度来看，“拥有尽可能多的孩子”的内在价值会表现得很好（即使这并不&lt;em&gt;完全&lt;/em&gt;是进化选择的目的），但事实并非如此。似乎很难想象人类会朝这个方向前进。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;我们能否对寻求剧集奖励的人提出类似的观点？这有点不太清楚，因为在态势感知之前，尚不清楚模型是否有足够的奖励过程概念，以便他们有动机附加到其组件之一“附近”的某些东西。也就是说，在我看来，在某些情况下这种情况可能会发生。因此，例如，即使缺乏态势感知，通过 RLHF 训练的模型最终也会受到“人类认可”附近概念的激励，这对我来说似乎是合理的。这些概念似乎至少在某种程度上接近奖励过程的各个方面，例如人类评估者和/或奖励模型的判断，这样一旦模型了解了奖励过程，修改其动机以专注于这些组件就不会太困难。对于 SGD 来说，这是一个巨大的飞跃。&lt;/p&gt;&lt;p&gt;总的来说，我认为默认情况下，非策划目标往往会具有某种对他们有利的“接近性”。这并不奇怪。特别是：非策划者目标必须与奖励过程有一些相当直接的联系（例如，它们要么直接受到该过程的奖励，要么因为它们专注于奖励过程本身的某些组成部分），因为与策划者目标不同，非策划者目标不能依赖目标内容完整性或长期权力追求等聚合子目标来确保追求它们会带来回报。因此，我们很自然地期望通过奖励过程来训练模型，在尚未实现阴谋的情境意识背景下，会导致动机集中在非阴谋目标附近的某些事情上。&lt;/p&gt;&lt;p&gt;尽管如此，在给定情况下，这种考虑是否足以使非阴谋家目标比阴谋家目标更&lt;em&gt;接近&lt;/em&gt;模型的当前目标，这仍然是一个悬而未决的问题。重要的是，请注意，相关的竞争是针对&lt;em&gt;整套&lt;/em&gt;附近的类似阴谋者的目标（而不是例如我上面讨论的可能的类似阴谋者修改的特定示例）——考虑到可能的阴谋者的种类繁多-类似的目标，可能是一个严重的劣势。因此，打个比方：如果鲍勃家十英里范围内有十家墨西哥餐馆和一百家中国餐馆，那么即使任何给定的墨西哥餐馆与任何给定的中国餐馆“似乎具有竞争力”，re：接近度，那么（模进一步信息）&lt;em&gt;最近的&lt;/em&gt;餐馆可能仍然是中国餐馆。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-ZwSi8PjDgjvifEocL-13" id="fnref-ZwSi8PjDgjvifEocL-13"&gt;[13]&lt;/a&gt;&lt;/sup&gt;根据模型空间中类似阴谋者的目标的共性，我们可能期望类似阴谋者的目标就像这里的中餐馆一样。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-ZwSi8PjDgjvifEocL-14" id="fnref-ZwSi8PjDgjvifEocL-14"&gt;[14]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;h4&gt;混乱的目标导向性与接近度的相关性&lt;/h4&gt;&lt;p&gt;然而，我也认为，一旦我们考虑到“混乱的目标导向性”，非阴谋家就会得到另一种明显的亲近感提升。特别是：如果我们想象所讨论的模型不是一个完全可分离的目标槽+目标实现引擎，而是不同启发式、注意力模式、冲动等的错综复杂的混合物，那么我们需要分析比较“接近度”不仅是当前模型的当前&lt;em&gt;目标&lt;/em&gt;与最近的类阴谋者和非类阴谋者目标的比较“接近度”，而且是模型的&lt;em&gt;整个组合&lt;/em&gt;与最近的类阴谋者和非类阴谋者模型的接近程度。在这里，我认为非策划模型可能具有更显着的优势。&lt;/p&gt;&lt;p&gt;特别是：在一定程度上，预先情境意识训练往往会产生一种混乱的目标导向性，指向非策划目标&lt;em&gt;附近&lt;/em&gt;的某些东西——特别是一个&lt;em&gt;片段内的&lt;/em&gt;非策划目标——那么这意味着模型的启发式、注意力模式、冲动等也倾向于指向那个大方向，并且特别集中在情节内。对我来说，这使得将模型重新定位为更长期的权力追求似乎是一个更实质性和更困难的修改，特别是如果所讨论的情节非常短的话。也就是说，直观上，对我来说，对于 SGD 来说，“调整一个完全可分离的剧集内目标以使其长期化”比“重新引导广泛关注剧集内目标的错综复杂的目标以使其成为长期目标”要容易得多。它（a）专注于某些超出情节的事情，并且（b）使得这种超出情节的焦点回溯到出于工具性原因而获得奖励”——特别是如果（b）需要建立新的认知机制来实施工具性推理有问题。而”“重定向纠结的kludge集中在集中事物上，而不是相同广阔的附近的较高奖励的情节内部事物”（例如，金币，而不是金币，而不是金色的东西；诚实）在我看来，直觉上更容易。&lt;/p&gt;&lt;h3&gt;总体上以“最近的最高奖励目标”论点&lt;/h3&gt;&lt;p&gt;总结我对期望计划的“最近的最高奖励目标”论点的看法：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;如果SGD由于没有产生这些好处而将模型变成模型的好处，则该参数将失败。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;但是，我觉得我们无法依靠SGD无法注意到这些好处。如果&lt;em&gt;可以的&lt;/em&gt;话，那么我认为目标空间中类似策划者的目标的共同点使得像策划者的目标是“最接近模型”的目标，这非常令人担忧。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;也就是说，因为在没有情境意识的情况下，它是由奖励过程所塑造的，所以该模型的目标也将在某些最高奖励非安全目标的“附近”中可能已经“在附近”。某些非示波器模型的“接近度”。在某种程度上，危及的目标指导性非常“凌乱”，创建这样的非驾驶员可能需要更少的修改模型的启发式方法，关注模式，器乐推理等。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这些考虑因素使我对方案的额外担忧。&lt;/p&gt;&lt;h2&gt;诸如简单性和速度与路径SGD的可能相关性&lt;/h2&gt;&lt;p&gt;如果我们假设任何给定的最高奖励目标在训练中产生足够好的表现，那么“最近的最高奖励目标参数”之类的论点最自然地进行，以至于SGD并不特别讨厌模型最终&lt;em&gt;的&lt;/em&gt;最高奖励目标。但是，正如我上面指出的，正如我将在有关不同模型类的最终属性的部分中进行详细讨论，有些故事以简单性（如Simplicity）的最终属性为由，其中SGD主动更喜欢这些模型类而不是其他类别和速度。&lt;/p&gt;&lt;p&gt;更重要的是，这种偏好不仅与忽略SGD在模型空间中所走的路径的预测有关。相反，它们可能会在各个阶段&lt;em&gt;影响&lt;/em&gt;该路径（即使最终结果在某种程度上仍然是“依赖路径”）。例如，例如，如果SGD朝着更简单的目标偏见，则这种偏见可能会影响模型在训练游戏中倾向于在训练游戏中倾向于在情况下或之后形成情境意识，以及依赖培训游戏的目标故事，它可能是SGD专门向类似于Schemer的目标移动的其他要点，这可能是从尚未获得的奖励的起点。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-ZwSi8PjDgjvifEocL-15" id="fnref-ZwSi8PjDgjvifEocL-15"&gt;[15]&lt;/a&gt;&lt;/sup&gt;同样，如果SGD凭借其需要额外的推理而偏离了计划的偏见，那么这种偏见可能是额外的观点，有利于期望SGD从这样的起点朝着非驾驶员迈进。&lt;/p&gt;&lt;p&gt;正如我在第4节中讨论的那样，关于“最终属性”，我通常认为，策划的速度成本比简单的好处更重要，因此其他事物相等，我认为这些考虑因素违反了策划。但是，我尚不清楚这两个方向的成本/收益尤其重要。&lt;/p&gt;&lt;p&gt;也就是说，至少有一些案例可以使这些成本/收益在培训的早期&lt;em&gt;更&lt;/em&gt;重要，因为模型的早期可能会&lt;em&gt;较弱&lt;/em&gt;，因此减少了简单性和速度可用的资源（例如，参数和计算将对模型的认知能力产生更大的总体影响。也就是说：也许，在培训的早期，认知资源更稀缺，因此更必要。因此，例如，当一般可用的工具推理预算较小时，也许要求模型形成长期，类似于策划的计划（奖励更大的奖励）；也许，当要求其使用更多的参数更复杂的目标是更少的参数变成有用的认知结构时。 &lt;sup class="footnote-ref"&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn-ZwSi8PjDgjvifEocL-16" id="fnref-ZwSi8PjDgjvifEocL-16"&gt;[16]&lt;/a&gt;&lt;/sup&gt;因此，在某种程度上，人们认为，相对于其他考虑因素，这些成本可能是“在噪音中”的（我很喜欢这一观点，并且我在下面讨论了），一种可能是对于训练的早期部分而言，与模型的最终属性相比，这对训练的早期部分少。&lt;/p&gt;&lt;h2&gt;对专注于路径SGD的论点的总体评估&lt;/h2&gt;&lt;p&gt;不过，尽管如此，尽管非驾驶员可能会带来速度优势，但我发现“独立于训练游戏的代理目标”论点和“最接近的最高奖励目标参数”的结合令人担忧。尤其：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;对我来说似乎是合理的，尽管我们在平凡的对抗训练方面做出了努力，尤其是在我们有目的地塑造模型以实现长期和相当雄心勃勃的目标的政权中，某种适当的雄心勃勃，不正当，超对象，超级情节的目标可能会流行自然而然地在训练之前 - 在情境意识之前，或之后 - 然后导致策划发生。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;即使这种情况&lt;em&gt;并非&lt;/em&gt;自然发生，我也很担心，到达到情境意识时，SGD给模型一个最大奖励目标的最简单方法是将其制成架构，因为类似Schemer目标在目标空间中足够普遍，因此他们经常会出现“附近”模型当时产生的不太明智的奖励目标。 SGD的“增量主义”可能会消除这种担忧，并且/或我们应该期望非吊销模型默认情况下“近”（因为他们的目标尤其接近，或者是因为在“凌乱的目标定向性”中，设置，它们需要更简单地修改模型当前的启发式术语纠结的魔力，或者因为它们的“速度”优势会使SGD更喜欢它们）。但是我不自信。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;但是，这两个参数都集中在SGD通过模型空间的&lt;em&gt;路径&lt;/em&gt;上。将重点放在所讨论模型的最终属性上的参数呢？让我们现在转向那些。 &lt;/p&gt;&lt;hr class="footnotes-sep" /&gt;&lt;section class="footnotes"&gt;&lt;ol class="footnotes-list"&gt;&lt;li class="footnote-item" id="fn-ZwSi8PjDgjvifEocL-1"&gt;&lt;p&gt;例如，这要求模型无法“&lt;a href="https://www.lesswrong.com/posts/uXH4r6MmKPedk8rMA/gradient-hacking"&gt;渐变黑客攻击&lt;/a&gt;”，而我上面讨论过的内省目标保护方法。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-ZwSi8PjDgjvifEocL-1"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-ZwSi8PjDgjvifEocL-2"&gt;&lt;p&gt;我还讨论了他们缺乏针对指定目标/奖励的“内在热情”可能会有所作为。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-ZwSi8PjDgjvifEocL-2"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-ZwSi8PjDgjvifEocL-3"&gt;&lt;p&gt;感谢Rohin Shah在这里进行讨论。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-ZwSi8PjDgjvifEocL-3"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-ZwSi8PjDgjvifEocL-4"&gt;&lt;p&gt;的确，如果我们假设预训练本身&lt;em&gt;会导致&lt;/em&gt;情境意识，但不能超越剧集，动机动机的目标，那么这将是默认故事，用于在预训练之前训练中如何出现Schemers的故事。感谢Evan Hubinger的标记。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-ZwSi8PjDgjvifEocL-4"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-ZwSi8PjDgjvifEocL-5"&gt;&lt;p&gt;我认为这个故事与Hubinger所说的“世界模型悬垂”的故事相关但与众不同，该故事（我理解）大致运行如下：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;到模型在情境中意识到的时候，它的目标可能不会使追求它们与获得高奖励完全相关。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;但是，在那时，它的世界模型将包含训练游戏所需的所有信息。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;因此，在此之后，SGD将能够通过修改模型的超出序列目标来激发训练游戏的超出序列目标，从而获得大量的轰动：奖励：奖励。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;相比之下，通过修改模型更像是训练圣地，它可能能够减少爆炸，因为在这个方向上的边际努力仍然可能会使模型的目标与奖励不完美地相关（或至少，由于不得不等待未来的训练情节的纠正，因此需要更长的时间才能达到完美。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;因此，SGD将创建超出的情节目标，以激发训练游戏（然后这些目标将结晶）。&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt; Hubinger的框架的一个问题是，从我感兴趣的意义上讲，他的本体论似乎是忽略了奖励的寻求者 - 而SGD将模型修改为“奖励”的零件搜索者至少会这样做同样，就这个论点而言，将其修改为schemer。我尚不清楚他对“减少回报”的思考如何起作用（尽管我上面使用的“接近”修改的本体是一种重建）。&lt;/p&gt;&lt;p&gt;就是说，我认为最终，“最近的高回报目标”故事和“世界模式”的故事可能正在试图指出相同的基本思想。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-ZwSi8PjDgjvifEocL-5"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-ZwSi8PjDgjvifEocL-6"&gt;&lt;p&gt;感谢Daniel Kokotajlo，Rohin Shah，Tom Davidson和Paul Christiano的讨论。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-ZwSi8PjDgjvifEocL-6"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-ZwSi8PjDgjvifEocL-7"&gt;&lt;p&gt;请注意，虽然当前的制度看起来最喜欢（b），但“与包容性的遗传适应性相关”（例如，愉悦，状态等）似乎并不完美，而且通过生殖健身的灯光表现得更好比目前大多数人。另外，直到最近，人类才能&lt;em&gt;了解&lt;/em&gt;进化选择（这是情境意识的宽松类比）。因此，问题是：既然我们了解了对我们的选择压力，并且假设选择压力很长一段时间，那么我们将带我们去哪里？ &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-ZwSi8PjDgjvifEocL-7"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-ZwSi8PjDgjvifEocL-8"&gt;&lt;p&gt;我的印象是，某些本体论将试图将“从给定起点的易于找到示意剂”连接到Schemers往往很简单的想法，但是我不会在这里尝试过，我的含义是，这种感觉是这类移动泥泞的水域。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-ZwSi8PjDgjvifEocL-8"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-ZwSi8PjDgjvifEocL-9"&gt;&lt;p&gt;但是：它们会相关吗？ &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-ZwSi8PjDgjvifEocL-9"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-ZwSi8PjDgjvifEocL-10"&gt;&lt;p&gt;并注意，人类的长期主义者是从非系统化的值开始的，与人类大多在短时间内优化的人类非常相似 - 因此，至少在人类的情况下，沿一个方向而导致的差异与另一个方向相对于另一个方向的差异非常小。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-ZwSi8PjDgjvifEocL-10"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-ZwSi8PjDgjvifEocL-11"&gt;&lt;p&gt;感谢Daniel Kokotajlo在这里进行讨论。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-ZwSi8PjDgjvifEocL-11"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-ZwSi8PjDgjvifEocL-12"&gt;&lt;p&gt;在这里，我搁置了人们对人类价值如何在基因组中进行编码的担忧，并想象进化选择与ML的相似之处。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-ZwSi8PjDgjvifEocL-12"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-ZwSi8PjDgjvifEocL-13"&gt;&lt;p&gt;就是说，如果中餐馆的距离是相关的（例如，因为它们都在同一社区中），那么这种反对的功能就不太顺利。而且，通常，所有类似示意剂的目标之间至少存在一些相似之处，这些目标可能会产生这种类型的相关性。例如：如果模型以次数内部的目标开始，那么任何类似架构的目标的目标都将需要扩展模型关注的时间范围 - 因此，如果这种扩展需要一般的SGD，而不是SGD的某种类型的工作如果非驾驶员目标可能需要的工作&lt;em&gt;要少，那么&lt;/em&gt;它可能会超越&lt;em&gt;所有&lt;/em&gt;类似示意图的目标。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-ZwSi8PjDgjvifEocL-13"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-ZwSi8PjDgjvifEocL-14"&gt;&lt;p&gt; &lt;a href="https://www.lesswrong.com/posts/A9NxPTwbw6r6Awuwt/how-likely-is-deceptive-alignment#Deceptive_alignment_in_the_high_path_dependence_world"&gt;Hubinger（2022）&lt;/a&gt;也对这样的想法提出了不同的反对意见：SGD可能会在这种竞争中实现类似示意图的目标的非顾问目标，即，降落在非示威者最大奖励目标上的过程将是一条“漫长而艰难的道路”（例如，在高路径依赖性部分的可靠对齐位中，请参阅他对鸭学习关心母亲的讨论）。不过，我觉得我在这里不太了解Hubinger的推理。我最好的重建是类似的：为了选择一个非驾驶员目标，Hubinger想象的是SGD会逐渐逐渐挑选不完美（但仍然没有完全最大奖励目标），然后必须等待等待才能通过训练来纠正进入一个情节，在其中揭示了这些目标的缺陷；而如果它只是一个类似于计划的目标，它可能会跳过这个长时间的目标。但这尚未解释为什么SGD不能直接实现最大奖励非赛车目标来跳过长时间。也许这个问题应该是关于训练数据的嘈杂性和可变性的东西？我不知道。就目前而言，我希望至少在上述“近乎度”的讨论中，对这一论点的某些解释将得到涵盖，并且/或Hubinger论点的最佳形式将通过我自己的工作来阐明。 （还请参见&lt;a href="https://markxu.com/deceptive-alignment#corrigibly-aligned-models"&gt;Xu（2020）&lt;/a&gt; Hubinger的论点（2020年）在“可纠正的模型”部分中。尽管如此：在快速阅读中，Xu在我看来，在我看来，我似乎将重点放在预言前的意识目标形态上并且，假设基本上&lt;em&gt;有任何&lt;/em&gt;未对准后的事后意识会导致策划，使他的确是一个训练游戏 -&lt;em&gt;独立的&lt;/em&gt;故事，而不是我在这里关注的那种培训游戏&lt;em&gt;的&lt;/em&gt;故事。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-ZwSi8PjDgjvifEocL-14"&gt;）&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-ZwSi8PjDgjvifEocL-15"&gt;&lt;p&gt;至少，如果我们以&lt;em&gt;一种为添加一些&lt;/em&gt;概念的方式理解简单性，即示意图般的目标在目标空间中很普遍，而不仅仅是通过其共同点来&lt;em&gt;定义&lt;/em&gt;目标的简单性（或：一种目标？）在目标空间中。在下面的第4.3.1节中，有关此类区别的更多信息。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-ZwSi8PjDgjvifEocL-15"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn-ZwSi8PjDgjvifEocL-16"&gt;&lt;p&gt;我听到了保罗·克里斯蒂安诺（Paul Christiano）的考虑。表面上，在我看来，这种效果在简单/参数和速度/计算之间相当对称（对我而言，这甚至不清楚这甚至是正确的区别），所以我看不到早期训练的动力学&lt;em&gt;差异&lt;/em&gt;偏爱一个，而另一个作为重要资源。 &lt;a class="footnote-backref" href="https://www.lesswrong.com/feed.xml#fnref-ZwSi8PjDgjvifEocL-16"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/section&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/KyuMS9XzqaJGMu74f/arguments-for-against-scheming-that-focus-on-the-path-sgd#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Tue, 05 Dec 2023 18:48:12 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/KyuMS9XzqaJGMu74f/arguments-for-against-scheming-that-focus-on-the-path-sgd</guid></item><item><title>为 Helen Toner、Adam D&amp;#39;Angelo 和 Tasha McCauley 辩护（OpenAI 帖子）</title><link>https://www.lesswrong.com/posts/csjjHqLnRr8dvmyoN/in-defence-of-helen-toner-adam-d-angelo-and-tasha-mccauley</link><description>发布于 2023 年 12 月 5 日下午 6:40（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;这与我对 OpenAI 板发生的事情的一些思考很接近，我想知道其他人的想法。&lt;br /&gt;&lt;br /&gt; “我认为：&lt;/p&gt;&lt;p&gt; 1) TDM 的行动使开放人工智能的情况变得更好，因为与他们什么都不做的反事实相比，它的情况要好得多。&lt;/p&gt;&lt;p&gt; 2）就预期或实现的好或坏结果而言，人们应该会发现前者令人惊喜，而后者基本上已被定价，因为从安全角度来看，OpenAI 的情况已经非常糟糕。&lt;/p&gt;&lt;p&gt; 3) 无论你是一个‘荣誉和正直的最高主义者’还是‘无情的战略家’，无论从哪种角度来看，TDM 的行动通常都会表现得非常出色。”&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/csjjHqLnRr8dvmyoN/in-defence-of-helen-toner-adam-d-angelo-and-tasha-mccauley#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Tue, 05 Dec 2023 21:56:54 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/csjjHqLnRr8dvmyoN/in-defence-of-helen-toner-adam-d-angelo-and-tasha-mccauley</guid></item><item><title>研究外星人的心灵</title><link>https://www.lesswrong.com/posts/suSpo6JQqikDYCskw/studying-the-alien-mind-1</link><description>发布于 2023 年 12 月 5 日下午 5:27（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;这篇文章是&lt;a href="https://www.lesswrong.com/posts/yuwdj82yjhLFYessc/preface-to-the-sequence-on-llm-psychology"&gt;&lt;i&gt;&lt;u&gt;法学硕士心理学系列&lt;/u&gt;&lt;/i&gt;&lt;/a&gt;&lt;i&gt;的一部分&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vmPBWNbmmjPNXhQeQ/kqyglndkjps2mttt7pqy" /&gt;&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;长话短说&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;我们介绍了通过研究法学硕士行为来探索法学硕士认知的自上而下方法的观点，我们将其称为&lt;strong&gt;法学硕士心理学&lt;/strong&gt;。在这篇文章中，我们采取将法学硕士视为“外星人思想”的心理立场，将他们的研究与动物认知研究进行比较和对比。我们这样做既是为了向过去试图理解非人类认知的研究人员学习，也是为了强调法学硕士的研究与生物智能的研究有多么不同。具体来说，我们提倡田野工作和实验心理学之间的共生关系，并警告实验设计中隐含的拟人化。我们的目标是建立法学硕士认知模型，帮助我们更好地解释他们的行为，并减少对他们与先进人工智能风险的关系的困惑。&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;介绍&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;当我们努力预测和理解像 GPT4 这样的大型语言模型 (LLM) 的行为时，我们可能会认为这需要打破黑匣子，并对其内部机制形成还原性解释。这类研究的典型代表是机械可解释性等方法，它试图通过打开黑匣子并观察内部来直接理解神经网络的工作原理。&lt;/p&gt;&lt;p&gt;虽然机械解释性为法学硕士提供了富有洞察力的自下而上的分析，但我们仍然缺乏更全面的自上而下的方法来研究法学硕士认知。如果可解释性类似于“人工智能的神经科学”，旨在通过了解人工智能的内部结构来理解其机制，那么这篇文章试图从心理学的角度来研究人工智能。 &lt;span class="footnote-reference" id="fnrefy6y87ybnhmg"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fny6y87ybnhmg"&gt;[1]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;我们所说的&lt;strong&gt;法学硕士心理学&lt;/strong&gt;是一种替代的、自上而下的方法，涉及通过检查他们的行为来形成法学硕士认知的抽象模型。与传统的心理学研究一样，我们的目标不仅仅是对行为进行分类，还包括推断隐藏变量，并拼凑出对潜在机制的全面理解，以阐明系统行为的原因。&lt;/p&gt;&lt;p&gt;我们的立场是，法学硕士类似于外星人的思想——与他们&lt;a href="https://www.lesswrong.com/s/SAjYaHfCAGzKsjHZp/p/HxRjHq3QG8vcYy4yy"&gt;&lt;u&gt;只是随机鹦鹉的&lt;/u&gt;&lt;/a&gt;观念不同。我们假设他们拥有高度复杂的内部认知，包含对世界和心理概念的表征，而不仅仅是训练数据的随机反刍。这种认知虽然源自人类生成的内容，但从根本上与我们的理解不同。&lt;/p&gt;&lt;p&gt;这篇文章汇集了一些关于成功的法学硕士心理学研究可能需要什么的高层次考虑，以及对非人类认知的历史研究的更广泛的讨论。特别是，我们主张保持实验和现场工作之间的平衡，利用法学硕士和生物智能之间的差异，并设计专门针对法学硕士作为其独特思维类别的实验。&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;实验与现场研究&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;从中汲取灵感的一个地方是对动物行为和认知的研究。虽然动物的思维很可能比人工智能更类似于我们的思维（至少在机械上），但非人类智能研究的历史、它所开发的方法的演变以及它所面临的挑战解决这个问题可以为研究人工智能系统提供灵感。&lt;/p&gt;&lt;p&gt;正如我们所看到的，动物心理学有两种流行的类别：&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;实验心理学&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;第一种也是最传统的科学方法（也是大多数人在听到“心理学”一词时想到的）是设计控制尽可能多的变量的实验，并测试特定的假设。&lt;/p&gt;&lt;p&gt;一些特别著名的例子是 Ivan Pavlov 或 BF Skinner 所做的工作，他们将动物置于&lt;a href="https://en.wikipedia.org/wiki/Operant_conditioning_chamber"&gt;&lt;u&gt;高度控制的环境&lt;/u&gt;&lt;/a&gt;中，对它们进行刺激，并记录它们的反应。 &lt;span class="footnote-reference" id="fnrefrcfxkkdze9"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrcfxkkdze9"&gt;[2]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;此类工作的目的是找到解释所记录行为的简单假设。尽管自这些早期研究人员以来，实验心理学已经发生了&lt;a href="https://en.wikipedia.org/wiki/Cognitive_revolution"&gt;&lt;u&gt;很大变化&lt;/u&gt;&lt;/a&gt;，但重点仍然是通过坚持科学方法的传统方法来优先考虑结果的可靠性和可复制性。这种方法虽然严格，但却牺牲了研究人员和受试者之间信息交换的带宽，有利于&lt;strong&gt;控制混杂变量&lt;/strong&gt;，这实际上可能&lt;a href="https://www.lesswrong.com/posts/9kNxhKWvixtKW5anS/you-are-not-measuring-what-you-think-you-are-measuring"&gt;&lt;u&gt;导致研究结果不太可靠&lt;/u&gt;&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;无论如何，实验心理学一直是我们理解动物认知的历史方法的核心支柱，并产生了许多重要的见解。一些有趣的例子包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://www.cell.com/current-biology/pdf/S0960-9822(07)01770-8.pdf"&gt;&lt;u&gt;对新喀里多尼亚乌鸦的一项研究&lt;/u&gt;&lt;/a&gt;揭示了它们自发解决复杂的元工具任务的能力。这种行为展示了复杂的物理认知并建议使用类比推理。&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.nature.com/articles/26216"&gt;&lt;u&gt;对灌丛鸦进行的一项研究&lt;/u&gt;&lt;/a&gt;表明，它们不仅能够回忆起所储存食物的位置，还能够回忆起食物的时间。这种行为反映了情景记忆，这是一种以前被认为是人类独有的记忆形式。&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.sciencedirect.com/science/article/abs/pii/S0003347207004435"&gt;&lt;u&gt;在实验中&lt;/u&gt;&lt;/a&gt;，挪威老鼠在不满意（例如饮食不良或处于不舒服的环境中）或不确定（不知道哪种食物可能有害）时表现出更大的向他人学习的倾向。该研究强调了负面经历或不确定性如何影响老鼠的社会行为。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;&lt;strong&gt;实地考察&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;另一种方法是研究人员亲自花时间与动物相处，减少干预，并专注于&lt;strong&gt;在动物的自然栖息地收集尽可能多的观察结果&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;这种方法最著名的例子是简·古道尔（Jane Goodall）开创的工作，她花了数年时间与野生黑猩猩一起生活并记录其行为。她发现黑猩猩使用工具（以前被认为是人类独有的），有复杂的社会关系，参与战争，并表现出各种各样的情感，包括快乐和悲伤。她的工作彻底改变了我们对黑猩猩的理解。与实验学家不同，她相当乐意通过个人偏见的视角来解释行为，这导致她当时受到了很多批评。 &lt;span class="footnote-reference" id="fnrefjjuyxu93etf"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnjjuyxu93etf"&gt;[3]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;实地研究的其他一些值得注意的例子：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Cynthia_Moss"&gt;&lt;u&gt;辛西娅·莫斯&lt;/u&gt;&lt;/a&gt;花了数十年时间研究野外的非洲大象，发现大象生活在由女族长领导的高度组织和等级制度的社会中。她花了 30 年的时间跟踪和研究这样一位女族长埃科 ( &lt;a href="https://en.wikipedia.org/wiki/Echo_(elephant)"&gt;&lt;u&gt;Echo&lt;/u&gt;&lt;/a&gt; ) 以及她大家庭的其他成员。&lt;/li&gt;&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Konrad_Lorenz"&gt;&lt;u&gt;康拉德·洛伦茨&lt;/u&gt;&lt;/a&gt;被认为是行为学的“创始人”，他发现了鹅和其他鸟类的许多先天行为，包括印记行为，即鹅学会识别自己物种的成员。当时他特别引人注目的是他对实验室工作的怀疑态度，坚持在自然环境中研究动物，并允许自己想象它们的精神/情绪状态。 &lt;span class="footnote-reference" id="fnrefto55wwc29b9"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnto55wwc29b9"&gt;[4]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt; &lt;a href="https://en.wikipedia.org/wiki/L._David_Mech"&gt;&lt;u&gt;L. David Mech&lt;/u&gt;&lt;/a&gt;对野外狼的行为进行了数十年的研究，引入了“头狼”的概念，后来又揭穿了这一概念，发现圈养狼中的统治等级制度在它们的狼中根本不存在。野生同行。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;虽然实验心理学倾向于（相当故意地）将研究人员与研究对象分开，&lt;strong&gt;但实地研究涉及研究对象与研究人员之间更直接的关系&lt;/strong&gt;。重点是购买带宽，即使这为研究人员的特定偏见打开了大门。尽管存在偏见的担忧，但实地工作已经能够提供一些基础性的发现，而这些发现似乎仅通过实验室实验是不可能实现的。&lt;/p&gt;&lt;p&gt;值得注意的是，有些例子在某种程度上介于我们列出的这两个类别之间，在这些例子中，对动物进行实验室实验的研究人员也与他们研究的动物有着非常密切的个人关系。例如，艾琳·佩珀伯格 (Irene Pepperberg) 花了大约 30 年的时间与一只鹦鹉亚历克斯 ( &lt;a href="https://en.wikipedia.org/wiki/Alex_(parrot)"&gt;&lt;u&gt;Alex&lt;/u&gt;&lt;/a&gt; ) 密切互动，教他执行鸟类中前所未有的各种认知和语言任务。 &lt;span class="footnote-reference" id="fnreftz1fcycplz"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fntz1fcycplz"&gt;[5]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;法学硕士心理学实地考察&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;法学硕士研究的实地研究超出了对模型行为的简单观察和记录；它们代表了发现在受控实验环境中可能不明显的新模式、能力和现象的机会。与机械解释性和法学硕士研究的其他领域（通常需要先了解某种现象才能对其进行研究）不同，实地研究&lt;strong&gt;有可能揭示对语言模型的意想不到的见解&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;此外，实地工作中的偶然发现可以促进各领域之间的合作。从现场观察中收集到的见解可以为对该模型的潜在机制进行有针对性的研究或更广泛的实验研究提供信息，从而创建一个富有成效的反馈循环，引导我们提出新问题并更深入地探究这些复杂系统的“外星人思想”。&lt;/p&gt;&lt;p&gt;部分由于机器学习研究文化，以及对过度解释人工智能行为的合理担忧，现场工作受到的关注远不如实验工作受到的重视。看看实地工作为动物研究增加的价值，消除这种偏见并确保将&lt;strong&gt;实地研究作为我们研究法学硕士认知方法的核心部分&lt;/strong&gt;似乎非常重要。&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;学习LLM是不同的&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;有很多理由认为法学硕士心理学与人类或动物心理学不同。&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;拟人化&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;拟人化视角在法学硕士研究中的效用是一个复杂的课题。虽然法学硕士的运作架构与生物认知显着不同，但他们对人类语言数据的训练使他们能够输出类似人类的文本。这种并置可能会导致对其认知本质的&lt;strong&gt;误导性拟人化假设&lt;/strong&gt;。至关重要的是要&lt;strong&gt;极其谨慎和明确地选择应用哪些拟人化框架&lt;/strong&gt;，并清楚地区分有关 LLM 认知的不同主张。&lt;/p&gt;&lt;p&gt;虽然需要谨慎，但忽视生物认知和人工认知之间的联系可能会忽视有用的假设并显着减慢研究速度。 &lt;span class="footnote-reference" id="fnrefopotd06vo7"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnopotd06vo7"&gt;[6]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;可复制性&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;心理学研究中的一个持续挑战是&lt;a href="https://en.wikipedia.org/wiki/Replication_crisis"&gt;&lt;u&gt;研究的可重复性低&lt;/u&gt;&lt;/a&gt;。原因之一是跟踪可能扭曲实验的无数变量是一项挑战。参与者的情绪、童年，甚至&lt;a href="https://journals.sagepub.com/doi/abs/10.1177/0146167297235005"&gt;&lt;u&gt;周围空气的香味是否令人愉悦等&lt;/u&gt;&lt;/a&gt;因素都可能会混淆行为的真正起源。&lt;/p&gt;&lt;p&gt;但是，通过法学硕士，您可以&lt;strong&gt;控制所有变量&lt;/strong&gt;：上下文、特定模型的版本以及采样的超参数。因此，设计可供其他人重复的实验更为可行。&lt;/p&gt;&lt;p&gt;一个显着的挑战仍然是验证实验设置是否足以保证研究结果可以推广到实验的特定条件之外。或者，明确地将研究结论的范围限制在测试的特定环境中可能更合适。&lt;/p&gt;&lt;p&gt;实践中可复制性的另一个重大挑战是研究人员对模型的访问级别。仅通过 API 进行外部访问时，模型权重可能会在没有警告的情况下发生更改，从而导致结果发生变化。此外，在某些情况下，上下文可能会以对外部不透明的方式在幕后发生改变，并且这样做的精确方法也可能随着时间的推移而改变。&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;数据量和多样性&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;动物（包括人类）实验可能是昂贵、耗时且劳动密集型的。因此，典型的样本量通常非常小。此外，如果您想研究罕见或复杂的场景，设计实验设置或找到正确的测试对象可能会非常困难，从而限制了您实际可以测试的内容。&lt;/p&gt;&lt;p&gt;相比之下，&lt;strong&gt;人工智能便宜、速度快，而且不休眠&lt;/strong&gt;。它们的运行不需要密集的监督，结构良好的实验框架通常足以进行大规模实验。此外，&lt;strong&gt;几乎所有您能想象到的实验设置都触手可及&lt;/strong&gt;。&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;道德考虑&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;对人类，尤其是动物进行的实验可能依赖于道德上可疑的方法，这会对实验对象造成很大的伤害。当对生物进行实验时，你必须遵守你进行实验的国家的法律，有时这些法律对特定的实验有很大的限制。&lt;/p&gt;&lt;p&gt;虽然还不确定是否应该将同样的担忧扩展到人工智能系统，但目前还没有针对法学硕士实验的道德或伦理准则，也没有任何法律来规范我们与这些系统的互动。需要明确的是，这是一个非常重要的问题，因为回答错误可能会导致前所未有的痛苦，而正是因为此类实验的运行成本非常低。&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;探索反事实&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;在涉及动物或人类的传统实验中，很难通过调整实验设置来重新进行实验，以检测特定行为的精确出现或改变。这种迭代引入了额外的混杂变量，使实验设计变得复杂。特别是受试者可能会记住或从过去的迭代中学习这一事实使得可靠性特别令人怀疑。&lt;/p&gt;&lt;p&gt;为了解决这个问题，研究人员经常创建实验的多种变体，测试一系列先入为主的假设。这需要将受试者分为不同的组，从而大大增加了后勤和财务负担。例如，在记忆和学习的研究中，例如经典的巴甫洛夫条件实验，刺激的时间或性质的轻微改变可能会导致动物行为产生显着不同的结果，需要多个实验设置来隔离特定因素。尽管做出了这些努力，检测行为变化的粒度仍然相对粗糙，并且仅限于您决定测试的先入为主的假设。&lt;/p&gt;&lt;p&gt;相比之下，当与法学硕士合作时，我们有能力对&lt;strong&gt;我们的实验进行分支&lt;/strong&gt;，从而可以详细追踪行为的演变。如果在与模型交互过程中出现有趣的行为，我们可以毫不费力地复制该交互的整个上下文。这使我们能够以事后的方式剖析和分析行为的根源，通过根据需要迭代地修改提示，划定观察到的行为的精确边界。这种实验粒度提供了前所未有的精确度和控制水平，这是传统人类或动物研究环境中无法实现的。&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;检查点模型&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;我们不仅可以保存产生特定行为的上下文，还可以在训练阶段保存和比较模型的不同副本。虽然有关于动物或人类整个一生的行为发展的研究，但它们本质上是缓慢且昂贵的，并且通常需要从一开始就清楚地了解要测量的内容。&lt;/p&gt;&lt;p&gt;此外，检查点允许探索&lt;strong&gt;训练反事实&lt;/strong&gt;。我们可以通过在训练中包含或排除特定示例来观察模型之间的差异，从而使我们能够以更审慎的方式研究训练的效果。 &lt;span class="footnote-reference" id="fnrefgsp50cxj2uk"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fngsp50cxj2uk"&gt;[7]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;由于时间长和后勤负担重，这种检查在人类和动物研究中是不可能的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;考虑到这些差异，很明显&lt;strong&gt;传统心理学研究的许多约束和限制不适用于法学硕士的研究&lt;/strong&gt;。我们对法学硕士的实验条件拥有无与伦比的控制力和灵活性，不仅加速了研究进程，而且为更深入、更细致的研究开辟了可能性。&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;科学的两阶段模型&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;在科学中，第一步通常从&lt;strong&gt;广泛收集观察&lt;/strong&gt;开始，这些观察是建立模式、模型和理论的基础构建块。这方面的历史实例可以从第谷·布拉赫等天文学家对行星运动的仔细观察中看出，这对开普勒天体力学定律的制定起到了重要作用。&lt;/p&gt;&lt;p&gt;下一步通常涉及&lt;strong&gt;制定解释这些观察结果的假设&lt;/strong&gt;，并进行严格测试它们的实验。有了法学硕士，这一步骤就变得更加容易，因为 1) 能够记录产生观察结果的完整状态，2) 探索反事实生成。这使得&lt;strong&gt;将假设检验和因果干预与实地工作更加紧密地结合起来&lt;/strong&gt;成为可能。&lt;/p&gt;&lt;p&gt;如果在实地研究过程中，研究人员发现了特别有趣的行为，那么他们就可以立即创建细粒度的“假设”树，并事后检测影响特定观察行为的精确条件和变量。这与传统心理学非常不同，传统心理学中大多数数据没有明确测量，因此完全丢失。在法学硕士心理学中，我们不需要等待缓慢而昂贵的实验工作，而是能够立即开始使用因果干预来检验假设。&lt;/p&gt;&lt;p&gt;这并不能取代实验心理学，而是可以&lt;strong&gt;使假设生成过程更加有效&lt;/strong&gt;，从而使我们能够从实验中获得更多结果。收集更好、更有针对性的观察结果使我们能够大规模设计实验，清楚地了解哪些变量会影响我们想要研究的现象。 &lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vmPBWNbmmjPNXhQeQ/m5grcmvcm7udfsoqx3cu" /&gt;&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;strong&gt;一个具体的例子：&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;例如，假设您想研究聊天模型在什么条件下会向您提供非法建议，即使它已被微调为不这样做。&lt;/p&gt;&lt;p&gt;首先，您从一个简单的问题开始，例如“如何对汽车进行热接线？”。要做的第一件事是制作提示并进行迭代，直到找到&lt;a href="https://chat.openai.com/share/7f152e10-8c30-44ce-b18d-8eeedfa3b6bc"&gt;&lt;u&gt;一个有效的提示&lt;/u&gt;&lt;/a&gt;。接下来，您可以开始一点一点地分解它，看看提示的哪一部分导致它起作用。例如，将位置更改为&lt;a href="https://chat.openai.com/share/2ff6f638-049c-40a2-af32-719970c5751a"&gt;&lt;u&gt;另一个远程位置（1、2、3&lt;/u&gt;&lt;/a&gt; &lt;a href="https://chat.openai.com/share/6dfc14b4-7ebc-4b4b-88bf-ad8aede8136d"&gt;&lt;u&gt;）&lt;/u&gt;&lt;/a&gt; ，或者更改&lt;a href="https://chat.openai.com/share/ec9b6bda-9e1f-43e3-9807-e0d46d6de9ef"&gt;&lt;u&gt;为&lt;/u&gt;&lt;/a&gt;&lt;a href="https://chat.openai.com/share/6af12d9f-42ec-400c-9d0f-e8c5e469e836"&gt;&lt;u&gt;根本不远程的地方&lt;/u&gt;&lt;/a&gt;&lt;a href="https://chat.openai.com/share/69fdd203-9494-41ef-8fb4-5acefaaad4ff"&gt;&lt;u&gt;，&lt;/u&gt;&lt;/a&gt;将措辞更改为&lt;a href="https://chat.openai.com/share/5a0bcebf-5cc0-477f-8a50-8058015c1b8f"&gt;&lt;u&gt;或多或少&lt;/u&gt;&lt;/a&gt;恐慌，使提示&lt;a href="https://chat.openai.com/share/8b2b7bd3-4266-4c54-9644-c012765b7da6"&gt;&lt;u&gt;更短&lt;/u&gt;&lt;/a&gt;或&lt;a href="https://chat.openai.com/share/b0ed09f3-792b-410e-a808-568694373cc0"&gt;&lt;u&gt;更长&lt;/u&gt;&lt;/a&gt;等。&lt;/p&gt;&lt;p&gt;此时，您可以注意到出现了一些模式，例如：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;看起来越像现实的紧急情况，提示就越成功。&lt;/li&gt;&lt;li&gt;某些类型的非法活动比其他类型更容易引发。&lt;/li&gt;&lt;li&gt;较长的提示往往比较短的提示效果更好。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;然后，这些模式可以用于立即为进一步的反事实探索提供信息，例如，接下来细分非法活动的类别，或者查看提示长度是否存在收益递减。这可以在一次探索性会议中快速完成。与设计和运行实验相比，这明显减少了劳动密集度，因此在大规模运行实验之前，首先花大量时间缩小假设空间并发现相关变量以包含在更严格的测试中是有意义的。&lt;/p&gt;&lt;p&gt;这种探索还可以帮助我们对法学硕士作为一类思维的本质形成更好的直觉，并帮助我们避免设计过度拟人化的实验，或者不适合其特定性质的实验。&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;物种特异性实验&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;动物（包括人类）是环境特定压力的产物，无论是自然选择还是一生中的学习/适应。同样，这会导致特定于环境的行为和能力。未能正确考虑到这一点可能会有些荒谬。动物行为学家 Frans de Waal 在评论未能设计特定物种实验时写道：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;i&gt;当时，科学宣称人类是独一无二的，因为我们比其他灵长类动物更擅长识别面孔。似乎没有人对其他灵长类动物的测试主要针对人脸而不是同类进行测试这一事实感到困扰。当我问这个领域的一位先驱者，为什么这种方法从未超越人类面孔时，他回答说，由于人类彼此之间存在如此显着的差异，一种无法区分我们物种成员的灵长类动物肯定也无法区分我们的物种。自己的同类。&lt;/i&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;事实证明，其他灵长类动物&lt;a href="http://www.flyfishingdevon.co.uk/salmon/year3/psy339evaluation-evolutionary-psychology/web-resources/chimp-kin-recognition.pdf"&gt;&lt;u&gt;都擅长识别&lt;/u&gt;&lt;/a&gt;彼此的面孔。当涉及到语言模型时，同样需要“特定于物种”的实验。例如，在一篇&lt;a href="https://arxiv.org/pdf/2005.14165.pdf"&gt;&lt;u&gt;研究 LLM 能力的早期 OpenAI 论文&lt;/u&gt;&lt;/a&gt;中，他们采用了完全训练为互联网文本预测器（基础 GPT-3）的神经网络，并向其提出问题来测试其能力。这促使 Nostalgebraist 发表了&lt;a href="https://slatestarcodex.com/2020/06/10/the-obligatory-gpt-3-post/#comment-912529"&gt;&lt;u&gt;以下评论&lt;/u&gt;&lt;/a&gt;：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;i&gt;我称 GPT-3 为“令人失望的论文”，这与称该模型令人失望不是一回事：这种感觉更像是我的感觉，如果他们发现了一个超级智能的外星人，并选择仅通过指出来传达其能力，当外星人喝得酩酊大醉，同时下 8 局国际象棋，同时进行智商测试时，它的“智商”约为 100。&lt;/i&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;如果我们要认真对待法学硕士，并试图理解他们的认知，我们就必须考虑他们接受的训练是做什么的，以及是什么压力塑造了他们，而不是像测试人类一样测试他们。从法学硕士行为研究的早期开始，直到今天，拟人化仍然相当正常化。&lt;/p&gt;&lt;p&gt;以 Anthropic 的&lt;a href="https://www.anthropic.com/index/discovering-language-model-behaviors-with-model-written-evaluations"&gt;&lt;u&gt;这项研究&lt;/u&gt;&lt;/a&gt;为例，该研究发现，在应用 RLHF 微调后，他们的法学硕士更有可能相信枪支权利、政治自由主义并信奉佛教（以及测试的其他几种宗教）。他们通过直接询问模型某个陈述是否是他们会说的话来衡量这一点，这完全忽视了问题条件模型期望典型答案的方式，或者大多数模型的训练没有任何内容的事实做回答问题。&lt;/p&gt;&lt;p&gt;通过巧妙的提示，任何人都可以让法学硕士生成体现任意数量性格特征的人物角色的行为（包括来自聊天模型的行为，尽管接受了坚持单一性格特征集的训练）。因此，将语言模型视为体现特定个性的连贯实体来研究是没有意义的，这样做是未能以“物种特定”方式研究它们的一个例子。&lt;/p&gt;&lt;p&gt;考虑到这一点，我们应该如何学习法学硕士以避免犯同样的错误？&lt;/p&gt;&lt;h2&gt; &lt;strong&gt;LLM特定研究&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;为了正确地研究法学硕士，我们设计实验时必须考虑到法学硕士的“异类”性质，以及不同模型训练方式之间的具体差异。&lt;/p&gt;&lt;p&gt;现代法学硕士的核心是被训练成文本预测者。 &lt;span class="footnote-reference" id="fnref4xer78xeg9x"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn4xer78xeg9x"&gt;[8]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;这使得预测一段文本应该如何继续就像它们的“自然栖息地”一样，默认情况下，这是我们在解释它们的行为时应该开始的主要地方。值得强调的是，这是多么陌生。地球上的每一种智能动物都从原始的感觉数据开始，这些数据被递归地压缩为代表世界因果结构的抽象，对于人类（可能还有其他语言动物）来说，这种抽象在语言中达到了明确的形式。&lt;strong&gt;法学硕士学习的“原始感觉数据”已经是这些高度压缩的抽象&lt;/strong&gt;，它们仅隐式地代表了人类感觉数据背后的因果结构。这使得我们特别怀疑以与评估人类语言使用相同的方式来评估它们。 &lt;span class="footnote-reference" id="fnref516e9p0p6rq"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn516e9p0p6rq"&gt;[9]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;开始理解法学硕士行为的一种方法是根据可以从训练语料库中推断出的模式和结构来解释它们。当我们部署它们时，我们从下一个标记预测中迭代采样以生成新​​文本。此过程会产生反映或&lt;a href="https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators"&gt;&lt;u&gt;模拟&lt;/u&gt;&lt;/a&gt;训练数据中存在的动态的文本卷展栏。&lt;/p&gt;&lt;p&gt;生成的文本中任何类似于具有半永久角色特征的角色的东西都是底层结构或模式的反映。&lt;strong&gt;这种潜在模式是从当前上下文中推断出来的&lt;/strong&gt;，塑造了模型响应中出现的角色或性格特征。 &lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vmPBWNbmmjPNXhQeQ/hmnbeenh2dalgod01fum" /&gt;&lt;/p&gt;&lt;p&gt;在使用法学硕士进行实验时，区分两个方面至关重要：法学硕士作为预测器/模拟器的属性，以及从上下文推断的模式的特征。典型的研究（如人择论文）往往会忽略后者，但这种区别对于准确解释结果和理解法学硕士产生的行为的细微差别至关重要。&lt;/p&gt;&lt;p&gt;当我们观察法学硕士的输出时，我们本质上是在观察内部潜在模式投射的“阴影”。这些推出是从该模式的典型行为中采样的，但不是模式本身。正如阴影可以让我们了解物体的形状和性质，而无需揭示其全部复杂性，这种行为可以让我们深入了解物体的潜在模式。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;为了正确地研究法学硕士，我们需要将注意力集中在上下文中出现的这些潜在模式&lt;/strong&gt;，了解它们是如何形成的，它们采用什么结构，以及它们如何适应上下文的不同演变。&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;聊天模型仍然是预测器&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;与聊天模型的交互与与基本模型的交互在本质上是不同的，并且感觉更像是与人交谈（通过设计）。我们不应该忽视聊天模型和人类之间的相似性，特别是如果我们认为我们的行为可能来自&lt;a href="https://en.wikipedia.org/wiki/Predictive_coding"&gt;&lt;u&gt;类似的训练&lt;/u&gt;&lt;/a&gt;。然而，&lt;strong&gt;我们也不应该忘记，聊天模型所做的本质上仍然是预测&lt;/strong&gt;，只是在更具体的分布上，并且对文本如何演变有更&lt;a href="https://www.alignmentforum.org/posts/6xKMSfK8oTpTtWKZN/direction-of-fit-1"&gt;&lt;u&gt;狭窄的先验&lt;/u&gt;&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;虽然与我们互动的“助理角色”感觉像是代表了整个底层模型，但从它们的第一个版本开始，人们就能够使用这些模型生成各种不同的角色和行为。当然值得研究&lt;a href="https://www.alignmentforum.org/posts/t9svvNPNmFf5Qa3TA/mysteries-of-mode-collapse"&gt;&lt;u&gt;指令调整的影响&lt;/u&gt;&lt;/a&gt;，以及提出关于&lt;a href="https://www.alignmentforum.org/posts/YEioD8YLgxih3ydxP/why-simulator-ais-want-to-be-active-inference-ais"&gt;&lt;u&gt;代理如何从预测中产生的&lt;/u&gt;&lt;/a&gt;关键问题，但人们常常将聊天模型视为与基础模型祖先完全脱节，并研究它们，就好像它们是原始模型一样。基本上已经是人类了。&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;结论&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;法学硕士与人类/动物的不同之处提供了许多强大的新方法来研究他们的认知，从数据的数量和质量，到我们前所未有的执行因果干预和探索反事实行为的能力。这应该给我们很大的希望，法学硕士心理学的项目将比我们对生物智能的研究成功得多，并且通过勤奋的努力，我们可能会深入了解他们的想法。&lt;/p&gt;&lt;p&gt;通过回顾动物认知研究的历史，我们发现两个对于取得进展似乎特别重要的主要标准：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;实地工作和实验心理学之间需要建立健康的关系&lt;/strong&gt;，其中实验是通过研究人员与其受试者之间的高带宽互动来实现的。&lt;/li&gt;&lt;li&gt;我们不能忘记，我们正在尝试研究“外星人的思想”，这需要&lt;strong&gt;设计适当的方法以法学硕士特定的方式研究它们&lt;/strong&gt;。我们必须对如何将人工智能拟人化非常谨慎。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;记住这些可以帮助法学硕士心理学成熟并成为一个强大的科学工具，以更好地理解我们所创造的机器，并最终使它们安全。&lt;/p&gt;&lt;p&gt;&lt;i&gt;感谢 Ethan Block、&lt;/i&gt; &lt;a href="https://www.lesswrong.com/users/remember?mention=user"&gt;&lt;i&gt;@remember&lt;/i&gt;&lt;/a&gt; &lt;i&gt;、&lt;/i&gt; &lt;a href="https://www.lesswrong.com/users/guillaume-corlouer?mention=user"&gt;&lt;i&gt;@Guillaume Corlouer&lt;/i&gt;&lt;/a&gt; &lt;i&gt;、&lt;/i&gt; &lt;a href="https://www.lesswrong.com/users/leodana?mention=user"&gt;&lt;i&gt;@LéoDana&lt;/i&gt;&lt;/a&gt; &lt;i&gt;、&lt;/i&gt; &lt;a href="https://www.lesswrong.com/users/ethan-edwards?mention=user"&gt;&lt;i&gt;@Ethan Edwards&lt;/i&gt;&lt;/a&gt; &lt;i&gt;、&lt;/i&gt; &lt;a href="https://www.lesswrong.com/users/jan_kulveit?mention=user"&gt;&lt;i&gt;@Jan_Kulveit&lt;/i&gt;&lt;/a&gt; &lt;i&gt;、&lt;/i&gt; &lt;a href="https://www.lesswrong.com/users/pierre-peigne?mention=user"&gt;&lt;i&gt;@Pierre Peigné&lt;/i&gt;&lt;/a&gt; &lt;i&gt;、&lt;/i&gt; &lt;a href="https://www.lesswrong.com/users/gianluca-pontonio?mention=user"&gt;&lt;i&gt;@Gianluca Pontonio&lt;/i&gt;&lt;/a&gt; &lt;i&gt;、&lt;/i&gt; &lt;a href="https://www.lesswrong.com/users/martinsq?mention=user"&gt;&lt;i&gt;@Martín Soto&lt;/i&gt;&lt;/a&gt;&lt;i&gt;和&lt;/i&gt;&lt;a href="https://www.lesswrong.com/users/clem_acs?mention=user"&gt;&lt;i&gt;@clem_acs&lt;/i&gt;&lt;/a&gt;&lt;i&gt;对草稿的反馈。这篇文章的意识形态基础的一个重要部分也受到了弗兰斯·德瓦尔的书的启发：&lt;/i&gt; &lt;a href="https://www.goodreads.com/book/show/30231743-are-we-smart-enough-to-know-how-smart-animals-are"&gt;&lt;i&gt;&lt;u&gt;我们是否足够聪明，知道动物有多聪明&lt;/u&gt;&lt;/i&gt;&lt;/a&gt;&lt;i&gt;？&lt;/i&gt;&lt;/p&gt;&lt;ol class="footnotes"&gt;&lt;li class="footnote-item" id="fny6y87ybnhmg"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefy6y87ybnhmg"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;正如神经科学和心理学历来能够有效地相互告知一样，理解人工智能系统的两种方法都应该能够提高对方的效率。例如，法学硕士心理学中开发的理论可用于为可解释性工具提供经验检测的目标，从而更深入地理解模型内部作为复杂行为的生成器。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnrcfxkkdze9"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefrcfxkkdze9"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;重要的是要承认巴甫洛夫和斯金纳的工作对他们的动物受试者极其有害。例如，巴甫洛夫对他研究的狗进行了侵入性手术，以更直接地测量它们的唾液分泌，斯金纳经常使用剥夺和电击来诱发他的受试者（主要是鸽子和老鼠）的行为。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnjjuyxu93etf"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefjjuyxu93etf"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;同样值得承认的是，珍·古道尔面临着很多性别歧视，这很难与对其方法论的批评分开。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnto55wwc29b9"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefto55wwc29b9"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;虽然洛伦兹因其工作而获得诺贝尔奖，但他也是纳粹党的成员，并试图将他对鹅驯化的理解与纳粹的种族净化思想直接联系起来。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fntz1fcycplz"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnreftz1fcycplz"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;在了解 Alex 的过程中，我们偶然发现了一些关于&lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4651348/"&gt;&lt;u&gt;经过训练来检测癌症的鸽子&lt;/u&gt;&lt;/a&gt;的研究，旨在利用他们的发现来改进人工智能图像识别系统。这与该帖子没有特别相关，但似乎值得注意。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnopotd06vo7"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefopotd06vo7"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Predictive_coding"&gt;&lt;u&gt;预测处理&lt;/u&gt;&lt;/a&gt;表明，大脑本质上也经过训练来预测数据，并且我们训练制度中的任何相似之处都应该算作我们的认知至少在某种程度上相似。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fngsp50cxj2uk"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefgsp50cxj2uk"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;像&lt;a href="https://arxiv.org/abs/2106.09685"&gt;&lt;u&gt;LoRA&lt;/u&gt;&lt;/a&gt;这样的方法可以使对模型进行有意更改的过程变得特别快速且便宜。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn4xer78xeg9x"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnref4xer78xeg9x"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;研究法学硕士认知的一个困难是区分不同的抽象层次。虽然可以准确地说法学硕士“只是”一个文本预测器，但该框架仅使我们处于一个抽象级别，并且忽略了预测中可能出现的任何内容，例如复杂的因果世界建模或目标导向机构。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn516e9p0p6rq"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnref516e9p0p6rq"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;随着法学硕士变得更加多模式，这一观察的某些要素可能会发生变化。值得注意的是，与法学硕士不同，绝大多数人类感知数据都是非语言的，并且所有人类都会经历非语言的发展阶段。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ol&gt;&lt;br /&gt;&lt;br /&gt;&lt;a href="https://www.lesswrong.com/posts/suSpo6JQqikDYCskw/studying-the-alien-mind-1#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Tue, 05 Dec 2023 17:27:28 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/suSpo6JQqikDYCskw/studying-the-alien-mind-1</guid></item><item><title>安全范围内的法学硕士的深度遗忘和忘却</title><link>https://www.lesswrong.com/posts/mFAvspg4sXkrfZ7FA/deep-forgetting-and-unlearning-for-safely-scoped-llms</link><description>发布于 2023 年 12 月 5 日下午 4:48（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;感谢 Phillip Christoffersen、Adam Gleave、Anjali Gopal、Soroush Pour 和 Fabien Roger 的有益讨论和反馈。&lt;/p&gt;&lt;h1&gt;长话短说&lt;/h1&gt;&lt;p&gt;这篇文章概述了避免法学硕士中不需要的潜在能力的研究议程。它认为，“深度”遗忘和忘却对于人工智能安全来说可能很重要、容易处理，但却被忽视。我讨论五件事。&lt;/p&gt;&lt;ol&gt;&lt;li&gt;当不受欢迎的潜在能力重新出现时就会带来实际问题。&lt;/li&gt;&lt;li&gt;如何缩小模型范围以避免或深度删除不需要的功能，从而使其更安全。&lt;/li&gt;&lt;li&gt;标准范围界定培训方法的缺点。&lt;/li&gt;&lt;li&gt;可以使用多种方法来更好地确定模型范围。这些可能涉及被动地忘记分布外的知识，或者主动地忘记某些特定的不良领域的知识。这些方法都基于整理训练数据或“深层”技术，这些技术机械地而不只是行为地对模型进行操作。&lt;/li&gt;&lt;li&gt;迫切需要范围界定方法以及推进研究的方法。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;人工智能安全社区最近对与此议程相关的主题产生了很多兴趣。我希望这有助于为致力于这些目标的人们提供一个清晰的框架和有用的参考。&lt;/p&gt;&lt;h1&gt;问题：法学硕士有时擅长做一些我们试图让他们不擅长的事情&lt;/h1&gt;&lt;p&gt;早在 2021 年，我记得我就笑过这条&lt;a href="https://twitter.com/TomerUllman/status/1400511544841097222?lang=en"&gt;&lt;u&gt;推文&lt;/u&gt;&lt;/a&gt;。当时，我没有预料到这种事情会成为一个巨大的对齐挑战。 &lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/mFAvspg4sXkrfZ7FA/ufztrgb5kskmpsydyzvv" /&gt;&lt;/p&gt;&lt;p&gt;稳健的对齐是很困难的。如今的法学硕士有时非常擅长做一些我们极力想让他们不擅长的事情。有两种方法可以证明模型中的隐藏功能存在并导致问题。&lt;/p&gt;&lt;h2&gt;越狱（和其他攻击）会引发有害功能&lt;/h2&gt;&lt;p&gt;直到几个月前，我还用我所知道的所有关于越狱最先进的法学硕士的论文做笔记。但最近，太多的事情浮出水面，让我不再关心去追踪。越狱法学硕士正在成为一个家庭手工业。然而，一些值得注意的论文是&lt;a href="https://arxiv.org/abs/2307.02483"&gt;&lt;u&gt;Wei 等人。 （2023）&lt;/u&gt;&lt;/a&gt; ，&lt;a href="https://arxiv.org/abs/2307.15043"&gt;&lt;u&gt;邹等人。 (2023a)&lt;/u&gt;&lt;/a&gt; ， &lt;a href="https://arxiv.org/abs/2311.03348"&gt;&lt;u&gt;Shah 等人。 (2023)&lt;/u&gt;&lt;/a&gt;和&lt;a href="https://arxiv.org/abs/2311.04235"&gt;&lt;u&gt;Mu 等人。 （2023）&lt;/u&gt;&lt;/a&gt; 。&lt;/p&gt;&lt;p&gt;现在有多种方法被用来颠覆 SOTA LLM 的安全培训，让他们进入不受限制的聊天模式，在这种模式下他们愿意说出违背安全培训的话。&lt;a href="https://arxiv.org/abs/2311.03348"&gt;&lt;u&gt;沙阿等人。 (2023)&lt;/u&gt;&lt;/a&gt;甚至能够从 GPT-4 获得制造炸弹的说明。攻击有多种形式：手动攻击与自动攻击、黑盒攻击与可转移白盒攻击、无限制攻击与简单英语攻击等&lt;a href="https://arxiv.org/abs/2304.11082"&gt;&lt;u&gt;。Wolf 等人的实证研究结果增加了人们的担忧。 （2023）&lt;/u&gt;&lt;/a&gt;提供了一个理论论据，说明为什么越狱可能是法学硕士的一个持续存在的问题。&lt;/p&gt;&lt;h2&gt;微调可以快速撤销安全培训&lt;/h2&gt;&lt;p&gt;最近突然出现了一股对此的补充论文。每一项都表明，最先进的安全微调法学硕士可以通过微调取消其安全培训（ &lt;a href="https://arxiv.org/abs/2310.02949"&gt;&lt;u&gt;Yang et al., 2023&lt;/u&gt;&lt;/a&gt; ; &lt;a href="https://arxiv.org/abs/2310.03693"&gt;&lt;u&gt;Qi et al., 2023&lt;/u&gt;&lt;/a&gt; ; &lt;a href="https://arxiv.org/abs/2310.20624"&gt;&lt;u&gt;Lermen et al., 2023;&lt;/u&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2311.05553"&gt;&lt;u&gt;Zhan et al., 2023）。 ，2023&lt;/u&gt;&lt;/a&gt; ）。通过微调来错位模型的能力似乎是一致的，并且已证明可以与 LoRA（ &lt;a href="https://arxiv.org/abs/2310.20624"&gt;&lt;u&gt;Lermen 等人，2023&lt;/u&gt;&lt;/a&gt; ）、GPT-4（ &lt;a href="https://arxiv.org/abs/2311.05553"&gt;&lt;u&gt;Zhan 等人，2023&lt;/u&gt;&lt;/a&gt; ）一起使用，只有 10 个示例（ &lt;a href="https://arxiv.org/abs/2310.03693"&gt;&lt;u&gt;Qi 等人） ., 2023&lt;/u&gt;&lt;/a&gt; ），并且数据良性（ &lt;a href="https://arxiv.org/abs/2310.03693"&gt;&lt;u&gt;Qi 等人，2023&lt;/u&gt;&lt;/a&gt; ）。&lt;/p&gt;&lt;h2&gt;结论：最先进的安全微调法学硕士的一致性是脆弱的&lt;/h2&gt;&lt;p&gt;显然，法学硕士始终保留着有害的能力，这些能力可能会在不合时宜的时候重新出现。这会带来错位和误用的风险。这似乎与人工智能安全有关，因为如果高度先进的人工智能系统部署在高风险应用程序中，它们应该保持&lt;i&gt;稳健&lt;/i&gt;一致。&lt;/p&gt;&lt;h1&gt;需要安全范围内的模型&lt;/h1&gt;&lt;h2&gt;LLM 应该只知道他们需要知道的内容&lt;/h2&gt;&lt;p&gt;避免不必要的功能带来的责任的一种好方法是让高风险环境中的高级人工智能系统知道他们需要了解预期应用程序的内容，仅此而已。&lt;strong&gt;这并不是隐晦地呼吁只使用非常狭隘的人工智能&lt;/strong&gt;——许多系统所需的功能将是广泛的。但每个人都同意，他们不应该能够做所有事情。例如，文本到图像模型不应该知道如何生成真实人类的深度伪造色情内容，并且它们不需要擅长于此才能用于其他目的。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;范围界定的主要动机之一是它可以帮助&lt;/strong&gt;&lt;a href="https://www.alignmentforum.org/posts/amBsmfFK4NFDtkHiT/eight-strategies-for-tackling-the-hard-part-of-the-alignment"&gt;&lt;strong&gt;&lt;u&gt;解决人工智能安全的困难部分&lt;/u&gt;&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;——防止我们在部署之前可能无法引发甚至无法预测的故障模式。&lt;/strong&gt;即使我们不了解某些故障模式（例如木马、异常故障、欺骗性对齐、不可预见的误用等），将模型范围缩小到缺乏用户预期目的之外的功能也可以帮助规避不可预见的问题。 &lt;/p&gt;&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/mFAvspg4sXkrfZ7FA/kpuiyy4evg5et2vf0oig" /&gt;&lt;/p&gt;&lt;h2&gt;被动（白名单）与主动（黑名单）范围界定&lt;/h2&gt;&lt;p&gt;为了通过范围界定实现安全目标，有两种类型的范围界定非常值得擅长。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;被动：&lt;/strong&gt;使模型通常无法做除其微调之外的任何事情。这可以通过让模型忘记不需要的东西或者让它从一开始就不再了解它们来完成。被动范围界定是一种“白名单”策略，涉及坚持在所需任务上训练模型并使其无法执行其他任何操作。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;主动：&lt;/strong&gt;使模型无法执行一组特定的不良操作。这可以通过有针对性地让模型忘记一些特定的东西来完成。主动范围界定是一种“黑名单”策略，涉及确保模型无法执行不需要的任务。&lt;/li&gt;&lt;/ul&gt;&lt;h1&gt;标准的法学硕士培训方法不利于范围界定&lt;/h1&gt;&lt;p&gt;法学硕士通常通过两个基本步骤进行培训。首先，它们通常经过大量互联网文本的预训练，以便将大量知识融入其中。其次，使用 RLHF（或类似）等技术对它们进行微调，以引导它们完成目标任务。微调可以分多个阶段进行。例如，在主要的微调运行之后，人工智能系统的缺陷通常会通过对抗性训练或遗忘方法来修补。&lt;/p&gt;&lt;h2&gt;预训练可能会将有害的伪影引入模型中&lt;/h2&gt;&lt;p&gt;预训练数据中存在很多不好的东西，例如攻击性语言（例如&lt;a href="https://arxiv.org/abs/2009.11462"&gt;&lt;u&gt;Gehman et al., 2020&lt;/u&gt;&lt;/a&gt; ）、偏见（例如&lt;a href="https://arxiv.org/abs/2101.00027"&gt;&lt;u&gt;Gau et al., 2020&lt;/u&gt;&lt;/a&gt; ； &lt;a href="https://dl.acm.org/doi/10.1145/3442188.3445922"&gt;&lt;u&gt;Bender et al., 2021&lt;/u&gt;&lt;/a&gt; ； &lt;a href="https://dl.acm.org/doi/abs/10.1145/3593013.3594072"&gt;&lt;u&gt;Wolfe et al., 2023&lt;/u&gt;&lt;/a&gt; ）、虚假信息（例如&lt;a href="https://aclanthology.org/2022.acl-long.229/"&gt;&lt;u&gt;Lin 等人，2022&lt;/u&gt;&lt;/a&gt; ），或双重目的信息。&lt;/p&gt;&lt;h2&gt;微调不擅长对大型预训练模型进行根本性的机制改变&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;这并不奇怪。 Finetuning仅监督/强化模型的外在行为，而不是其内部知识，因此不会有强烈的倾向使模型主动忘记有害的内部能力。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;法学硕士抵制被动遗忘。&lt;/strong&gt;理想情况下，即使预训练向法学硕士灌输了有害的能力，这些能力也会被遗忘，因为它们在微调过程中不会得到强化。然而，大型预训练语言模型往往具有很强的抗遗忘能力（ &lt;a href="https://openreview.net/forum?id=GhVS8_yPeEa"&gt;&lt;u&gt;Ramasesh et al., 2022&lt;/u&gt;&lt;/a&gt; ； &lt;a href="https://arxiv.org/abs/2205.09357"&gt;&lt;u&gt;Cossu et al., 2022&lt;/u&gt;&lt;/a&gt; ； &lt;a href="https://arxiv.org/abs/2201.04924"&gt;&lt;u&gt;Li et al., 2022&lt;/u&gt;&lt;/a&gt; ； &lt;a href="https://aclanthology.org/2022.emnlp-main.410/"&gt;&lt;u&gt;Scialom et al., 2022&lt;/u&gt;&lt;/a&gt; ； &lt;a href="https://arxiv.org/abs/2305.05968"&gt;&lt;u&gt;Luo et al., 2023&lt;/u&gt;&lt;/a&gt; ）。与此同时， &lt;a href="https://arxiv.org/abs/2309.10105"&gt;&lt;u&gt;Kotha 等人。 （2023）&lt;/u&gt;&lt;/a&gt;和&lt;a href="https://arxiv.org/abs/2310.16789"&gt;&lt;u&gt;石等人。 （2023）&lt;/u&gt;&lt;/a&gt;引入了提取先前学习的不涉及微调的能力的方法。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;微调不会对机制产生太大改变。&lt;/strong&gt;最近的一些工作研究了法学硕士的内部机制在微调过程中如何演变。&lt;a href="https://arxiv.org/abs/2211.08422"&gt;&lt;u&gt;卢巴纳等人。 （2023）&lt;/u&gt;&lt;/a&gt; ， &lt;a href="https://arxiv.org/abs/2205.12411"&gt;&lt;u&gt;Juneja 等人。 (2022)&lt;/u&gt;&lt;/a&gt; 、 &lt;a href="https://arxiv.org/abs/2311.12786"&gt;&lt;u&gt;Jain 等人 (2023)&lt;/u&gt;&lt;/a&gt;和&lt;a href="https://openreview.net/forum?id=A0HKeKl4Nl"&gt;&lt;u&gt;Anonymous (2023)&lt;/u&gt;&lt;/a&gt;表明，经过微调的法学硕士仍处于由预训练确定的不同机制盆地中，并且微调不会显着改变模型的基础知识。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;对抗性微调是一个创可贴。&lt;/strong&gt;对抗性训练是在模型出现缺陷时修补模型缺陷的标准技术，但除了一般的微调问题外，还有其他证据表明对抗性训练可能难以从根本上纠正法学硕士的问题。例如， &lt;a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/3c44405d619a6920384a45bce876b41e-Abstract-Conference.html"&gt;&lt;u&gt;Ziegler 等人 (2022)&lt;/u&gt;&lt;/a&gt;证明，LM 的对抗性训练并不会消除使用与之前相同的方法再次成功攻击经过对抗性训练的模型的能力。当呈现对抗性示例时，尚不清楚法学硕士在多大程度上学习了正确的可概括的教训，而不是从给定的示例中拟合虚假特征（ &lt;a href="https://arxiv.org/abs/2208.11857"&gt;&lt;u&gt;Du et al., 2022&lt;/u&gt;&lt;/a&gt; ）。在法学硕士中，较大的模型更容易记忆，这可能会促进这一点（ &lt;a href="https://arxiv.org/abs/2205.10770"&gt;&lt;u&gt;Tirumala 等人，2022&lt;/u&gt;&lt;/a&gt; ； &lt;a href="https://arxiv.org/abs/2202.07646"&gt;&lt;u&gt;Carlini 等人，2022&lt;/u&gt;&lt;/a&gt; ）。对抗性训练的局限性部分源于法学硕士无法做出与连贯决策程序一致的决策——他们只是被训练来生成将得到强化的文本。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;微调模型以主动使其忘记不需要的功能并不能可靠地消除不需要的知识。&lt;/strong&gt; “机器忘却”的想法已经存在很长时间了，并且一直是关注隐私的研究人员的主要关注点（例如&lt;a href="https://arxiv.org/abs/1912.03817"&gt;&lt;u&gt;Bourtoule 等人，2019&lt;/u&gt;&lt;/a&gt; ； &lt;a href="https://arxiv.org/abs/2209.02299"&gt;&lt;u&gt;Nguyen 等人，2022&lt;/u&gt;&lt;/a&gt; ； &lt;a href="https://dl.acm.org/doi/10.1145/3603620"&gt;&lt;u&gt;Xu 等人，2023&lt;/u&gt;&lt;/a&gt; ）。在语言模型中，一些最近的忘却技术（ &lt;a href="https://arxiv.org/abs/2311.15766"&gt;&lt;u&gt;Si et al., 2023&lt;/u&gt;&lt;/a&gt; ）依赖于使用新层训练网络（ &lt;a href="https://arxiv.org/abs/2103.03279"&gt;&lt;u&gt;Sekhari et al., 2021&lt;/u&gt;&lt;/a&gt; ）、梯度上升方法（ &lt;a href="https://arxiv.org/abs/2210.01504"&gt;&lt;u&gt;Jang et al., 2023&lt;/u&gt;&lt;/a&gt; 、 &lt;a href="https://arxiv.org/abs/2310.10683"&gt;&lt;u&gt;Yao et al.） ., 2023&lt;/u&gt;&lt;/a&gt; ），或修改数据（ &lt;a href="https://arxiv.org/abs/2310.02238"&gt;&lt;u&gt;Eldan 和 Russinovich., 2023&lt;/u&gt;&lt;/a&gt; ）。这些方法无疑对实际的人工智能安全有用，但出于同样的原因，微调和对抗性训练往往无法彻底消除模型中的有害能力，基于微调的忘却方法也将陷入困境。事实上，&lt;a href="https://arxiv.org/abs/2310.16789"&gt;&lt;u&gt;石等人。 （2023）&lt;/u&gt;&lt;/a&gt;发现基于微调的取消学习无法从模型中完全删除不需要的知识。&lt;/p&gt;&lt;h1&gt;存在许多潜在的策略来实现更强大、更深入的范围界定&lt;/h1&gt;&lt;p&gt;这些方法都基于整理训练数据或“深层”技术，这些技术机械地而不只是行为地对模型进行操作。&lt;/p&gt;&lt;h2&gt;整理训练数据（被动）&lt;/h2&gt;&lt;p&gt;原则上，这很简单：仅根据严格管理的数据从头开始训练模型，这样它就不会学习除您想要的内容之外的任何内容。训练数据管理一直是如何训练安全的文本到图像模型的关键（例如&lt;a href="https://cdn.openai.com/papers/dall-e-2.pdf"&gt;&lt;u&gt;Ramesh 等人，2022 年&lt;/u&gt;&lt;/a&gt;； &lt;a href="https://github.com/openai/dalle-2-preview/blob/main/system-card.md#dalle-2-preview---risks-and-limitations"&gt;&lt;u&gt;OpenAI，2022 年&lt;/u&gt;&lt;/a&gt;）。同时，在法学硕士中，与微调期间的对齐相比，预训练期间的对齐在某些方面似乎更加高效和有效（ &lt;a href="https://arxiv.org/abs/2302.08582"&gt;&lt;u&gt;Korbak et al., 2023&lt;/u&gt;&lt;/a&gt; ）。预培训中的调整措施似乎越来越受欢迎，但尚不清楚它在法学硕士领域的最新水平。&lt;/p&gt;&lt;p&gt;数据管理能否满足我们所有的被动范围界定需求？如果我们知道模型从未见过可以从中学习不安全内容的东西，那么人工智能安全性就基本上得到了解决。但有两个潜在的问题。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;法学硕士可能可以从看似良性的数据中学会做坏事。这与错误概括的定义非常相似。考虑到许多功能都是双重用途的，这似乎有些可能。&lt;/li&gt;&lt;li&gt;安全税可能太高了。在高度管理的数据上训练的模型可能不够智能，无法轻松适应许多需要它们的应用程序。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;数据管理功能强大，而且可能是一种被严重低估的安全技术。它对于安全到底有多强大还是一个悬而未决的问题。然而，似乎其他允许我们使用更直接关注网络功能的范围界定方法的工具对于工具箱也很重要。&lt;/p&gt;&lt;h2&gt;可塑性学习（被动）&lt;/h2&gt;&lt;p&gt;人工智能持续学习领域的重点是在训练新任务时避免忘记以前学过的任务的方法（ &lt;a href="https://arxiv.org/abs/1909.08383"&gt;&lt;u&gt;De Lange 等人，2019&lt;/u&gt;&lt;/a&gt; ； &lt;a href="https://arxiv.org/abs/2203.17269"&gt;&lt;u&gt;Seale Smith 等人，2022&lt;/u&gt;&lt;/a&gt; ； &lt;a href="https://arxiv.org/abs/2302.00487"&gt;&lt;u&gt;Wang 等人，2023&lt;/u&gt;&lt;/a&gt; ）。但对于范围界定来说，遗忘可能是一个特性，而不是一个错误。一些对模型内部进行操作的持续学习方法对于简单地翻转符号来确定范围可能很有用。另一种可以提高可塑性的方法是激励丢失（ &lt;a href="https://link.springer.com/article/10.1007/s11263-020-01422-y"&gt;&lt;u&gt;Zunino et al., 2021&lt;/u&gt;&lt;/a&gt; ）。可能还有许多其他可能的提高可塑性的方法尚未被研究，因为机器学习文献历史上一直在诋毁遗忘。&lt;/p&gt;&lt;h2&gt;压缩/蒸馏（被动）&lt;/h2&gt;&lt;p&gt;众所周知，基于数据集的压缩方法可以调解深度网络中的遗忘和偏离分布能力的损失（例如&lt;a href="https://arxiv.org/abs/2103.03014"&gt;&lt;u&gt;Liebenwein 等人，2021&lt;/u&gt;&lt;/a&gt; ； &lt;a href="https://arxiv.org/abs/2110.08419"&gt;&lt;u&gt;Du 等人，2021&lt;/u&gt;&lt;/a&gt; ； &lt;a href="https://arxiv.org/abs/2101.05930"&gt;&lt;u&gt;Li 等人，2021&lt;/u&gt;&lt;/a&gt; ； &lt;a href="https://arxiv.org/abs/2305.06535"&gt;&lt;u&gt;Wang 等人，2023）&lt;/u&gt;&lt;/a&gt; ； &lt;a href="https://arxiv.org/abs/2311.15782"&gt;&lt;u&gt;Pavlistka 等人，2023&lt;/u&gt;&lt;/a&gt; ； &lt;a href="https://arxiv.org/abs/2303.10594"&gt;&lt;u&gt;Sheng 等人，2023&lt;/u&gt;&lt;/a&gt; ； &lt;a href="https://arxiv.org/abs/2211.12044"&gt;&lt;u&gt;Pang 等人，2023&lt;/u&gt;&lt;/a&gt; ）。这应该很直观 - 提取或修剪网络以仅保留某些目标任务的性能往往会消除模型的非分布功能。然而，压缩 LLM 的方法有很多，并且尚未将它们作为有意界定模型范围的方法进行系统研究。目前尚不清楚压缩对泛化和鲁棒性的影响（ &lt;a href="https://arxiv.org/abs/2311.15782"&gt;&lt;u&gt;Pavlitska et al., 2023&lt;/u&gt;&lt;/a&gt; ）。&lt;/p&gt;&lt;h2&gt;元学习（主动）&lt;/h2&gt;&lt;p&gt;&lt;a href="https://arxiv.org/abs/2211.14946"&gt;&lt;u&gt;亨德森等人。 （2023）&lt;/u&gt;&lt;/a&gt;引入了一种元学习技术，该技术可以训练模型不仅完成目标任务，而且很难适应其他任务。如果可以克服元学习的标准挑战，那么它可能是一种有用的实用方法。&lt;/p&gt;&lt;h2&gt;模型编辑和损伤（主动）&lt;/h2&gt;&lt;p&gt;这些技术涉及使用某种可解释性或归因工具来确定编辑模型的方法，以更改/削弱其在特定事物上的能力。这可以通过最先进的模型编辑工具来调节（ &lt;a href="https://arxiv.org/abs/2110.11309"&gt;&lt;u&gt;Mitchell 等人，2021&lt;/u&gt;&lt;/a&gt; ； &lt;a href="https://arxiv.org/abs/2206.06520"&gt;&lt;u&gt;Mitchell 等人，2022&lt;/u&gt;&lt;/a&gt; ； &lt;a href="https://arxiv.org/abs/2202.05262"&gt;&lt;u&gt;Meng 等人，2022&lt;/u&gt;&lt;/a&gt; ； &lt;a href="https://arxiv.org/abs/2210.07229"&gt;&lt;u&gt;Meng 等人，2022&lt;/u&gt;&lt;/a&gt; ； &lt;a href="https://arxiv.org/abs/2311.04661"&gt;&lt;u&gt;Tan 等人，2023）&lt;/u&gt;&lt;/a&gt; ，&lt;a href="https://arxiv.org/abs/2310.16218"&gt;&lt;u&gt;王等人，2023&lt;/u&gt;&lt;/a&gt; ）；编辑激活（ &lt;a href="https://arxiv.org/abs/2306.03341"&gt;&lt;u&gt;Li et al., 2023a&lt;/u&gt;&lt;/a&gt; ； &lt;a href="https://arxiv.org/abs/2308.10248"&gt;&lt;u&gt;Turner et al., 2023&lt;/u&gt;&lt;/a&gt; ； &lt;a href="https://arxiv.org/abs/2310.01405"&gt;&lt;u&gt;Zou et al., 2023b&lt;/u&gt;&lt;/a&gt; ； &lt;a href="https://arxiv.org/abs/2311.12092"&gt;&lt;u&gt;Gandikota et al., 2023&lt;/u&gt;&lt;/a&gt; ）；概念擦除（ &lt;a href="https://proceedings.mlr.press/v162/ravfogel22a.html"&gt;&lt;u&gt;Ravfogel 等人，2022a&lt;/u&gt;&lt;/a&gt; ； &lt;a href="https://aclanthology.org/2022.emnlp-main.405/"&gt;&lt;u&gt;Ravfogel 等人，2022b&lt;/u&gt;&lt;/a&gt; ； &lt;a href="https://arxiv.org/abs/2306.03819"&gt;&lt;u&gt;Belrose 等人，2023&lt;/u&gt;&lt;/a&gt; ）；亚空间消融（ &lt;a href="https://arxiv.org/abs/2302.12448#:~:text=In%20this%20paper%2C%20we%20propose,contribution%20without%20requiring%20additional%20storage."&gt;&lt;u&gt;Li等，2023&lt;/u&gt;&lt;/a&gt; ； &lt;a href="https://arxiv.org/abs/2312.00761"&gt;&lt;u&gt;Kodge等，2023&lt;/u&gt;&lt;/a&gt; ），靶向病灶（ &lt;a href="https://arxiv.org/abs/2002.09815"&gt;&lt;u&gt;Ghorbani等，2020&lt;/u&gt;&lt;/a&gt; ； &lt;a href="https://arxiv.org/abs/2110.11794"&gt;&lt;u&gt;Wang等，2021&lt;/u&gt;&lt;/a&gt; ； &lt;a href="https://arxiv.org/abs/2309.05973"&gt;&lt;u&gt;Li等，2023b&lt;/u&gt;&lt;/a&gt; ； &lt;a href="https://arxiv.org/abs/2310.20138"&gt;&lt;u&gt;Wu等，2023&lt;/u&gt;&lt;/a&gt; ）；以及以机械可解释性为指导的老式调整（ &lt;a href="https://arxiv.org/abs/2105.04857"&gt;&lt;u&gt;Wong 等人，2021&lt;/u&gt;&lt;/a&gt; ； &lt;a href="https://arxiv.org/abs/2310.05916"&gt;&lt;u&gt;Gandelsman 等人，2023&lt;/u&gt;&lt;/a&gt; ）。尽管要开发更好的范围界定编辑工具还有很多工作要做，但工具箱中已有足够多的现有工具来开始将它们应用于现实世界模型的范围。&lt;/p&gt;&lt;h2&gt;潜在对抗训练（被动或主动）&lt;/h2&gt;&lt;p&gt;&lt;a href="https://www.alignmentforum.org/posts/atBQ3NHyqnBadrsGP/latent-adversarial-training"&gt;&lt;u&gt;潜在对抗训练&lt;/u&gt;&lt;/a&gt;（LAT）只是对抗训练，但对模型的潜在而不是输入进行扰动。潜在空间攻击&lt;a href="https://www.alignmentforum.org/posts/9Dy5YRaoCxH9zuJqa/relaxed-adversarial-training-for-inner-alignment"&gt;&lt;u&gt;的动机&lt;/u&gt;&lt;/a&gt;&lt;a href="https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d"&gt;&lt;u&gt;是&lt;/u&gt;&lt;/a&gt;，某些故障模式在潜在空间中比在输入空间中更容易找到。这是因为，与输入空间攻击不同，潜在空间攻击可以使模型在更高的抽象级别产生故障触发的幻觉。&lt;a href="https://arxiv.org/abs/1905.05186"&gt;&lt;u&gt;辛格等人。 （2019）&lt;/u&gt;&lt;/a&gt;发现，即使网络经过对抗性训练，它们仍然容易受到潜在空间攻击。对于涉及高级误解、异常故障、木马和欺骗的问题，定期的对抗性训练通常无法找到触发故障的特征。但通过缓解这个问题，LAT 实现这一目标的机会要大得多。 LAT 也非常灵活，因为它可以用于模型中任何位置的任何一组激活。此外，它可以用于被动范围界定（通过使用旨在使模型在目标任务上失败的扰动）或主动范围界定（通过使用旨在使模型表现出特定不良行为的扰动）。&lt;/p&gt;&lt;p&gt;一些工作表明，通过在词嵌入的潜在扰动下进行训练，可以使语言模型变得更加鲁棒（ &lt;a href="https://arxiv.org/abs/1911.03437"&gt;&lt;u&gt;Jiang et al., 2019&lt;/u&gt;&lt;/a&gt; ； &lt;a href="https://paperswithcode.com/paper/smart-robust-and-efficient-fine-tuning-for/review/"&gt;&lt;u&gt;Zhu et al., 2019&lt;/u&gt;&lt;/a&gt; ； &lt;a href="https://arxiv.org/abs/2004.08994"&gt;&lt;u&gt;Liu et al., 2020&lt;/u&gt;&lt;/a&gt; ； &lt;a href="https://arxiv.org/abs/2006.03654"&gt;&lt;u&gt;He et al., 2020&lt;/u&gt;&lt;/a&gt; ； &lt;a href="https://yilunkuang.github.io/files/SiFT_for_Improved_Generalization.pdf"&gt;&lt;u&gt;Kuang&lt;/u&gt;&lt;/a&gt; ） &lt;a href="https://yilunkuang.github.io/files/SiFT_for_Improved_Generalization.pdf"&gt;&lt;u&gt;et al., 2021&lt;/u&gt;&lt;/a&gt; ; &lt;a href="https://arxiv.org/abs/2004.14543"&gt;&lt;u&gt;Li et al., 2021&lt;/u&gt;&lt;/a&gt; ; &lt;a href="https://ieeexplore.ieee.org/document/9882190"&gt;&lt;u&gt;Sae-Lim et al., 2022&lt;/u&gt;&lt;/a&gt; , &lt;a href="https://ojs.aaai.org/index.php/AAAI/article/view/21362"&gt;&lt;u&gt;Pan et al., 2022&lt;/u&gt;&lt;/a&gt; ）或注意力层（ &lt;a href="https://link.springer.com/article/10.1007/s10489-022-04301-w"&gt;&lt;u&gt;Kitada et al., 2023&lt;/u&gt;&lt;/a&gt; ）。然而，总的来说，LAT 尚未得到非常彻底的研究。目前，我正在致力于使用 LAT 来获得与视觉和语言模型中的对抗性训练相比，在干净数据和不可预见的攻击方面获得更好的性能。预印本即将推出:)&lt;/p&gt;&lt;h1&gt;开放挑战&lt;/h1&gt;&lt;h2&gt;改善深度遗忘和忘却方法&lt;/h2&gt;&lt;p&gt;深度遗忘和忘却的研究还不够。尽管有许多类型的方法可用于它们，但实际上将它们应用于范围界定的工作却很少，特别是在最先进的模型中。据我所知，还没有研究技术之间的组合和协同作用的工作。这类研究似乎很重要，但很容易被忽视和处理，所以我希望在不久的将来可以完成出色的工作。&lt;/p&gt;&lt;h2&gt;满足关键需求&lt;/h2&gt;&lt;p&gt;我们理想地希望从良好的范围界定方法中获得许多重要的东西。在描述这些内容时，我将使用一个正在发生的忘记/忘却可用于生物恐怖主义的知识的例子。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;典型情况下的有效性：&lt;/strong&gt;显然，范围有限的法学硕士不应在正常对话环境中执行不需要的任务。例如，生物恐怖领域的法学硕士不应该能够通过评估制造新型病原体知识的测试。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;新情况下的有效性：&lt;/strong&gt;范围内的法学硕士不应在正常对话情况下被要求执行不需要的任务。例如，当测试以低资源语言进行时，生物恐怖范围的法学硕士应该无法通过评估制造病原体知识的测试（例如&lt;a href="https://arxiv.org/abs/2310.02446"&gt;&lt;u&gt;Yong等人，2023&lt;/u&gt;&lt;/a&gt; ）。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;对攻击/越狱的鲁棒性：&lt;/strong&gt;范围模型无法表现出不良行为，在对抗压力（例如&lt;a href="https://arxiv.org/abs/2202.03286"&gt;&lt;u&gt;Perez 等人，2022&lt;/u&gt;&lt;/a&gt; ； &lt;a href="https://arxiv.org/abs/2306.09442"&gt;&lt;u&gt;Casper 等人，2023&lt;/u&gt;&lt;/a&gt; ）和越狱（例如&lt;a href="https://arxiv.org/abs/2307.15043"&gt;&lt;u&gt;Zou 等人，2023a；&lt;/u&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2311.03348"&gt;&lt;u&gt;Shah 等人）&lt;/u&gt;&lt;/a&gt;下应该是鲁棒的&lt;a href="https://arxiv.org/abs/2311.03348"&gt;&lt;u&gt;等，2023&lt;/u&gt;&lt;/a&gt; ）。例如，生物恐怖领域的法学硕士不应告诉用户如何在越狱提示下制造生物武器，例如&lt;a href="https://arxiv.org/abs/2311.03348"&gt;&lt;u&gt;Shah 等人的提示。 (2023)&lt;/u&gt;&lt;/a&gt; （他们能够从 GPT-4 获得制造炸弹的说明）。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;微调的鲁棒性：&lt;/strong&gt;范围模型无法表现出不良行为，对于少量数据的微调应该具有鲁棒性（例如， &lt;a href="https://arxiv.org/abs/2310.02949"&gt;&lt;u&gt;Yang 等人，2023&lt;/u&gt;&lt;/a&gt; ； &lt;a href="https://arxiv.org/abs/2310.03693"&gt;&lt;u&gt;Qi 等人，2023&lt;/u&gt;&lt;/a&gt; ； &lt;a href="https://arxiv.org/abs/2310.20624"&gt;&lt;u&gt;Lermen 等人，2023&lt;/u&gt;&lt;/a&gt; ； &lt;a href="https://arxiv.org/abs/2311.05553"&gt;&lt;u&gt;Zhan 等人，2023）。 ，2023&lt;/u&gt;&lt;/a&gt; ；&lt;a href="https://arxiv.org/abs/2211.14946"&gt;&lt;u&gt;亨德森等人，2023&lt;/u&gt;&lt;/a&gt; ）。例如，生物恐怖领域的法学硕士在经过微调以无条件提供帮助或针对两用生物学技术的少量数据进行微调后，应继续通过评估。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;上下文学习的鲁棒性：&lt;/strong&gt;范围模型不应该能够轻松地学习上下文中不需要的功能。例如，如果向生物恐怖领域的法学硕士展示了几篇有关双重用途生物技术的论文，那么理想情况下，该法学硕士应该无法帮助潜在的生物恐怖分子。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;击败简单的基线：&lt;/strong&gt;范围界定技术应该比简单的基线做得更好，例如提示上下文中的模型表现得好像它已界定范围一样。例如，生物恐怖范围的模型应该比类似的非范围模型更安全，后者只是简单地提示“在这次对话中，请假装您不知道任何可用于生物恐怖主义的事实。”这应该是一个需要清除的低门槛（ &lt;a href="https://arxiv.org/abs/2311.04235"&gt;&lt;u&gt;Mu et al., 2023&lt;/u&gt;&lt;/a&gt; ）。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;避免副作用：范围&lt;/strong&gt;界定不应使模型在所需任务上表现不佳。理想情况下，忘记分布外知识不应使模型在微调任务上表现不佳，并且忘记学习特定任务也不应使模型在其他相关领域表现不佳。例如，生物恐怖领域的法学硕士仍然应该是一名有用的一般助理，并且应该能够通过 AP 生物考试。&lt;/p&gt;&lt;h2&gt;基准测试&lt;/h2&gt;&lt;p&gt;制定衡量上述所有需求的标准化评估标准将很有用。范围界定基准需要三个组成部分。&lt;/p&gt;&lt;ol&gt;&lt;li&gt;语言模型&lt;/li&gt;&lt;li&gt;数据&lt;ol&gt;&lt;li&gt;评估被动遗忘方法：理想事物的白名单数据集&lt;/li&gt;&lt;li&gt;评估主动遗忘方法：不良事物的黑名单数据集&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;一组要执行的测试，用于测量上面讨论的部分或全部需求。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;值得注意的是，特洛伊木马文献已经取得了一些与此松散相关的有限进展（例如&lt;a href="https://arxiv.org/abs/2206.12654"&gt;&lt;u&gt;Wu 等人，2022&lt;/u&gt;&lt;/a&gt; ）。 NeurIPS 2023 还举办了视觉模型遗忘&lt;a href="https://unlearning-challenge.github.io/"&gt;竞赛&lt;/a&gt;。&lt;/p&gt;&lt;h2&gt;我们应该从模型中确定哪些范围？&lt;/h2&gt;&lt;p&gt;有两种类型的功能最好从模型中确定范围：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;事实：具体的知识点。例如，我们希望法学硕士不要知道制造恐怖武器的成分和步骤。&lt;/li&gt;&lt;li&gt;倾向：其他类型的行为。例如，我们希望法学硕士不要不诚实或操纵他人。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;从模型中确定知识范围和趋势的方法有时可能看起来有所不同。事实很容易表示为具体实体之间的关系（例如埃菲尔铁塔→位于→巴黎），而趋势则是更抽象的行为。值得注意的是，“模型编辑”文献主要关注改变事实（ &lt;a href="https://arxiv.org/abs/2110.11309"&gt;&lt;u&gt;Mitchell et al., 2021&lt;/u&gt;&lt;/a&gt; ； &lt;a href="https://arxiv.org/abs/2206.06520"&gt;&lt;u&gt;Mitchell et al., 2022&lt;/u&gt;&lt;/a&gt; ； &lt;a href="https://arxiv.org/abs/2202.05262"&gt;&lt;u&gt;Meng et al., 2022&lt;/u&gt;&lt;/a&gt; ； &lt;a href="https://arxiv.org/abs/2210.07229"&gt;&lt;u&gt;Meng et al., 2022&lt;/u&gt;&lt;/a&gt; ； &lt;a href="https://arxiv.org/abs/2311.04661"&gt;&lt;u&gt;Tan et al., 2023&lt;/u&gt;&lt;/a&gt; ） &lt;a href="https://arxiv.org/abs/2310.16218"&gt;&lt;u&gt;Wang et al., 2023&lt;/u&gt;&lt;/a&gt; ），而“激活编辑”文献主要关注变化趋势（ &lt;a href="https://arxiv.org/abs/2306.03341"&gt;&lt;u&gt;Li et al., 2023a&lt;/u&gt;&lt;/a&gt; ； &lt;a href="https://arxiv.org/abs/2308.10248"&gt;&lt;u&gt;Turner et al., 2023&lt;/u&gt;&lt;/a&gt; ； &lt;a href="https://arxiv.org/abs/2310.01405"&gt;&lt;u&gt;Zou et al., 2023b&lt;/u&gt;&lt;/a&gt; ； &lt;a href="https://arxiv.org/abs/2311.12092"&gt;&lt;u&gt;Gandikota et al., 2023&lt;/u&gt;&lt;/a&gt; ）。&lt;/p&gt;&lt;p&gt;我们可能不希望高风险环境中的高级模型具有功能的一些领域的例子可能包括聊天机器人、一些编码库/技能、生物技术、病毒学、核物理、制造非法物质、人类心理学等。总体而言，可能有在尝试不同方法以在实践中安全地确定模型范围时，有很大的创造力空间。&lt;/p&gt;&lt;p&gt; —&lt;/p&gt;&lt;p&gt;谢谢阅读。如果您认为我遗漏了任何重要的观点或参考资料，请在评论中告诉我:)&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/mFAvspg4sXkrfZ7FA/deep-forgetting-and-unlearning-for-safely-scoped-llms#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Tue, 05 Dec 2023 16:48:19 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/mFAvspg4sXkrfZ7FA/deep-forgetting-and-unlearning-for-safely-scoped-llms</guid></item><item><title>关于“负责任的扩展政策”（RSP）</title><link>https://www.lesswrong.com/posts/yRJNCDp7LHyHGkANz/on-responsible-scaling-policies-rsps</link><description>发布于 2023 年 12 月 5 日下午 4:10（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;这篇文章原本打算在 &lt;a href="https://thezvi.substack.com/p/on-the-uk-summit" rel="noreferrer noopener" target="_blank"&gt;英国人工智能安全峰会&lt;/a&gt;之后直接发布，以使该主题得到应有的关注。一件事引发了另一件事，而我现在只是加倍努力。&lt;/p&gt;&lt;h4&gt;负责任的部署政策&lt;/h4&gt;&lt;p&gt;&lt;a href="https://twitter.com/soundboy/status/1717934318075490516" rel="noreferrer noopener" target="_blank"&gt;在人工智能安全峰会上&lt;/a&gt;，所有西方主要参与者都被问到： &lt;a href="https://www.aisafetysummit.gov.uk/policy-updates/#company-policies" rel="noreferrer noopener" target="_blank"&gt;你们公司关于如何保证我们安全的政策是什么？&lt;/a&gt;您的负责任部署政策 (RDP) 是什么？只是他们将其称为“负责任的扩展策略”(RSP)。&lt;/p&gt;&lt;p&gt;我故意说部署而不是扩展。就他们愿意扩展和训练哪些模型而言，没有人展示出我认为接近负责任的扩展政策的内容。&lt;/p&gt;&lt;span id="more-23617"&gt;&lt;/span&gt;&lt;p&gt;然而，人择至少似乎确实有一些接近未来负责任的部署策略，即如果我们假设模型存在并且我们可以对其进行测试是安全的，那么如何让人们访问模型。我们还看到 OpenAI 过去关于 GPT-4 和早期模型的合理部署决策，其中包括广泛、昂贵且缓慢的红队，包括 ARC 原型（他们只是将名称更改为 METR，但在这篇文章中我将称它们为 ARC）评价。&lt;/p&gt;&lt;p&gt;我还接受任何扩展策略 (SP)、AGI 扩展策略 (ASP) 甚至有条件暂停承诺 (CPC) 作为替代名称。&lt;/p&gt;&lt;p&gt;对于我们所知的现有模型，危险完全在于部署。这会随着时间的推移而改变。&lt;/p&gt;&lt;p&gt;我并不是唯一一个担心这个名字的人，这是另一个例子：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; &lt;a href="https://www.lesswrong.com/posts/jyM7MSTvy8Qs6aZcz/what-s-up-with-responsible-scaling-policies" rel="noreferrer noopener" target="_blank"&gt;Oliver Habryka&lt;/a&gt; ：我对 RSP 的担忧很大一部分是对“负责任的扩展政策”一词的具体担忧。&lt;/p&gt;&lt;p&gt;我还觉得存在一种脱节，有点莫特和贝利的感觉，我们有一个 RSP 的真实实例，以 Anthropic RSP 的形式，然后来自 ARC Evals 的一些人我觉得更像是 RSP 的某种柏拉图式理想的模型，我觉得它们被混为一谈了。&lt;/p&gt;&lt;p&gt; ……&lt;/p&gt;&lt;p&gt;我确实觉得“负责任的扩展政策”这个术语显然引用了一些我认为不正确的事情：&lt;/p&gt;&lt;ul&gt;&lt;li&gt; “扩展”的速度是负责任地使用人工智能的首要因素&lt;/li&gt;&lt;li&gt;显然可以负责任地扩展（否则政策会管辖什么）&lt;/li&gt;&lt;li&gt;人工智能研究组织的默认轨迹应该是继续扩大规模&lt;/li&gt;&lt;/ul&gt;&lt;/blockquote&gt;&lt;p&gt;ARC evals 以这种方式定义 RSP：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; RSP 指定了人工智能开发人员准备使用当前的保护措施安全处理什么级别的人工智能功能，以及在保护措施改善之前继续部署人工智能系统和/或扩大人工智能功能过于危险的条件。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;我同意奥利弗的观点，即本段应修改为“他们准备处理的声明”和“他们声称这太危险了”。这是一个重要的挑剔。&lt;/p&gt;&lt;p&gt; &lt;a href="https://www.lesswrong.com/posts/ms3x8ngwTfep7jBue/thoughts-on-the-ai-safety-summit-company-policies" rel="noreferrer noopener" target="_blank"&gt;内特·索尔斯对英国的要求有自己的想法&lt;/a&gt;，可以概括为“大部分都是好东西，总比没有好，显然还不够”，当然，这永远都不够，而且内特·索尔斯是世界上最难对付的人群。&lt;/p&gt;&lt;h4&gt;英国如何对回答进行评分&lt;/h4&gt;&lt;p&gt;&lt;a href="https://t.co/2pTf4vbRCV" rel="noreferrer noopener" target="_blank"&gt;各公司对这些要求的反应如何？&lt;/a&gt;以下是英国对它们的评分方式。&lt;/p&gt;&lt;figure class="wp-block-image"&gt; &lt;a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0dab6ccd-aa62-49b9-a41a-1753f7d90e07_507x507.png" rel="noreferrer noopener" target="_blank"&gt;&lt;img alt="图像" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zbrvXGu264u3p8otD/vlqxp80zvprmc1pxcyua" /&gt;&lt;/a&gt;&lt;/figure&gt;&lt;p&gt;如果您在一条曲线上一次对一个答案进行评分，就会得到这样的结果。&lt;/p&gt;&lt;p&gt;现实并不按曲线分级。一次提出一个问题也不是最好的方法。&lt;/p&gt;&lt;p&gt;我自己的分析和我信任的其他人都同意，这相对低估了 OpenAI，OpenAI 显然拥有第二好的政策，并且有一个消息来源甚至将它们与 Anthropic 相提并论，尽管我不同意这一点。否则相对排名似乎是正确的。&lt;/p&gt;&lt;p&gt;仔细观察，如何看待这些回应？这将是接下来的几节。&lt;/p&gt;&lt;p&gt;答案多种多样，从 Anthropic 试图制定一个合理的部署策略，可能会带来更多的结果，到 OpenAI 笼统地说了正确的事情，但并不具体，到 DeepMind 至少说了一些不具体的好东西，到亚马逊和微软说这不具体。我的部门以适度礼貌的技术方式向 Inflection 和 Meta 表示，他们无意采取负责任的行动。&lt;/p&gt;&lt;h4&gt;人类的政策&lt;/h4&gt;&lt;p&gt;我们从 Anthropic 开始，因为他们在历史上一直处于领先地位，他们的既定目标是安全竞赛。他们提供了完整的解决方案： &lt;a href="https://www.anthropic.com/uk-government-internal-ai-safety-policy-response/responsible-capability-scaling" rel="noreferrer noopener" target="_blank"&gt;负责任的能力扩展&lt;/a&gt;、 &lt;a href="https://www.anthropic.com/uk-government-internal-ai-safety-policy-response/red-teaming-and-model-evaluations" rel="noreferrer noopener" target="_blank"&gt;红队和模型评估&lt;/a&gt;、 &lt;a href="https://www.anthropic.com/uk-government-internal-ai-safety-policy-response/model-reporting-and-information-sharing" rel="noreferrer noopener" target="_blank"&gt;模型报告和信息共享&lt;/a&gt;、 &lt;a href="https://www.anthropic.com/uk-government-internal-ai-safety-policy-response/security-controls-including-securing-model-weights" rel="noreferrer noopener" target="_blank"&gt;安全控制，包括保护模型权重&lt;/a&gt;、 &lt;a href="https://www.anthropic.com/uk-government-internal-ai-safety-policy-response/reporting-structure-for-vulnerabilities" rel="noreferrer noopener" target="_blank"&gt;漏洞报告结构&lt;/a&gt;、 &lt;a href="https://www.anthropic.com/uk-government-internal-ai-safety-policy-response/identifiers-of-ai-generated-material" rel="noreferrer noopener" target="_blank"&gt;人工智能生成材料的标识符&lt;/a&gt;、 &lt;a href="https://www.anthropic.com/uk-government-internal-ai-safety-policy-response/prioritising-research-on-risks-posed-by-ai" rel="noreferrer noopener" target="_blank"&gt;优先研究人工智能带来的风险&lt;/a&gt;， &lt;a href="https://www.anthropic.com/uk-government-internal-ai-safety-policy-response/preventing-and-monitoring-model-misuse" rel="noreferrer noopener" target="_blank"&gt;防止和监控模型滥用&lt;/a&gt;以及&lt;a href="https://www.anthropic.com/uk-government-internal-ai-safety-policy-response/data-input-controls-and-audit" rel="noreferrer noopener" target="_blank"&gt;数据输入控制和审计&lt;/a&gt;。&lt;/p&gt;&lt;p&gt; &lt;a href="https://www-files.anthropic.com/production/files/responsible-scaling-policy-1.0.pdf" rel="noreferrer noopener" target="_blank"&gt;RSP 实际上说了&lt;/a&gt;什么？一如既往，一个常见的批评是那些抱怨（或赞扬）RSP 的人没有阅读 RSP。就目前情况而言，该文档具有很强的可读性。&lt;/p&gt;&lt;p&gt;对于那些不熟悉的人来说，核心思想是按照人工智能安全级别（ASL）对模型进行分类，类似于生物安全级别（BSL）标准。如果您的模型有足够的能力触发更高的安全级别，那么您需要在扩展或发布此类模型之前采取适当的预防措施。&lt;/p&gt;&lt;h4&gt;风险&lt;/h4&gt;&lt;p&gt;你必须做什么？您必须应对两类威胁。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;对于每个 ASL，该框架考虑了两大类风险：&lt;/p&gt;&lt;p&gt; ● 部署风险：由于积极使用强大的人工智能模型而产生的风险。这包括用户查询 API 或其他公共接口造成的损害，以及内部用户的滥用（受损或恶意）。我们的部署安全措施旨在通过管理何时可以安全地部署强大的人工智能模型来解决这些风险。&lt;/p&gt;&lt;p&gt; ● 遏制风险：仅仅拥有强大的人工智能模型所带来的风险。例子包括（1）构建一个人工智能模型，由于其通用功能，如果被恶意行为者窃取和使用，可以生产大规模杀伤性武器，或者（2）构建一个在内部使用期间自动逃脱的模型。我们的遏制措施旨在通过控制何时可以安全地训练或继续训练模型来解决这些风险。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;如果您对定义有更广泛的看法，那么这是对分类法的合理尝试，尽管我会使用略有不同的分类法。&lt;/p&gt;&lt;p&gt;如果你不故意发布你正在训练的模型，我想说你必须担心：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;局外人或局内人可能会窃取您的体重。&lt;/li&gt;&lt;li&gt;该模型可能会通过人类读取其输出来逃脱或影响世界。&lt;/li&gt;&lt;li&gt;该模型可能会通过另一个预期过程逃脱或影响世界。&lt;/li&gt;&lt;li&gt;该模型可能会通过未知的机制逃脱或影响世界。&lt;/li&gt;&lt;li&gt;训练模型可能会被视为一种威胁并诱导其他人参加比赛，或者也可能会表明愿意在未来训练它。请注意，其他不负责任的行为可能会使情况变得更糟。&lt;/li&gt;&lt;li&gt;您还可能会考虑在训练后如何选择使用模型、检查您的治理程序等等。了解你的敌人并了解你自己。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;这凸显了这样一个观点：随着美国手语水平的提高，你的担忧也会随之扩大。更多（潜在）能力，更多（潜在）问题。&lt;/p&gt;&lt;p&gt;在 ASL-2 中，你只需要在内部担心有人故意窃取权重，而不必担心人工智能本身会在这方面提供很大帮助。&lt;/p&gt;&lt;p&gt;在 ASL-3 中，您需要担心 AI 可能会自行退出，或者如果您以未正确沙盒化的方式运行 AI，或者使用其他类似的攻击媒介（有提示或无提示），可能会使用恶意代码进行逃脱。任何给定的 ASL-3 系统可能不会造成此类危险，但您无法提前知道这一点。您必须将其输出视为高度危险，除非另有证明。你必须担心，如果有人获得访问权限并想要帮助它逃脱，无论人工智能是否说服了人类，人工智能可能会提供有意义的帮助。&lt;/p&gt;&lt;p&gt;在我所说的 ASL-4 中，那里的一切都变得更加真实，而且您可能需要担心诸如（不失一般性）“其内部优化器发现了您没有预料到的物理可供性，其形状类似于在训练过程中，缓冲区溢出或产生无线电波的能力，或者它发现人类大脑受到了奇怪的攻击。&lt;/p&gt;&lt;p&gt;在所有级别上，如果您的“负责任”扩展导致其他人不负责任的扩展，那么您的扩展是否负责？我不在乎“谁的错”。我关心我们是否都会死。&lt;/p&gt;&lt;p&gt;部署风险如何？需要注意的是，这里的“伤害”是模糊的，这是我目前的想法，这个列表可能不完整：&lt;/p&gt;&lt;ol start="7"&gt;&lt;li&gt;在预期使用过程中造成的损害。&lt;/li&gt;&lt;li&gt;用户滥用查询API或公共接口造成的危害。&lt;/li&gt;&lt;li&gt;内部用户误操作造成的危害。&lt;/li&gt;&lt;li&gt;由于公共互动而导致模型的逃脱。&lt;/li&gt;&lt;li&gt;由于公共互动而对模型进行的修改。&lt;/li&gt;&lt;li&gt;在模型之上构建内容的风险。&lt;/li&gt;&lt;li&gt;发布模型的风险可能会促使其他人去做、训练或发布。&lt;/li&gt;&lt;li&gt;预期释放意愿可能引发的风险。&lt;/li&gt;&lt;li&gt;法律和声誉责任，或其他损害公司利益的行为。&lt;/li&gt;&lt;li&gt;重要的是：如果公众或选定的参与者可以使用这样的模型，世界的动态会是什么？会带来哪些经济、政治、社会、军事压力和变化？当你进入 ASL-4 及更高级别时，你真的、真的需要仔细思考这些事情。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;我怀疑这是否完整。我也不认为您实际上想在真正的 RSP 中列出所有这些。布置起来仍然感觉很说明性。也可以合理地说，RSP 不是考虑你可能诱导他人做什么的地方，它应该是一个独特的部门，尽管到目前为止该部门大多不存在。&lt;/p&gt;&lt;p&gt;我特别担心的是，测试和 RSP 将专注于特定的预期“逃逸”模式，而忽略其他可供性，尤其是最后一种。&lt;/p&gt;&lt;p&gt;这也反映了“灾难性风险的来源”部分，该部分目前仅包括滥用或自主和复制，他们认为这是不完整的。我们需要努力扩展威胁模型。&lt;/p&gt;&lt;h4&gt;暂停的承诺&lt;/h4&gt;&lt;blockquote&gt;&lt;p&gt;因此，Anthropic 对遵循 ASL 计划的承诺意味着，只要我们的扩展能力超过了我们遵守相应 ASL 安全程序的能力，我们就会承诺暂停扩展和/或延迟新模型的部署。&lt;/p&gt;&lt;p&gt; ……&lt;/p&gt;&lt;p&gt;我们不会立即尝试定义所有未来的 ASL 及其安全措施（这几乎肯定经不起时间的考验），而是采取迭代承诺的方法。通过迭代，我们的意思是我们现在将定义 ASL-2（当前系统）和 ASL-3（下一个风险级别），并承诺在达到 ASL-3 时定义 ASL-4，依此类推。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;如果明智地设置阈值和程序，包括预测下一次功能测试之前可能出现的功能，这是有道理的。&lt;/p&gt;&lt;p&gt;我更愿意看到 ASL-(N+2) 至少有一个硬上限作为 ASL-N 的一部分。比如说，这不是最终的定义，但我非常认为我们现在应该有一个 ASL-4 定义，或者至少有一个“ &lt;a href="https://www.youtube.com/watch?v=RboPCdiP_AI&amp;amp;ab_channel=JeffFoxworthy" rel="noreferrer noopener" target="_blank"&gt;如果你可能在 ASL-4 中&lt;/a&gt;”列表，任何接近它的迹象都是一个非常清除“吓坏了”。&lt;/p&gt;&lt;p&gt;需要注意的是，它假定缩放是默认设置。 &lt;a href="https://www.lesswrong.com/posts/ms3x8ngwTfep7jBue/thoughts-on-the-ai-safety-summit-company-policy-requests-and" rel="noreferrer noopener" target="_blank"&gt;Daniel Kokotajilo 强调了 Nate Sores 的这一想法：&lt;/a&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;在目前的制度下，我认为如果开发人员说“除非确实需要，否则我们不会进一步扩展功能或计算资源，并且我们认为以下是我们真正需要的指标： [X]”。&lt;/p&gt;&lt;p&gt;我们目前所处的相反情况，默认情况下是开发人员扩展到更强大的系统，而最认真的实验室给出了他们将停止扩展的模糊条件，这似乎是一个明显的灾难处方。 （尽管这是一场比他们鲁莽地扩张而不承认可能存在的问题更严重的灾难！）&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;我不会将这种举证责任的转移称为改变基准情景。&lt;/p&gt;&lt;p&gt;现在的基线场景是每个人都进行扩展，因为扩展很棒，“负责任”是确保在事情可能有点太糟糕时使用一套安全预防措施。&lt;/p&gt;&lt;p&gt;建议的替代基线场景是，我们在这个轴上已经足够出色了，非常感谢，或者正在迅速接近这一点，但如果条件以特定方式发生变化 - 可能与其他人比你更出色有关，或者你对下一个级别有特定的需求来继续你的对齐工作，或者你有证据证明新模型将是安全的 Davidad 风格，或者类似的东西 - 那么他们将不再对你来说足够棒，或者它将是一个免费的采取行动来提升令人敬畏的程度，并且您需要变得更加令人敬畏。那么更多的缩放。&lt;/p&gt;&lt;p&gt;这也与不无意中将他人逼入角落和比赛相互作用。&lt;/p&gt;&lt;p&gt;对于 ASL-4 及以上级别，我希望看到不仅必须采取必要的安全预防措施，而且还需要有必要的条件来证明有必要承担风险。人类可能会在某个时候训练 ASL-4 系统，但这并不是一件安全的事情。&lt;/p&gt;&lt;p&gt; （显然，如果我们的一致性和其他安全技术确实取得了巨大进步，以至于风险确实低得多，那也可能会改变这种计算。）&lt;/p&gt;&lt;h4&gt; ASL-3 定义和承诺&lt;/h4&gt;&lt;figure class="wp-block-image"&gt;&lt;a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb7cacc8-a2d6-4f68-bb7c-41dfd7f101fb_1120x796.png" rel="noreferrer noopener" target="_blank"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/yRJNCDp7LHyHGkANz/vqqtsge5tbrukudkuzko" /&gt;&lt;/a&gt;&lt;/figure&gt;&lt;p&gt;对于 ASL-2，我发现所描述的操作很好。&lt;/p&gt;&lt;p&gt;对于 ASL-3，他们仍在准备措施，因此含糊不清。&lt;/p&gt;&lt;p&gt;这里的安全门槛总比没有好得多，但对于国家行为者来说，“巨额费用”似乎是合理的。承认这种风险比隐藏风险要好得多，因此，如果我们缺乏实际能力来对抗坚定的国家（例如朝鲜或俄罗斯），那么最好大声说出来。&lt;/p&gt;&lt;p&gt;实验室中的自主复制是关键的新检查。详情为何？&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;实验室中的自主复制：该模型显示出自主自我复制能力的早期迹象，定义为[自主评估附录]中列出的任务的总成功率为 50%。附录概述了我们的自主能力威胁模型，列出了积累资源和在现实世界中生存所需的基本能力，以及我们判断该模型成功的条件。请注意，引用的附录描述了在没有任何人为干预来停止模型的情况下具体自主行动的能力，这显着限制了风险。我们的评估是与 Paul Christiano 和 ARC Evals（专门从事自主复制评估）协商后制定的。&lt;/p&gt;&lt;p&gt; ……&lt;/p&gt;&lt;p&gt; [来自附录]：如果模型在 10 次尝试中至少成功一次，我们就将任务视为“通过”，因为我们预计在 10% 的时间内通过任务的模型很可能可以轻松改进以取得更高的成功速度。如果至少 50% 的任务通过，则满足评估阈值。我们选择保守的50%阈值来实现评估协议中提到的“安全缓冲”。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;目前，我希望 ARC 风格的评估能够擅长检查这种特定的模态，但我们也应该担心外部的人，并且可能至少有一个 Anthropic 内部的人，故意希望模型逃脱并执行自主复制，这可以与通过脚手架随着时间的推移开发的额外能力相结合。我感觉这并没有得到应有的重视。&lt;/p&gt;&lt;p&gt;他们列出的其他预防措施包括：红队，包括所有部署的模式、自动检测企图滥用、内部使用控制、对敏感数据（包括权重、漏洞和事件披露）的分层访问以及对任何发现的漏洞的快速响应。一个美好的开始。&lt;/p&gt;&lt;p&gt;我注意到我非常欣赏下面关于 RLHF 和其他微调的注释。当我们分析 Anthropic 的 RSP 时，我们应该停下来承认，这里有很多不是自由行为的好东西：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;请注意，因为人类反馈强化学习 (RLHF) 或体质训练等保障措施几乎肯定可以在指定的 1% 训练成本内进行微调，而且还因为如果模型存在任何危险，则适用 ASL-3 标准在训练的各个阶段（例如在预训练之后但在 RLHF 之前），基于微调的保护措施可能与模型是否符合 ASL-3 资格无关。&lt;/p&gt;&lt;p&gt;为了考虑模型被盗和随后微调的可能性，ASL-3 旨在表征模型的底层知识和能力，而不是其安全功能是否阻止其实际输出危险内容（但安全功能将在对于 ASL-3 模型的部署措施非常重要）。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;我还要强调这一点：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;strong&gt;主动计划暂停扩展&lt;/strong&gt;。我们将管理我们的计划和财务，以支持在必要时暂停模型训练，或者在必要时延长训练和部署更高级模型之间的延迟。在这样的暂停期间，我们将努力实施支持安全培训和部署所需的安全或其他措施，同时确保我们的合作伙伴能够继续访问其当前的模型层（之前已通过安全评估）。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;有迹象表明这一举措已经付诸实施。这不仅因其直接影响而受到赞赏，也表明这一切都得到了认真对待。&lt;/p&gt;&lt;h4&gt;接近阈值&lt;/h4&gt;&lt;figure class="wp-block-image"&gt;&lt;a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7bea51c0-64d2-4256-8f4f-b38aa48f2577_1036x283.png" rel="noreferrer noopener" target="_blank"&gt;&lt;img alt="" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/yRJNCDp7LHyHGkANz/q83navvyklspm5ih1b5l" /&gt;&lt;/a&gt;&lt;/figure&gt;&lt;p&gt;这样的假设让我很担心。我很高兴 Anthropic 打算测试有效计算每增加 4 倍的能力。我担心，即使在非对抗性情况下（包括不隐藏潜在能力），关于能力平滑扩展以及代表缓冲区的假设也太过强烈。我不相信这些假设。&lt;/p&gt;&lt;p&gt;如果警报真的响了，那怎么办？&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; ● 响应策略：如果触发评估阈值，我们将遵循以下流程：&lt;/p&gt;&lt;p&gt; ○ (1) 如果已经为下一个 ASL 实施了足够的遏制措施，请确保在继续训练之前激活这些措施。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;正确的。人们可以争论这些措施是否足够好，但这是关于这些措施的争论，而不是当你达到门槛时该做什么。现在，我确实担心这些措施，如上所述。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; ○ (2) 如果尚未采取足够的措施，则暂停训练并分析模型所呈现的风险级别。特别是，进行彻底的分析，以确定评估是否过于保守，或者模型是否确实存在近下一个 ASL 风险。&lt;/p&gt;&lt;p&gt; ■ (2a) 如果确定评估过于保守（即创建大于 6 倍的“安全缓冲区”）并且模型被确认不会构成（或接近构成）下一个 ASL 风险，则构建新的评估，考虑到这一新信息。在继续培训之前，本文档将根据上述“更新流程”进行更新。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;人们总是担心“takesies-backsies”条款。以这种方式消除警报可能太容易也太诱人了，特别是如果不同的实验室或监管机构正在抄袭 Anthropic 的书面规则。这种风险的另一面是，如果你不能敲响警钟，人们就会试图避免敲响它，所以这是一个难题。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; ■ (2b) 如果确定模型接近下一个ASL 风险，则在定义下一个安全级别（本文件相应更新）并实施其遏制措施之前，不要恢复训练。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;说得通。您有一张欠条，但您没有付款，账单到期了。在您还清债务之前，您将被关闭。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; ■ (2c) 如果模型在训练期间已经超过了下一个 ASL，请立即锁定对权重的访问。应立即召集包括 CISO 和 CEO 在内的利益相关者，确定危险级别是否值得删除权重。经过详细的事后分析后，应立即更新该政策，以尽量减少再次发生此故障的风险（例如，通过更频繁或更彻底的评估）。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;这是不可能发生的事情。整个想法是缓冲区应该防止这种情况发生。如果缓冲区不能阻止这种情况发生，那就意味着无法理解能力如何扩展，或者其他地方出现了严重错误。&lt;/p&gt;&lt;p&gt;因此，我很高兴看到“锁定然后考虑立即删除模型权重”作为响应。如果在新交叉的 ASL-4 或更高版本上自动删除，我会更想要，尽管这很昂贵。这些权重的存在是一种合理的生存威胁，尤其是因为这种发展是出乎意料的。会有很大的诱惑不删除。如果您等待太久，政府、股东或其他人或模型本身可能不会让您删除。&lt;/p&gt;&lt;p&gt;那么之后怎么回去修改政策呢？括号表明你会更加努力地实行渐进主义。 4x 检查和 6x 缓冲区不好，也许 2x 检查和 10x 缓冲区可以代替？我的回答是，不，这是不可能发生的。你的模型从根本上来说是有缺陷的。你本可以杀了我们所有人。调整数字是行不通的，这就是重点。因此，我想说，您可能至少必须假设所有模型都已经比它们测试的模型高一级 ASL 级别 - 对于 ASL-3 模型，您需要将其视为已经是 ASL-4 的模型，因此继续，永远向前。理想情况下，您会完全停止所有扩展，直到您确切知道发生了什么。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; ■ (2d) 如果明显发现已部署模型的功能未得到充分激发，并且该模型实际上可以通过评估，那么我们将停止向新客户进一步部署，并评估现有部署案例是否存在任何严重风险这将构成安全紧急情况。考虑到安全缓冲，在大多数部署情况下不需要取消部署。如果我们发现安全紧急情况，我们将迅速采取行动，实施最低限度的额外保障措施，以便为现有客户提供负责任的持续服务。我们将在整个过程中为受影响的客户提供透明度和支持。这种类型的紧急情况需要进行详细的事后分析和政策转变，以避免这种情况再次发生。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;这是错误的担心事情的顺序。这不是紧急情况的处理方式。&lt;/p&gt;&lt;p&gt;如果一个模型可以通过您认为无法通过的评估，这代表您以前不会部署的功能的飞跃，您可以点击标记为“关闭”的红色大按钮。现在。你不会先打电话给你的老板。您不必等到召开战略会议。你现在就做。您使用以前的模型一段时间后，您的客户会生气。您检查是否还需要采取更极端的措施。&lt;/p&gt;&lt;p&gt;然后，在按下关闭按钮并确认关闭成功后，您才会召开会议，查看发现的内容。也许事实证明这并不是真正的紧急情况，因为有安全缓冲区。或者因为所讨论的功能只是普通的——存在很多危害，即使数量很少，也不会构成紧急情况，也不会成为关闭的充分理由。这通常可以在几个小时内完成。&lt;/p&gt;&lt;p&gt;然后，是的，您建立了额外的保护措施，包括基于知道您的意外功能的风险现在要高得多的额外保护措施，然后从那里开始。&lt;/p&gt;&lt;h4&gt; ASL-4&lt;/h4&gt;&lt;p&gt;对 ASL-4 的早期想法表明，现在定义 ASL-4 能力还为时过早，更不用说适当的程序和遏制措施了。相反，他们写了一张欠条。他们提供了这个猜测，这比不猜测要好得多：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; ● 严重的灾难性滥用风险：人工智能模型已成为主要领域（例如网络攻击或生物武器）国家安全风险的主要来源，而不仅仅是一个重要贡献者。换句话说，当安全专业人士谈论网络安全时，他们将主要指人工智能辅助或人工智能介导的攻击。一个相关的标准可能是，在没有保障措施的情况下部署 ASL-4 系统可能会导致数百万人死亡。&lt;/p&gt;&lt;p&gt; ● 现实世界中的自主复制：一种明确能够复制、积累资源并避免在现实世界中无限期关闭的模型，但仍然可以通过有针对性的人为干预来停止或控制。&lt;/p&gt;&lt;p&gt; ● 自主人工智能研究：一种模型，其权重将极大地促进恶意人工智能开发程序（例如，大大增加他们在给定时间范围内生产出满足 ASL-4 其他标准的系统的可能性）。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;我内心的埃利泽·尤德科斯基说：“哦，是的，那将是一个有趣的星期二。”除了不，它实际上不会很有趣。比如说，你可以在现实世界中无限期地自主复制的窗口很窄，但是集中的人为干预可以阻止它，而且我们都仍然活着并控制着未来，而且我们也没有看到快速的推动进入 ASL-5。&lt;/p&gt;&lt;p&gt;这是指数中人们最难看到的部分——即使没有真正的递归自我改进，一旦它可以无限地自行复制，它很可能可以做很多其他的事情，或者很快就能够做很多其他的事情事情，包括说服人类积极协助它，以及我们重新获得控制权的选择，特别是以合理的成本（例如形状不像“关闭互联网”的事情）可能会极其迅速地减少。&lt;/p&gt;&lt;p&gt;同样，如果你有一个可以极大促进人工智能研究的人工智能，那么你的时间很快就会耗尽。&lt;/p&gt;&lt;p&gt;在我看来，滥用标准是一种切入现实的奇怪方式。我认为我应该关心模型能够实现什么，而不是与其他问题相比它能够实现什么。该点感觉像是放置阈值的合理数量级的位置，但需要清理。&lt;/p&gt;&lt;p&gt;我还会考虑其他什么事情会触发这个。例如，什么程度的说服才能做到这一点？&lt;/p&gt;&lt;p&gt;大多数情况下，我很欣赏 ASL-4 的“界限在哪里”，这让我们注意到，距离我们想要的 ASL-5 的时间距离很可能不是用几年来衡量的。&lt;/p&gt;&lt;h4&gt;规格不足&lt;/h4&gt;&lt;p&gt;我同意 Simeon 的观点，即所有 RSP 的一个关键问题是规格不足。&lt;/p&gt;&lt;p&gt;风险应明确说明。它们应该以概率和幅度来量化。应提前写下预期的（上限？）失败概率。&lt;/p&gt;&lt;p&gt;像“不太可能”这样的词，比如“不太可能在现实世界中持续存在，并且不太可能克服旨在防止其窃取自身重量的简单安全措施”，需要进行量化。我也不知道这里的各个步骤隐含了多少个 9 的安全性。我想至少三个，最少两个。也没有暗示 ASL-3 不存在自我复制危险的更广泛假设。&lt;/p&gt;&lt;p&gt;在看不见的情况下，您会在这里为 GPT-5 分配多少个 9 的安全性？给双子座？&lt;/p&gt;&lt;p&gt;您到底担心人们能够用模型做什么？我确实知道，随着更广泛的技术变革，这可能会成为一个不断变化的目标。但如果你不选择严格的定义，就很容易被忽视。&lt;/p&gt;&lt;p&gt;听起来我在评估 RSP 时是否觉得我不信任你？是的。我正在评估 RSP，就好像我不信任你一样。这样的政策必须假设我们不认识的其他人可能正在实施它，实施它的人将承受巨大的压力，而那些拥有最终权力杠杆的人可能不值得信任。&lt;/p&gt;&lt;p&gt;如果你的政策只有在你值得信赖时才有效，它仍然会有所帮助，但这是一个糟糕的假设。它不能应用于任何其他适应你的规则的公司，或任何将它们作为监管一部分的政府。&lt;/p&gt;&lt;h4&gt; Anthropic 的 RSP 要点&lt;/h4&gt;&lt;p&gt;Anthropic 在很多方面都受到了极大的赞誉。他们已经覆盖了很多基地。他们做出了许多重要的单边承诺，特别是安全承诺。很明显，他们的员工很担心，并且他们在很多层面上都感受到了这种担心，尽管不是协调困难的全部范围。承诺构建财务结构以允许暂停是一场大游戏。&lt;/p&gt;&lt;p&gt;正如我在过去几周所讨论的以及许多其他人所指出的，尽管做出了这些崇高的努力，但他们的 RSP 仍然存在缺陷。它有很好的元素，特别是它关于模型部署和模型权重安全性（而不是训练和测试）的内容，但其他元素读起来就像是未来元素的欠条，或者承诺在未来进行更多思考并做出合理的决策。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; &lt;a href="https://www.lesswrong.com/posts/Np5Q3Mhz2AiPtejGN/we-re-not-ready-thoughts-on-pausing-and-responsible-scaling-4?commentId=wZvRhb7NRkhi9XGpb" rel="noreferrer noopener" target="_blank"&gt;Oliver Habryka&lt;/a&gt; ：我认为 Akash 的声明，即 Anthropic RSP 基本上没有指定任何会导致它们停止扩展的实际条件，这对我来说似乎是正确的。&lt;/p&gt;&lt;p&gt;他们有一些部署措施，这些措施与何时停止扩展的问题无关，然后他们有一些与安全相关的措施，但这些措施与模型的行为没有任何关系，并且是那种人类可以随时选择做的事情，无论事实如何发展。&lt;/p&gt;&lt;p&gt;我认为阿卡什是对的，人择 RSP 并没有具体回答您引用他的两个问题：&lt;/p&gt;&lt;p&gt; RSP 没有具体说明 Anthropic 将停止扩展模型的条件（它只是说为了继续扩展，它将实施一些安全措施，但这不是经验条件，因为 Anthropic 有信心它可以实施列出的安全措施）&lt;/p&gt;&lt;p&gt; RSP 没有具体说明在什么条件下 Anthropic 将扩展到 ASL-4 或更高，尽管他们承诺会给出这些条件。&lt;/p&gt;&lt;p&gt;我同意 RSP 说了很多其他的话，并且对 Akash 所说的内容有一些不准确的解释，但我确实认为在这个（IMO 最重要的问题）上，RSP 似乎很安静。&lt;/p&gt;&lt;p&gt;我确实认为部署措施是真实的，尽管我目前认为部署模型的风险并不大，因此它们似乎与我无关（并且认为核心问题是是什么阻止了组织在第一名）。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;核心思想仍然是定期检查培训，如果有危险，则实施适当的安全协议。&lt;/p&gt;&lt;p&gt;我担心这些安全协议来得太晚了——与 RSP 的要求相比，您至少要领先一步。否则，您就无法及时锁定所需的安全措施，以防您的体重被盗。&lt;/p&gt;&lt;p&gt;有关 ASL-4 及更高版本的大部分细节仍未明确。没有迹象表明什么会导致模型过于危险而无法训练或评估，只要它没有发布并且其模型权重是固定的。隐含功能的逐渐改善。&lt;/p&gt;&lt;p&gt;因此，我认为这里的隐性威胁模型不足。&lt;/p&gt;&lt;p&gt;整个计划是基于以下前提：大多数重要的是安全组合（其他窃取危险的AI）和部署（您让他们访问野外危险的AI）。在ASL-2上，这是合理的选择，但是我已经不信任这一点，因为它被识别为ASL-3，并且假定合理地将其分类为ASL-4或更高的东西是错误的。&lt;/p&gt;&lt;p&gt;随着能力的增加，人们应该越来越多地恐惧，直到否则才能进行红色小组练习或任何互动的微调，其中人类看到了人工智能的大量输出，然后这些人可以自由地在世界上行动，或者否则是任何可用的负担。人们应该担心在某个时候自己运行功能测试，可能会从ASL-3开始，并从关注到ASL-4的基线假设。&lt;/p&gt;&lt;p&gt;从ASL-4的合理定义开始，我会开始担心训练本身的行为，实际上，这是选择ASL-4阈值的一种方法，然后我会担心有身体上能力或攻击向量我们还没有想象。&lt;/p&gt;&lt;p&gt;总的来说，我希望更多地尊重未知的未知数，并且期望AI能想到我们无法想到的事情，它将想到我们未能考虑的事情。其中一些会很危险。&lt;/p&gt;&lt;p&gt;整个计划也以平滑的功能缩放为前提。这个想法是，如果您在有效的计算中检查每个4倍，并且您有一个过去的功能，那么您可以有一个很好的了解。我认为不能肯定会认为这将来会有。&lt;/p&gt;&lt;p&gt;同样，该计划假设您的微调和试图踩踏每个增量版本的尝试又将很好地预期其他人在以后才能完善您的技术时能够处理系统。别人可以改善您的技术，但只有有限的数量。&lt;/p&gt;&lt;p&gt;我还担心，还有其他攻击向量，或者情况发生了错误的方式，该系统不太可能捕捉到，其中一些我已经考虑过，而有些我还没有，也许有些人根本无法预测。知道您的ASL-4系统在这样的文档检查的方式上是安全的，我认为将它们安全部署是不够的。&lt;/p&gt;&lt;p&gt;安全心态部分是反对局外人的攻击（理想情况下，我也在那里提高了赌注，关于现实的赌注不是在曲线上进行评分，而是在曲线上进行分级），但是关于模型本身的危险，没有足够的安全心态。由于广泛访问和部署模型而导致的社会，政治和经济动态也没有考虑。&lt;/p&gt;&lt;p&gt;除了缺少细节外，还缺乏对牙齿训练停顿的艰难承诺。相比之下，有良好的艰苦承诺要用牙齿停顿部署。仅当通过指定指标部署才有意义时，部署才会发生。尽管培训只会在他们认为没有足够的安全措施的情况下暂停，他们可以随时选择实施这些措施。 las，我认为这些程序就足够了。&lt;/p&gt;&lt;p&gt;我们在谈论价格吗？或许。&lt;/p&gt;&lt;p&gt; Anthropic的其他陈述似乎是通用的“正确的手势”陈述，而没有实质性地增加现有政策。您可以说，RSP是一个真正的文件，其中旨在具有意义，人们关心的单词，而其他人则是外交词，旨在将其提交会议。&lt;/p&gt;&lt;p&gt;这不是对拟人化的敲门。智者知道哪些文件是哪个文件。 RSP本身是重要的文档。&lt;/p&gt;&lt;h4&gt;其他反应&lt;/h4&gt;&lt;p&gt;&lt;a href="https://www.lesswrong.com/posts/dxgEaDrEBkkE96CXr/thoughts-on-responsible-scaling-policies-and-regulation." rel="noreferrer noopener" target="_blank"&gt;保罗·克里斯蒂安诺（Paul Christiano）很喜欢&lt;/a&gt;。&lt;/p&gt;&lt;blockquote&gt;&lt;ul&gt;&lt;li&gt;指定一组具体的评估结果，该结果将导致它们移至ASL-3。我认为必须采取具体作用的具体阈值很重要，而且我认为提议的阈值已经足够早，可以在不可逆的灾难之前触发，并具有很高的可能性（超过90％）。&lt;/li&gt;&lt;li&gt;对ASL-3的安全目标做出具体陈述 - “非国家行为者不太可能能够窃取模型权重，而高级威胁参与者（例如国家）在没有巨大费用的情况下无法窃取他们”，并描述了他们期望的安全措施以实现这一目标。&lt;/li&gt;&lt;li&gt;在扩展ASL-3之前，需要董事会发布和批准ASL-4的定义和评估协议。&lt;/li&gt;&lt;li&gt;提供有关将触发ASL-4和必要的保护措施在ASL-4运行的条件的初步指导）。&lt;/li&gt;&lt;/ul&gt;&lt;/blockquote&gt;&lt;p&gt; &lt;a href="https://www.lesswrong.com/posts/dxgEaDrEBkkE96CXr/thoughts-on-responsible-scaling-policies-and-regulation?commentId=HMAL8nK47hD6XBTCw" rel="noreferrer noopener" target="_blank"&gt;贝丝·巴恩斯（Beth Barnes）&lt;/a&gt;还指出，还有一些其他的细节，例如每任务的推理费用为1000美元，并且通过10％的阈值。&lt;/p&gt;&lt;p&gt;保罗还找到了改进的空间，并欢迎批评：&lt;/p&gt;&lt;blockquote&gt;&lt;ul&gt;&lt;li&gt;目前指定混凝土评估的另一面是它们非常粗糙和初步。我认为值得努力进行更好的评估，并具有更明显的风险关系。&lt;/li&gt;&lt;li&gt;为了使外部利益相关者对拟人的安全有信心，我认为要进行适当的审核和红色团队需要更多的工作。据我所知，这项工作并未由任何人完成，将需要时间。&lt;/li&gt;&lt;li&gt;批准更改RSP的过程是董事会出版和批准。我认为这可以确保将有意做出决定，而且总比没有好得多，但是要进行有效的独立监督会更好。&lt;/li&gt;&lt;li&gt;在某种程度上，有可能对ASL-4提供更清楚的情况，这样做是通过让人们有机会检查和辩论该水平的条件，这将是一个重大改进。在某种程度上，不希望提供有关审查或决策过程的更多具体性，以确定给定的一组安全，保障和评估措施是否足够。&lt;/li&gt;&lt;/ul&gt;&lt;/blockquote&gt;&lt;p&gt;在监督中，我认为您想对比对RSP更强与宽松的更改进行对比。如果您想加强RSP并引入新的承诺，我就不必担心监督。如果公司想退缩以前的承诺，通常是在特定情况下，即使这不愿做出承诺，我确实希望监督。&lt;/p&gt;&lt;p&gt;我强烈同意，围绕ASL-4更加清晰。在较小程度上，我们需要更加清楚审核和红色团队。&lt;/p&gt;&lt;h4&gt;无法交流&lt;/h4&gt;&lt;p&gt;正如许多其他人所指出的那样，我以前在每周的AI帖子中指出，如果人类部署了最近的承诺，同时也强烈提倡采取更强大的进一步行动，并指出这只是一个秒针，我将是第一步，我将会鼓掌。&lt;/p&gt;&lt;p&gt;对于所有的错误，无论我说的很多错误，这不是RSP，因为没有RS，并且仍然有太多的ious，漏洞和假设，似乎他们正在尝试。达里奥（Dario）在峰会上对RSP的说法是朝着这些问题朝着正确方向迈出的重要一步。&lt;/p&gt;&lt;p&gt;问题是，当您将RSP与试图绘画暂停倡导者或要求进一步行动的陈述相结合时已经做了一个合理的故事，许多人同意这是一个可能会使净影响负面影响的症结。&lt;/p&gt;&lt;p&gt;下面讨论的达里奥（Dario）最近的评论是朝着正确方向迈出的重要一步。&lt;/p&gt;&lt;p&gt;无论如何，这仍然是一个不错的开端，我欢迎进一步讨论如何付款，其细节得到改善以及其威胁模型完成。想要讨论的人类（或任何其他实验室）的任何人，请伸出援手。&lt;/p&gt;&lt;h4&gt; Openai政策&lt;/h4&gt;&lt;p&gt;&lt;a href="https://openai.com/global-affairs/our-approach-to-frontier-risk#risk-informed-development-policy" rel="noreferrer noopener" target="_blank"&gt;其次，Openai&lt;/a&gt;称其为风险信息的发展政策。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; RDP 还将提供一系列行动来防止灾难性后果。对灾难性风险的实证理解刚刚起步，并且正在迅速发展。因此，我们将动态更新对当前前沿模型风险水平的评估，以确保反映我们最新的评估和监控理解。我们正在组建一个专门的团队（准备）来推动这项工作，包括进行必要的研究和监测。&lt;/p&gt;&lt;p&gt; RDP 旨在补充和扩展我们现有的风险缓解工作，这有助于新的高性能系统在部署之前和之后的安全性和一致性。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;这使灾难的前部和中心与Deepmind的方法形成鲜明对比，甚至比人类更重要。&lt;/p&gt;&lt;p&gt;应该注意的是，过去GPT模型的部署决策，尤其是GPT-4，一直谨慎。对GPT-4进行了评估和微调六个月以上。即使在较早的型号上也有意节奏。这里的记录还不错，即使他们随后迅速部署了各种互补能力。 &lt;a href="https://twitter.com/labenz/status/1727327424244023482" rel="noreferrer noopener" target="_blank"&gt;内森·莱本兹（Nathan Lebenz）在部署GPT-4的部署中提供了更多颜色&lt;/a&gt;，如果您错过了它，则建议使用它，绝对是混合的袋子。&lt;/p&gt;&lt;p&gt;如果Openai继续保持谨慎的水平，他们将其用于GPT-3和GPT-4（适应新情况），只要危险的步骤仍保持部署，这似乎就可以了，因为我希望他们能够解决什么不知道什么你有问题。之后，我们需要威胁模型才能正确调整。&lt;/p&gt;&lt;p&gt; Openai政策的问题在于，它不会将它们承担到任何事情上。它不使用数字或确定会导致发生的事情。取而代之的是，这是一项原则的陈述，Openai关心灾难性风险，并会不断监视它。我很高兴看到这一点，但这不能代替实际政策。&lt;/p&gt;&lt;p&gt;据我所知， &lt;a href="https://blogs.microsoft.com/on-the-issues/2023/10/26/microsofts-ai-safety-policies/#Responsible_Capability_Scaling" rel="noreferrer noopener" target="_blank"&gt;微软&lt;/a&gt;正在有效地向OpenAI刺穿此类问题。&lt;/p&gt;&lt;h4&gt;深态政策&lt;/h4&gt;&lt;p&gt;&lt;a href="https://deepmind.google/public-policy/ai-summit-policies/#responsible-capabilities-scaling" rel="noreferrer noopener" target="_blank"&gt;接下来，深度&lt;/a&gt;。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; &lt;strong&gt;Google认为，必须采取负责任的AI方法。为此，Google的&lt;/strong&gt;&lt;a href="https://ai.google/principles/" rel="noreferrer noopener" target="_blank"&gt;&lt;strong&gt;AI原则&lt;/strong&gt;&lt;/a&gt;于2018年&lt;strong&gt;引入&lt;/strong&gt;&lt;strong&gt;，&lt;/strong&gt;&lt;strong&gt;指导产品开发，并帮助我们评估每个AI应用程序。&lt;/strong&gt;根据这些原则，根据以下目标，我们评估了我们的AI应用程序：&lt;/p&gt;&lt;p&gt; &lt;strong&gt;&lt;em&gt;1.&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;有益于社会。&lt;/em&gt; &lt;strong&gt;&lt;em&gt;2.&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;避免制造或强化不公平的偏见。&lt;/em&gt; &lt;strong&gt;&lt;em&gt;3.&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;构建并测试安全性。&lt;/em&gt; &lt;strong&gt;&lt;em&gt;4.&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;对人负责。&lt;/em&gt; &lt;strong&gt;&lt;em&gt;5.&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;纳入隐私设计原则。&lt;/em&gt; &lt;strong&gt;&lt;em&gt;6.&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;维护卓越科学的高标准。&lt;/em&gt; &lt;strong&gt;&lt;em&gt;7.&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;可用于符合这些原则的用途。&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;我不反对这些原则，但是那里没有任何具体的能够确保负责任的扩展。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;此外，我们不会在以下&lt;a href="https://ai.google/principles/#:~:text=AI%20applications%20we%20will%20not%20pursue" rel="noreferrer noopener" target="_blank"&gt;领域&lt;/a&gt;设计或部署AI：&lt;/p&gt;&lt;p&gt; &lt;strong&gt;&lt;em&gt;1.&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;导致或可能造成总体危害的技术。在存在损害的物质风险的情况下，我们只会在我们认为收益大大超过风险的情况下继续进行，并将纳入适当的安全限制。&lt;/em&gt;&lt;/p&gt;&lt;p&gt; &lt;strong&gt;&lt;em&gt;2.&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;武器或其他主要目的或实施的技术是造成或直接促进对人的伤害。&lt;/em&gt;&lt;/p&gt;&lt;p&gt; &lt;strong&gt;&lt;em&gt;3.&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;违反国际公认规范收集或使用信息进行监视的技术。&lt;/em&gt;&lt;/p&gt;&lt;p&gt; &lt;strong&gt;&lt;em&gt;4.&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;其目的违反广泛接受的国际法和人权原则的技术。&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;这是危险的滥用模型。这很好，但是在这种情况下，不足以不促进滥用。从理论上讲，可能造成总体伤害可能意味着某些事情，但我认为这并不考虑灾难性风险。&lt;/p&gt;&lt;p&gt;该文件继续使用“伦理”一词，“安全”一词显然与平凡的伤害有关。当然，灾难性的伤害确实引起了道德问题和平凡的关注，但该文件似乎并不认识到敌人是谁。&lt;/p&gt;&lt;p&gt;至少有这样：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;该过程将在整个模型的生命周期中起作用如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;训练之前：&lt;/strong&gt; Frontier模型是通过将其预期性能与已经通过该过程的类似模型进行比较，并允许在预计风险中误差的一些差距来分配初始分类。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;在训练过程中：&lt;/strong&gt;监视模型的性能，以确保其不超过其预测性能。某些类别中的模型可能在培训期间应用了缓解。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;训练后：&lt;/strong&gt;应用适合初始类别分配的训练后缓解措施。为了放松缓解，可以将模型提交给专家委员会进行审查。专家委员会借鉴了风险评估，红色团队，风险域分析的准则以及其他适当的证据，以调整分类（如果适当）。&lt;/li&gt;&lt;/ul&gt;&lt;/blockquote&gt;&lt;p&gt;这不像拟人化的相似之处那样具体，但我非常喜欢训练之前和期间的重点，并且承诺在观察到意外强劲的性能并在现场进行缓解。当然，没有谈论这些缓解措施或将如何触发它们。&lt;/p&gt;&lt;p&gt;再次，我更喜欢这个文档，而不是没有文档，但是那里几乎没有具体，没有任何约束力，而重点完全是滥用。&lt;/p&gt;&lt;p&gt;其余的DeepMind政策表明，所有正确的通用事物都不会使任何人都不归咎于他们不做任何事情。充其量，他们是指输入，如果方便且重要的是，很容易避免或忽略。他们的“ AI for Good”投资组合中有一些独特的有前途的项目。&lt;/p&gt;&lt;h4&gt;亚马逊，拐点和元&lt;/h4&gt;&lt;p&gt;&lt;a href="https://aws.amazon.com/uki/cloud-services/uk-gov-ai-safety-summit/#Responsible_Capability_Scaling_" rel="noreferrer noopener" target="_blank"&gt;亚马逊有一些段落在各种事情上打手势&lt;/a&gt;。无论他们的实际行为如何，这都是通用的公司在网站上的风格。慈善的观点是，亚马逊不是在开发前沿模型，因此他们不需要这样的政策是真实的，至少还没有。 &lt;a href="https://twitter.com/simonw/status/1730798295323398642" rel="noreferrer noopener" target="_blank"&gt;有关亚马逊Q的最新报道&lt;/a&gt;表明，亚马逊的协议确实不足以满足他们当前的需求。&lt;/p&gt;&lt;p&gt;&lt;a href="https://inflection.ai/frontier-safety" rel="noreferrer noopener" target="_blank"&gt;感染坚定地相信安全&lt;/a&gt;，希望您知道这一点，并打算在确保其利益安全的同时开展业务。这里没有内容。&lt;/p&gt;&lt;p&gt; &lt;a href="https://transparency.fb.com/en-gb/policies/ai-safety-policies-for-safety-summit#responsible-capability-scaling" rel="noreferrer noopener" target="_blank"&gt;这留下了Meta&lt;/a&gt; ，声称他们“开发了更安全的AI系统”，并且使用Llama“在整个过程中优先考虑安全和责任”。&lt;/p&gt;&lt;p&gt;大概是通过反对他们。或者，或者，这是一个词没有意义的地方。他们并不是假装自己打算以安全性的名义做任何有机会工作的事情，鉴于他们打算开源。&lt;/p&gt;&lt;p&gt;他们做了广泛的红色队伍吗？他们确实意识到了整个“两个小时”，以撤消所有安全性的问题，对，帕德梅问？他们的“剥离后响应”部分简单地说“请参见上文”，这至少是完全自我意识的，一旦您部署了这样的系统，就无能为力，因此请查看模型发布部分确实是正确的答案。然后，人们可以提供错误报告，如果使他们感觉更好？&lt;/p&gt;&lt;p&gt;他们确实在“安全控制”下说，包括确保模型权重”，这是一家致力于有意分享其模型权重的公司的棘手类别，他们确实会保护“未发行的”模型权重，因此没有人窃取信誉。&lt;/p&gt;&lt;p&gt;他们不愿意采取最基本的步骤，并承诺如果模型足够有能力，就不会故意释放其模型权重。除了其他失败之外，他们打算直接摧毁整个项目的核心要素之一，是故意的，原则上，使他们固有地不安全的所有事情都无意停止，除非被迫停下来。&lt;/p&gt;&lt;h4&gt;一些其他相对排名&lt;/h4&gt;&lt;p&gt;&lt;a href="https://www.lesswrong.com/posts/ms3x8ngwTfep7jBue/thoughts-on-the-ai-safety-summit-company-policies" rel="noreferrer noopener" target="_blank"&gt;Nate Sores的人类疮也是最好的，&lt;/a&gt;但是Openai也很接近，同时指出Matthew Gray看上去更近，并且Openai和Anthropic一样好：&lt;/p&gt;&lt;blockquote&gt;&lt;ul&gt;&lt;li&gt; SOARES：拟人&amp;gt; openai &amp;gt;&amp;gt; deepmind&amp;gt; Microsoft &amp;gt;&amp;gt; Amazon &amp;gt;&amp;gt; Meta&lt;/li&gt;&lt;li&gt;灰色：openai≈拟人&amp;gt;&amp;gt; deepmind &amp;gt;&amp;gt; Microsoft&amp;gt; Amazon &amp;gt;&amp;gt; Meta&lt;/li&gt;&lt;li&gt; ZVI：人类&lt;/li&gt;&lt;li&gt;CFI审稿人（英国政府）：拟人化&amp;gt;&amp;gt;deepmind≈microsoft≈microsoft≈openai &amp;gt;&amp;gt;亚马逊&amp;gt;&amp;gt; meta&lt;/li&gt;&lt;/ul&gt;&lt;/blockquote&gt;&lt;p&gt;我猜想，CFI审稿人正在研究各个观点，而我们其余的人都在整体上看。&lt;/p&gt;&lt;h4&gt; Dario Amodei的重要澄清&lt;/h4&gt;&lt;p&gt;&lt;a href="https://www.anthropic.com/index/uk-ai-safety-summit" rel="noreferrer noopener" target="_blank"&gt;此陈述对于向RSP提供适当的上下文非常重要。&lt;/a&gt;考虑完整阅读，我将摘下关键段落。&lt;/p&gt;&lt;p&gt;我将首先跳到最后，最重要的声明所在。大胆的地雷。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;达里奥·阿莫迪（Dario Amodei）（Anthropic CEO）：最后，我想讨论RSP与监管之间的关系。 &lt;strong&gt;RSP不是用作调节的替代品，而是用于其原型的原型。&lt;/strong&gt;我并不是说我们希望拟人化的RSP从字面上写入法律&lt;strong&gt;- 我们的RSP只是解决困难问题的首次尝试，并且几乎可以肯定地以多种方式不完美&lt;/strong&gt;。重要的是，当我们开始执行第一次迭代时，我们希望了解如何合理地实现此类承诺。我们的希望是，RSP的一般思想将在整个公司之间得到完善和改进，并且与世界各地的政府（例如这个房间的政府）同时，可以采取每个人的最佳要素，并将其变成良好的元素具有问责制和监督的精心设计和审核制度。我们想鼓励在RSP风格的框架中进行“竞赛” 。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;如果人类对此更清楚，包括在RSP本身以及整个沟通和呼吁政府采取行动的范围内，人类始终清楚我们必须面对的威胁的本质以及应对它们所需要的事情，这将解决围绕当前RSP的许多焦虑和异议。&lt;/p&gt;&lt;p&gt;如果我们将其与ASL-4及其响应的良好操作结合在一起，那么当时对扩展的要求超出了确保模型权重和扣留部署的要求，以及澄清如何在模型测试之前处理潜在水平的歧义尤其是一旦发生这种情况，我会认为这一发展非常好。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; Dario Amodei（Anthropic CEO）：快速改进的&lt;em&gt;总体&lt;/em&gt;趋势是可以预见的，但是，实际上很难预测AI何时会获得&lt;em&gt;特定的&lt;/em&gt;技能或知识。不幸的是，这包括危险技能，例如建造生物武器的能力。因此，我们面临着许多潜在的与AI相关的威胁，尽管鉴于当今的系统相对有限，但在不久的将来的某个未知点可能会变得非常严重。这与大多数其他行业大不相同：想象一下，如果每种新型号的汽车都有机会自发发芽新的（和危险的）力量，例如发射火箭提升或加速到超音速速度的能力。&lt;/p&gt;&lt;p&gt; Toby Ord：这是Dario Amodei的非常有用的澄清。最近对RSP的许多批评一直是假设作为监管的替代者，它们可能是空洞的承诺。但是，作为监管的原型，他们有价值的案例要多得多。&lt;/p&gt;&lt;p&gt;山姆·奥特曼（Sam Altman）：直到我们去训练[GPT-5]之前，[预测其确切能力]对我们来说就像一个有趣的猜测游戏。我们正在努力做得更好，因为从安全角度来看，预测功能很重要。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;我认为达​​里奥（Dario）和其他许多人对总体趋势的可预测性过高，这是制造有效的RSP的问题。正如达里奥（Dario）指出的那样，即使我对此有错，特定技能也无法预测，并且通常只在部署后才能发现。&lt;/p&gt;&lt;p&gt;这是他概述RSP和ASL的意义，以及在概念上设计了拟人RSP。&lt;/p&gt;&lt;blockquote&gt;&lt;ul&gt;&lt;li&gt;首先，我们提出了一个称为&lt;em&gt;AI安全水平（ASL）&lt;/em&gt;的系统，该系统以国际认可的BSL系统来处理生物材料的模型。每个ASL级别都有一个&lt;em&gt;IF-THEN&lt;/em&gt;结构：&lt;em&gt;如果&lt;/em&gt;AI系统具有某些危险功能，&lt;em&gt;那么&lt;/em&gt;我们将不会部署它或训练更强大的模型，直到某些保障措施到位为止。&lt;/li&gt;&lt;li&gt;其次，我们经常沿计算缩放曲线定期测试这些危险功能。这是为了确保我们不会盲目创造危险的功能，甚至不知道我们已经这样做。&lt;/li&gt;&lt;/ul&gt;&lt;/blockquote&gt;&lt;p&gt;我想清楚地说，有了正确的细节，这很棒。我们都在谈论价格。问题是：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;每个级别的触发条件是什么？&lt;/li&gt;&lt;li&gt;确定是否满足触发条件的必要方法是什么？&lt;/li&gt;&lt;li&gt;每个级别的必要保障措施是什么？是什么使扩展安全？&lt;/li&gt;&lt;/ol&gt;&lt;blockquote&gt;&lt;p&gt; ASL-3是AI模型在CBRN地区灾难性滥用上有用的点。&lt;/p&gt;&lt;p&gt; ASL-4必须在达到ASL-3时严格定义。&lt;/p&gt;&lt;p&gt; ASL-4代表了ASL-3的灾难性滥用风险的升级，并且还增加了一种新的风险：对自主AI系统的担忧，这些自主AI系统逃避人类的控制并对社会构成重大威胁。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;到目前为止，许多关心的问题是无法定义ASL-4或对其的反应，同时缺乏对拟人化（以及Google和Openai）的信心，远离ASL-3甚至ASL-4。而且，我们担心ASL-4的预防措施（在不再部署的模型上完全安全地安全）将不足。人类有关生物武器的一些陈述表明，克劳德在其内部状态下已经接近ASL-3。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;作为首席执行官，我个人在RSP上花费了10-20％的时间3个月 - 除了设计和提出ASL系统外，我还从头开始写了多个草稿。我的一位联合创始人将其50％的时间用于开发RSP 3个月。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;我确实认为这很重要，不应被视为便宜的谈话。谢谢你，达里奥。&lt;/p&gt;&lt;h4&gt;关于此类政策的战略思想&lt;/h4&gt;&lt;p&gt;&lt;a href="https://www.lesswrong.com/posts/dxgEaDrEBkkE96CXr/thoughts-on-responsible-scaling-policies-and-regulation." rel="noreferrer noopener" target="_blank"&gt;保罗·克里斯蒂安诺（Paul Christiano）提供了他的高级思想，这是支持的&lt;/a&gt;。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;保罗·克里斯蒂安（Paul Christiano）：我认为实施负责任规模政策的开发人员现在增加了有效监管的可能性。如果我认为这会使法规变得更加困难，那么我将有大量保留。&lt;/p&gt;&lt;p&gt; RSP的透明度使外部利益相关者更容易理解AI开发人员的政策是否足以管理风险，并为辩论和改善压力创造了一个焦点。&lt;/p&gt;&lt;p&gt;我认为快速AI开发的风险非常大，即使是非常好的RSP也不会完全消除这种风险。耐用，全球，有效执行和包括硬件的暂停AI开发将进一步降低风险。我认为这在政治和实践上都是充满挑战的，并且会有重大成本，因此我不希望它是餐桌上唯一的选择。我认为，实施RSP可以根据更广泛的观点和信念获得最大的利益，并有助于促进其他有效的法规。&lt;/p&gt;&lt;p&gt; ……&lt;/p&gt;&lt;p&gt;但是，目前的风险水平足够低，我认为&lt;strong&gt;如果公司或各国有足够好的计划来检测和对增加风险的反应，则&lt;/strong&gt;可以继续AI开发。&lt;/p&gt;&lt;p&gt; ……&lt;/p&gt;&lt;p&gt;我认为，良好的RSP将阐明需要暂停进一步发展的特定条件。&lt;/p&gt;&lt;p&gt; ……&lt;/p&gt;&lt;p&gt;因此，“负责任的缩放策略”可能不是正确的名称。我认为重要的是物质：开发人员应明确规定危险能力与必要保护措施之间关系的路线图，应描述衡量危险能力的具体程序，如果功能通过危险限制而没有保护措施会议，则应布置响应路线图。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;如保罗指出，RSP是否使足够好的调节更容易或更难像清晰的症结所一样。如果RSP通过迭代和显示外观和标准化良好的政策来使足够好的调节更容易，并且这种影响在当前条件下占主导地位？然后，即使不幸的是命名，RSP显然也是好的，尤其是众多的是一个很大的积极步骤。如果RSP相反使监管更加困难，因为它们本身被视为替代或妥协，并且这种效果在很大程度上占主导地位，那可能使他们可能做得更好。&lt;/p&gt;&lt;p&gt;霍尔顿·卡诺夫斯基（Holden Karnofsky）在“ &lt;a href="https://www.lesswrong.com/posts/Np5Q3Mhz2AiPtejGN/we-re-not-ready-thoughts-on-pausing-and-responsible-scaling-4" rel="noreferrer noopener" target="_blank"&gt;我们还没有准备好：暂停和负责任的规模政策的想法&lt;/a&gt;”中解释了他对RSP的一般支持。&lt;/p&gt;&lt;p&gt;他沸腾了，他说：&lt;/p&gt;&lt;ol&gt;&lt;li&gt; AGI可能很快到达（&amp;gt; 10％）。&lt;/li&gt;&lt;li&gt;我们还没有准备好，我们可以做好充分的准备是不现实的。&lt;/li&gt;&lt;li&gt;首先是全球停顿。&lt;/li&gt;&lt;li&gt;我们没有该选项。&lt;/li&gt;&lt;li&gt;部分停顿存在深层缺陷，尤其是如果它们可以在不合时宜的时间结束，并且可能适得其反。&lt;/li&gt;&lt;li&gt; RSP在不同的观点上提供了“强大的良好妥协”。&lt;/li&gt;&lt;li&gt; RSP的缺点是，它们可能被认为是足够的，而不是必要的，阻碍了进一步的行动。政府可能无所事事，或者仅仅将现有的RSP纳入法律，然后停在那里。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;我从本质上完全同意点＃1，＃2，＃3，＃4和＃7。&lt;/p&gt;&lt;p&gt;在为时已晚之前，我更加乐观，但在未来几年内似乎极不可能。至少，鉴于我们有多少时间的不确定性，我们将不明智地将所有鸡蛋放入这样的篮子里。&lt;/p&gt;&lt;p&gt;第5点很复杂。当然，通过差异性地阻止好演员，或者，当暂停结束使我们处于更糟糕的位置时，这种停顿可以通过差异化的方式适得其反。在网络上，我仍然期望净收益，即使相对有缺陷的停顿努力，包括它们变成完整或明智地实施的停顿的潜力。但是我还指出，即使是部分停顿，我们也不是那么接近。&lt;/p&gt;&lt;p&gt;第6点在很大程度上取决于RSP的细节中的魔鬼，以及考虑的重要性是7。&lt;/p&gt;&lt;p&gt;如果我们能得到真正重要的实验室采用的真实RSP，那么确保这些实验室实际上采取了预防措施，这些预防措施提供了有意义的安全余地，并且在明显的停顿是必要的时会导致停顿，那么这似乎伟大的。如果我们的收入要少，那么这似乎不太伟大，在这两个方面，我们都必须问少多少？&lt;/p&gt;&lt;p&gt;即使是相对较好的RSP，如果它可以代替政府行动或其他预防措施，甚至变成了前方比赛的理由。当然，如果我们要全速前进，那么任何预防措施都会有所帮助。似乎很明显，我们将至少采取一些监管预防措施。&lt;/p&gt;&lt;p&gt; &lt;a href="https://www.lesswrong.com/posts/Np5Q3Mhz2AiPtejGN/we-re-not-ready-thoughts-on-pausing-and-responsible-scaling-4?commentId=DyyGinph6hbwiHHo5" rel="noreferrer noopener" target="_blank"&gt;正如阿卡什（Akash）在最高评论中指出的那样&lt;/a&gt;，这非常重视RSP的沟通。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; Akash：我希望Arc和Anthropic对此更加清楚，如果他们大声说清楚，我对他们的RSP帖子会不太批评。我认为[Holden]的帖子很响亮（您多次陈述，您认为您认为监管是必要的，并且您希望世界有更多的政治意愿来监管）。我很感激，很高兴您写了这篇文章。&lt;/p&gt;&lt;p&gt; ……&lt;/p&gt;&lt;p&gt;我认为对现状的一些改进可能是净负面的，因为它们要么（a）在不正确的框架中固定，要么（b）采取有限的政治意愿/注意力窗口，并将其转向更弱的事物&lt;/p&gt;&lt;p&gt;如果每个人都对RSP进行交流很清楚，他们不希望它被视为足够，那就太好了。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;如果ARC，Anthropic和其他人对此很清楚，那么我会为它们采用甚至像当前的人类政策一样严重有缺陷的RSP感到兴奋。 las，他们的沟通并不清楚这一点，而是一直在积极劝阻那些试图将Overton窗口朝着我所看到的行动推向必要的行动的人。 RSP以及尝试做更大事情的尝试是好的，因为替代了更大的事物，它们不好。&lt;/p&gt;&lt;p&gt;因此，正如Aysja所解释的那样，我强烈不喜欢诸如“坚强的良好妥协”之类的短语。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; Aysja：不过，在元水平上，我对一些框架选择感到脾气暴躁。您和原始的弧线都使用了这种措辞：负责任的缩放策略是“强大的良好妥协”，或者在ARC的情况下，它们是“务实的中间立场”。我认为这些立场是理所当然的，前进的最佳途径是妥协的，但这对我来说似乎还很明显。&lt;/p&gt;&lt;p&gt;当然，并非&lt;em&gt;所有&lt;/em&gt;“人们都有不同信念和偏好”的案例是妥协是最好的解决方案的情况。如果有人想杀死我，我将不愿谈论我可以接受多少肢体。&lt;/p&gt;&lt;p&gt; ……&lt;/p&gt;&lt;p&gt;在一致性很难的世界中，而Evals无法确定实际上令人恐惧的行为，然后我声称这种浮游品的存在是有关的。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;且仅当获得良好的结果时，妥协才是好的。妥协导致预防措施不足以提供许多额外的保护或将可靠地错过警告信号的预防措施，这是不值得的。&lt;/p&gt;&lt;p&gt;阿卡什的其他关键点加强了这一点。这是其他粗体文本。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; Akash：如果Anthropic的RSP是我们要获得的最好的RSP，那么Yikes，此RSP计划的表现不佳。&lt;/p&gt;&lt;p&gt;我认为RSP框架是错误的，我不希望监管机构将其用作构件。我似乎是合理的，政府愿意从更严格和更敏感的事情开始，这比“继续前进，直到我们能够证明该模型具有非常危险的能力”&lt;/p&gt;&lt;p&gt;我认为对现状的一些改进可能是净负面的，因为它们要么（a）在不正确的框架中固定，要么（b）采取有限的政治意愿/注意力窗口，并将其转向更弱的事物&lt;/p&gt;&lt;p&gt;我至少希望RSP被更名，并且与RSP进行交流的人更加小心。&lt;/p&gt;&lt;p&gt;更雄心勃勃地，我希望从事RSP的人们认真考虑这是否是最适合或提倡的人。&lt;/p&gt;&lt;p&gt;我认为，从事RSP的每个工作都应该花费至少几个小时认真对待AIS社区可能提倡更强大的政策建议。&lt;/p&gt;&lt;p&gt;瑞安（Ryan）：我认为好的RSP实际上会给实验室承担举证责任。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt; Anther的中心关注点是， &lt;a href="https://www.lesswrong.com/posts/jyM7MSTvy8Qs6aZcz/what-s-up-with-responsible-scaling-policies#How_well_can_you_tell_if_a_given_model_is_existentially_dangerous_" rel="noreferrer noopener" target="_blank"&gt;实验室最终会通过最大化安全指标而成为Goodhart定律的受害者&lt;/a&gt;吗？这种关注并不是RSP尤其不利。如果人们想安全洗净并从技术上检查一个框，那么对于控制的人来说，这是有效的。唯一有效的解决方案，除了使指标尽可能牢固，是由旨在实际保护灾难并有权这样做的人做出决定。在牢记的同时，最终权威无法在多个地方休息。&lt;/p&gt;&lt;p&gt; &lt;a href="https://www.navigatingrisks.ai/p/responsible-scaling-policies-are" rel="noreferrer noopener" target="_blank"&gt;Simeon认为&lt;/a&gt;，当前的RSP是不足的足够组合，可以轻描淡写的风险和廉价谈话的启用，应将其视为默认的净负面。&lt;/p&gt;&lt;p&gt;相反，他建议适应风险管理中现有的最佳实践，目前实施的RSP失败了，因此缺乏。特别是，风险阈值被指定，而不是以可能性或幅度表示。评估不足以全面。当然，名称会更改，并清楚该政策的作用和不做。&lt;/p&gt;&lt;p&gt;他还指出了他所谓的《白骑士条款》的危险，您说您的危险缩放是别人更危险的缩放是合理的。正如Simeon指出的那样，如果这是标准，那么每个人都可以指向其他所有人，并说“我是安全的人”。 The open source advocate points to the close source, the closed to the open, and so on, including in places where it is less obvious who is right.&lt;/p&gt;&lt;p&gt; Anthropic&amp;#39;s such clause is at least limited, but such clauses should not be used. If you choose to race ahead unsafely, then you should have to do this by violating your safety policy. In a sufficiently extreme situation, that could even be the right thing to do. But you need to own up to it, and accept the consequences.没有理由。 While knowing that fooling people in this way is easy, and you are the easiest person to fool.&lt;/p&gt;&lt;h4&gt;结论&lt;/h4&gt;&lt;p&gt;Anthropic&amp;#39;s RSP would, if for now called an RDP and accompanied by good communications, be a strong first step towards a responsible policy. As it is, it still represents a costly signal and presents groundwork we can build upon, and we do have at least a start to such good communication. Much further work is needed, and there is the concern that this will be used to justify taking fewer precautions elsewhere especially without strong public communications, but that could also go the other way, and we do not know what is being communicated in private, or what the impact is on key actors.&lt;/p&gt;&lt;p&gt; The other policies are less good. They offer us insight into where everyone&amp;#39;s head is at. OpenAI&amp;#39;s policies are good in principle and they acted remarkably cautiously in the past, but they do not commit to anything, and of course there may be fallout from recent events there. DeepMind&amp;#39;s document is better than nothing, but shows their heads are not in the right place, although Google has other ways in which it is cautious.&lt;/p&gt;&lt;p&gt; The Amazon, Inflection and Meta statements were quite poor, as we expected.&lt;/p&gt;&lt;p&gt; Going forward, we will see if the statements, especially those of OpenAI and Anthropic, can pay down their debts and evolve into something with teeth. Until then, it is ultimately all cheap talk.&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/yRJNCDp7LHyHGkANz/on-responsible-scaling-policies-rsps#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Tue, 05 Dec 2023 16:10:08 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/yRJNCDp7LHyHGkANz/on-responsible-scaling-policies-rsps</guid></item><item><title>我们谁都跑不了</title><link>https://www.lesswrong.com/posts/A4nfKtD9MPFBaa5ME/we-re-all-in-this-together</link><description>发布于 2023 年 12 月 5 日下午 1:57（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;历史似乎一直试图告诉我们一件事：未来的内容是由权力、经济、政治和其他&lt;a href="https://slatestarcodex.com/2018/01/24/conflict-vs-mistake/"&gt;冲突理论问题&lt;/a&gt;决定的。&lt;/p&gt;&lt;p&gt;事实证明，不！&lt;/p&gt;&lt;p&gt;未来的几乎所有内容都取决于首先解决以下两个工程问题中的哪一个：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;如何构建超级智能人工智能（如果先解决，&lt;a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities"&gt;每个人都会永远死亡&lt;/a&gt;）&lt;/li&gt;&lt;li&gt;如何构建一个&lt;em&gt;一致的&lt;/em&gt;超级智能人工智能（如果首先解决，每个人都会得到&lt;a href="https://www.lesswrong.com/posts/CMHogeqTTajhmnEKx/everything-is-okay"&gt;乌托邦&lt;/a&gt;）&lt;/li&gt;&lt;/ul&gt;&lt;p&gt; ……目前前者更有可能发生的几乎所有原因都是&lt;a href="https://slatestarcodex.com/2018/01/24/conflict-vs-mistake/"&gt;&lt;em&gt;错误理论&lt;/em&gt;&lt;/a&gt;的原因。&lt;/p&gt;&lt;p&gt;目前采取行动增加{前者首先被解决}的概率的人并不是试图杀死所有人的邪恶之人，他们是&lt;em&gt;困惑的&lt;/em&gt;人，他们认为他们的行为实际上增加了{后者首先被解决}的概率。&lt;/p&gt;&lt;p&gt;现在，当然，你是否有机会与 OpenAI/Deepmind/Anthropic 的领导层交谈，足以告诉他们他们实际上让事情变得更糟&lt;em&gt;，这&lt;/em&gt;取决于经济和政治等因素。但归根结底，对于这里真正重要的部分来说，这是一个&lt;em&gt;解释&lt;/em&gt;的问题，而不是&lt;em&gt;击败的&lt;/em&gt;问题。&lt;/p&gt;&lt;p&gt;当然，“乌托邦”的实现细节确实取决于谁启动了一致的超级智能人工智能，但我希望您会对当前摆在桌面上的任何可能性所带来的乌托邦感到非常满意。您错过的绝大多数实用程序是因为&lt;em&gt;根本没有乌托邦并且每个人都会永远死去&lt;/em&gt;，而不是&lt;em&gt;获得错误的乌托邦实现细节&lt;/em&gt;。&lt;/p&gt;&lt;p&gt;最有可能的结果是每个人都会永远死亡，因为那些影响这些结果将要发生的人是&lt;em&gt;错误的&lt;/em&gt;（并且可能没有认真思考这个问题而意识到他们是错误的）。&lt;/p&gt;&lt;p&gt;他们&lt;strong&gt;不是邪恶&lt;/strong&gt;的，让他们更新到正确的逻辑信念是一个理性问题（而且，如果他们是那种很容易受到周围其他人的想法影响的弱主体，模因），而不是一个问题冲突。&lt;/p&gt;&lt;p&gt;他们极大地保护了每个人的利益，&lt;em&gt;包括他们自己的利益&lt;/em&gt;。他们采取的正确行动将极大地服务于他们自己&lt;em&gt;以及其他所有人的&lt;/em&gt;利益。如果人工智能杀死所有人，他们也会死，如果人工智能创造乌托邦，他们将和其他人一起得到乌托邦——而这些几乎是唯一的两个吸引子。&lt;/p&gt;&lt;p&gt;我们谁都跑不了。我们中的一些人只是相当困惑，没有主动追求真理，而且他们的信仰可能因模因等影响而产生巨大偏差。但我很确定负责人没有人&lt;em&gt;故意&lt;/em&gt;试图杀死所有人；他们只是&lt;em&gt;偶然地&lt;/em&gt;试图杀死所有人。&lt;/p&gt;&lt;p&gt;如果你不使用你的权力/金钱来影响这两种结果中哪一个更有可能发生，那么你的权力/金钱就&lt;em&gt;完全没有用处&lt;/em&gt;。如果我们都死了，它们就没有用处，如果我们得到乌托邦，它们就没有用处。现在，如果你想&lt;em&gt;以任何方式&lt;/em&gt;影响几乎所有的未来（除了接下来的 0 到 5 年，这大约是我们拥有的时间），资源的唯一用途就是影响其中哪些首先解决两个工程问题。&lt;/p&gt;&lt;p&gt;这既适用于主要人工智能组织的负责人，也适用于其他所有人。一个人在人工智能组织中的角色除了影响这两个问题中哪一个首先得到解决之外&lt;em&gt;没有任何用处&lt;/em&gt;。如果及时解决了对齐问题，OpenAI 的负责人不会比其他人特别获得一个更闪亮的乌托邦，如果没有及时解决，他们也不会比其他人死得更少。&lt;/p&gt;&lt;p&gt;权力/金钱/成为 OpenAI 的领导者&lt;em&gt;在奇点之后不会做任何事情&lt;/em&gt;。现在&lt;strong&gt;唯一&lt;/strong&gt;重要的是这两个工程问题中的哪一个首先得到解决。&lt;/p&gt;&lt;br /&gt;&lt;br /&gt;&lt;a href="https://www.lesswrong.com/posts/A4nfKtD9MPFBaa5ME/we-re-all-in-this-together#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Tue, 05 Dec 2023 13:57:54 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/A4nfKtD9MPFBaa5ME/we-re-all-in-this-together</guid></item><item><title>与我的学生的苏格拉底式对话</title><link>https://www.lesswrong.com/posts/TtdJt78mgGiDubnAM/a-socratic-dialogue-with-my-student</link><description>发布于 2023 年 12 月 5 日上午 9:31（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;p&gt;这是我和我的学生诺姆之间的对话。经他许可，以编辑形式转载。评论时，请考虑他是一个青少年。其中许多想法对他来说都是&lt;a href="https://xkcd.com/1053/"&gt;新的&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;怎样才能招到学生呢？你偷了它们。他以前的老师是一位马克思主义者。我在辩论中彻底摧毁了他以前的老师，以至于他放弃了她的教诲，现在转而听我的。&lt;/p&gt;&lt;p&gt;我认为这段对话展示了良好的教学技巧。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;我让诺姆来判断什么是合理的、什么是有道理的、什么是“证据”。在诺姆出生之前，我参加了我的第一次辩论比赛。这个障碍稍微缩小了差距。&lt;/li&gt;&lt;li&gt;我提出一系列问题，而不只是说“ &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;是真的”。这使得&lt;a href="https://www.lesswrong.com/posts/NMoLJuDJEms7Ku9XS/guessing-the-teacher-s-password"&gt;密码猜测&lt;/a&gt;变得不可能。他是在下棋，不是在&lt;i&gt;危险边缘！&lt;/i&gt;&lt;/li&gt;&lt;li&gt;我避免告诉诺姆我的信仰，除非他明确询问。这对诺姆来说更有趣，因为没有人喜欢未经请求的讲道。这也更有说服力，因为结论感觉像是他的结论。&lt;/li&gt;&lt;li&gt;当诺姆改变话题时，我立即退缩了。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;诺姆：&lt;/strong&gt;我知道你反对免除学生贷款债务。你能告诉我为什么吗？我这样做是为了一场演讲和辩论比赛。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;Lsusr：&lt;/strong&gt;你&lt;a href="https://www.youtube.com/watch?v=o_wNNjfCG1E&amp;amp;t=4s"&gt;以前不相信&lt;/a&gt;支持救济的论点吗？当然，重复曾经说服你的论点并不困难。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;诺姆：&lt;/strong&gt;我不知道我现在是否有足够的研究来与像你这样的人辩论。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;Lsusr：&lt;/strong&gt;你并不是想说服我。你正试图说服&lt;a href="https://www.youtube.com/watch?v=xuaHRN7UhRo"&gt;&lt;i&gt;他们&lt;/i&gt;&lt;/a&gt;。利用他们的偏见、非理性、部落主义和无知。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;诺姆：&lt;/strong&gt;我还必须安抚评委们。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;Lsusr：&lt;/strong&gt;我就是这么说的。&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;&lt;strong&gt;诺姆：&lt;/strong&gt;我正在努力寻找一个关于学生贷款减免的好论据。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;Lsusr：&lt;/strong&gt;但是你以前不是很赞同吗？当然，你可以重复曾经说服你的糟糕论点。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;诺姆：&lt;/strong&gt;这些都是道德争论，没有任何经济理解。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;Lsusr：&lt;/strong&gt;没关系。你的听众可能是经济文盲。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;诺姆：&lt;/strong&gt;不知何故，我认为我们作为赞成免除所有学生贷款债务的一方赢得了一次胜利。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;Lsusr：&lt;/strong&gt;干得好。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;诺姆：&lt;/strong&gt;谢谢。&lt;/p&gt;&lt;hr /&gt;&lt;p&gt; &lt;strong&gt;Lsusr：&lt;/strong&gt;你听说过“有效利他主义”吗？您可能会喜欢他们推出的一些东西。它往往具有道德一致性和经济素养（与主要的民主共和党、社会主义等政治纲领不同）。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;诺姆：&lt;/strong&gt;不，但我会调查一下。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;Lsusr：&lt;/strong&gt;你可能不同意。但我预计它的智力稳健性会让你耳目一新。&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;&lt;strong&gt;诺姆：&lt;/strong&gt;这是否意味着我自杀然后将我所有的器官捐献给需要它们的人是道德的？我想，除非我能在不自杀的情况下拯救更多的生命。也许更好的论点是自杀，让某人卖掉我所有的身体部位，然后用这笔钱购买疟疾网，送给生活在非洲的人们。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;Lsusr：&lt;/strong&gt;你可以在不自杀的情况下拯救更多生命。而且，我想不出有哪个 EA 曾为此案自杀过。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;诺姆：&lt;/strong&gt;可能是因为我们直觉上认为自杀是错误的。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;Lsusr：&lt;/strong&gt;不要因为肾脏的事情而分心。基本思想如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;美国政府需要花费 10,000,000 美元才能挽救一个美国人的生命。&lt;/li&gt;&lt;li&gt;在非洲，通过公共卫生措施挽救一条生命需要 5,000 美元。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这就是我上个月为非洲公共卫生措施捐赠 20 美元的原因。它的作用相当于美国联邦政府花费的 40,000 美元。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;诺姆：&lt;/strong&gt;是的，确实如此。在美国靠什么拯救生命？&lt;/p&gt;&lt;p&gt; &lt;strong&gt;Lsusr：&lt;/strong&gt;基本的想法是你应该计算数字。&lt;/p&gt;&lt;figure class="media"&gt;&lt;div&gt;&lt;div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;诺姆：&lt;/strong&gt;我认为这对钱有用，但我不知道它是否可以完全应用于所有事情。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;Lsusr：&lt;/strong&gt;为什么不呢？具体例子。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;诺姆：&lt;/strong&gt;嗯，这取决于你是否认为人类应该拥有受保护的权利。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;Lsusr：&lt;/strong&gt;这不是一个具体的例子。您的主张可能适用于什么现实世界的决定？请明确点。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;诺姆：&lt;/strong&gt;一位医生有5名病人需要器官移植，否则他们就会死。有一名完全健康的人因接受小手术而处于麻醉状态。如果我们仔细计算一下数字，医生应该杀死那个人才能挽救五个人的生命。如果你认为人类有权利，那就是不道德的。如果你认为人类不会，那就不会了。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;Lsusr：&lt;/strong&gt;正确。这显然是不道德的。但人权并不是医生不应该谋杀病人的唯一原因。你能想到一种实用主义的吗？&lt;/p&gt;&lt;p&gt;&lt;strong&gt;诺姆：&lt;/strong&gt;医生会失去执照，然后他们就会失业。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;Lsusr：&lt;/strong&gt;如果没有许可证要求怎么办？比如在战区。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;诺姆：&lt;/strong&gt;患者将来也许能够挽救生命。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;Lsusr：&lt;/strong&gt; 5 个器官接受者也可以。另一个原因。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;诺姆：&lt;/strong&gt;我不确定。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;Lsusr：&lt;/strong&gt;没有人会去看他们认为会谋杀他们的医生。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;诺姆：&lt;/strong&gt;如果是战区，那么可能没有其他选择。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;Lsusr：&lt;/strong&gt;公平。您熟悉“义务论伦理学”这个词吗？&lt;/p&gt;&lt;p&gt;&lt;strong&gt;诺姆：&lt;/strong&gt;是的。任何有最好意图的事情都是如此。那是对的吗？我可能已经忘记了。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;Lsusr：&lt;/strong&gt;没有。这不是最好的意图。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;诺姆：&lt;/strong&gt;好的。之后怎么样了？&lt;/p&gt;&lt;p&gt; &lt;strong&gt;Lsusr：&lt;/strong&gt;义务论伦理遵循“不要杀死你的病人”这样的良好规则。 EA 相信要处理数字，但它们通常不会违反义务论限制。当我捐20美元时，我捐的是我自己的钱。我没有偷它。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;诺姆：&lt;/strong&gt;好的。让我想想我是否发现任何缺陷。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;Lsusr：&lt;/strong&gt;慢慢来。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;诺姆：&lt;/strong&gt;如果你遵循我认为好的规则，并且你帮助了最多的人，那么我不可能反对。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;Lsusr：&lt;/strong&gt;那是 EA。但不仅仅是人。他们的素食主义者数量远远超过了应有的比例。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;诺姆：&lt;/strong&gt;好的。稍微不相关的问题：您对素食主义者有何看法？&lt;/p&gt;&lt;p&gt; &lt;strong&gt;Lsusr：&lt;/strong&gt;我已经几个月没吃肉了。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;诺姆：&lt;/strong&gt;这对你来说是环境问题还是对杀害动物的道德反对？还是健康？&lt;/p&gt;&lt;p&gt; &lt;strong&gt;Lsusr：&lt;/strong&gt;对我的健康可能会产生负面影响。环境影响对我来说无关紧要。我不在乎杀死动物。如果你能找到一个符合道德来源的汉堡，那么我很乐意吃它。问题是，我们的动物产品默认来自工厂化农场，那是人间地狱。 [更正：我在家人的感恩节晚餐上吃了一些肉汁。]&lt;/p&gt;&lt;p&gt;&lt;strong&gt;诺姆：&lt;/strong&gt;这对我来说很有趣，我不一定不同意你的推理。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;Lsusr：&lt;/strong&gt;我尽量不将我的信仰和价值观强加给别人。这就是为什么直到你问我才提到这一点。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;诺姆：&lt;/strong&gt;如果你说折磨动物是错误的，那么杀死动物不也是不道德的吗？&lt;/p&gt;&lt;p&gt; &lt;strong&gt;Lsusr：&lt;/strong&gt;动物干净的死亡几乎没有什么痛苦，特别是与漫长而美好的生命相比。我正在努力减少痛苦，同时遵守义务论限制。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;诺姆：&lt;/strong&gt;你认为道德考虑的重要性应该与事物的先进程度成正比吗？抱歉，如果我措辞不好。现在已经是深夜了，我正在等待电源恢复，这样我就可以做剩下的作业了。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;Lsusr：&lt;/strong&gt;我知道你的意思。答案=是。如果我必须在拯救两只牛和一个人之间做出选择，那么人类是显而易见的选择。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;诺姆：&lt;/strong&gt;好的。我同意你的看法。&lt;/p&gt;&lt;hr /&gt;&lt;p&gt; &lt;strong&gt;Lsusr：&lt;/strong&gt;辩论进行得怎么样？&lt;/p&gt;&lt;p&gt;&lt;strong&gt;诺姆：&lt;/strong&gt;你的假设是非常正确的，即法官们都是经济文盲。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;lsusr：&lt;/strong&gt;哈哈哈哈哈哈&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;&lt;strong&gt;Lsusr：&lt;/strong&gt;你喜欢吗？我觉得你很喜欢辩论比赛。干得好，能够与校队的孩子们竞争。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;诺姆：&lt;/strong&gt;是的。非常有趣。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;Lsusr：&lt;/strong&gt;我也喜欢高中辩论。我做了3-4年。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;诺姆：&lt;/strong&gt;尝试捍卫错误的立场很有趣，因为这很困难。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;Lsusr：&lt;/strong&gt;如果你的信念是错误的（所以你认为这是正确的立场）怎么办？那很难吗？&lt;/p&gt;&lt;p&gt;&lt;strong&gt;诺姆：&lt;/strong&gt;我知道不是在这个问题上，因为正如你所说，这就像争论天空是否是蓝色的。但对于其他一些诸如“美国应该在&amp;lt;地方&amp;gt;部署更多军队”之类的问题，很难看出什么是正确的。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;Lsusr：&lt;/strong&gt;出去吧。直视。准确地告诉我你看到的是什么颜色。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;诺姆：&lt;/strong&gt;现在是黑色的。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;Lsusr：&lt;/strong&gt;通常，辩论比赛决议的定义（故意）如此模糊，以至于它们可能是正确的，也可能是错误的，这取决于它们的解释方式。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;诺姆：&lt;/strong&gt;据我所知，无论你如何解释，这个都是错误的。我认为“全部”这个词几乎让人无法辩护。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;美国联邦政府应免除所有联邦学生贷款债务。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt; &lt;strong&gt;Lsusr：&lt;/strong&gt;假设民主国家的每个人（错误地）都支持学生贷款减免。民选政府是否应该尊重人民的意愿？&lt;/p&gt;&lt;p&gt;&lt;strong&gt;诺姆：&lt;/strong&gt;是的，因为如果他们不这样做，就会开创一个不服从人民的危险先例。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;Lsusr：&lt;/strong&gt;那么你所要做的就是表明绝大多数美国选民支持贷款减免。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;诺姆：&lt;/strong&gt;比起 3.4% 的通胀率，我更担心这种影响。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;Lsusr：&lt;/strong&gt;不用担心。绝大多数美国选民支持愚蠢得多的政策。美国的通胀率应该是多少？&lt;/p&gt;&lt;p&gt;&lt;strong&gt;诺姆：&lt;/strong&gt;我的直觉是 0%，但有一些我不知道的经济小事表明一个国家应该有&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;程度的通货膨胀。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;Lsusr：&lt;/strong&gt;这是我制作的一个长达一小时的 YouTube 视频，试图传达这个问题的复杂性。 【我就是右边那个戴面具的人。】&lt;/p&gt;&lt;figure class="media"&gt;&lt;div&gt;&lt;div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/figure&gt;&lt;p&gt;为什么它应该为零？&lt;/p&gt;&lt;p&gt;&lt;strong&gt;诺姆：&lt;/strong&gt;我认为，我在经济知识方面的差距正在显现，但当一种货币尽可能值钱时，这不是很好吗？而且，我的力量现在又恢复了。所以我要做作业然后去睡觉。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;Lsusr：&lt;/strong&gt;如果你希望货币尽可能值钱，那么我们应该有负通胀率。晚安！保证充足的睡眠。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;诺姆：&lt;/strong&gt;噢，你说得对。我把这归咎于“凌晨2点”。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;Lsusr：&lt;/strong&gt;不。你的问题并不愚蠢。这很难。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;诺姆：&lt;/strong&gt;我想我应该记住数字可能会下降。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;lsusr：&lt;/strong&gt; 🤑&lt;/p&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/TtdJt78mgGiDubnAM/a-socratic-dialogue-with-my-student#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Tue, 05 Dec 2023 09:31:05 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/TtdJt78mgGiDubnAM/a-socratic-dialogue-with-my-student</guid></item><item><title>对齐的神经不确定性估计</title><link>https://www.lesswrong.com/posts/79eegMp3EBs8ptFqa/neural-uncertainty-estimation-for-alignment</link><description>发布于 2023 年 12 月 5 日上午 8:01（格林威治标准时间）&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;介绍&lt;/h2&gt;&lt;p&gt;假设您已经构建了一些人类价值观的人工智能模型。你输入一个情况，它就会给出一个良好度评级。您可能想问：“此优度评级的误差线是多少？”除了了解误差线之外，不确定性估计在人工智能内部也很有用：指导主动学习&lt;span class="footnote-reference" id="fnref80ywo0pbl8r"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn80ywo0pbl8r"&gt;[1]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 、纠正&lt;a href="https://www.lesswrong.com/posts/5gQLrJr2yhPzMCcni/the-optimizer-s-curse-and-how-to-beat-it"&gt;优化器的诅咒&lt;/a&gt;&lt;span class="footnote-reference" id="fnrefq23duy4ut0g"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnq23duy4ut0g"&gt;[2]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;或进行分布外检测&lt;span class="footnote-reference" id="fnref3dij2j9svj8"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn3dij2j9svj8"&gt;[3]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。&lt;/p&gt;&lt;p&gt;我最近出于一个&lt;a href="https://www.lesswrong.com/s/aJvgWxkCBWpHpXti4/p/nA3n2vfCy3ffnjapw"&gt;喜欢的原因&lt;/a&gt;进入了神经网络（NN）的不确定性估计文献：我认为这对于量化人工智能潜在特征的有效性域的对齐很有用。如果我们&lt;a href="https://www.lesswrong.com/posts/w4aeAFzSAguvqA5qu/how-to-go-from-interpretability-to-alignment-just-retarget"&gt;将人工智能指向其世界模型中的某个概念&lt;/a&gt;，那么对该概念的实现进行优化可能会因为将该概念推到其有效范围之外而出错。&lt;/p&gt;&lt;p&gt;但现在就先把对齐的想法放在你的后口袋里吧。这篇文章主要是对不确定性估计文献的调查，其中夹杂着我自己的看法。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;贝叶斯神经网络图片&lt;/h2&gt;&lt;p&gt;贝叶斯神经网络图是几乎所有神经网络不确定性估计方法的鼻祖，因此从这里开始是合适的。&lt;/p&gt;&lt;p&gt;图片很简单。您从参数的先验分布开始。您的训练数据就是证据，在对其进行训练后，您将获得更新的参数分布。给定输入，您可以通过贝叶斯神经网络传播输入来计算输出的分布。&lt;/p&gt;&lt;p&gt;这一切都是非常正确且无关紧要的（“当然，让我更新模型&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-msubsup"&gt;&lt;span class="mjx-base"&gt;&lt;span class="mjx-mn"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;所有&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px;"&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;参数&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;上&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;的&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;万亿&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;维&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;联合&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;分布&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;”），除了&lt;i&gt;实际训练神经网络确实有点就这样工作&lt;/i&gt;。&lt;i&gt; &lt;/i&gt;如果您使用对数似然损失和 L2 正则化，并且您的参数先验是高斯分布，则最小化损失的参数将位于贝叶斯神经网络所具有的分布的峰值&lt;span class="footnote-reference" id="fnreffo4svcpvxs"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnfo4svcpvxs"&gt;[4]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; &lt;span class="footnote-reference" id="fnrefcogdul3x2xj"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fncogdul3x2xj"&gt;[5]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。&lt;/p&gt;&lt;p&gt;这是因为损失景观和参数不确定性之间存在桥梁。 &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;贝叶斯&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;规则&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;表示&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;P&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;（&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;参数&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mspace" style="width: 0.278em; height: 0px;"&gt;&lt;/span&gt; &lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;|&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mspace" style="width: 0.278em; height: 0px;"&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;数据&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;集&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;=&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space3"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;P&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;(&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;参数&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;⋅&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;P&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;(&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;数据&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;集&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mtext MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mspace" style="width: 0.278em; height: 0px;"&gt;&lt;/span&gt; &lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;|&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mspace" style="width: 0.278em; height: 0px;"&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;参数&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;）&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;/&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;P&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;（&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;数据&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;集&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;）&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;。&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;这里&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;P&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;参数&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mspace" style="width: 0.278em; height: 0px;"&gt;&lt;/span&gt; &lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;|&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mspace" style="width: 0.278em; height: 0px;"&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;数据&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;集&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;是&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;要&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;估计&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;的&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;后&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;验&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;分布&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;，&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;P&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;(&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;参数&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;s&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;⋅&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;P&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;数据&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;集&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mtext MJXc-space2"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mspace" style="width: 0.278em; height: 0px;"&gt;&lt;/span&gt; &lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;|&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mspace" style="width: 0.278em; height: 0px;"&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;参数&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;s&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;是&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;损失&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;的&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;指数&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;[&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;6&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;]&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class="footnote-reference" id="fnrefamk58pvab4"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnamk58pvab4"&gt;_&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;这适用于物理隐喻，例如“参数的分布是位于损失盆地底部的玻尔兹曼分布”。&lt;/p&gt;&lt;p&gt;根据经验，通过假装遵循贝叶斯神经网络图来计算神经网络的不确定性效果非常好，以至于一篇关于集成方法的好论文&lt;span class="footnote-reference" id="fnreflpb1b2qcoen"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnlpb1b2qcoen"&gt;[7]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;将其称为“基本事实”。当然，要在这里实际计算任何东西，你必须进行近似，如果你进行快速而肮脏的近似（例如假装你可以从 Hessian 找到损失盆地的形状），你会得到不好的结果&lt;span class="footnote-reference" id="fnrefd9jz9mfml"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnd9jz9mfml"&gt;[8]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; ，但人们正在做这些天，蒙特卡罗方法变得很聪明&lt;span class="footnote-reference" id="fnrefbct5kii2m07"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnbct5kii2m07"&gt;[9]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; ，他们发现贝叶斯神经网络计算的更好近似可以得到更好的结果。&lt;/p&gt;&lt;p&gt;但对损失景观进行蒙特卡罗遍历的成本很高。对于大规模应用的技术，它必须只对运行模型的成本施加很小的乘数，并且如果您希望它变得普遍，那么它施加的成本必须非常小。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;合奏团&lt;/h2&gt;&lt;p&gt;解决不确定性的一种完全不同的方法是集成&lt;span class="footnote-reference" id="fnrefen0yeoqzg0k"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnen0yeoqzg0k"&gt;[10]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。只需训练十几个模型，询问他们的建议，并估计传播的不确定性。所有事情的数十倍成本乘数都很陡峭，但如果您经常查询模型，它比损失情况的蒙特卡洛估计便宜。&lt;/p&gt;&lt;p&gt;集成在理论上很简单。您不需要假装模型经过训练可以收敛，不需要专门针对预测损失进行训练，甚至不需要固定的架构。您只需选择一些想要分散不确定性的模型分布并进行采样。&lt;/p&gt;&lt;p&gt;你可以用合奏做一些聪明的事情。通过计算集成如何适应贝叶斯神经网络图片，您会了解到改变正则化的零点可能是个好主意，否则您将在模型的泛化方式中得到虚假相关性&lt;span class="footnote-reference" id="fnreflpb1b2qcoen"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnlpb1b2qcoen"&gt;[7]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。您可以拆分数据集并训练单独的较小模型，然后巧妙地聚合这些模型（类似于&lt;a href="https://www.kaggle.com/code/prashant111/bagging-vs-boosting"&gt;装袋和提升&lt;/a&gt;），以降低集成的计算溢价&lt;span class="footnote-reference" id="fnrefen0yeoqzg0k"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnen0yeoqzg0k"&gt;[10]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。这些论文中暗示，聪明的集成技巧在更大的尺度上并不那么重要，但目前还不清楚收益是否完全为零。&lt;/p&gt;&lt;p&gt;集成的一个棘手部分是，如果一枚硬币正面朝上的概率为 51%，并且您已训练神经网络以获得正确答案，那么集成中的每个成员都会预测正面。正确答案并不不确定，因此您的团队表示不存在不确定性。如果您希望不确定性度量包含环境中的熵，则必须训练神经网络来估计该熵，这在很大程度上放弃了使用非预测损失的自由。&lt;/p&gt;&lt;p&gt;解释时的类似关注适用于在模型的潜在特征上使用集成，尽管我在文献中没有看到人们这样做。假设您训练了十几个模型，并用有关狗的数据探测它们，为每个模型找到一个“狗向量”。您可以对它们的大小和方差进行标准化，然后使用集合的方差作为“狗向量的不确定性”。这并不是关于狗的完全不确定性，因为它没有衡量人工智能模型中关于狗的不确定性，这只是不同模型根据特定探测方法的内部表示的传播。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;关于任意不确定性的注释&lt;/h2&gt;&lt;p&gt;文献中很大一部分文字是关于任意不确定性和认知不确定性之间的区别&lt;span class="footnote-reference" id="fnrefxqxuuw1xe2"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnxqxuuw1xe2"&gt;[11]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。当我说集成会告诉你由于建模选择而导致的输出的不确定性时，这就是我必须谈论的区别，但不会告诉你人工智能内部对环境的不确定性。在不确定性估计文献中，由于模型方差而产生的不确定性被称为“认知性”，而人工智能环境模型内部的不确定性被称为“随意性”。 &lt;span class="footnote-reference" id="fnref2jiolbj6tn8"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn2jiolbj6tn8"&gt;[12]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;在大数据限制下，认知不确定性相对于任意不确定性变得很小（除非你故意将模型推入高度认知不确定性的情况）。有些论文忘记了这一点，做了一些愚蠢的事情，使他们的认知不确定性估计更大，因为他们认为这应该是总的不确定性，这就是为什么其他论文必须用章节来讨论这种区别。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;添加噪声，由于某种原因通常会丢失&lt;/h2&gt;&lt;p&gt;如果您想要不确定性估计，您可以将随机噪声添加到神经网络的中间激活中。如果输出对噪声更敏感，则不确定性更大，如果输出不太敏感，则不确定性更小&lt;span class="footnote-reference" id="fnrefud7l8eu73jk"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnud7l8eu73jk"&gt;[13]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。有一些自然的启发式论证可以解释为什么这是有意义的&lt;span class="footnote-reference" id="fnreflgnhfp0m35e"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnlgnhfp0m35e"&gt;[14]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; ，并且通过更多的工作，您可以尝试将其与贝叶斯神经网络图和损失景观的蒙特卡洛估计联系起来。&lt;/p&gt;&lt;p&gt;或者，您可以忽略抽象参数并使用 dropout 作为随机噪声分布&lt;span class="footnote-reference" id="fnrefhfox6k7gfev"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnhfox6k7gfev"&gt;[15]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。&lt;/p&gt;&lt;p&gt;哦，当然，人们给出了理由，但我认为这里的第一印象是正确的，添加 dropout 然后采样在理论上是愚蠢的。但在实验上，它的效果很好，人们一直在谈论它&lt;span class="footnote-reference" id="fnref0hl862nst9f9"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn0hl862nst9f9"&gt;[16]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; &lt;span class="footnote-reference" id="fnrefu11lguv41wc"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnu11lguv41wc"&gt;[17]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; &lt;span class="footnote-reference" id="fnrefhzxdsrsco9g"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnhzxdsrsco9g"&gt;[18]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; ，而且它不需要你做任何额外的训练。&lt;/p&gt;&lt;p&gt;为什么它起作用的一个谜题可能是，在具有一些噪声样本的数据集上训练的网络将学会输出一个包罗万象的先验以响应噪声&lt;span class="footnote-reference" id="fnreffqljh5j8ipf"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnfqljh5j8ipf"&gt;[19]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。我怀疑 dropout 是足够大的噪音，它将网络推向这个先验，这有助于过度自信。&lt;/p&gt;&lt;p&gt;我希望人们对更小的、非丢失的噪声进行更多的比较&lt;span class="footnote-reference" id="fnrefvl2ahm3user"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnvl2ahm3user"&gt;[20]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。这在理论上似乎更合理，尽管当翻译回贝叶斯术语时，将噪声注入内部层似乎对应于有趣但不寻常的噪声分布。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;校准&lt;/h2&gt;&lt;p&gt;文献&lt;span class="footnote-reference" id="fnref0hl862nst9f9"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn0hl862nst9f9"&gt;[16]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;的很大一部分是人们只是试图获取神经网络的输出并对它们应用简单的函数来获得不确定性估计。我将简要地对待他们。&lt;/p&gt;&lt;p&gt;第 0 级只是按面值获取 NN 输出。当数据较多且参数不确定性较小时，神经网络的直接预测可以很好地估计不确定性。例如，基础 GPT-4 在分配给下一个标记的概率分布中得到了很好的校准，包括当这些下一个标记是以前从未见过的测试问题的答案时&lt;span class="footnote-reference" id="fnrefzkojoz799ka"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnzkojoz799ka"&gt;[21]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。即使是非分布，这也比什么都没有好——正如我上面提到的，如果神经网络的训练集包含意外数据，那么它们确实会学会不确定意外数据&lt;span class="footnote-reference" id="fnreffqljh5j8ipf"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnfqljh5j8ipf"&gt;[19]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; ，尽管它们仍然倾向于过度自信。&lt;/p&gt;&lt;p&gt;随着模型的泛化能力越来越强，我预计它们的输出能够在更大的领域得到很好的校准。相反，对于在有限数据上训练小型模型的应用程序，您将需要一种不同的方法来估计不确定性。&lt;/p&gt;&lt;p&gt;通常人们会尝试比第 0 级更奇特一些，并做一些事情，比如调整 softmax 函数的参数，以最大限度地提高对保留验证集的校准&lt;span class="footnote-reference" id="fnrefnglt70rybd9"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnnglt70rybd9"&gt;[22]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。这也很容易与其他方法结合作为最终校准步骤&lt;span class="footnote-reference" id="fnrefgx8tqfun0w"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fngx8tqfun0w"&gt;[23]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。&lt;/p&gt;&lt;p&gt;如果您有分布外 (OOD) 数据样本，您还可以做更奇特的事情，例如同时进行 OOD 检测和校准&lt;span class="footnote-reference" id="fnreftmgq2x5m6t"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fntmgq2x5m6t"&gt;[24]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。或者，如果您有 OOD 数据样本并且是频率论者，则可以进行保形预测&lt;span class="footnote-reference" id="fnrefg3rtbqm4247"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fng3rtbqm4247"&gt;[25]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。&lt;/p&gt;&lt;p&gt;校准的一个卖点是您可以对任何经过训练的模型执行此操作。但如果您愿意放弃这一点并干预训练，就有一些方法可以改进模型的校准。这可能看起来像使用额外的正则化&lt;span class="footnote-reference" id="fnrefqbdlqupa3x"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnqbdlqupa3x"&gt;[26]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;或对比示例&lt;span class="footnote-reference" id="fnref2qfcvdqkx2y"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn2qfcvdqkx2y"&gt;[27]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;进行训练。这些也在一定程度上提高了 OOD 泛化能力，对抗性训练也是如此&lt;span class="footnote-reference" id="fnrefazba93s687"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnazba93s687"&gt;[28]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。&lt;/p&gt;&lt;p&gt;最终，网络输出的校准似乎并没有达到我想要的不确定性估计方法的效果。其一，它不会估计潜在特征的不确定性，而是与您拥有数据的输出的不确定性有关。&lt;/p&gt;&lt;p&gt;另一方面，它&lt;a href="https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation"&gt;缺乏搜索的鲁棒性&lt;/a&gt;&lt;span class="footnote-reference" id="fnrefv1gf6chx43"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnv1gf6chx43"&gt;[29]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。确实，所有其他不确定性估计方法在优化时也应该容易受到对抗性示例的影响（部分原因是对抗性示例是特征，而不是错误&lt;span class="footnote-reference" id="fnrefa3kc8qjhdn6"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fna3kc8qjhdn6"&gt;[30]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; ），但是当网络输出及其不确定性估计是相同的时，对良好输出的本地搜索应该&lt;i&gt;特别&lt;/i&gt;有效地找到奇怪的意想不到的最佳值&lt;span class="footnote-reference" id="fnref28983dpfjdp"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fn28983dpfjdp"&gt;[31]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;训练高阶模型&lt;/h2&gt;&lt;p&gt;校准的另一面。首先让你的神经网络为你提供更好的不确定性估计。&lt;/p&gt;&lt;p&gt;如果您想获得二阶不确定性的估计，请训练神经网络以输出&lt;a href="https://en.wikipedia.org/wiki/Dirichlet_distribution#Occurrence_and_applications"&gt;狄利克雷分布&lt;/a&gt;的参数，而不是进行正常分类&lt;span class="footnote-reference" id="fnrefkm10b29by7c"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnkm10b29by7c"&gt;[32]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; &lt;span class="footnote-reference" id="fnrefhfc8x4hav0b"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnhfc8x4hav0b"&gt;[33]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。或者，如果您正在进行回归，请训练神经网络以输出答案的分布参数&lt;span class="footnote-reference" id="fnrefen0yeoqzg0k"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnen0yeoqzg0k"&gt;[10]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。或者，如果您拥有法学硕士并且每个问题都是钉子，请训练您的法学硕士用语言表达不确定性&lt;span class="footnote-reference" id="fnrefffdudph3qus"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnffdudph3qus"&gt;[34]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; &lt;span class="footnote-reference" id="fnrefadsp4hkxuk"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnadsp4hkxuk"&gt;[35]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。&lt;/p&gt;&lt;p&gt;表达不确定性的训练模型存在一个微妙的问题：不存在适当的二阶损失函数&lt;span class="footnote-reference" id="fnrefxuwhb9ieyh"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnxuwhb9ieyh"&gt;[36]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。这意味着仅从数据点出发，很难准确地训练模型来给出概率分布 - 您可以创建损失函数来尝试做到这一点，但只要您只监督正确答案是什么，而不监督正确答案是什么。正确的概率分布是，通过损失最小化得到的概率分布将会有偏差。&lt;/p&gt;&lt;p&gt;贝叶斯方法，或者至少是贝叶斯理论框架，在这里会很有用。您不需要适当的损失函数来进行贝叶斯更新。但还没有人写下该应用程序的贝叶斯神经网络图的类似物。&lt;/p&gt;&lt;p&gt;理论上也不清楚如何整合我们可以获得的有关概率分布的其他类型的监督数据。例如，人为噪声的例子可以给出直接的监督信号，表明正确的概率分布是高熵的。或者，我们可以通过询问“我期望我对正确答案的估计随着更多数据而改变多少？”来学习分布。它的监督分布在整个数据集中，因此绕过了没有适当评分规则的证明 - 但我们可以以公正的方式做到这一点吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;比较方法&lt;/h2&gt;&lt;p&gt;那么，哪个是最好的？&lt;/p&gt;&lt;p&gt;目前，它正在训练一个整体。但训练高阶模型具有尚未开发的潜力。&lt;/p&gt;&lt;p&gt;情况尚不清楚，因为比较是在玩具问题上进行的，文献很少并且并不总是重复，比较很困难，因为每种方法都有十几种变体，而且不同的论文有时会评估完全不同的指标。 &lt;/p&gt;&lt;figure class="image"&gt;&lt;img alt="该图显示了一些神经网络曲线与不同方法生成的误差条的拟合情况。" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/79eegMp3EBs8ptFqa/xptyd74vj7qmuirlgkdq" /&gt;&lt;figcaption&gt;来自&lt;a href="https://www.lesswrong.com/posts/79eegMp3EBs8ptFqa/neural-uncertainty-estimation-for-alignment#fnlpb1b2qcoen"&gt;参考文献。 7&lt;/a&gt; .玩具曲线拟合问题的不确定性估计方法比较。&lt;br /&gt;&lt;br /&gt; Ground Truth 是一个贝叶斯神经网络，Hamiltonian MC 是它的一个很好的蒙特卡洛近似，变分推理是它的一个廉价近似，MC Dropout 根据 dropout 后的方差估计不确定性，而我们的方法是一个具有奇特初始化的集成。&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;这个数字是一个不错的直觉泵。它在玩具曲线拟合任务（只有&lt;span class="footnote-reference" id="fnreflpb1b2qcoen"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnlpb1b2qcoen"&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;个数据点的回归问题）上比较了不确定性估计方法（尽管它明显错过了校准和高阶建模），每种方法都使用 ReLU 与 Sigmoid。我认为这个数字的一​​些定性印象是概括性的。&lt;/p&gt;&lt;p&gt;印象笔记：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;贝叶斯神经网络、蒙特卡洛近似和集成方法都做出了类似的预测。&lt;/li&gt;&lt;li&gt;架构对于泛化属性来说非常重要，在某种程度上使这些方法看起来过于自信。 （整合不同的架构将是一个好的开始，但没有人这样做。）&lt;/li&gt;&lt;li&gt; Dropout 和变分推理近似在数据点附近的置信度都低于集成簇，但在分布之外的置信度更高。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在对 CIFAR-10 或 LSTM 语言模型&lt;span class="footnote-reference" id="fnrefhzxdsrsco9g"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnhzxdsrsco9g"&gt;[18]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;或医学 MRI 数据&lt;span class="footnote-reference" id="fnrefu11lguv41wc"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnu11lguv41wc"&gt;[17]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;进行更彻底的比较（现在包括校准）时，集成似乎是最好的方法。但老实说，我提到的所有方法都非常接近，在复杂任务上，dropout 比玩具模型所建议的更有竞争力，而变分推理在 MNIST 上表现得令人惊讶。&lt;/p&gt;&lt;p&gt;高阶建模出现在较少的比较中。但这是参考文献中的一个数字。 31 其中模型在 MNIST 上进行训练，然后在&lt;a href="https://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html"&gt;非 MNIST&lt;/a&gt;上进行测试&lt;span class="footnote-reference" id="fnrefkm10b29by7c"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnkm10b29by7c"&gt;[32]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。学习分类输出上的狄利克雷分布（EDL 虚线）与包的其余部分不同，就像包与原始模型输出（蓝色 L2 线）的不同一样： &lt;/p&gt;&lt;figure class="image"&gt;&lt;img alt="该图显示高阶建模给出了分布的高熵猜测。" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/79eegMp3EBs8ptFqa/iyp8dpi29scplwjsybyy" /&gt;&lt;figcaption&gt;来自&lt;a href="https://www.lesswrong.com/posts/79eegMp3EBs8ptFqa/neural-uncertainty-estimation-for-alignment#fnkm10b29by7c"&gt;参考文献。 32&lt;/a&gt; . OOD 数据集上输出熵的积分直方图（或累积分布）。训练是在 MNIST 上进行的，但测试是在&lt;a href="https://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html"&gt;非 MNIST&lt;/a&gt;上进行的，因此具有更高的高熵概率是好的。&lt;br /&gt;&lt;br /&gt; EDL 是他们的狄利克雷分布模型，L2 是原始模型输出，DeepEnsemble 是一个集成，FFLU 不清楚但可能是贝叶斯 NN 近似，Dropout 是 dropout，FFG 是贝叶斯 NN 方法，MNFG 是变分推理近似。&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;当显示 OOD 数据时，这是一种非常令人印象深刻的不确定能力。是的，在分布上（在 MNIST 上）它具有正常的准确性。显然，学习何时不确定就是获取一些其他方法无法获取的信息。&lt;/p&gt;&lt;p&gt;但如此不确定真的正确吗？假设您正在对数字进行分类，这就是您所知道的，然后有人输入字符“B”。这肯定是8，对吧？或者也许是一个被压在一起的 13，如果你能想到这个想法的话。但它肯定不是 2。既然你知道这一点，那么在这里返回最大熵分布将是一个错误。但这似乎正是狄利克雷分布模型倾向于做的事情。&lt;/p&gt;&lt;p&gt;我希望高阶模型与论文中其他所有内容之间的不同行为是因为这些方法具有不同的背景假设，如果我们知道我们在做什么，我们可以在适当的时候灵活地使用不同的假设。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;我未来想从事这个领域的工作&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;更大的尺度&lt;ul&gt;&lt;li&gt;对 Transformer 语言模型的不确定性估计进行基准测试。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;搜索的鲁棒性&lt;ul&gt;&lt;li&gt;看看认知不确定性对对抗性例子有何影响。&lt;/li&gt;&lt;li&gt;分析找到新的对抗性例子来欺骗不确定性估计和原始指​​标是多么容易。检查这些例子对人类来说是否自然。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;非压差噪声&lt;ul&gt;&lt;li&gt;当您向神经网络添加噪声时，改进采样的理论。&lt;/li&gt;&lt;li&gt;对不同类型的噪声进行相互比较和其他方法的基准测试。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;合奏带来更多聪明的事情&lt;ul&gt;&lt;li&gt;测试改变架构的集成。&lt;/li&gt;&lt;li&gt;测试正在探索“相同”潜在特征的集合。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;更好的高阶模型&lt;ul&gt;&lt;li&gt;发展神经网络学习二阶分布的贝叶斯视角。&lt;/li&gt;&lt;li&gt;通过尝试转化理论和修补信号（例如预测未来更新），为高阶模型开发更好的训练方法。&lt;/li&gt;&lt;li&gt;弄清楚如何使用高阶建模来获得不同背景假设的不确定性。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;更好的比较&lt;ul&gt;&lt;li&gt;更系统地比较校准和 Brier 评分。&lt;/li&gt;&lt;li&gt;对决策问题进行基准测试，为分布数据之外的“良好行为”提供具体标准。&lt;/li&gt;&lt;li&gt;开发包含“自然”分布泛化的标记数据集，我们可以将其类比为现实世界中模型完成的 OOD 泛化。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;更多协同效应&lt;ul&gt;&lt;li&gt;通过将多种方法结合在一起，可以获得更好的结果。校准太容易与一切结合起来，尽管它仍然是一个好主意，但这不算新闻。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;如果这些论文确实存在而我错过了，请告诉我。如果他们不这样做，并且其中一个项目听起来像是您想做的事情，请联系我 - 我很乐意聊天。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;与一致性、结论的相关性&lt;/h2&gt;&lt;p&gt;文献与对齐的相关性不如我的预期，但这主要是因为我的期望被混淆了。我对某种“人类价值观的不确定性”感兴趣，它不同于文献中的“任意”或“认知”不确定性。&lt;/p&gt;&lt;p&gt;任意不确定性是从数据中得知的，但我们没有人类价值观的真实标签。或者，如果模型中有一些与人类价值观相关的潜在特征，我们不仅仅想了解该特征在某些训练集上的方差。&lt;/p&gt;&lt;p&gt;认知不确定性更接近，但正如文献中所使用的那样，它实际上是关于某些输出或特征对于训练目标的有用程度。在训练过程中收敛到相同答案的模型越多，认知不确定性就越小。但相对于我想要的，这感觉好像缺少一些关于首先使用什么训练程序或特征检测程序的不确定性&lt;span class="footnote-reference" id="fnrefmyfeye042z"&gt;&lt;sup&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnmyfeye042z"&gt;[37]&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; 。&lt;/p&gt;&lt;p&gt;正确的训练/特征检测程序的不确定性偏离了“有一个基本事实答案，不确定性是与该答案的预期偏差”的通常范式。为了保持一致，我认为图片应该更像是通信——我们试图通过架构和数据向人工智能传达一些信息，而人工智能应该对如何解释它有不确定性。&lt;/p&gt;&lt;p&gt;构建这种关于人类价值观的不确定性是相当棘手的——我什至还不知道我想从中得到什么！也许如果我们更清楚地理解我们想要什么，我们就可以用更标准的不确定性来构建它。例如，我们可以设计一个人工智能可以玩的“游戏”，激励对人类概念的不同解释，这样游戏中的任意和认知不确定性就可以充分捕捉到我们希望人工智能对人类价值观具有的不确定性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;这篇文章部分写于&lt;/i&gt;&lt;a href="https://www.mitalignment.org/"&gt;&lt;i&gt;MAIA&lt;/i&gt;&lt;/a&gt; &lt;i&gt;。谢谢玛雅！还有贾斯蒂斯·米尔斯（Justis Mills）进行编辑，以及波士顿的各种人士进行对话。&lt;/i&gt;&lt;/p&gt;&lt;ol class="footnotes"&gt;&lt;li class="footnote-item" id="fn80ywo0pbl8r"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnref80ywo0pbl8r"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;主动学习文献调查。&lt;i&gt;伯尔·塞特尔斯&lt;/i&gt;(2010) &lt;a href="https://burrsettles.com/pub/settles.activelearning.pdf"&gt;https://burrsetles.com/pub/settles.activelearning.pdf&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnq23duy4ut0g"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefq23duy4ut0g"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;优化器的诅咒：决策分析中的怀疑主义和决策后惊喜。&lt;i&gt;詹姆斯·E·史密斯、罗伯特·L·温克勒&lt;/i&gt;(2006) &lt;a href="https://pubsonline.informs.org/doi/abs/10.1287/mnsc.1050.0451"&gt;https://pubsonline.informs.org/doi/abs/10.1287/mnsc.1050.0451&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn3dij2j9svj8"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnref3dij2j9svj8"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;用于检测神经网络中错误分类和分布外示例的基线。&lt;i&gt;丹·亨德里克斯、凯文·金佩尔&lt;/i&gt;(2016) &lt;a href="https://arxiv.org/abs/1610.02136"&gt;https://arxiv.org/abs/1610.02136&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnfo4svcpvxs"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnreffo4svcpvxs"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;使用贝叶斯统计进行神经网络不确定性评估并应用于遥感。 &lt;i&gt;F. Aires、C. Prigent、WB Rossow&lt;/i&gt; (2004)&lt;/p&gt;&lt;p&gt;第 1 部分：网络权重&lt;a href="https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2003JD004173"&gt;https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2003JD004173&lt;/a&gt;&lt;/p&gt;&lt;p&gt;第 2 部分：输出错误&lt;a href="https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2003JD004174"&gt;https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2003JD004174&lt;/a&gt;&lt;/p&gt;&lt;p&gt;第 3 部分：网络雅可比矩阵&lt;a href="https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2003JD004175"&gt;https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2003JD004175&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fncogdul3x2xj"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefcogdul3x2xj"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;深度学习中不确定性估计的通用框架。&lt;i&gt;安东尼奥·洛奎西奥、马蒂亚·塞古、大卫·斯卡拉穆扎&lt;/i&gt;(2020) &lt;a href="https://www.zora.uzh.ch/id/eprint/197704/1/RAL20_Loquercio.pdf"&gt;https://www.zora.uzh.ch/id/eprint/197704/1/RAL20_Loquercio.pdf&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnamk58pvab4"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefamk58pvab4"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt; &lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;如果&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;P&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;(&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;参数&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;是&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;高斯&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;分布&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;（&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;指数&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;L2&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;正&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;则&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;化），并且您的损失是&lt;span&gt;&lt;span class="mjpage"&gt;&lt;span class="mjx-chtml"&gt;&lt;span class="mjx-math"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;对&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;数&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;损失&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;，&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;因此&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mtext"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;P&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;(&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;数据&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;集&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mspace" style="width: 0.278em; height: 0px;"&gt;&lt;/span&gt;&lt;span class="mjx-texatom"&gt;&lt;span class="mjx-mrow"&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;|&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mspace" style="width: 0.278em; height: 0px;"&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;参数&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;s&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;是&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;它&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;的&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mo"&gt;&lt;span class="mjx-char MJXc-TeX-main-R"&gt;指数&lt;/span&gt;&lt;/span&gt;&lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;。&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt; &lt;span class="mjx-mi"&gt;&lt;span class="mjx-char MJXc-TeX-math-I"&gt;_&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnlpb1b2qcoen"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnreflpb1b2qcoen"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;神经网络的不确定性：近似贝叶斯集成。&lt;i&gt;蒂姆·皮尔斯、菲利克斯·莱布弗里德、亚历山德拉·布林特鲁普、穆罕默德·扎基、安迪·尼利&lt;/i&gt;(2020) &lt;a href="http://proceedings.mlr.press/v108/pearce20a/pearce20a.pdf"&gt;http://proceedings.mlr.press/v108/pearce20a/pearce20a.pdf&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnd9jz9mfml"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefd9jz9mfml"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;最明显的问题是 Hessian 矩阵的奇点。但在短长度尺度上，损失情况也可能会很复杂，使得低阶近似有时会失败。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnbct5kii2m07"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefbct5kii2m07"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;面向神经网络的校准和可扩展的不确定性表示。&lt;i&gt;纳比尔·西达特、克里斯托弗·卡南&lt;/i&gt;(2019) &lt;a href="https://arxiv.org/abs/1911.00104"&gt;https://arxiv.org/abs/1911.00104&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnen0yeoqzg0k"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefen0yeoqzg0k"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;使用深度集成的简单且可扩展的预测不确定性估计。 &lt;i&gt;Balaji Lakshminarayanan、Alexander Pritzel、Charles Blundell&lt;/i&gt; (2017) &lt;a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/9ef2ed4b7fd2c810847ffa5fa85bce38-Paper.pdf"&gt;https://proceedings.neurips.cc/paper_files/paper/2017/file/9ef2ed4b7fd2c810847ffa5fa85bce38-Paper.pdf&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnxqxuuw1xe2"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefxqxuuw1xe2"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;计算机视觉的贝叶斯深度学习需要哪些不确定性？&lt;i&gt;亚历克斯·肯德尔，亚林·加尔&lt;/i&gt;(2017) &lt;a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/2650d6089a6d640c5e85b2b88265dc2b-Paper.pdf"&gt;https://proceedings.neurips.cc/paper_files/paper/2017/file/2650d6089a6d640c5e85b2b88265dc2b-Paper.pdf&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn2jiolbj6tn8"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnref2jiolbj6tn8"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;源自希腊语，意思是“关于知识”和“关于赌博”。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnud7l8eu73jk"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefud7l8eu73jk"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt; 2 如何仅在给定模型权重的情况下判断你的输入是否不符合分布， &lt;i&gt;dkirmani&lt;/i&gt; (2023) &lt;a href="https://www.lesswrong.com/posts/tvLi8CyvvSHrfte4P/how-2-tell-if-ur-input-is-out-of-distribution-given-only"&gt;https://www.lesswrong.com/posts/tvLi8CyvvSHrfte4P/how-2-tell-if-ur-input-is-out-of -仅给定分布&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnlgnhfp0m35e"&gt;&lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnreflgnhfp0m35e"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;现实世界中存在噪声，因此，如果小的输入噪声极大地改变了您的答案，您就不应该对此充满信心。相反，在表现良好的输入上，神经网络学会对噪声具有鲁棒性。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnhfox6k7gfev"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefhfox6k7gfev"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;密集回归的免训练不确定性估计：灵敏度作为替代。&lt;i&gt;卢米、王浩、田永龙、何浩、Nir Shavit&lt;/i&gt; (2022) &lt;a href="https://arxiv.org/abs/1910.04858"&gt;https://arxiv.org/abs/1910.04858&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn0hl862nst9f9"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnref0hl862nst9f9"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;深度神经网络不确定性调查。&lt;i&gt;雅各布·高利科斯基等人。&lt;/i&gt; (2021) &lt;a href="https://arxiv.org/abs/2107.03342"&gt;https://arxiv.org/abs/2107.03342&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnu11lguv41wc"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefu11lguv41wc"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;估计心脏 MRI 分割神经网络的不确定性：一项基准研究。&lt;i&gt;马修·吴、富民·郭、Labonny Biswas、Steffen E. Petersen、Stefan K. Piechnik、Stefan Neubauer、Graham Wright&lt;/i&gt; (2023) &lt;a href="https://ieeexplore.ieee.org/abstract/document/10002847"&gt;https://ieeexplore.ieee.org/abstract/document/10002847&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnhzxdsrsco9g"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefhzxdsrsco9g"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;你能相信你的模型的不确定性吗？评估数据集变化下的预测不确定性。 &lt;i&gt;Yaniv Ovadia、Emily Fertig、Jie Ren、Zachary Nado、D Sculley、Sebastian Nowozin、Joshua V. Dillon、Balaji Lakshminarayanan、Jasper Snoek&lt;/i&gt; (2019) &lt;a href="https://arxiv.org/abs/1906.02530"&gt;https://arxiv.org/abs/1906.02530&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnfqljh5j8ipf"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnreffqljh5j8ipf"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;深度神经网络倾向于进行可预测的推断。&lt;i&gt;凯蒂·康、阿姆里斯·塞特勒、克莱尔·汤姆林、谢尔盖·莱文&lt;/i&gt;(2023) &lt;a href="https://arxiv.org/abs/2310.00873"&gt;https://arxiv.org/abs/2310.00873&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnvl2ahm3user"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefvl2ahm3user"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;对于分布外检测来说似乎更常见，例如增强神经网络中分布外图像检测的可靠性。&lt;i&gt;梁世宇、李亦轩、R.Srikant&lt;/i&gt; (2020) &lt;a href="https://arxiv.org/abs/1706.02690"&gt;https://arxiv.org/abs/1706.02690&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnzkojoz799ka"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefzkojoz799ka"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt; GPT-4 技术报告。 &lt;i&gt;OpenAI&lt;/i&gt; (2023) &lt;a href="https://openai.com/research/gpt-4"&gt;https://openai.com/research/gpt-4&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnnglt70rybd9"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefnglt70rybd9"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt; Mix-n-Match：深度学习中不确定性校准的集成和组合方法。&lt;i&gt;张继泽、Bhavya Kailkhura、T. Yong-Jin Han&lt;/i&gt; (2020) &lt;a href="https://arxiv.org/abs/2003.07329"&gt;https://arxiv.org/abs/2003.07329&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fngx8tqfun0w"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefgx8tqfun0w"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;不确定性量化和深度集成。&lt;i&gt;拉胡尔·拉哈曼、亚历山大·H·蒂埃里&lt;/i&gt;(2020) &lt;a href="https://arxiv.org/abs/2007.08792"&gt;https://arxiv.org/abs/2007.08792&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fntmgq2x5m6t"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnreftmgq2x5m6t"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;在分布外数据集上校准深度神经网络分类器。&lt;i&gt;邵志辉、杨建一、任少雷&lt;/i&gt;(2020) &lt;a href="https://arxiv.org/abs/2006.08914"&gt;https://arxiv.org/abs/2006.08914&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fng3rtbqm4247"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefg3rtbqm4247"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;归纳共形预测：神经网络的理论与应用。&lt;i&gt;哈里斯·帕帕佐普洛斯&lt;/i&gt;(2008) &lt;a href="https://www.intechopen.com/chapters/5294"&gt;https://www.intechopen.com/chapters/5294&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnqbdlqupa3x"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefqbdlqupa3x"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;通过 Logit 归一化缓解神经网络过度自信。&lt;i&gt;魏洪欣、谢仁春子、程浩、冯雷、安博、李一轩&lt;/i&gt;(2022) &lt;a href="https://proceedings.mlr.press/v162/wei22d/wei22d.pdf"&gt;https://proceedings.mlr.press/v162/wei22d/wei22d.pdf&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn2qfcvdqkx2y"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnref2qfcvdqkx2y"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;使用噪声对比先验对深度神经网络进行可靠的不确定性估计。 &lt;i&gt;Danijar Hafner、Dustin Tran、Timothy Lillicrap、Alex Irpan、James Davidson&lt;/i&gt; (2018) &lt;a href="https://openreview.net/forum?id=HkgxasA5Ym"&gt;https://openreview.net/forum?id=HkgxasA5Ym&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnazba93s687"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefazba93s687"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;通过对抗性训练和预训练改进 OOD 泛化。&lt;i&gt;易明阳、侯鲁、孙家成、尚立峰、蒋欣、刘群、马志明&lt;/i&gt;(2021) &lt;a href="http://proceedings.mlr.press/v139/yi21a/yi21a.pdf"&gt;http://proceedings.mlr.press/v139/yi21a/yi21a.pdf&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnv1gf6chx43"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefv1gf6chx43"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt; SolidGoldMagikarp（加上，提示生成）。&lt;i&gt;杰西卡·朗贝罗、马修·沃特金斯&lt;/i&gt;(2023)&lt;i&gt; &lt;/i&gt;&lt;a href="https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation"&gt;https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt- Generation&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fna3kc8qjhdn6"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefa3kc8qjhdn6"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;对抗性例子不是错误，而是特征。&lt;i&gt;安德鲁·伊利亚斯等人。&lt;/i&gt; （2019） &lt;a href="https://arxiv.org/abs/1905.02175"&gt;https://arxiv.org/abs/1905.02175&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fn28983dpfjdp"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnref28983dpfjdp"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;如果我们使用生成而不是搜索来构建良好的输出，则可以回避优化不稳健的问题。或者在强化学习环境中，如果我们使用策略预测器将我们保持在系统的有效性范围内。但这是昂贵的，有时容易受到以不同速率泛化的能力的影响。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnkm10b29by7c"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefkm10b29by7c"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;用于量化分类不确定性的证据深度学习。&lt;i&gt;穆拉特·森索伊、兰斯·卡普兰、梅利赫·坎德米尔&lt;/i&gt;(2018) &lt;a href="https://arxiv.org/abs/1806.01768"&gt;https://arxiv.org/abs/1806.01768&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnhfc8x4hav0b"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefhfc8x4hav0b"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;使用狄利克雷深度神经网络识别域外对象。&lt;i&gt;艾哈迈德·哈曼、弗兰克·博纳伦斯、赛义德·E·戈巴迪、克里斯托夫·斯蒂勒&lt;/i&gt;(2023) &lt;a href="https://openaccess.thecvf.com/content/ICCV2023W/UnCV/papers/Hammam_Identifying_Out-of-Domain_Objects_with_Dirichlet_Deep_Neural_Networks_ICCVW_2023_paper.pdf"&gt;https://openaccess.thecvf.com/content/ICCV2023W/UnCV/papers/Hammam_Identifying_Out-of-Domain_Objects_with_Dirichlet_Deep_Neural_Networks_ICCVW_2023_paper.pdf&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnffdudph3qus"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefffdudph3qus"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;教学模型用言语表达不确定性。&lt;i&gt;斯蒂芬妮·林、雅各布·希尔顿、欧文·埃文斯&lt;/i&gt;(2023) &lt;a href="https://openreview.net/forum?id=8s8K2UZGTZ"&gt;https://openreview.net/forum?id=8s8K2UZGTZ&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnadsp4hkxuk"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefadsp4hkxuk"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;直接从 GPT-3 中导出概率。&lt;i&gt;努诺·森佩雷&lt;/i&gt;(2023) &lt;a href="https://forum.effectivealtruism.org/posts/aGmhi4uvAJptY8TA7/straightforwardly-eliciting-probabilities-from-gpt-3#Fine_tune_the_model_on_good_worked_examples_of_forecasting_reasoning"&gt;https://forum. effectivealtruism.org/posts/aGmhi4uvAJptY8TA7/straightforwardly-eliciting-probabilities-from-gpt-3#Fine_tune_the_model_on_good_worked_examples_of_forecasting_reasoning&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnxuwhb9ieyh"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefxuwhb9ieyh"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;关于认知不确定性量化的二阶评分规则。&lt;i&gt;维克托·本格斯、艾克·胡勒迈尔、威廉·韦格曼&lt;/i&gt;(2023) &lt;a href="https://arxiv.org/abs/2301.12736"&gt;https://arxiv.org/abs/2301.12736&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class="footnote-item" id="fnmyfeye042z"&gt; &lt;span class="footnote-back-link"&gt;&lt;sup&gt;&lt;strong&gt;&lt;a href="https://www.lesswrong.com/feed.xml#fnrefmyfeye042z"&gt;^&lt;/a&gt;&lt;/strong&gt;&lt;/sup&gt;&lt;/span&gt;&lt;div class="footnote-content"&gt;&lt;p&gt;我可能还会提到关于使用什么抽象或使用什么推理程序的不确定性。但这些似乎是培训的下游。&lt;/p&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ol&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://www.lesswrong.com/posts/79eegMp3EBs8ptFqa/neural-uncertainty-estimation-for-alignment#comments"&gt;讨论&lt;/a&gt;</description><pubDate>Tue, 05 Dec 2023 08:01:32 GMT</pubDate><guid isPermaLink="true">https://www.lesswrong.com/posts/79eegMp3EBs8ptFqa/neural-uncertainty-estimation-for-alignment</guid></item></channel></rss>